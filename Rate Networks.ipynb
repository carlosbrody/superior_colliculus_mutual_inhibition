{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** DIFFERENTIABLE FUNCTIONS FOR RUNNING RATE-BASED NEURAL NETWORKS**\n",
    "\n",
    "This notebook contains functions to run, both in forward and backward time a rate-based network model (using equations similar to those of Hopfield, PNAS, 1984 and many others), plus some simple examples using them. The code (without the illustrative examples) gets extracted into\n",
    "\n",
    "    rate_networks.jl\n",
    "\n",
    "There is also an example of using those networks together with ForwardDiff to differentiate one of the outputs with respect to various network parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">TABLE OF CONTENTS</h1>\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "**Updates to the table of contents are periodic, but run the cell below to first start or force an update.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "macro javascript_str(s) display(\"text/javascript\", s); end\n",
    "\n",
    "javascript\"\"\"\n",
    "$.getScript('make_table_of_contents.js')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#@include_me  rate_networks.jl\n",
    "\n",
    "using PyCall\n",
    "using PyPlot\n",
    "using ForwardDiff\n",
    "using DiffBase\n",
    "using MAT\n",
    "\n",
    "# pygui(true)\n",
    "\n",
    "include(\"general_utils.jl\")\n",
    "include(\"constrained_parabolic_minimization.jl\")\n",
    "include(\"hessian_utils.jl\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup -- definitions of forwardModel() and backwardsModel()\n",
    "\n",
    "These are functions that run arbitrary $0.5 * (1+\\tanh(x))$ - style rate networks, either forwards in time, or backwards in time.  The backwards in time part was for an idea that we're no longer pursuing, but was kept here for completeness.\n",
    "\n",
    "The equations are similar to those in Hopfield, PNAS, 1984 and in many papers since: For unit $i$,\n",
    "\n",
    "$$\n",
    "    \\tau \\frac{{\\rm d}U_i}{{\\rm d}t} \\; = \\; g_{\\rm leak} \\cdot (U_{\\rm rest} - U) \\; + \\; \n",
    "    \\Sigma_j W_{ij} g(U_j) \\; + I_i + \\; \\sigma \\eta\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "    g(U) = 0.5\\cdot \\left(1 + \\tanh\\frac{U-\\beta}{\\theta} \\right)\n",
    "$$\n",
    "\n",
    "The **forwards integration** is done using simple Euler integration \n",
    "\n",
    "$$ \n",
    "U(t+\\delta t) =  U(t) + \\delta t \\frac{{\\rm d}U}{{\\rm d}t}.\n",
    "$$\n",
    "\n",
    "The **backwards integration** is more complicated, and involves doing a minimization search to find the $U(t-\\delta t)$ that would most closely produce $U(t)$ after one $\\delta t$ timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#@include_me rate_networks.jl\n",
    "\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "forwardModel(startU; dt=0.01, tau=0.1, nsteps=100, input=[0.1, 0], noise=[], W=[0 -5;-5 0], \n",
    "init_add=0, start_add=0, const_add=0, sigma=0, gleak=1, U_rest=0, \n",
    "    do_plot=false, nderivs=0, difforder=0, clearfig=true, fignum=1, dUdt_mag_only=false,\n",
    "    warn_if_unused_params=false, opto_strength=1, opto_units=[], opto_times=zeros(0,2),)\n",
    "\n",
    "Runs a tanh() style-network forwards in time, given its starting point, using simple Euler integration\n",
    "    tau dU/dt = -U + W*V + I\n",
    "    V = 0.5*tanh(U)+ 0.5\n",
    "\n",
    "# PARAMETERS:\n",
    "\n",
    "- startU     A column vector, nunits-by-1, indicating the values of U at time zero\n",
    "\n",
    "\n",
    "# OPTIONAL PARAMETERS\n",
    "\n",
    "- dt      Scalar, timestep size\n",
    "\n",
    "- tau     Scalar, in seconds\n",
    "\n",
    "- gleak   dUdt will have a term equal to gleak*(U_rest - U)\n",
    "\n",
    "- U_rest  dUdt will have a term equal to gleak*(U_rest - U)\n",
    "\n",
    "- nsteps  Number of timesteps to run, including time=0.\n",
    "\n",
    "- input   Either an nunits-by-1 vector, in which case inputs to each unit are constant\n",
    "        across time, or a matrix, nunits-by-nsteps, indicating input for each unit at each timepoint.\n",
    "\n",
    "- W       Weight matrix, nunits-by-nunits\n",
    "\n",
    "- init_add    DEPRECATED: Vector or scalar that gets added to the input current at very first timestep.\n",
    "            Deprecated because this made it dt-dependent. Replaced by start_add.\n",
    "\n",
    "- start_add   Vector or scalar that gets added, once, to the initial U[:,1], before the integration process begins.\n",
    "\n",
    "- const_add   Scalar that gets added to U after every timestep\n",
    "\n",
    "- sigma       After each timestep, add sigma*sqrt(dt)*randn() to each element of U\n",
    "\n",
    "- opto_strength    The outputs V, after being computed, will get multiplied by this number. opto_strength should *EITHER* be a scalar, in which case optional params opto_units and opto_times below are also relevant; *OR* it should be an nunits-by-nsteps matrix, completely specifying how much each unit's V should be multiplied by at each timestep, in which case opto_times and opto_units are irrelevant\n",
    "\n",
    "- opto_units       A list of the unit numbers that will have their V multiplied by opto_strength. For example, [1,3] would affect only units 1 and 3.  Can be the empty matrix (equivalent to no opto effect). Irrelevant if opto_strength = 1\n",
    "\n",
    "- opto_times    An n-by-2 matrix, where each row lists t_start_of_opto_effect, t_end_of_opto_effect. For example,\n",
    "                [1 3 ; 6 8]  would mean \"have an opto effect during both 1 <= t <=3 and 6 <= t <= 8]. With the \n",
    "                code as currently configured, this would mean the same opto_strength and opto_units across all \n",
    "                the relevant time intervals in a run.\n",
    "\n",
    "- do_plot   Default false, if true, plots V of up to the first two dimensions\n",
    "\n",
    "- fignum     Figure number on which to plot\n",
    "\n",
    "- clearfig  If true, the figure is first cleared, otherwise any plot ois overlaid\n",
    "\n",
    "- nderivs, difforder     Required for making sure function can create its own arrays and \n",
    "                       still be differentiated\n",
    "\n",
    "- dUdt_mag_only  If true, returns |dUdt|^2 from the first timestep only, then stops.\n",
    "\n",
    "- warn_if_unused_params     If true, pronts out a warning of some of the passed parameters are not used.\n",
    "\n",
    "\n",
    "\n",
    "** RETURNS:**\n",
    "\n",
    "- Uend Vend       nunits-by-1 vectors representing the final values of U and V that were found.\n",
    "\n",
    "- U, V            nunits-by-nsteps matrices containing the full trajectories\n",
    "\n",
    "- t               A time vector, so one could things like plot(t, U[1,:])\n",
    "\n",
    "\"\"\"\n",
    "function forwardModel(startU; opto_strength=1, opto_units=[], opto_times=zeros(0,2),\n",
    "    dt=0.01, tau=0.1, nsteps=100, input=[], noise=[], W=[0 -5;-5 0], \n",
    "    init_add=0, start_add=0, const_add=0, do_plot=false, nderivs=0, difforder=0, clearfig=true, fignum=1,\n",
    "    dUdt_mag_only=false, sigma=0, g_leak=1, U_rest=0, theta=0, beta=1, \n",
    "    warn_if_unused_params=false, other_unused_params...)\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    o = g(z)    squashing tanh function, running from 0 to 1, is equal to 0.5 when input is 0.\n",
    "    \"\"\"\n",
    "    function g(z)\n",
    "        return 0.5*tanh.(z)+0.5\n",
    "    end\n",
    "    \n",
    "    if warn_if_unused_params && length(other_unused_params)>0\n",
    "        @printf(\"\\n\\n=== forwardModel warning, had unused params \")\n",
    "        for k in keys(Dict(other_unused_params))\n",
    "            @printf(\"%s, \", k)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    if length(size(opto_times))==1\n",
    "        opto_times = reshape(opto_times, 1, 2)\n",
    "    end\n",
    "    \n",
    "    my_input = ForwardDiffZeros(size(input,1), size(input,2), nderivs=nderivs, difforder=difforder)\n",
    "    for i=1:prod(size(input)); my_input[i] = input[i]; end\n",
    "    input = my_input;\n",
    "    \n",
    "    nunits = length(startU)\n",
    "    if size(startU,2) > size(startU,1)\n",
    "        error(\"startU must be a column vector\")\n",
    "    end\n",
    "    \n",
    "    # --- formatting input ---\n",
    "    if ~(typeof(input)<:Array) || prod(size(input))==1  # was a scalar\n",
    "        input = input[1]*(1+ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    elseif length(input)==0 # was the empty matrix\n",
    "        input = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    elseif size(input,2)==1     # was a column vector\n",
    "        input = input*(1+ForwardDiffZeros(1, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    end    \n",
    "    # --- formatting noise ---\n",
    "    if ~(typeof(noise)<:Array) || prod(size(noise))==1  # was a scalar\n",
    "        noise = noise*(1+ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    elseif length(noise)==0 # was the empty matrix\n",
    "        noise = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    elseif size(noise,2)==1     # was a column vector\n",
    "        noise = noise*(1+ForwardDiffZeros(1, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    end    \n",
    "    # --- formatting opto fraction ---\n",
    "    if typeof(opto_strength)<:Array\n",
    "        if size(opto_strength,1) != nunits || size(opto_strength,2) != nsteps\n",
    "            error(\"opto_strength must be either a scalar or an nunits-by-nsteps matrix\")\n",
    "        end\n",
    "        opto_matrix = opto_strength\n",
    "    else # We assume that if opto_strength is not an Array, then it is a scalar\n",
    "        opto_matrix = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder) + 1\n",
    "        time_axis = dt*(0:nsteps-1)\n",
    "        for i=1:size(opto_times,1)\n",
    "            opto_matrix[opto_units, (opto_times[i,1] .<= time_axis) & (time_axis .<= opto_times[i,2])] = opto_strength\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    U = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    V = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    \n",
    "    if ~(typeof(W)<:Array); W = [W]; end\n",
    "\n",
    "    W     = reshape(W, nunits, nunits)\n",
    "    U     = reshape(U, nunits, nsteps)\n",
    "    V     = reshape(V, nunits, nsteps)\n",
    "    input = reshape(input, nunits, nsteps)\n",
    "    noise = reshape(noise, nunits, nsteps)\n",
    "\n",
    "    input[:,1] += init_add\n",
    "    input      += const_add\n",
    "\n",
    "    #@printf(\"size(U) is (%d,%d), and size(startU) is (%d,%d) and size(noise) is (%d,%d)\", \n",
    "    #    size(U,1), size(U,2), size(startU,1), size(startU,2), size(noise,1), size(noise,2))\n",
    "    # @printf(\"U[1]=%g, noise[1]=%g\\n\", startU, noise[1])\n",
    "    U[:,1] = startU + noise[:,1] + start_add; # @printf(\"Resulting U=%g\\n\", U[1])\n",
    "    V[:,1] = g((U[:,1]-theta)/beta); \n",
    "#    @printf(\"U[1U[1,1])\n",
    "    V[:,1] .*= opto_matrix[:,1]\n",
    "    \n",
    "    for i=2:nsteps\n",
    "        dUdt = g_leak*(U_rest -U[:,i-1]) + W*V[:,i-1] + input[:,i-1]\n",
    "        if dUdt_mag_only; return sum(dUdt.*dUdt); end;\n",
    "        # @printf(\"dUdt=%g\\n\", dUdt[1])\n",
    "        # @printf(\"i=%g\\n\", i)\n",
    "        # @printf(\"noise[2]=%g\\n\", noise[2])\n",
    "        U[:,i] = U[:,i-1] + (dt/tau)*dUdt + noise[:,i] + sigma*sqrt(dt)*randn(size(U,1),1)\n",
    "        # @printf(\"Resulting U[2]=%g\\n\", U[2])\n",
    "        V[:,i] = g((U[:,i]-theta)/beta)\n",
    "        V[:,i] .*= opto_matrix[:,i]\n",
    "        # @printf(\"Resulting V[2]=%g\\n\", V[2])\n",
    "    end\n",
    "\n",
    "    if do_plot\n",
    "        figure(fignum)\n",
    "        if length(startU)==1\n",
    "            if clearfig; clf(); end;\n",
    "            t = (0:nsteps-1)*dt\n",
    "            plot(t, V[1,:], \"b-\")\n",
    "            plot(t[1], V[1,1], \"g.\")\n",
    "            plot(t[end], V[1,end], \"r.\")\n",
    "            xlabel(\"t\"); ylabel(\"V1\"); ylim([-0.01, 1.01])\n",
    "        elseif length(startU)>=2\n",
    "            if clearfig; clf(); end;\n",
    "            plot(V[1,:], V[2,:], \"b-\")\n",
    "            plot(V[1,1], V[2,1], \"g.\")\n",
    "            plot(V[1,end], V[2,end], \"r.\")\n",
    "            xlabel(\"V1\"); ylabel(\"V2\"); \n",
    "            xlim([-0.01, 1.01]); ylim([-0.01, 1.01])\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return U[:,end], V[:,end], U, V, (0:nsteps-1)*dt\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "backwardsModel(endU; dt=0.01, tau=0.1, nsteps=100, input=[0],noise=[],  W=[0 -5;-5 0], \n",
    "    do_plot=false, nderivs=0, difforder=0, clearfig=true, fignum=1, tol=1e-15, start_eta=10)\n",
    "\n",
    "Runs a tanh() style-network BACKWARDS in time, given its ending point, by making a backwards\n",
    "guess at each timepoint and then using Hessian minimization to find the backwards vector that correctly\n",
    "leads to the current timestep value.  Uses forwardModel() . The forwards equations are:\n",
    "\n",
    "    tau dU/dt = -U + W*V + I\n",
    "    V = 0.5*tanh(U)+ 0.5\n",
    "\n",
    "**PARAMETERS:**\n",
    "\n",
    "endU     A column vector, nunits-by-1, indicating the values of U at time=end\n",
    "\n",
    "\n",
    "**OPTIONAL PARAMETERS:**\n",
    "\n",
    "dt      Scalar, timestep size\n",
    "\n",
    "tau     Scalar, in seconds\n",
    "\n",
    "nsteps  Number of timesteps to run, including time=0.\n",
    "\n",
    "input   Either an nunits-by-1 vector, in which case inputs to each unit are constant\n",
    "        across time, or a matrix, nunits-by-nsteps, indicating input for each unit at each timepoint.\n",
    "\n",
    "W       Weight matrix, nunits-by-nunits\n",
    "\n",
    "do_plot   Default false, if true, plots V of up to the first two dimensions\n",
    "\n",
    "tol       Tolerance in the minimization procedure for finding each backwards timestep. Passed on\n",
    "          to trust_region_Hessian_minimization()\n",
    "\n",
    "start_eta   Passed on to trust_region_Hessian_minimization()\n",
    "\n",
    "fignum     Figure number on which to plot\n",
    "\n",
    "clrearfig  If true, the figure is first cleared, otherwise any plot ois overlaid\n",
    "\n",
    "nderivs, difforder     Required for making sure function can create its own arrays and \n",
    "                       still be differentiated\n",
    "\n",
    "\n",
    "\n",
    "** RETURNS:**\n",
    "\n",
    "Ustart Vstart   nunits-by-1 vectors representing the starting values of U and V that were found.\n",
    "U, V            nunits-by-nsteps matrices containing the full trajectories\n",
    "costs           1-by-nsteps vector with the final cost from the minimization procedure for each\n",
    "                timestep. This is the squared difference between the U[t+1] produced by the U[t] \n",
    "                guess and the actual U[t+1]\n",
    "\n",
    "\"\"\"\n",
    "function backwardsModel(endU; nsteps=100, start_eta=10, tol=1e-15, maxiter=400, \n",
    "    do_plot=false, init_add=0, start_add=0, dt=0.01, \n",
    "    input=[], noise=[], nderivs=0, difforder=0, clearfig=false, fignum=1, params...)    \n",
    "\n",
    "    \"\"\"\n",
    "    o = g(z)    squashing tanh function, running from 0 to 1, is equal to 0.5 when input is 0.\n",
    "    \"\"\"\n",
    "    function g(z)\n",
    "        return 0.5*tanh.(z)+0.5\n",
    "    end\n",
    "    \n",
    "    nunits = length(endU)\n",
    "\n",
    "    # --- formatting input ---\n",
    "    if ~(typeof(input)<:Array) || prod(size(input))==1  # was a scalar\n",
    "        input = input[1]*(1+ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    elseif length(input)==0 # was the empty matrix\n",
    "        input = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    elseif size(input,2)==1     # was a column vector\n",
    "        input = input*(1+ForwardDiffZeros(1, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    end    \n",
    "    # --- formatting noise ---\n",
    "    if ~(typeof(noise)<:Array)  # was a scalar\n",
    "        noise = noise*(1+ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    elseif length(noise)==0 # was the empty matrix\n",
    "        noise = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    elseif size(noise,2)==1     # was a column vector\n",
    "        noise = noise*(1+ForwardDiffZeros(1, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    end    \n",
    "    \n",
    "    function J(U1, U2; nderivs=0, difforder=0, noise=[], inputs=[], pars...)\n",
    "        U2hat = forwardModel(U1; nsteps=2, noise=noise, input=input, nderivs=nderivs, difforder=difforder, pars...)[1]\n",
    "        U2hat = U2hat\n",
    "        DU = U2hat - U2\n",
    "    \n",
    "        return sum(DU.*DU)\n",
    "    end\n",
    "    \n",
    "    if length(noise)==0\n",
    "        noise = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    end\n",
    "\n",
    "    U = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    U = reshape(U, nunits, nsteps)\n",
    "    costs = ForwardDiffZeros(nsteps, 1, nderivs=nderivs, difforder=difforder)    \n",
    "    \n",
    "    U[:,end] = endU\n",
    "    for i=(nsteps-1):-1:1\n",
    "        if i==1\n",
    "            my_init_add = init_add\n",
    "            my_start_add = start_add\n",
    "        else\n",
    "            my_init_add = 0\n",
    "            my_start_add = 0\n",
    "        end\n",
    "                \n",
    "        U[:,i], costs[i] = trust_region_Hessian_minimization(U[:,i+1], \n",
    "            (x) -> J(x, U[:,i+1]; nderivs=length(endU), difforder=2, \n",
    "            input=input[:,i:i+1], noise = noise[:,i:i+1], \n",
    "            init_add=my_init_add, start_add=my_start_add, params...); \n",
    "            verbose=false, start_eta=start_eta, tol=tol, maxiter=maxiter)\n",
    "        if i>1; U[:,i] += noise[:,i]; end\n",
    "    end\n",
    "    \n",
    "    \n",
    "    V = g(U)  # REALLY???? HOW ABOUT THETA AND BETA?\n",
    "    \n",
    "    if do_plot\n",
    "        figure(fignum)   \n",
    "        if typeof(params)<:Array; params = Dict(params); end;\n",
    "        if haskey(params, :dt);     dt     = params[:dt];     end\n",
    "        if haskey(params, :nsteps); nsteps = params[:nsteps]; end\n",
    "        if length(endU)==1\n",
    "            if clearfig; clf(); end;\n",
    "            t = (0:nsteps-1)*dt\n",
    "            plot(t, V[1,:], \"m-\")\n",
    "            plot(t[1], V[1,1], \"go\")\n",
    "            plot(t[end], V[1,end], \"ro\")            \n",
    "            ylim([-0.01, 1.01])\n",
    "        elseif length(endU)>=2\n",
    "            if clearfig; clf(); end;            \n",
    "            plot(V[1,:], V[2,:], \"m-\")\n",
    "            plot(V[1,1], V[2,1], \"go\")\n",
    "            plot(V[1,end], V[2,end], \"ro\")\n",
    "            xlim([-0.01, 1.01]); ylim([-0.01, 1.01])\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return U[:,1], V[:,1], U, V, costs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing forward and backwards models with only 1 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First run a simple one-dimensional model (only one unit).  \n",
    "#\n",
    "# We've included the start_add parameter here, but the truth is that is a bad parameter\n",
    "# and is best not used: the reson is that it does not scale with dt.\n",
    "pygui(true); figure(1); clf();\n",
    "params = Dict(:noise => [0.1], :W => [-2], :nsteps=>10, :start_add=>-1.9)\n",
    "Uend = forwardModel([1.1]; do_plot=true, params...)[1]\n",
    "\n",
    "# Now an example of running the backwards model.\n",
    "# Note that because of the start_add parameter, the plot and the backwards model\n",
    "# look a little different: the backwards model correctly returns the starting value of U\n",
    "# while the forwards plot uses the initial value of V(U) *after( the start_add)\n",
    "#\n",
    "Ustart = backwardsModel(Uend; do_plot=true, tol=1e-30, params...)[1]\n",
    "@printf(\"Ustart came back as %g\\n\", Ustart[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing forward and backwards models now with 2 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If instead of letting the noise be generated internally in forwardModel(), \n",
    "# we pass a snapshot of it to forwardModel(), that same snapshot can then be passed to backwardsModel()\n",
    "# which can then know what the noise sample at each timestep is, and therefore can take it into account.\n",
    "\n",
    "nsteps=50\n",
    "params = Dict(:noise =>0.03*randn(2,nsteps) + [0.1,0]*ones(1,nsteps), :W => [0 -5; -5 0], :nsteps=>nsteps)\n",
    "\n",
    "Uend, Vend, U, V              = forwardModel([0.1,0.1]; do_plot=true, params...);\n",
    "Ustart, Vstart, bU, bV, costs = backwardsModel(Uend; do_plot=true, tol=1e-30, params...)\n",
    "\n",
    "@printf(\"Ustart came back as : \"); print_vector_g(Ustart); print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Testing opto in forwardsModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "#  Specifying opto_strength as a scalar\n",
    "#\n",
    "\n",
    "pygui(true)\n",
    "\n",
    "nsteps=50\n",
    "params = Dict(:sigma=>0.02, :opto_strength=>0.1, :opto_units=>1, :opto_times=>[0.05 0.15 ; 0.4 0.45],\n",
    ":W => [0 -5; -5 0], :nsteps=>nsteps)\n",
    "\n",
    "Uend, Vend, U, V, t = forwardModel([0.1,0.1]; input=[0.1,0], do_plot=true, fignum=1, params...);\n",
    "\n",
    "figure(2); clf();\n",
    "subplot(2,1,1)\n",
    "plot(t, V[1,:], t, V[2,:])\n",
    "ylabel(\"V\")\n",
    "remove_xtick_labels\n",
    "subplot(2,1,2)\n",
    "plot(t, U[1,:], t, U[2,:])\n",
    "ylabel(\"U\")\n",
    "xlabel(\"t\")\n",
    "\n",
    "\n",
    "# And now a derivative w.r.t. opto_strength\n",
    "func = (;pars...) -> forwardModel([0.1, 0.1]; input=[0.1,0], do_plot=false, merge(params, Dict(pars))...)[1][1]\n",
    "\n",
    "val, grad, hess = keyword_vgh(func, [\"opto_strength\"], [0.09])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "#  Specifying opto_strength as a full nunits-by-nsteps matrix\n",
    "#\n",
    "\n",
    "\n",
    "nsteps=50\n",
    "opto_strength = ones(2, nsteps)\n",
    "opto_strength[1, 10:20] = 0.2\n",
    "params = Dict(:sigma=>0.2, :opto_strength=>opto_strength, :opto_units=>1:2, :opto_times=>[0.05 0.15 ; 0.4 0.45],\n",
    ":W => [0 -5; -5 0], :nsteps=>nsteps)\n",
    "\n",
    "Uend, Vend, U, V, t = forwardModel([0.1,0.1]; input=[0.1,0], do_plot=true, fignum=1, params...);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring dt-dependence of gradients and hessian\n",
    "\n",
    "If we're doing things correctly, and $dt$ is small enough that we're starting to approximate the continuous-time solution, we should find that the output of our network does not depend very much on the choice of timestep $dt$. Correspondingly, gradients of the output with respect to network parameters should also be relatively $dt$-independent.\n",
    "\n",
    "In this example, we'll work with a two-dimensional mutual-inhibition network. We'll have one parameter, $W$, that represents the weight of the connection between the two units. Note, however that `forwardModel()` takes in a full connection matrix $W$, since `forwardModel()` makes no assumptions about any structure in that connection matrix. So what we do is wrap `forwardModel()` in a local function called `forward()` that transforms the scalar $W$ into a full 2-by-2 matrix $W$ and then calls `forwardModel()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# If you want to freeze the noise, provide a seed to the random number generator, e.g.: srand(111)\n",
    "# If you want a new random seed every time you run, but also want to preserve the seed so as to be able to re-run\n",
    "# the exact same noise again, you can use the local time, e.g. here in tenths of milliseconds:\n",
    "sr = Int64(round(10000*time()))\n",
    "srand(sr)\n",
    "startU=randn(100,2)-3\n",
    "\n",
    "sigma = 0  # This will be the default; below we set it to something else as a parameter\n",
    "\n",
    "\n",
    "dt = 0.02\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "W = -4\n",
    "noise = 0\n",
    "input = 0\n",
    "sigma = 0\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    ":noise=>noise, :input=>input, :sigma=>sigma, :const_add=>0, :init_add=>0)\n",
    "\n",
    "# wrapper function that will take our scalar W, indicating weight between the two units, and \n",
    "# turn that into a 2-by-2 weight matrix:\n",
    "forward = (startU; pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;     \n",
    "    forwardModel(startU; pars...)\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "args = [\"W\", \"const_add\", [\"start_add\" 2], \"sigma\"]\n",
    "params = [-4.01, 0.5, 0.2, -0.2, 0.01]\n",
    "\n",
    "# --- first with dt = 0.02\n",
    "figure(1); clf();\n",
    "value1, grad1, hess1 = keyword_vgh((;pars...)->forward([-0.2, 0.3]; do_plot=true, merge(model_params, Dict(pars))...)[1][1], args, params)\n",
    "title(@sprintf(\"Running with dt=%g\", dt))\n",
    "\n",
    "\n",
    "# --- now with dt = 0.005\n",
    "\n",
    "dt = 0.005\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    ":noise=>noise, :input=>input, :sigma=>sigma, :const_add=>0, :init_add=>0)\n",
    "\n",
    "figure(2); clf();\n",
    "value2, grad2, hess2 = keyword_vgh((;pars...)->forward([-0.2, 0.3]; do_plot=true, fignum=2, merge(model_params, Dict(pars))...)[1][1], args, params)\n",
    "title(@sprintf(\"Running with dt=%g\", dt))\n",
    "\n",
    "\n",
    "# --- and again with dt = 0.02 but different instantiation of the noise at each timestep\n",
    "\n",
    "dt = 0.02\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    ":noise=>noise, :input=>input, :sigma=>sigma, :const_add=>0, :init_add=>0)\n",
    "\n",
    "figure(3); clf();\n",
    "value3, grad3, hess3 = keyword_vgh((;pars...)->forward([-0.2, 0.3]; do_plot=true, fignum=3, merge(model_params, Dict(pars))...)[1][1], args, params)\n",
    "title(@sprintf(\"Running again with dt=%g\", dt))\n",
    "\n",
    "# As you will see below, the gradient values are all pretty stable across runs except for the gradient with respect to\n",
    "# sigma, but that is largely because of different noise instantiations, the two runs at dt=0.02 differ by about\n",
    "# as much as their difference w.r.t. the run at dt=0.005\n",
    "[\"first dt=0.02 run\" grad1[:]' ; \"dt=0.005 run\" grad2[:]'; \"second dt=0.02 run\" grad3[:]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.2",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
