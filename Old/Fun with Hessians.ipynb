{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import ForwardDiff\n",
    "using PyCall\n",
    "# import PyPlot\n",
    "using PyPlot\n",
    "using ForwardDiff\n",
    "using DiffBase\n",
    "\n",
    "pygui(true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fun with Hessians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function x, cost = one_d_minimizer(seed, func; tol=1e-5, maxiter=100, start_eta=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "one_d_minimizer"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "function x, cost = one_d_minimizer(seed, func; tol=1e-5, maxiter=100, start_eta=0.1)\n",
    "\n",
    "Minimizes a 1-d function using constrained Hessian minimization. \n",
    "We don't trust the long-range info from the Hessian too much, meaning that there's\n",
    "a given (adaptive) step size. If Newton's method suggests a step smaller than that step\n",
    "size, we take it. Otherwise, we only move in the direction of the gradient by the stepsize.\n",
    "\n",
    "Adaptive step size: Every step that the cost function gets smaller, the step grows by a factor \n",
    "of 1.1. If a step would have led to a larger cost function, the step is not taken, and\n",
    "the size falls by a factor of 2.\n",
    "\n",
    "PARAMETERS:\n",
    "===========\n",
    "\n",
    "seed       A float, the starting point for the minimization\n",
    "\n",
    "func       A one dimensional function, takes a float and returns a float that represents the current cost.\n",
    "\n",
    "\n",
    "OPTIONAL PARAMETERS:\n",
    "====================\n",
    "\n",
    "tol=1e-5   If a step would lead to a change in cost that is smaller in magnitude than tol, stop the minimization.\n",
    "\n",
    "maxiter=100   Maximum number of iterations for the minimization\n",
    "\n",
    "start_eta=0.1  The starting value of the step size\n",
    "\n",
    "\n",
    "RETURNS:\n",
    "========\n",
    "\n",
    "x0     the value of x that minimizes f(x)\n",
    "\n",
    "cost   The value of f(x0) \n",
    "\n",
    "\"\"\"\n",
    "function one_d_minimizer(seed, func; tol=1e-5, maxiter=100, start_eta=0.1)\n",
    "    eta = start_eta;\n",
    "    lambdavec = [seed]\n",
    "\n",
    "\n",
    "    out = DiffBase.HessianResult(lambdavec)\n",
    "    ForwardDiff.hessian!(out, func, lambdavec)\n",
    "    cost = DiffBase.value(out)\n",
    "    grad = DiffBase.gradient(out)\n",
    "    hess = DiffBase.hessian(out)\n",
    "\n",
    "\n",
    "    for i in [1:maxiter;]\n",
    "        h_delta = - grad./hess;\n",
    "        if abs(h_delta[1]) < eta\n",
    "            new_lambdavec = lambdavec + h_delta\n",
    "        else\n",
    "            new_lambdavec = lambdavec - eta*sign(grad)\n",
    "        end\n",
    "\n",
    "        if abs(new_lambdavec[1] - lambdavec[1]) < tol\n",
    "            break\n",
    "        end\n",
    "        \n",
    "        ForwardDiff.hessian!(out, func, new_lambdavec)\n",
    "        new_cost = DiffBase.value(out)\n",
    "        new_grad = DiffBase.gradient(out)\n",
    "        new_hess = DiffBase.hessian(out)\n",
    "\n",
    "        if new_cost .< cost\n",
    "            lambdavec[1] = new_lambdavec[1]\n",
    "            cost = new_cost;\n",
    "            grad = new_grad;\n",
    "            hess = new_hess;\n",
    "\n",
    "            eta = eta*1.1\n",
    "        else\n",
    "            eta = eta/2\n",
    "        end\n",
    "\n",
    "        # @printf \"%d: cost=%.3f lambda=%.3f\\n\" i cost lambdavec[1]\n",
    "    end\n",
    "    \n",
    "    return lambdavec[1], cost\n",
    "end\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function constrained_parabolic_minimization(H, G, r; tol=1e-6, min_only=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-0.148833,-0.20087],-6.096351909334731,-95.08495660219029,7.932154938695469e-23)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "function constrained_parabolic_minimization(H, G, r; tol=1e-6, min_only=true)\n",
    "\n",
    "Given a Hessian matrix, a gradient vector, and a desired radius from the origin, finds the vector \n",
    "that minimizes the parabola defined by the Hessian and the gradient, subject to the constraint that the\n",
    "vector's length equals the desired radius.\n",
    "\n",
    "PARAMETERS:\n",
    "===========\n",
    "\n",
    "H      A square symmetric matrix. It should have all positive eigenvalues.\n",
    "\n",
    "G      A vector, length equal to the size(H,2)\n",
    "\n",
    "r      desired radius\n",
    "\n",
    "OPTIONAL PARAMETERS:\n",
    "====================\n",
    "\n",
    "tol=1e-6        Numerical tolerance on the computations\n",
    "\n",
    "min_only=true   Return only the minimum, or, if false, all xs for all lambdas that match x'*x = r^2\n",
    "\n",
    "\n",
    "RETURNS:\n",
    "========\n",
    "\n",
    "x        The vector that minimizes 0.5*x'*H*x + x'*G subject to x'*x = r\n",
    " \n",
    "J        0.5*x'*H*x + x'*G at the returned x\n",
    "\n",
    "lambda   value of the Lagrange multiplier at which the radius constraint is satisfied\n",
    "\n",
    "c        The squared difference between the length of x and r. Should be small, otherwise somthing went wrong!\n",
    "\n",
    "\"\"\"\n",
    "function constrained_parabolic_minimization(H, G, r; tol=1e-6, min_only=true, \n",
    "    doplot=false, efactor=3.0)\n",
    "\n",
    "    #  --- First a couple of helper functions ----\n",
    "    \n",
    "    \"\"\"\n",
    "    function x_of_lambda(lambda)\n",
    "\n",
    "    Given square matrix H, vector G, and passed scalar lambda, returns the vector x that minimizes\n",
    "    \n",
    "    0.5 x'*H*x + x'*G - lambda *x'*x\n",
    "\n",
    "    \"\"\"\n",
    "    function x_of_lambda(lambda)\n",
    "        return inv(H - lambda*eye(size(H,1)))*(-G)\n",
    "    end\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    function q(lambda, r)\n",
    "\n",
    "    Returns the squared difference between r and the norm of x_of_lambda(lambda).\n",
    "    \"\"\"\n",
    "    function q(lambda, r)\n",
    "        return (r - norm(x_of_lambda(lambda)))^2\n",
    "    end\n",
    "\n",
    "    \n",
    "    # First scan lambda to find good candidates for minimizing the parabolic \n",
    "    # surface under the x'*x = r^2 constraint\n",
    "    L = eig(H)[1]\n",
    "    L0 = maximum(abs(L))\n",
    "    lambdas = L0*[-efactor:0.01:efactor;]\n",
    "    costs = zeros(size(lambdas))\n",
    "    for i in [1:length(lambdas);]\n",
    "        try \n",
    "            costs[i] = q(lambdas[i], r)\n",
    "        catch\n",
    "            costs[i] = Inf\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    if doplot\n",
    "        figure(2); clf();\n",
    "        plot(lambdas, costs, \"b.-\")\n",
    "        xlabel(\"lambda\")\n",
    "        ylabel(\"cost\")\n",
    "    end\n",
    "\n",
    "    # Take all candidates where the derivative of costs changes sign \n",
    "    # from negative to positive (those would be minima),\n",
    "    # plus the smallest and the largest lambdas tested, as candidates\n",
    "    g = append!(prepend!(find(diff(sign(diff(costs))) .> 0.99), [1]), [length(lambdas)])\n",
    "\n",
    "    lambdas_out = zeros(size(g))\n",
    "    costs_out   = zeros(size(g))\n",
    "    for i in [1:length(g);]\n",
    "        lambdas_out[i], costs_out[i] = one_d_minimizer(lambdas[g[i]], x -> q(x[1], r), start_eta=1, tol=tol)\n",
    "    end\n",
    "\n",
    "    # Eliminate any lambdas where x'*x doesn't match our desired value r\n",
    "    I = find(costs_out .< tol)\n",
    "    lambdas_out = lambdas_out[I]; costs_out = costs_out[I];\n",
    "    \n",
    "    # Eliminate any repeated lambdas, to within the specified numerical tolerance.\n",
    "    I = setdiff(1:length(lambdas_out), find(diff(lambdas_out) .< tol))\n",
    "    lambdas_out = lambdas_out[I]; costs_out = costs_out[I];\n",
    "    \n",
    "    # Find the parabolic estimate of the cost function at these points\n",
    "    J  = zeros(size(lambdas_out))\n",
    "    xs = zeros(length(G), length(lambdas_out))\n",
    "    for i in [1:length(J);]\n",
    "        xs[:,i] = x_of_lambda(lambdas_out[i])\n",
    "        J[i] = (0.5*xs[:,i]'*H*xs[:,i] + xs[:,i]'*G)[1]\n",
    "    end\n",
    "\n",
    "    # Find and return only the x that has the smallest J\n",
    "    if min_only\n",
    "        I = indmin(J)    \n",
    "    else\n",
    "        I = 1:length(J)\n",
    "    end\n",
    "    return xs[:,I], J[I], lambdas_out[I], costs_out[I]\n",
    "end\n",
    "\n",
    "    \n",
    "x, J, lo, co = constrained_parabolic_minimization([3 2; 2 3], [15,20], 0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: hess not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: hess not defined",
      ""
     ]
    }
   ],
   "source": [
    "constrained_parabolic_minimization(hess, grad'', eta, doplot=true, efactor=910.0)\n",
    "ylim([-1, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.01,0.1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ylim(-0.01, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x_of_lambda (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global H = [3 2; 2 3];\n",
    "global G = reshape([5,7], 2, 1)\n",
    "\n",
    "function x_of_lambda(lambda)\n",
    "    global H\n",
    "    global G\n",
    "    return inv(H - lambda*eye(size(H,1)))*(-G)\n",
    "end\n",
    "\n",
    "\n",
    "# figure(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the parabolic minimization\n",
    "\n",
    "Define your Hessian and Gradient at the top, and watch it minimize.  \n",
    "The green dot in Fig 1 should come out at the minimum contour, constrained to the red circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Array{Any,1}:\n",
       " PyObject <matplotlib.lines.Line2D object at 0x33b542950>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H = [6 5; 5 -6]    # Hessian\n",
    "G = [20, 15]''    # Gradient    H & G together define the contour surface\n",
    "# \n",
    "# H = [3 2; 2 3]\n",
    "# G = [10, 15]''\n",
    "# G = [5, 7]''\n",
    "G = [12, 15]''\n",
    "r = 8            # radius of red circle\n",
    "\n",
    "\n",
    "# -------------\n",
    "\n",
    "x = [-10:0.2:10;]'\n",
    "y = [-10:0.2:10;]\n",
    "\n",
    "X = repmat(x, length(x), 1); \n",
    "Y = repmat(y, 1, length(y)); \n",
    "\n",
    "Z = zeros(length(x), length(x))\n",
    "for i in [1:length(x);]\n",
    "    for j in [1:length(x);]\n",
    "        myx = [x[i] y[j]]'\n",
    "        Z[i,j] = (0.5*myx'*H*myx + G'*myx)[1]\n",
    "    end\n",
    "end\n",
    "\n",
    "figure(1); clf();\n",
    "ax = gca(); # projection=\"3d\")\n",
    "\n",
    "ax[:contour](Y, X, Z, 200); # , cmap=ColorMap(\"hot\"))\n",
    "\n",
    "vlines(0, -10, 10)\n",
    "hlines(0, -10, 10)\n",
    "\n",
    "\n",
    "\n",
    "angles = [1:360;]\n",
    "c = zeros(size(angles))\n",
    "xs = zeros(size(angles))\n",
    "ys = zeros(size(angles))\n",
    "for i in angles\n",
    "    xs[i] = r*cos(i*pi/180)\n",
    "    ys[i] = r*sin(i*pi/180)\n",
    "    myx = [xs[i] ys[i]]'\n",
    "    c[i] = (0.5*myx'*H*myx + G'*myx)[1]\n",
    "end\n",
    "plot(xs, ys, \"r-\")\n",
    "title(\"Green is min of contours constrained to red circle\")\n",
    "\n",
    "figure(2); clf()\n",
    "plot(angles, c, \"b.-\")\n",
    "xlabel(\"angle\")\n",
    "ylabel(\"J\")\n",
    "title(\"Cost function J as we go around round the red circle of Fig 1\")\n",
    "\n",
    "x = constrained_parabolic_minimization(H, G, r)[1]\n",
    "figure(1); \n",
    "plot(x[1], x[2], \"g.\", markersize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sandlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition constrained_Hessian_minimization(Any, Any) in module Main at In[13]:4 overwritten at In[15]:4.\n",
      "WARNING: Method definition #constrained_Hessian_minimization(Array{Any, 1}, Main.#constrained_Hessian_minimization, Any, Any) in module Main overwritten.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "constrained_Hessian_minimization (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "function constrained_Hessian_minimization(seed, func; start_eta=10, tol=1e-6, maxiter=400,\n",
    "    verbose=false)\n",
    "\n",
    "    params = seed\n",
    "    eta = start_eta\n",
    "\n",
    "    out = DiffBase.HessianResult(params)\n",
    "    ForwardDiff.hessian!(out, func, params)\n",
    "    cost = DiffBase.value(out)\n",
    "    grad = DiffBase.gradient(out)\n",
    "    hess = DiffBase.hessian(out)\n",
    "\n",
    "    chessdelta = zeros(size(params))\n",
    "\n",
    "    for i in [1:maxiter;]\n",
    "        hessdelta  = - inv(hess)*grad\n",
    "        try\n",
    "            chessdelta = constrained_parabolic_minimization(hess, grad'', eta)[1]\n",
    "            jumptype = \"not failed\"\n",
    "        catch\n",
    "            jumptype = \"failed\"\n",
    "        end\n",
    "\n",
    "        if norm(hessdelta) <= eta\n",
    "            new_params = params + hessdelta\n",
    "            jumptype = \"Newton\"\n",
    "        elseif jumptype != \"failed\" \n",
    "            new_params = params + chessdelta\n",
    "            jumptype  = \"constrained\"\n",
    "        end\n",
    "\n",
    "        if jumptype != \"failed\"\n",
    "            ForwardDiff.hessian!(out, func, new_params)\n",
    "            new_cost = DiffBase.value(out)\n",
    "            new_grad = DiffBase.gradient(out)\n",
    "            new_hess = DiffBase.hessian(out)\n",
    "\n",
    "            if abs(new_cost - cost) < tol\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "\n",
    "        if jumptype == \"failed\" || new_cost >= cost\n",
    "            eta = eta/2\n",
    "        else\n",
    "            eta = eta*1.1\n",
    "            params = new_params\n",
    "            cost = new_cost\n",
    "            grad = new_grad\n",
    "            hess = new_hess\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            @printf \"%d: eta=%.3f cost=%.4f  jtype=%s\\n\" i eta cost jumptype  \n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return params\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11.954988 seconds (32.39 M allocations: 3.942 GB, 9.16% gc time)\n",
      "  0.085113 seconds (54.85 k allocations: 6.255 MB)\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition energy(Any, Any, Any) in module Main at In[82]:13 overwritten at In[86]:13.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.068754 seconds (61.75 k allocations: 2.710 MB, 6.02% gc time)\n"
     ]
    }
   ],
   "source": [
    "xs = [-4:0.001:4;]\n",
    "ideal_params = [-1, 3, 1, 4]\n",
    "sigma = 0.5\n",
    "a = ideal_params[1]\n",
    "b = ideal_params[2]\n",
    "theta = ideal_params[3]\n",
    "bias = ideal_params[4]\n",
    "\n",
    "ys = a + b*tanh.((xs-bias)./theta) \n",
    "ys += sigma*randn(size(ys))\n",
    "\n",
    "function energy(params, x, y)\n",
    "    a = params[1]\n",
    "    b = params[2]\n",
    "    theta = params[3]\n",
    "    bias = params[4]\n",
    "    \n",
    "    J = 0\n",
    "    for i in [1:length(x);]\n",
    "        J += (y[i] - (a + b*tanh((x[i]-bias)/theta)))^2\n",
    "    end\n",
    "\n",
    "    return J\n",
    "end\n",
    "\n",
    "\n",
    "params = [1.0, 1, -1, 1]\n",
    "\n",
    "params = @time(constrained_Hessian_minimization(params, x -> energy(x, xs, ys), verbose=false,\n",
    "    tol=1e-12))\n",
    "\n",
    "# figure(2); clf();\n",
    "# plot(xs', ys', \"b.-\")\n",
    "\n",
    "myx = [0:0.01:1;]*(maximum(xs) - minimum(xs)) + minimum(xs)\n",
    "# plot(myx, params[1] + params[2]*tanh((myx-params[4])/params[3]), \"r-\")\n",
    "\n",
    "out = DiffBase.HessianResult(params)\n",
    "@time(ForwardDiff.hessian!(out, x -> energy(x, xs, ys), params));\n",
    "\n",
    "out = DiffBase.GradientResult(params)\n",
    "@time(ForwardDiff.gradient!(out, x -> energy(x, xs, ys), params));\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition regular_newton_minimization(Any, Any) in module Main at In[88]:2 overwritten at In[89]:2.\n",
      "WARNING: Method definition #regular_newton_minimization(Array{Any, 1}, Main.#regular_newton_minimization, Any, Any) in module Main overwritten.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "regular_newton_minimization (generic function with 1 method)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function regular_newton_minimization(seed, func; start_eta=0.05, tol=1e-6, maxiter=400, verbose=false)\n",
    "    params = seed\n",
    "    eta    = start_eta\n",
    "    \n",
    "    out = DiffBase.HessianResult(params)\n",
    "    ForwardDiff.hessian!(out, func, params)\n",
    "    cost = DiffBase.value(out)\n",
    "    grad = DiffBase.gradient(out)\n",
    "    hess = DiffBase.hessian(out)\n",
    "\n",
    "    for i in [1:maxiter;]\n",
    "        @printf \"NEED TO FIX: WHAT IF THE PARABOLIC APPROX DOESN'T LEAD TO A MINIMUM?\\n\"\n",
    "        new_params = params - eta * inv(hess)*grad\n",
    "        \n",
    "        ForwardDiff.hessian!(out, func, new_params)\n",
    "        new_cost = DiffBase.value(out)\n",
    "        new_grad = DiffBase.gradient(out)\n",
    "        new_hess = DiffBase.hessian(out)\n",
    "        \n",
    "        if abs(new_cost - cost)<tol\n",
    "            break\n",
    "        end\n",
    "        \n",
    "        if new_cost >= cost\n",
    "            eta=eta/2\n",
    "        else\n",
    "            eta = eta*1.1\n",
    "            if ( eta > 1 )\n",
    "                eta=1\n",
    "            end\n",
    "            params = new_params\n",
    "            cost = new_cost\n",
    "            grad = new_grad\n",
    "            hess = new_hess\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            @printf \"%d: eta=%.3f cost=%.4f params=[\" i eta cost \n",
    "            for p in [1:length(params);]\n",
    "                @printf \"%.3f\" params[p]\n",
    "                if p<length(params) @printf \", \"; end\n",
    "            end\n",
    "            @printf \"]\\n\"\n",
    "        end\n",
    "    end\n",
    "   \n",
    "    return params\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition adaptive_gradient_minimization(Any, Any) in module Main at In[27]:4 overwritten at In[29]:4.\n",
      "WARNING: Method definition #adaptive_gradient_minimization(Array{Any, 1}, Main.#adaptive_gradient_minimization, Any, Any) in module Main overwritten.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "adaptive_gradient_minimization (generic function with 4 methods)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function adaptive_gradient_minimization(seed, func; start_eta=0.1, tol=1e-6, maxiter=400,\n",
    "    verbose=false)\n",
    "    \n",
    "    params = seed\n",
    "    eta = start_eta\n",
    "\n",
    "    out = DiffBase.GradientResult(params)\n",
    "    ForwardDiff.gradient!(out, func, params)\n",
    "    cost = DiffBase.value(out)\n",
    "    grad = DiffBase.gradient(out)\n",
    "\n",
    "    for i in [1:maxiter;]\n",
    "        new_params = params - eta*grad\n",
    "\n",
    "        ForwardDiff.gradient!(out, func, new_params)\n",
    "        new_cost = DiffBase.value(out)\n",
    "        new_grad = DiffBase.gradient(out)\n",
    "\n",
    "        if abs(new_cost - cost) < tol\n",
    "            break\n",
    "        end\n",
    "    \n",
    "        if new_cost >= cost\n",
    "            eta = eta/2\n",
    "        else\n",
    "            eta = eta*1.1\n",
    "            params = new_params\n",
    "            cost = new_cost\n",
    "            grad = new_grad\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            @printf \"%d: eta=%.3f cost=%.4f\\n\" i eta cost  \n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return params\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.325773 seconds (4.97 M allocations: 240.401 MB, 13.46% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       "  -5.61208\n",
       "  -2.07047\n",
       "  -6.5679 \n",
       " -11.1208 "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "figure(2); clf()\n",
    "\n",
    "params = [1.0, 1, -1, 1]\n",
    "\n",
    "params = @time(adaptive_gradient_minimization(params, x -> energy(x, xs, ys), verbose=false,\n",
    "tol=1e-12, maxiter=2000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 30.445459 seconds (577.86 M allocations: 27.269 GB, 16.24% gc time)\n"
     ]
    }
   ],
   "source": [
    "params = [1.0, 1, -1, 1]\n",
    "\n",
    "params = @time(adaptive_gradient_minimization(params, x -> energy(x, xs, ys), verbose=false,\n",
    "tol=1e-12, maxiter=24000))\n",
    "\n",
    "# figure(2); clf();\n",
    "# plot(xs', ys', \"b.-\")\n",
    "\n",
    "myx = [0:0.01:1;]*(maximum(xs) - minimum(xs)) + minimum(xs)\n",
    "# plot(myx, params[1] + params[2]*tanh((myx-params[4])/params[3]), \"r-\")\n",
    "\n",
    "out = DiffBase.GradientResult(params)\n",
    "# @time(ForwardDiff.gradient!(out, x -> energy(x, xs, ys), params));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Array{Any,1}:\n",
       " PyObject <matplotlib.lines.Line2D object at 0x322010e50>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "figure(2); clf();\n",
    "plot(xs', ys', \"b.-\")\n",
    "\n",
    "myx = [0:0.01:1;]*(maximum(xs) - minimum(xs)) + minimum(xs)\n",
    "plot(myx, params[1] + params[2]*tanh((myx-params[4])/params[3]), \"r-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lambdas = setdiff([-10.001:0.01:10;], eig(H))\n",
    "Js = zeros(size(lambdas))\n",
    "for i in [1:length(lambdas);]\n",
    "    try\n",
    "        Js[i] = (norm(x_of_lambda(lambdas[i]))-5)^2\n",
    "    catch\n",
    "        println(i)\n",
    "    end\n",
    "end\n",
    "\n",
    "figure(2); clf();\n",
    "plot(lambdas, Js)\n",
    "ylim(-10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition one_d_minimizer(Any, Any) in module Main at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:40 overwritten at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:40.\n",
      "WARNING: Method definition #one_d_minimizer(Array{Any, 1}, Main.#one_d_minimizer, Any, Any) in module Main overwritten.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'one_d_minimizer :: Tuple{Any,Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition constrained_parabolic_minimization(Any, Any, Any) in module Main at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:128 overwritten at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:128.\n",
      "WARNING: Method definition #constrained_parabolic_minimization(Array{Any, 1}, Main.#constrained_parabolic_minimization, Any, Any, Any) in module Main overwritten.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'constrained_parabolic_minimization :: Tuple{Any,Any,Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition constrained_Hessian_minimization(Any, Any) in module Main at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:219 overwritten at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:219.\n",
      "WARNING: Method definition #constrained_Hessian_minimization(Array{Any, 1}, Main.#constrained_Hessian_minimization, Any, Any) in module Main overwritten.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'constrained_Hessian_minimization :: Tuple{Any,Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition adaptive_gradient_minimization(Any, Any) in module Main at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:281 overwritten at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:281.\n",
      "WARNING: Method definition #adaptive_gradient_minimization(Array{Any, 1}, Main.#adaptive_gradient_minimization, Any, Any) in module Main overwritten.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "adaptive_gradient_minimization (generic function with 4 methods)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"hessian_utils.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed = 1.3\n",
    "eta = 0.1;\n",
    "lambdavec = [seed]\n",
    "\n",
    "\n",
    "out = DiffBase.GradientResult(lambdavec)\n",
    "ForwardDiff.gradient!(out, x -> q(x[1], 5), lambdavec)\n",
    "cost = DiffBase.value(out)\n",
    "grad = DiffBase.gradient(out)\n",
    "\n",
    "\n",
    "for i in [1:100;]\n",
    "    new_lambdavec = lambdavec - eta*grad;\n",
    "\n",
    "    ForwardDiff.gradient!(out, x -> q(x[1], 5), new_lambdavec)\n",
    "    new_cost = DiffBase.value(out)\n",
    "    new_grad = DiffBase.gradient(out)\n",
    "\n",
    "    if new_cost < cost && new_lambdavec[1]>1.0001 && new_lambdavec[1]<4.999 \n",
    "        lambdavec = new_lambdavec\n",
    "        cost = new_cost\n",
    "        grad = new_grad\n",
    "        eta = eta*1.1\n",
    "    else\n",
    "        eta = eta/2\n",
    "    end\n",
    "\n",
    "    # @printf \"%d: eta=%g cost=%.3f lambda=%.3f\\n\" i eta cost lambdavec[1]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "J = zeros(2,1)\n",
    "\n",
    "J[1] = size(x[:,1]'*H*x[:,1] + x[:,1]'*G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L = eig(H)[1]\n",
    "r = 0.25\n",
    "\n",
    "L0 = abs(maximum(L))\n",
    "lambdas = L0*[-3:0.01:3;]\n",
    "costs = zeros(size(lambdas))\n",
    "for i in [1:length(lambdas);]\n",
    "    try \n",
    "        costs[i] = q(lambdas[i], r)\n",
    "    catch\n",
    "        costs[i] = Inf\n",
    "    end\n",
    "end\n",
    "\n",
    "figure(2); clf(); plot(lambdas, costs, \"-\"); ylim(-10, 20); grid()\n",
    "g = [1 find(diff(sign(diff(costs))) .> 0.99) length(lambdas)]\n",
    "println(lambdas[g])\n",
    "vlines(lambdas[g], ylim()[1], ylim()[2])\n",
    "\n",
    "for i in [1:length(g);]\n",
    "    lambda, cost = one_d_minimizer(lambdas[g[i]], x -> q(x[1], r), start_eta=1)\n",
    "    @printf \"%d: lambda=%g, cost=%g\\n\" i lambda cost\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lambda, cost = minimizer(20.0, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norm(x_of_lambda(39.5388))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = [2, 3;]\n",
    "\n",
    "b = [1; a; 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = plot(lambdas+lambdavec[1], 0.5*hess[1]*lambdas.^2 + grad[1]*lambdas + cost, \"r-\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.2",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
