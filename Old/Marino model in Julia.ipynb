{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We define functions to convert Duals, the variable types used by ForwardDiff, \\nto Floats. This is useful if we want to print out the value of a variable \\n(since print doesn't know how to Duals). Note that after being converted to a Float, no\\ndifferentiation by ForwardDiff can happen!  e.g. after\\n    x = convert(Float64, y)\\nForwardDiff can still differentiate y, but it can't differentiate x\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import ForwardDiff\n",
    "using PyCall\n",
    "# import PyPlot\n",
    "using PyPlot\n",
    "using ForwardDiff\n",
    "using DiffBase\n",
    "\n",
    "pygui(true)\n",
    "\n",
    "import Base.convert\n",
    "convert(::Type{Float64}, x::ForwardDiff.Dual) = Float64(x.value)\n",
    "function convert(::Array{Float64}, x::Array{ForwardDiff.Dual}) \n",
    "    y = zeros(size(x)); \n",
    "    for i in 1:prod(size(x)) \n",
    "        y[i] = convert(Float64, x[i]) \n",
    "    end\n",
    "    return y\n",
    "end\n",
    "\n",
    "include(\"hessian_utils.jl\")\n",
    "\n",
    "\"\"\"\n",
    "We define functions to convert Duals, the variable types used by ForwardDiff, \n",
    "to Floats. This is useful if we want to print out the value of a variable \n",
    "(since print doesn't know how to Duals). Note that after being converted to a Float, no\n",
    "differentiation by ForwardDiff can happen!  e.g. after\n",
    "    x = convert(Float64, y)\n",
    "ForwardDiff can still differentiate y, but it can't differentiate x\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The main model dynamics function\n",
    "\n",
    "Note that the documenation indicates some default values for the optional parameters; these values need to be updated to what the code actually says below. The actual defaults are much closer to what Marino's May 2017 model has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "run_dynamics"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "function t, U, V, W = run_dynamics(trial_type, params)\n",
    "\n",
    "    Runs the 4-way mutual inhibition model\n",
    "    \n",
    "    OBLIGATORY PARAMS:\n",
    "    ------------------\n",
    "\n",
    "        trial_type    Must be either \"pro\" or \"anti\"\n",
    "        params        A 4-element vector, whose components are interpreted, in turn, as\n",
    "            vwi   vertical inhibitory weights between Pro and Anti units. \n",
    "                    \"36\" means a -36 connection.\n",
    "            hwi   horizontal inhibitory weights between like units on the two sides. \n",
    "                    \"36\" means a -36 connection.\n",
    "            const_pro_bias   Extra positive input to the two Pro units\n",
    "            const_E          Constant positive input to all four units\n",
    "\n",
    "\n",
    "    OPTIONAL PARAMS:\n",
    "    ----------------\n",
    "    \n",
    "        U_rest = -1     Resting point for U in the absance of other inputs\n",
    "        g_leak = 0.5    Mutliplies (U_rest - U) for the dynamics\n",
    "        theta = 1       Threshold on U for sigmoidal transform from U to V \n",
    "        beta  = 1       Scaling on sigmoid going from U to V:   V = 0.5*tanh((U-theta)/beta) + 0.5\n",
    "        dt=0.02         Timestep\n",
    "        sigma=0.1,      added standard deviation on U per unit time\n",
    "        rule_period = 0.5       in seconds\n",
    "        delay_period = 0.5      in seconds. Opto will happen during this period\n",
    "        target_period = 0.1     right_light_input and target_extra_E will happen during this period; \n",
    "                                Pro v Anti input will be turned off; \n",
    "        post_target_period = 0.5  target_extra_E will still be on, but right_side_input won't\n",
    "        tau=0.1         Time constant of dynamics, in secs\n",
    "        marino_tau = False   If True, tau applies only to leak term; if False, applies to entire dUdt\n",
    "        start_U = [-7, -7, -7, -7]\n",
    "        const_E = 0.15  Constant excitation added to all units\n",
    "        right_light_input=1     Extra excitation to right side of the brain units during the target period\n",
    "        right_light_pro_extra   Even further excitation added to pro Right side units during target\n",
    "        vwi = 1.5       Weight between ProContra and AntiIpsi units (on each side of the brain)\n",
    "        hwi = 1.5       Weight between ProContra units across the brain; also between AntiContra\n",
    "        pro_self_ex = 0   Self excitation weight of Pro units\n",
    "        anti_self_ex = 0  Self excitation weight of Anti units\n",
    "        pfc_anti_input = 0.05    Input to Anti units during rule and delay periods\n",
    "        pfc_pro_input = np.nan   Input to Pro units during rule and delay periods (default means same as Anti)\n",
    "        const_pro_bias = 0       A constant extra input to the Pro units\n",
    "        target_extra_E = 0.25    Extra excitation added to all units during target and post_target periods\n",
    "        opto='off'    Whether there is optogenetic-induced scaling of outputs.\n",
    "            ='on'     Opto will be done during the delay period only\n",
    "            ='dt'     Opto will be done during the delay plus the target period\n",
    "           opto_scaling=0.5      Factor by which to scale the weight matrix during opto \n",
    "           opto_scale_on_E=1     Factor by which to scale the constant excitation during opto\n",
    "           opto_conductance=0    How much conductance to add to gleak during opto\n",
    "           opto_current=0        Added to dUdt at each time step during opto\n",
    "        unilateral_opto = False If True, then opto_scale_on_E will be forced to 0, and opto_scaling will apply\n",
    "                                to only one side\n",
    "        do_plot = True  whether or not to plot the results\n",
    "        fignum=1        figure on which to plot\n",
    "        decision_threshold   If |V(Pro_R) - V(Pro_L)| >= this number, a proper answer is produced. \n",
    "                The target light is presented to the right, so Right means \"pro\"\n",
    "\n",
    "\n",
    "    RETURNS:\n",
    "    --------\n",
    "\n",
    "        response     +1 for Pro, -1 for Anti, 0 for undefined \n",
    "                        if |V(Pro_R) - V(Pro_L)| < decision_threshold\n",
    "        t    Time vector\n",
    "        U   U matrix, size 4-by-len(t). Order is ProContra on right side, AntiIpsi on right\n",
    "                    side, ProContra on left side, AntiIpsi on left.\n",
    "        V   V matrix = 0.5*np.tanh((U-theta)/beta) + 0.5\n",
    "        W   Weight matrix between units\n",
    "        \n",
    "\"\"\"\n",
    "function run_dynamics(trial_type, params::Vector ; opto=\"off\", opto_scaling=0.8, opto_scale_on_E=1,\n",
    "    opto_conductance = 0, opto_current=0, \n",
    "    right_light_pro_extra = 0, right_light_input=12, \n",
    "    pro_self_ex = 0, anti_self_ex = 0, \n",
    "    tau=4.4, marino_tau = true, dt=0.05, target_extra_E = 0,\n",
    "    pfc_anti_input = 1.6, pfc_pro_input = 0.05,\n",
    "    sigma=3.2, start_U = [-25, -25, -25, -25], do_plot = false, fignum=1, \n",
    "    g_leak = 1, U_rest = 0, theta = 5, beta = 50,\n",
    "    rule_period = 200, delay_period = 200, target_period = 50,\n",
    "    post_target_period = 0.01, decision_threshold = 0.3)\n",
    "    \n",
    "    vwi = params[1]; hwi = params[2]; const_pro_bias = params[3]; const_E = params[4]\n",
    "    \n",
    "    t = [0 : dt : rule_period + delay_period + target_period + post_target_period;] \n",
    "\n",
    "    V = zeros(eltype(params), 4, length(t))   # the eltype(params) is for ForwardDiff\n",
    "    U = zeros(eltype(params), 4, length(t))\n",
    "\n",
    "    U[:,1] = start_U\n",
    "\n",
    "    W = [pro_self_ex -vwi -hwi 0; -vwi anti_self_ex 0 -hwi; \n",
    "        -hwi 0 pro_self_ex -vwi; 0 -hwi -vwi anti_self_ex]\n",
    "\n",
    "    E = const_E\n",
    "    \n",
    "    for i in [2:length(t);]  # the funny semicolon appears to be necessary in Julia\n",
    "        if marino_tau\n",
    "            dUdt = E + W * V[:,i-1] + g_leak*(U_rest - U[:,i-1])/tau\n",
    "        else\n",
    "            dUdt = E + W * V[:,i-1] + g_leak*(U_rest - U[:,i-1])\n",
    "        end\n",
    "    \n",
    "        if t[i] < rule_period + delay_period\n",
    "            if trial_type==\"anti\"\n",
    "                dUdt[[2,4]] += pfc_anti_input\n",
    "            elseif trial_type == \"pro\"\n",
    "                dUdt[[1,3]] += pfc_pro_input\n",
    "            else\n",
    "            end\n",
    "            \n",
    "        elseif t[i] < rule_period + delay_period + target_period\n",
    "            dUdt[[1,2]] += right_light_input\n",
    "            dUdt[1]     += right_light_pro_extra\n",
    "            dUdt        += target_extra_E\n",
    "        else\n",
    "            dUdt        += target_extra_E\n",
    "        end\n",
    "    \n",
    "        dUdt[[1,3]] += const_pro_bias\n",
    "        \n",
    "        if marino_tau\n",
    "            try\n",
    "                U[:,i] = U[:,i-1] +       dt*dUdt + sigma*randn(4)*sqrt(dt)\n",
    "            catch\n",
    "                @printf \"yep\\n\"\n",
    "            end\n",
    "        else\n",
    "            U[:,i] = U[:,i-1] + (dt/tau)*dUdt + sigma*randn(4)*sqrt(dt)\n",
    "        end\n",
    "    \n",
    "        V[:,i] = 0.5*tanh((U[:,i]-theta)/beta) + 0.5\n",
    "    end    \n",
    "\n",
    "    if do_plot\n",
    "        figure(fignum); \n",
    "        subplot(3,1,1)\n",
    "        h = plot(t, V'); \n",
    "        setp(h[1], color=[0, 0, 1])\n",
    "        setp(h[2], color=[1, 0, 0])\n",
    "        setp(h[3], color=[1, 0.5, 0.5])\n",
    "        setp(h[4], color=[0, 1, 1])\n",
    "        ylabel(\"V\")\n",
    "\n",
    "        ax = gca()\n",
    "        yl = [ylim()[1], ylim()[2]]\n",
    "        vlines([rule_period, rule_period+delay_period, \n",
    "            rule_period+delay_period+target_period], \n",
    "            -0.05, 1.05, linewidth=2)\n",
    "        if yl[1]<0.02\n",
    "            yl[1] = -0.02\n",
    "        end\n",
    "        if yl[2]>0.98\n",
    "            yl[2] = 1.02\n",
    "        end\n",
    "        ylim(yl)\n",
    "        grid(true)\n",
    "        \n",
    "        subplot(3,1,2)\n",
    "        hu = plot(t, U')\n",
    "        setp(hu[1], color=[0, 0, 1])\n",
    "        setp(hu[2], color=[1, 0, 0])\n",
    "        setp(hu[3], color=[1, 0.5, 0.5])\n",
    "        setp(hu[4], color=[0, 1, 1])\n",
    "        ylabel(\"U\"); ylim(-100, 100)\n",
    "        vlines([rule_period, rule_period+delay_period, \n",
    "            rule_period+delay_period+target_period], \n",
    "            ylim()[1], ylim()[2], linewidth=2)\n",
    "\n",
    "        grid(true)\n",
    "    \n",
    "        subplot(3,1,3)\n",
    "        hr = plot(t, V[1,:] - V[3,:])\n",
    "        ylim([-0.9, 0.9])\n",
    "        vlines([rule_period, rule_period+delay_period, \n",
    "            rule_period+delay_period+target_period], \n",
    "            ylim()[1], ylim()[2], linewidth=2)\n",
    "        xlabel(\"t\"); ylabel(\"Pro R - Pro L\")\n",
    "        grid(true)\n",
    "        \n",
    "    end\n",
    "    \n",
    "#    if V[1,end] - V[3,end] > decision_threshold\n",
    "#        answer = 1\n",
    "#    elseif V[1,end] - V[3,end] < -decision_threshold\n",
    "#        answer = -1\n",
    "#    else\n",
    "#        answer = 0\n",
    "#    end\n",
    "\n",
    "    answer1 = 0.5 + 0.5*tanh(((V[1,end] - V[3,end]) - decision_threshold)/0.1)\n",
    "    answer2 = 0.5 + 0.5*tanh(((V[3,end] - V[1,end]) - decision_threshold)/0.1)\n",
    "    answer  = answer1 - answer2\n",
    "    \n",
    "    return answer, t, U, V, W \n",
    "end\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the dynamics with Marino's parameters just to test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trial_type = \"pro\"\n",
    "params = [36, 1.8, 0.854, 1]\n",
    "ntrys = 4\n",
    "\n",
    "figure(1); clf();\n",
    "for i in [1:ntrys;]\n",
    "    answer, t, U, V, W = run_dynamics(\"pro\", params, do_plot=true, sigma=0.4, \n",
    "    pfc_anti_input = 1.6, pfc_pro_input = 0.05, fignum=1)\n",
    "    # println(answer)\n",
    "end\n",
    "\n",
    "figure(2); clf();\n",
    "for i in [1:ntrys;]\n",
    "    answer, t, U, V, W = run_dynamics(\"anti\", params, do_plot=true, sigma=0.4,\n",
    "    pfc_anti_input = 1.6, pfc_pro_input = 0.05, fignum=2)\n",
    "    # println(answer)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we play with testing with differentiating the main dynamics function, and defining a cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "function test_params(params; ntrials=10, sigma=3.2, random_seed=321)\n",
    "\n",
    "Computes the final pro unit outputs given params. \n",
    "When run with the same parameters as J() above, including the random seed,\n",
    "will go through the exact same sequence of trials and dynamics and responses.\n",
    "\n",
    "OBLIGATORY PARAMS:\n",
    "------------------\n",
    "\n",
    "params     The same obligatory vector as in run_dynamics()\n",
    "\n",
    "OPTIONAL PARAMS:\n",
    "----------------\n",
    "\n",
    "ntrials=10\n",
    "sigma=3.2\n",
    "random_seed=321\n",
    "\n",
    "RETURNS:\n",
    "--------\n",
    "\n",
    "pro_answers     A 2-by-ntrials vector. First row is final value of Pro R unit in \"pro\" trials; second row is final value of Pro L unit\n",
    "anti_answers    As pro_answers, but for anti trials\n",
    "\n",
    "\"\"\"\n",
    "function test_params(params, targets; ntrials=10, sigma=3.2, random_seed=321)\n",
    "    srand(random_seed)\n",
    "    cost = 0;\n",
    "    pro_answers  = zeros(eltype(params), 2, ntrials)\n",
    "    anti_answers = zeros(eltype(params), 2, ntrials)\n",
    "    cost      = 0;\n",
    "    pro_perf  = 0;\n",
    "    anti_perf = 0;\n",
    "    \n",
    "    for i in [1:ntrials;]\n",
    "        answer, t, U, V, W = run_dynamics(\"pro\", params, do_plot=false, sigma=sigma) # , pfc_anti_input = 0.8, pfc_pro_input=0.8)\n",
    "        pro_perf += V[1,end] - V[3,end]        \n",
    "        cost += (V[1,end]- V[3,end])^2\n",
    "        pro_answers[1,i] = V[1,end]\n",
    "        pro_answers[2,i] = V[3,end]\n",
    "        \n",
    "        answer, t, U, V, W = run_dynamics(\"anti\", params, do_plot=false, sigma=sigma) # , pfc_anti_input = 0.8, pfc_pro_input=0.8)\n",
    "        anti_perf += V[3,end] - V[1,end]        \n",
    "        cost += (V[1,end]- V[3,end])^2\n",
    "        anti_answers[1,i] = V[1,end]\n",
    "        anti_answers[2,i] = V[3,end]\n",
    "    end\n",
    "\n",
    "    RMSp = sqrt(sum((pro_answers[1,:]  - pro_answers[2,:]).^2) / ntrials)\n",
    "    RMSa = sqrt(sum((anti_answers[1,:] - anti_answers[2,:]).^2) / ntrials)\n",
    "\n",
    "    # @printf \"pro_perf = %f   ntrials=%d   targets[1]=%f \\n\" pro_perf ntrials targets[1]\n",
    "    cost1 = (pro_perf - ntrials*targets[1])^2 + (anti_perf - ntrials*targets[2])^2 \n",
    "    cost2 = - cost\n",
    "\n",
    "    return pro_answers, anti_answers, (RMSa+RMSp)/2, cost1, cost2\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "FIX:  THIS IS THE DOC FOR J(), NOT JCOST2() !!!!\n",
    "\n",
    "function J(params, targets; ntrials=10, sigma=3.2, random_seed=321)\n",
    "\n",
    "Computes a cost function for certain parameters and fraction correct targets\n",
    "\n",
    "OBLIGATORY PARAMS:\n",
    "------------------\n",
    "\n",
    "params     The same obligatory vector as in run_dynamics()\n",
    "targets    A 2-element vector, first element is target fraction correct for Pro, second for Anti. E.g., [0.8 0.7]\n",
    "\n",
    "OPTIONAL PARAMS:\n",
    "----------------\n",
    "\n",
    "ntrials=10\n",
    "sigma=3.2\n",
    "random_seed=321\n",
    "\n",
    "RETURNS:\n",
    "--------\n",
    "\n",
    "cost      A scalar\n",
    "\"\"\"\n",
    "function Jcost2(params, targets; ntrials=20, theta1=0.15, theta2=0.15, beta=0.5, verbose=false)\n",
    "    pro_answers, anti_answers = test_params(params, targets, ntrials=ntrials)\n",
    "    \n",
    "    pro_out = 0.5*(1 + tanh.((pro_answers[1,:]  - pro_answers[2,:])/theta1))\n",
    "    ant_out = 0.5*(1 + tanh.((anti_answers[2,:] - anti_answers[1,:])/theta1))\n",
    "\n",
    "    pro_dif = tanh((pro_answers[1,:]  - pro_answers[2,:]) /theta2).^2\n",
    "    ant_dif = tanh((anti_answers[1,:] - anti_answers[2,:])/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(pro_out) - targets[1])^2 + (mean(ant_out) - targets[2])^2\n",
    "    cost2 = -mean(pro_dif) \n",
    "    cost2 -= mean(ant_dif)\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"                              cost1=%.3f, cost2=%.3f, pro_out=%.3f, anti_out=%.3f\\n\", convert(Float64, cost1),\n",
    "        beta*convert(Float64, cost2), convert(Float64, mean(pro_out)), convert(Float64, mean(ant_out)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + beta*cost2, pro_out, ant_out, pro_dif, ant_dif\n",
    "end\n",
    "\n",
    "\n",
    "targets = [0.8, 0.7]   # Fraction correct in Pro and Anti\n",
    "params  = [36, 1, 0.854, 1.8]\n",
    "\n",
    "println(Jcost2(params, targets))\n",
    "\n",
    "grad = ForwardDiff.gradient(x -> Jcost2(x, targets)[1], params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual test of the derivatives for sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Does this match what we got above?\n",
    "\n",
    "delta = 0.001; i=4; params2 = copy(params); params2[i] += delta; (Jcost2(params2, targets)[1] - Jcost2(params, targets)[1])/delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Getting the derivatives doesn't seem to add too much time, only about 60%:\n",
    "\n",
    "@time(Jcost2(params, targets, ntrials=20))\n",
    "@time(ForwardDiff.gradient(x -> Jcost2(x, targets, ntrials=20)[1], params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# === RUN THE MINIMIZATION WITH CONSTRAINED HESSIAN MINIMIZATION ===\n",
    "#\n",
    "\n",
    "targets = [0.8, 0.7]  # Pro ant Anti desired per cent correct, respectively\n",
    "\n",
    "ntrials=20\n",
    "theta1=0.15\n",
    "theta2=0.2\n",
    "\n",
    "params = @time(constrained_Hessian_minimization([2.0, 2.0, 1, 1], \n",
    "x -> Jcost2([exp(x[1]), exp(x[2]), x[3], x[4]], targets, ntrials=ntrials, theta1=theta1, theta2=theta2, beta=0.05, verbose=true)[1], \n",
    "    verbose=true, tol=1e-7, start_eta=2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST THE RESULTS GIVEN PARAMS, TARGETS, NTRIALS, THETA1, THETA2\n",
    "\n",
    "tc, pro_out, ant_out, pro_dif, ant_dif = Jcost2(params, targets, \n",
    "    ntrials=ntrials, theta1=theta1, theta2=theta2, beta=0.05, verbose=true);\n",
    "\n",
    "figure(1); clf();\n",
    "subplot(4,1,1)\n",
    "plot(1:ntrials, pro_out, \"b.\")\n",
    "subplot(4,1,2);\n",
    "plot(1:ntrials, ant_out, \"r.\")\n",
    "subplot(4,1,3)\n",
    "plot(1:ntrials, pro_dif, \"b.\")\n",
    "subplot(4,1,4);\n",
    "plot(1:ntrials, ant_dif, \"r.\")\n",
    "\n",
    "# Should really fold reporting this proans and antans into Jcost2, we don't need to run the model twice!\n",
    "proans, antans = test_params(params, targets, ntrials=ntrials)\n",
    "\n",
    "figure(2); clf();\n",
    "subplot(2,1,1)\n",
    "plot(1:ntrials, proans[1,:], \"b.\")\n",
    "plot(1:ntrials, proans[2,:], \"r.\")\n",
    "title(\"pro\")\n",
    "subplot(2,1,2)\n",
    "plot(1:ntrials, antans[1,:], \"b.\")\n",
    "plot(1:ntrials, antans[2,:], \"r.\")\n",
    "title(\"anti\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# === RUN THE MINIMIZATION WITH ADAPTIVE GRADIENT DESCENT ===\n",
    "#\n",
    "\n",
    "targets = [0.8, 0.7]  # Pro ant Anti desired per cent correct, respectively\n",
    "\n",
    "ntrials=20\n",
    "theta1=0.15\n",
    "theta2=0.2\n",
    "\n",
    "params = @time(adaptive_gradient_minimization([18, 1.8, 0.854, 1], \n",
    "x -> Jcost2(x, targets, ntrials=ntrials, theta1=theta1, theta2=theta2, beta=0.05, verbose=true)[1], \n",
    "verbose=true, tol=1e-5, start_eta=2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = @time(constrained_Hessian_minimization([18, 1.8, 0.854, 1], \n",
    "x -> Jcost2(x, targets, ntrials=ntrials, theta1=theta1, theta2=theta2, beta=0.05, verbose=true)[1], \n",
    "    verbose=true, tol=1e-5, start_eta=2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING PASSING KWARGS TO THE DIFFERENTIATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function tester(; a=10, b=20, c=30)\n",
    "    @printf \"eltype(a)=%s\\n\" eltype(a)\n",
    "    @printf \"eltype(b)=%s\\n\" eltype(b)\n",
    "    @printf \"eltype(c)=%s\\n\" eltype(b)\n",
    "    return a*10 + b*20 + c*30\n",
    "end\n",
    "\n",
    "function cost_function(;args...)\n",
    "    return tester(;args...)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function tester2(x::Vector; par1=10, par2=20, pars...)\n",
    "    println(pars)\n",
    "    @printf \"eltype is: %s\" eltype(x)\n",
    "    return x[1]+ 10*x[2]+100*x[3]^2\n",
    "end\n",
    "\n",
    "ForwardDiff.hessian(x->tester2(x, par1=5), [1.0,0, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "function make_dict(args, x::Vector)\n",
    "    kwargs = Dict();\n",
    "    for i in [1:length(args);]\n",
    "        kwargs = merge(kwargs, Dict(Symbol(args[i]) => x[i]))\n",
    "    end\n",
    "    return kwargs\n",
    "end\n",
    "\n",
    "function ForwardDiffZeros(m, n; nderivs=0, derivorder=0)\n",
    "    if nderivs == 0 || derivorder == 0\n",
    "        return zeros(m, n)\n",
    "    elseif derivorder == 1\n",
    "        return zeros(ForwardDiff.Dual{nderivs, Float64}, m , n)\n",
    "    elseif derivorder == 2\n",
    "        return zeros(ForwardDiff.Dual{nderivs, ForwardDiff.Dual{nderivs, Float64}}, m, n)\n",
    "    else\n",
    "        error(\"Don't know how to do that order of derivatives!\", nderivs)\n",
    "    end\n",
    "end\n",
    "                \n",
    "\n",
    "args = [\"a\", \"b\"]\n",
    "init = [200, 300]\n",
    "make_dict(args, init)\n",
    "\n",
    "# ForwardDiffZeros(1, 2, nderivs=2, derivorder=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function tester(; a=10, b=20, c=30)\n",
    "    # @printf \"eltype(a)=%s\\n\" eltype(a)\n",
    "    # @printf \"eltype(b)=%s\\n\" eltype(b)\n",
    "    # @printf \"eltype(c)=%s\\n\" eltype(b)\n",
    "    return a*10 + b*20 + c*30\n",
    "end\n",
    "\n",
    "function cost_function(;args...)\n",
    "    return tester(;args...)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "args = [\"c\", \"b\"]\n",
    "init = [200.0, 3000]\n",
    "\n",
    "\n",
    "# ForwardDiff.gradient(x -> tester(;make_dict(args, x)...), init)\n",
    "ForwardDiff.gradient(x -> cost_function(;make_dict(args, x)...), init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "typeof(eltype(ForwardDiff.Dual{2,Float64}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "function test_params2(targets; taking_derivative=true, ntrials=10, sigma=3.2, random_seed=321, params...)\n",
    "\n",
    "Computes the final pro unit outputs given params. \n",
    "When run with the same parameters as J() above, including the random seed,\n",
    "will go through the exact same sequence of trials and dynamics and responses.\n",
    "\n",
    "OBLIGATORY PARAMS:\n",
    "------------------\n",
    "\n",
    "params     The same obligatory vector as in run_dynamics()\n",
    "\n",
    "OPTIONAL PARAMS:\n",
    "----------------\n",
    "\n",
    "ntrials=10\n",
    "sigma=3.2\n",
    "random_seed=321\n",
    "\n",
    "RETURNS:\n",
    "--------\n",
    "\n",
    "pro_answers     A 2-by-ntrials vector. First row is final value of Pro R unit in \"pro\" trials; second row is final value of Pro L unit\n",
    "anti_answers    As pro_answers, but for anti trials\n",
    "\n",
    "\"\"\"\n",
    "function test_params2(targets; ntrials=10, sigma=3.2, random_seed=321, nderivs=0, derivorder=0, params...)\n",
    "    srand(random_seed)\n",
    "    cost = 0;\n",
    "    pro_answers  = ForwardDiffZeros(2, ntrials, nderivs=nderivs, derivorder=derivorder)\n",
    "    anti_answers = ForwardDiffZeros(2, ntrials, nderivs=nderivs, derivorder=derivorder)\n",
    "\n",
    "    cost      = 0;\n",
    "    pro_perf  = 0;\n",
    "    anti_perf = 0;\n",
    "    \n",
    "    for i in [1:ntrials;]\n",
    "        answer, t, U, V, W = run_dynamics2(\"pro\", do_plot=false, \n",
    "        nderivs=nderivs, derivorder=derivorder, sigma=sigma; params...) # , pfc_anti_input = 0.8, pfc_pro_input=0.8)\n",
    "        pro_perf += V[1,end] - V[3,end]        \n",
    "        cost += (V[1,end]- V[3,end])^2\n",
    "        pro_answers[1,i] = V[1,end]\n",
    "        pro_answers[2,i] = V[3,end]\n",
    "        \n",
    "        answer, t, U, V, W = run_dynamics2(\"anti\", do_plot=false, \n",
    "        nderivs=nderivs, derivorder=derivorder, sigma=sigma; params...) # , pfc_anti_input = 0.8, pfc_pro_input=0.8)\n",
    "        anti_perf += V[3,end] - V[1,end]        \n",
    "        cost += (V[1,end]- V[3,end])^2\n",
    "        anti_answers[1,i] = V[1,end]\n",
    "        anti_answers[2,i] = V[3,end]\n",
    "    end\n",
    "\n",
    "    RMSp = sqrt(sum((pro_answers[1,:]  - pro_answers[2,:]).^2) / ntrials)\n",
    "    RMSa = sqrt(sum((anti_answers[1,:] - anti_answers[2,:]).^2) / ntrials)\n",
    "\n",
    "    # @printf \"pro_perf = %f   ntrials=%d   targets[1]=%f \\n\" pro_perf ntrials targets[1]\n",
    "    cost1 = (pro_perf - ntrials*targets[1])^2 + (anti_perf - ntrials*targets[2])^2 \n",
    "    cost2 = - cost\n",
    "\n",
    "    return pro_answers, anti_answers, (RMSa+RMSp)/2, cost1, cost2\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function Jcost3(targets; ntrials=20, theta1=0.15, theta2=0.15, beta=0.5, verbose=false, params...)\n",
    "    pro_answers, anti_answers = test_params2(targets, ntrials=ntrials; params...)\n",
    "    \n",
    "    pro_out = 0.5*(1 + tanh.((pro_answers[1,:]  - pro_answers[2,:])/theta1))\n",
    "    ant_out = 0.5*(1 + tanh.((anti_answers[2,:] - anti_answers[1,:])/theta1))\n",
    "\n",
    "    pro_dif = tanh((pro_answers[1,:]  - pro_answers[2,:]) /theta2).^2\n",
    "    ant_dif = tanh((anti_answers[1,:] - anti_answers[2,:])/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(pro_out) - targets[1])^2 + (mean(ant_out) - targets[2])^2\n",
    "    cost2 = -mean(pro_dif) \n",
    "    cost2 -= mean(ant_dif)\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"                              cost1=%.3f, cost2=%.3f, pro_out=%.3f, anti_out=%.3f\\n\", convert(Float64, cost1),\n",
    "        beta*convert(Float64, cost2), convert(Float64, mean(pro_out)), convert(Float64, mean(ant_out)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + beta*cost2, pro_out, ant_out, pro_dif, ant_dif\n",
    "end\n",
    "\n",
    "\n",
    "# test_params2([0.8, 0.7], taking_derivative=true)\n",
    "# Jcost3([0.8, 0.7], taking_derivative=false)\n",
    "\n",
    "\n",
    "args = [\"vwi\", \"hwi\", \"const_pro_bias\", \"const_E\", \"pfc_anti_input\", \"pfc_pro_input\"]\n",
    "params = [36, 1.8, 0.854, 1, 0.05, 0.05]\n",
    "# args = [\"vwi\", \"hwi\", \"const_pro_bias\"]\n",
    "# params = [36, 1.8, 0.854]\n",
    "# args = [\"hwi\", \"vwi\", \"const_pro_bias\", \"const_E\"]\n",
    "# params = [1.8, 36, 0.854, 1]\n",
    "\n",
    "targets = [0.8, 0.7]\n",
    "\n",
    "figure(1); clf();\n",
    "run_dynamics2(\"pro\"; make_dict(args, params)..., do_plot=true)[4][end];\n",
    "# test_params2(targets)[1][1,1]\n",
    "\n",
    "srand(300)\n",
    "#ForwardDiff.hessian(x->run_dynamics2(\"anti\"; make_dict(args, x)..., nderivs=length(args),\n",
    "#    derivorder = 2, do_plot=true)[4][end], params)\n",
    "#ForwardDiff.hessian(x -> test_params2(targets; nderivs=length(args),\n",
    "#    derivorder=2, make_dict(args, x)...)[1][1,1], params)\n",
    "\n",
    "@time ForwardDiff.hessian(x -> Jcost3(targets; nderivs=length(args), derivorder=2, \n",
    "    make_dict(args, x)...)[1], params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i=1; dx = 0.001; new_params = copy(params); new_params[i] = params[i]+dx; srand(300); A = run_dynamics2(\"pro\"; nderivs=0, make_dict(args, new_params)...)[4][end]; srand(300); B= run_dynamics2(\"pro\"; nderivs=0, make_dict(args, params)...)[4][end]; (A-B)/dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i=4; dx = 0.0001; new_params = copy(params); new_params[i] = params[i]+dx; srand(300); A = test_params2(targets; make_dict(args, new_params)...)[1][1,1]; srand(300); B = test_params2(targets; make_dict(args, params)...)[1][1,1]; (A-B)/dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i=3; dx = 0.00001; new_params = copy(params); new_params[i] = params[i]+dx; srand(300); A = Jcost3(targets; make_dict(args, new_params)...)[1]; srand(300); B = Jcost3(targets; make_dict(args, params)...)[1]; (A-B)/dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# === RUN THE MINIMIZATION WITH CONSTRAINED HESSIAN MINIMIZATION ===\n",
    "#\n",
    "\n",
    "targets = [0.8, 0.7]  # Pro ant Anti desired per cent correct, respectively\n",
    "\n",
    "ntrials=20\n",
    "theta1=0.15\n",
    "theta2=0.2\n",
    "\n",
    "args = [\"vwi\", \"hwi\", \"const_pro_bias\", \"const_E\"]\n",
    "seed = [36, 1.8, 0.854, 1]\n",
    "args = [\"vwi\", \"hwi\", \"const_pro_bias\", \"const_E\", \"pfc_anti_input\", \"pfc_pro_input\"]\n",
    "seed = [36, 1.8, 0.854, 1, 1.6, 0.05]\n",
    "seed = [1, 1, 1, 1, 1.6, 0.05]\n",
    "\n",
    "# ForwardDiff.gradient(x -> Jcost3(targets; nderivs=length(args), ntrials=ntrials, theta1=theta1, theta2=theta2,\n",
    "#    beta=0.05, verbose=true, make_dict(args, x)...)[1], seed)\n",
    "\n",
    "# ForwardDiff.hessian(x->Jcost3(targets; nderivs=length(args), derivorder=2, \n",
    "#    ntrials=ntrials, theta1=theta1, theta2=theta2, beta=0.05, verbose=true, make_dict(args, x)...)[1], seed)\n",
    "\n",
    "# params = @time(constrained_Hessian_minimization(seed, \n",
    "# x -> Jcost3(targets; nderivs=length(args), derivorder=2, ntrials=ntrials, theta1=theta1, theta2=theta2, \n",
    "# beta=0.05, verbose=true, make_dict(args, [exp(x[1]), exp(x[2]), x[3], x[4], x[5], x[6]])...)[1], verbose=true, tol=1e-7, start_eta=2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "  \n",
    "\n",
    "        \n",
    "\n",
    "# ================ now test them\n",
    "\n",
    "function tester(;a=10, b=20, c=30, nderivs=0, difforder=0)\n",
    "    M = ForwardDiffZeros(3, 3; nderivs=nderivs, difforder=difforder)\n",
    "    M[1,1] = a^2*10\n",
    "    M[2,2] = b^3*20\n",
    "    M[3,3] = a*sqrt(c)*30.1\n",
    "    return trace(M)\n",
    "end\n",
    "\n",
    "res1 = keyword_gradient((;pars...) -> tester(;pars...), [\"a\", \"b\", \"c\"], [10, 20, 3.1])\n",
    "\n",
    "out = DiffBase.GradientResult([10, 20, 30.1])\n",
    "keyword_gradient!(out, (;pars...) -> tester(;pars...), [\"a\", \"b\", \"c\"], [10, 20, 3.1])\n",
    "res2 = DiffBase.gradient(out)\n",
    "\n",
    "println(res1)\n",
    "println(res2)\n",
    "\n",
    "res3 = keyword_hessian((;pars...) -> tester(;pars...), [\"a\", \"c\"], [1.0, 2.0])\n",
    "out = DiffBase.HessianResult([10, 20.1])\n",
    "keyword_hessian!(out, (;pars...) -> tester(;pars...), [\"a\", \"c\"], [1.0, 2.0])\n",
    "res4 = DiffBase.hessian(out)\n",
    "\n",
    "println(res3)\n",
    "println(res4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition tester() in module Main at In[6]:2 overwritten at In[7]:2.\n",
      "WARNING: Method definition #tester(Array{Any, 1}, Main.#tester) in module Main overwritten.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3×3 Array{Float64,2}:\n",
       " 20.0        0.0    8.54783\n",
       "  0.0      240.0    0.0    \n",
       "  8.54783    0.0  -13.7868 "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function tester(;a=10, b=20, c=30, nderivs=0, difforder=0)\n",
    "    M = ForwardDiffZeros(3, 3; nderivs=nderivs, difforder=difforder)\n",
    "    M[1,1] = a^2*10\n",
    "    M[2,2] = b^3*20\n",
    "    M[3,3] = a*sqrt(c)*30.1\n",
    "    return trace(M)\n",
    " end\n",
    "\n",
    "hess_b_c = keyword_hessian((;pars...) -> tester(;pars...), [\"b\", \"c\"], [10, 3.1])  # note initial values must be floats\n",
    "\n",
    "\n",
    "hess_a_b_c = keyword_hessian((;pars...) -> tester(;pars...), [\"a\", \"b\", \"c\"], [10, 2, 3.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Array{Float64,2}:\n",
       " 20.0    10.642  \n",
       " 10.642  -2.66049"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_hessian((;pars...)->tester(;pars...), [\"a\", \"c\"], [1.1, 2.2])\n",
    "\n",
    "out = DiffBase.HessianResult([10, 20.1])\n",
    "keyword_hessian!(out, (;pars...) -> tester(;pars...), [\"a\", \"c\"], [1.0, 2.0])\n",
    "DiffBase.hessian(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "function response, t, U, V, W = run_dynamics2(trial_type)\n",
    "\n",
    "    Runs the 4-way mutual inhibition model\n",
    "    \n",
    "    OBLIGATORY PARAMS:\n",
    "    ------------------\n",
    "\n",
    "        trial_type    Must be either \"pro\" or \"anti\"\n",
    "\n",
    "    OPTIONAL PARAMS:\n",
    "    ----------------\n",
    "    \n",
    "        vwi = 36        vertical inhibitory weights between Pro and Anti units. \n",
    "                          \"36\" means a -36 connection.\n",
    "        hwi=1.8         horizontal inhibitory weights between like units on the two sides. \n",
    "        const_pro_bias=0.854   Extra positive input to the two Pro units\n",
    "        const_E=1              Constant positive input to all four units\n",
    "        U_rest = -1     Resting point for U in the absance of other inputs\n",
    "        g_leak = 0.5    Mutliplies (U_rest - U) for the dynamics\n",
    "        theta = 1       Threshold on U for sigmoidal transform from U to V \n",
    "        beta  = 1       Scaling on sigmoid going from U to V:   V = 0.5*tanh((U-theta)/beta) + 0.5\n",
    "        dt=0.02         Timestep\n",
    "        sigma=0.1,      added standard deviation on U per unit time\n",
    "        rule_period = 0.5       in seconds\n",
    "        delay_period = 0.5      in seconds. Opto will happen during this period\n",
    "        target_period = 0.1     right_light_input and target_extra_E will happen during this period; \n",
    "                                Pro v Anti input will be turned off; \n",
    "        post_target_period = 0.5  target_extra_E will still be on, but right_side_input won't\n",
    "        tau=0.1         Time constant of dynamics, in secs\n",
    "        marino_tau = False   If True, tau applies only to leak term; if False, applies to entire dUdt\n",
    "        start_U = [-7, -7, -7, -7]\n",
    "        const_E = 0.15  Constant excitation added to all units\n",
    "        right_light_input=1     Extra excitation to right side of the brain units during the target period\n",
    "        right_light_pro_extra   Even further excitation added to pro Right side units during target\n",
    "        vwi = 1.5       Weight between ProContra and AntiIpsi units (on each side of the brain)\n",
    "        hwi = 1.5       Weight between ProContra units across the brain; also between AntiContra\n",
    "        pro_self_ex = 0   Self excitation weight of Pro units\n",
    "        anti_self_ex = 0  Self excitation weight of Anti units\n",
    "        pfc_anti_input = 0.05    Input to Anti units during rule and delay periods\n",
    "        pfc_pro_input = np.nan   Input to Pro units during rule and delay periods (default means same as Anti)\n",
    "        const_pro_bias = 0       A constant extra input to the Pro units\n",
    "        target_extra_E = 0.25    Extra excitation added to all units during target and post_target periods\n",
    "        opto='off'    Whether there is optogenetic-induced scaling of outputs.\n",
    "            ='on'     Opto will be done during the delay period only\n",
    "            ='dt'     Opto will be done during the delay plus the target period\n",
    "           opto_scaling=0.5      Factor by which to scale the weight matrix during opto \n",
    "           opto_scale_on_E=1     Factor by which to scale the constant excitation during opto\n",
    "           opto_conductance=0    How much conductance to add to gleak during opto\n",
    "           opto_current=0        Added to dUdt at each time step during opto\n",
    "        unilateral_opto = False If True, then opto_scale_on_E will be forced to 0, and opto_scaling will apply\n",
    "                                to only one side\n",
    "        do_plot = True  whether or not to plot the results\n",
    "        fignum=1        figure on which to plot\n",
    "        decision_threshold   If |V(Pro_R) - V(Pro_L)| >= this number, a proper answer is produced. \n",
    "                The target light is presented to the right, so Right means \"pro\"\n",
    "        taking_derivative = true    If true, will return ForwardDiff.Dual{, Float64} types; otherwise Float64 types\n",
    "\n",
    "\n",
    "    RETURNS:\n",
    "    --------\n",
    "\n",
    "        response     +1 for Pro, -1 for Anti, 0 for undefined \n",
    "                        if |V(Pro_R) - V(Pro_L)| < decision_threshold\n",
    "        t    Time vector\n",
    "        U   U matrix, size 4-by-len(t). Order is ProContra on right side, AntiIpsi on right\n",
    "                    side, ProContra on left side, AntiIpsi on left.\n",
    "        V   V matrix = 0.5*np.tanh((U-theta)/beta) + 0.5\n",
    "        W   Weight matrix between units\n",
    "        \n",
    "\"\"\"\n",
    "function run_dynamics2(trial_type  ; opto=\"off\", opto_scaling=0.8, opto_scale_on_E=1,\n",
    "    vwi = 36, hwi = 1.8, const_pro_bias = 0.854, const_E = 1,\n",
    "    opto_conductance = 0, opto_current=0, \n",
    "    right_light_pro_extra = 0, right_light_input=12, \n",
    "    pro_self_ex = 0, anti_self_ex = 0, \n",
    "    tau=4.4, marino_tau = true, dt=0.05, target_extra_E = 0,\n",
    "    pfc_anti_input = 1.6, pfc_pro_input = 0.05,\n",
    "    sigma=3.2, start_U = [-25, -25, -25, -25], do_plot = false, fignum=1, \n",
    "    g_leak = 1, U_rest = 0, theta = 5, beta = 50,\n",
    "    rule_period = 200, delay_period = 200, target_period = 50,\n",
    "    post_target_period = 0.01, decision_threshold = 0.3, nderivs=0, derivorder=0)\n",
    "    \n",
    "    t = [0 : dt : rule_period + delay_period + target_period + post_target_period;] \n",
    "\n",
    "    V = ForwardDiffZeros(4, length(t), nderivs=nderivs, derivorder=derivorder)   # the element type is for ForwardDiff obviously\n",
    "    U = ForwardDiffZeros(4, length(t), nderivs=nderivs, derivorder=derivorder)\n",
    "\n",
    "    U[:,1] = start_U\n",
    "\n",
    "    W = [pro_self_ex -vwi -hwi 0; -vwi anti_self_ex 0 -hwi; \n",
    "        -hwi 0 pro_self_ex -vwi; 0 -hwi -vwi anti_self_ex]\n",
    "\n",
    "    E = const_E\n",
    "    \n",
    "    for i in [2:length(t);]  # the funny semicolon appears to be necessary in Julia\n",
    "        if marino_tau\n",
    "            dUdt = E + W * V[:,i-1] + g_leak*(U_rest - U[:,i-1])/tau\n",
    "        else\n",
    "            dUdt = E + W * V[:,i-1] + g_leak*(U_rest - U[:,i-1])\n",
    "        end\n",
    "    \n",
    "        if t[i] < rule_period + delay_period\n",
    "            if trial_type==\"anti\"\n",
    "                dUdt[[2,4]] += pfc_anti_input\n",
    "            elseif trial_type == \"pro\"\n",
    "                dUdt[[1,3]] += pfc_pro_input\n",
    "            else\n",
    "            end\n",
    "            \n",
    "        elseif t[i] < rule_period + delay_period + target_period\n",
    "            dUdt[[1,2]] += right_light_input\n",
    "            dUdt[1]     += right_light_pro_extra\n",
    "            dUdt        += target_extra_E\n",
    "        else\n",
    "            dUdt        += target_extra_E\n",
    "        end\n",
    "    \n",
    "        dUdt[[1,3]] += const_pro_bias\n",
    "        \n",
    "        if marino_tau\n",
    "            try\n",
    "                U[:,i] = U[:,i-1] +       dt*dUdt + sigma*randn(4)*sqrt(dt)\n",
    "            catch\n",
    "                @printf \"yep\\n\"\n",
    "            end\n",
    "        else\n",
    "            U[:,i] = U[:,i-1] + (dt/tau)*dUdt + sigma*randn(4)*sqrt(dt)\n",
    "        end\n",
    "    \n",
    "        V[:,i] = 0.5*tanh((U[:,i]-theta)/beta) + 0.5\n",
    "    end    \n",
    "\n",
    "    if do_plot\n",
    "        figure(fignum); \n",
    "        subplot(3,1,1)\n",
    "        h = plot(t, V'); \n",
    "        setp(h[1], color=[0, 0, 1])\n",
    "        setp(h[2], color=[1, 0, 0])\n",
    "        setp(h[3], color=[1, 0.5, 0.5])\n",
    "        setp(h[4], color=[0, 1, 1])\n",
    "        ylabel(\"V\")\n",
    "\n",
    "        ax = gca()\n",
    "        yl = [ylim()[1], ylim()[2]]\n",
    "        vlines([rule_period, rule_period+delay_period, \n",
    "            rule_period+delay_period+target_period], \n",
    "            -0.05, 1.05, linewidth=2)\n",
    "        if yl[1]<0.02\n",
    "            yl[1] = -0.02\n",
    "        end\n",
    "        if yl[2]>0.98\n",
    "            yl[2] = 1.02\n",
    "        end\n",
    "        ylim(yl)\n",
    "        grid(true)\n",
    "        \n",
    "        subplot(3,1,2)\n",
    "        hu = plot(t, U')\n",
    "        setp(hu[1], color=[0, 0, 1])\n",
    "        setp(hu[2], color=[1, 0, 0])\n",
    "        setp(hu[3], color=[1, 0.5, 0.5])\n",
    "        setp(hu[4], color=[0, 1, 1])\n",
    "        ylabel(\"U\"); ylim(-100, 100)\n",
    "        vlines([rule_period, rule_period+delay_period, \n",
    "            rule_period+delay_period+target_period], \n",
    "            ylim()[1], ylim()[2], linewidth=2)\n",
    "\n",
    "        grid(true)\n",
    "    \n",
    "        subplot(3,1,3)\n",
    "        hr = plot(t, V[1,:] - V[3,:])\n",
    "        ylim([-0.9, 0.9])\n",
    "        vlines([rule_period, rule_period+delay_period, \n",
    "            rule_period+delay_period+target_period], \n",
    "            ylim()[1], ylim()[2], linewidth=2)\n",
    "        xlabel(\"t\"); ylabel(\"Pro R - Pro L\")\n",
    "        grid(true)\n",
    "        \n",
    "    end\n",
    "    \n",
    "#    if V[1,end] - V[3,end] > decision_threshold\n",
    "#        answer = 1\n",
    "#    elseif V[1,end] - V[3,end] < -decision_threshold\n",
    "#        answer = -1\n",
    "#    else\n",
    "#        answer = 0\n",
    "#    end\n",
    "\n",
    "    answer1 = 0.5 + 0.5*tanh(((V[1,end] - V[3,end]) - decision_threshold)/0.1)\n",
    "    answer2 = 0.5 + 0.5*tanh(((V[3,end] - V[1,end]) - decision_threshold)/0.1)\n",
    "    answer  = answer1 - answer2\n",
    "    \n",
    "    return answer, t, U, V, W \n",
    "end\n",
    "\n",
    "\n",
    "figure(1); clf();\n",
    "run_dynamics2(\"pro\", do_plot=true) # , nderivs=2, derivorder=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD -- stuff for testing the constrained Hessian minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = [18, 1.8, 0.854, 1]\n",
    "func = x -> Jcost2(x, targets, ntrials=ntrials, theta1=theta1, theta2=theta2, beta=0.05, verbose=true)[1]\n",
    "\n",
    "out = DiffBase.HessianResult(params)\n",
    "ForwardDiff.hessian!(out, func, params)\n",
    "cost = DiffBase.value(out)\n",
    "grad = DiffBase.gradient(out)\n",
    "hess = DiffBase.hessian(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grad''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    chessdelta = zeros(size(params))\n",
    "        hessdelta  = - inv(hess)*grad\n",
    "\n",
    "chessdelta = constrained_parabolic_minimization(hess, grad'', 2)[1] # , doplot=true, lambdastepsize=0.01, efactor=3)[1]\n",
    "norm(hessdelta)\n",
    "\n",
    "new_params = params + chessdelta\n",
    "            ForwardDiff.hessian!(out, func, new_params)\n",
    "            new_cost = DiffBase.value(out)\n",
    "            new_grad = DiffBase.gradient(out)\n",
    "            new_hess = DiffBase.hessian(out)\n",
    "\n",
    "println(params)\n",
    "println(chessdelta)\n",
    "println(new_params)\n",
    "ylim(-0.0001, 0.001); grid();\n",
    "[cost new_cost]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "println(dot(chessdelta,grad)/(norm(chessdelta)*norm(grad)))\n",
    "println(chessdelta)\n",
    "println(grad)\n",
    "ylim(-0.1, 10)\n",
    "diff(sort(eig(hess)[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure(2);\n",
    "clf();\n",
    "theta = 0.4\n",
    "ant_out = (0.5*(1 + tanh.((antans[2,:] - antans[1,:])/theta)))\n",
    "subplot(2,1,1)\n",
    "plot(antans[2,:] - antans[1,:], \"b.\"); grid()\n",
    "\n",
    "subplot(2,1,2)\n",
    "plot(ant_out, \"b.\")\n",
    "\n",
    "\n",
    "sqrt(var(antans[2,:] - antans[1,:]))\n",
    "grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD SANDLOT FROM HERE ON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beginning to test gradient descent\n",
    "\n",
    "It's kind of working?  Two issues:\n",
    "\n",
    "(a) I think we're trapped in the final attractor values-- maybe time to explore adding reaction time, or not computing unit values so late in the trial? Actually, on further inspection, it is really asking for something like 80% performance without having defined outputs as hit=1, miss=0.  Really need the sigmoid.\n",
    "\n",
    "(b) Right now I only know how to differentiate J when J computes a single scalar.  But sometimes we want to stash some values as we go. Don't yet know how to do that.\n",
    "\n",
    "(c) Would also be nice to save values in a file or something while the gradient descent search is occurring, so as to have a trace of what happened, for later debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# trial_type = \"pro\"\n",
    "# params = [36, 1.8, 0.854, 1]\n",
    "# params = [36, 0.3, 0.254, 1]\n",
    "targets = [0.8, 0.7]   # Fraction correct in Pro and Anti\n",
    "\n",
    "ntrials = 10\n",
    "# eta = 0.001;\n",
    "\n",
    "# # This is close to Marino's params. Not much changes.\n",
    "# params = [36.006098, 1.655863, 0.568341, 1.019982]; \n",
    "# eta = 0.000221\n",
    "# params =[36.023674, 1.694372, 0.619004, 1.092042]; \n",
    "# eta = 0.00001\n",
    "\n",
    "# # Starting from the one immediately below, which produces all-Pro responses,\n",
    "# # the algorithm seems to be working!  Certainly the cost goes down, and we\n",
    "# # get to 6 correct Pro, 8 correct Anti (after having started at [10,0]).\n",
    "# params =[18.023674, 0.894372, 0.619004, 1.092042]; \n",
    "# eta = 0.0001\n",
    "# # The next few are a few stops along the way as the algorithm went on. I halted it,\n",
    "# # but it was still going and still getting better.\n",
    "# params = [23.211391, 3.799635, -0.432648, 4.131169]\n",
    "# params = [24.649633, 3.440672, -0.157887, 3.355422]\n",
    "# eta = 0.013\n",
    "# params = [26.855623, 3.449642, -0.222252, 2.867700]\n",
    "\n",
    "\n",
    "eta = 0.004\n",
    "params = [34.296820, 2.514509, 0.535576, 1.585929]\n",
    "\n",
    "# 22050: eta=0.013335, cost=1.64289, [P,A : D : c1,c2]=[9,9 : 0.584 : 23.685, -7.256], \n",
    "# params=[36.384866, 2.597524, 0.570819, 0.942388]\n",
    "\n",
    "# --------------\n",
    "\n",
    "params = [34.296820, 2.514509, 0.535576, 1.585929] \n",
    "eta = 0.1\n",
    "\n",
    "# --------------\n",
    "\n",
    "out = DiffBase.GradientResult(params)\n",
    "ForwardDiff.gradient!(out, x -> J(x, targets, ntrials=ntrials), params)\n",
    "cost = DiffBase.value(out)\n",
    "grad = DiffBase.gradient(out)\n",
    "\n",
    "i=0; while eta > 1e-6\n",
    "\n",
    "    i=i+1\n",
    "    new_params = params - eta*grad\n",
    "    \n",
    "    ForwardDiff.gradient!(out, x -> J(x, targets, ntrials=ntrials), new_params)\n",
    "    new_cost = DiffBase.value(out)\n",
    "    new_grad = DiffBase.gradient(out)\n",
    "    \n",
    "    if new_cost < cost\n",
    "        params = new_params\n",
    "        cost   = new_cost\n",
    "        grad   = new_grad\n",
    "        eta = eta*1.1\n",
    "    else\n",
    "        eta = eta/2\n",
    "    end\n",
    "    \n",
    "    pro_answers, anti_answers, D, c1, c2 = test_params(params, targets, ntrials=ntrials)\n",
    "    P = sum((sign(pro_answers[1,:]  - pro_answers[2,:])+1)/2)\n",
    "    A = sum((sign(anti_answers[2,:] - anti_answers[1,:])+1)/2)\n",
    "    \n",
    "    # P is number of times Pro  trials had Pro_R > Pro_L  (i.e., were correct). If this is equal to ntrials*target[1], we're golden\n",
    "    # A is number of times Anti trials had Pro_R < Pro_L  (i.e., were correct). If this is equal to ntrials*target[2], we're golden\n",
    "    # D is the root mean square separation between Pro_R and Pro_L, across all trials.  Large D is good.\n",
    "    \n",
    "    if rem(i, 1)==0\n",
    "        @printf \"%d: eta=%f, cost=%.5f, [P,A : D : c1,c2]=[%d,%d : %.3f : %.3f, %.3f], params=[%.3f, %.3f, %.3f, %.3f]\\n\" i eta cost P A D c1 c2 params[1] params[2] params[3] params[4]\n",
    "    end\n",
    "end\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  Plot out some final values to see what things are looking like\n",
    "\n",
    "new_params = [26.855623, 3.449642, -0.222252, 2.867700]\n",
    "pro_answers2, anti_answers2 = test_params(new_params, ntrials=10)\n",
    "\n",
    "figure(3); clf()\n",
    "subplot(2,1,1)\n",
    "plot(pro_answers2[1,:], \"b.\", pro_answers2[2,:], \"r.\")\n",
    "title(\"PRO\")\n",
    "\n",
    "subplot(2,1,2)\n",
    "plot(anti_answers2[1,:], \"b.\", anti_answers2[2,:], \"r.\")\n",
    "title(\"ANTI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_params =[23.211391, 3.799635, -0.432648, 4.131169]; new_eta = 0.000221\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "function plot_many(params; ntrials=5, random_seed = 321, sigma=3.2, target_period=50, start_plotting_at=5)\n",
    "\n",
    "Plot full dynamics of some example trials, using same random seed and trial order as was used during the minimization\n",
    "\"\"\"\n",
    "function plot_many(params; ntrials=5, random_seed = 321, sigma=3.2, target_period=50, start_plotting_at=5)\n",
    "    srand(random_seed)    \n",
    "    pro_perf  = 0;\n",
    "    anti_perf = 0;\n",
    "    \n",
    "    for i in [1:start_plotting_at-1;]        \n",
    "        answer, t, U, V, W = run_dynamics(\"pro\", params, do_plot=true, fignum=2, \n",
    "            sigma=sigma, target_period=target_period) # , pfc_anti_input = 0.8, pfc_pro_input=0.8)\n",
    "        pro_perf += V[1,end] - V[3,end]        \n",
    "\n",
    "        answer, t, U, V, W = run_dynamics(\"anti\", params, do_plot=true, fignum=3, \n",
    "            sigma=sigma, target_period=target_period) # , pfc_anti_input = 0.8, pfc_pro_input=0.8)\n",
    "        anti_perf += V[3,end] - V[1,end]        \n",
    "    end\n",
    "\n",
    "    figure(2); clf();\n",
    "    figure(3); clf();\n",
    "\n",
    "    for i in [1:ntrials-start_plotting_at+1;]        \n",
    "        answer, t, U, V, W = run_dynamics(\"pro\", params, do_plot=true, fignum=2, \n",
    "            sigma=sigma, target_period=target_period) # , pfc_anti_input = 0.8, pfc_pro_input=0.8)\n",
    "        pro_perf += V[1,end] - V[3,end]        \n",
    "\n",
    "        answer, t, U, V, W = run_dynamics(\"anti\", params, do_plot=true, fignum=3, \n",
    "            sigma=sigma, target_period=target_period) # , pfc_anti_input = 0.8, pfc_pro_input=0.8)\n",
    "        anti_perf += V[3,end] - V[1,end]        \n",
    "    end\n",
    "    \n",
    "    @printf \"pro_perf=%.2f  anti_perf=%.2f\"  pro_perf  anti_perf\n",
    "    return pro_perf, anti_perf\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = [34.296820, 2.514509, 0.535576, 1.585929]\n",
    "# params = [36, 1.8, 0.854, 1]\n",
    "params=[36.384866, 2.597524, 0.570819, 0.942388]\n",
    "\n",
    "plot_many(params, start_plotting_at=1,sigma=0.8, ntrials=10)\n",
    "\n",
    "pa, aa, rms, c1, c2 = test_params(params, targets, ntrials=10)\n",
    "\n",
    "println(c1)\n",
    "println(c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wondering about playing with using the Hessian for faster searching..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From here on, various pieces of trash:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This doesn't work, has no effect\n",
    "\n",
    "@pyimport matplotlib as mpl\n",
    "\n",
    "mpl.rcParams[\"font.size\"] = 32\n",
    "mpl.rcParams[\"font.family\"] = \"Arial\"\n",
    "mpl.rcParams[\"lines.linewidth\"] = 1.5\n",
    "mpl.rcParams[\"lines.markersize\"] = 8"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.5.2",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
