{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">TABLE OF CONTENTS</h1>\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "**Updates to the table of contents are periodic, but run the cell below to first start or force an update.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://sites.google.com/site/brodylabhome/files/make_table_of_contents.js')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition @javascript_str(ANY<:Any) in module Main at In[850]:1 overwritten at In[863]:1.\n"
     ]
    }
   ],
   "source": [
    "macro javascript_str(s) display(\"text/javascript\", s); end\n",
    "\n",
    "javascript\"\"\"\n",
    "$.getScript('https://sites.google.com/site/brodylabhome/files/make_table_of_contents.js')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "@javascript_str (macro with 1 method)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We define functions to convert Duals, the variable types used by ForwardDiff, \\nto Floats. This is useful if we want to print out the value of a variable \\n(since print doesn't know how to Duals). Note that after being converted to a Float, no\\ndifferentiation by ForwardDiff can happen!  e.g. after\\n    x = convert(Float64, y)\\nForwardDiff can still differentiate y, but it can't differentiate x\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "using PyCall\n",
    "using PyPlot\n",
    "using ForwardDiff\n",
    "using DiffBase\n",
    "\n",
    "pygui(true)\n",
    "\n",
    "import Base.convert\n",
    "convert(::Type{Float64}, x::ForwardDiff.Dual) = Float64(x.value)\n",
    "function convert(::Array{Float64}, x::Array{ForwardDiff.Dual}) \n",
    "    y = zeros(size(x)); \n",
    "    for i in 1:prod(size(x)) \n",
    "        y[i] = convert(Float64, x[i]) \n",
    "    end\n",
    "    return y\n",
    "end\n",
    "\n",
    "include(\"general_utils.jl\")\n",
    "include(\"hessian_utils.jl\")\n",
    "\n",
    "\"\"\"\n",
    "We define functions to convert Duals, the variable types used by ForwardDiff, \n",
    "to Floats. This is useful if we want to print out the value of a variable \n",
    "(since print doesn't know how to Duals). Note that after being converted to a Float, no\n",
    "differentiation by ForwardDiff can happen!  e.g. after\n",
    "    x = convert(Float64, y)\n",
    "ForwardDiff can still differentiate y, but it can't differentiate x\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup -- definitions of forwardModel() and backwardsModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition g(Any) in module Main at In[326]:5 overwritten at In[613]:5.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'g :: Tuple{Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition ginverse(Any) in module Main at In[326]:12 overwritten at In[613]:12.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'ginverse :: Tuple{Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition forwardModel(Any) in module Main at In[326]:74 overwritten at In[613]:74.\n",
      "WARNING: Method definition #forwardModel(Array{Any, 1}, Main.#forwardModel, Any) in module Main overwritten.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'forwardModel :: Tuple{Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition backwardsModel(Any) in module Main at In[326]:212 overwritten at In[613]:212.\n",
      "WARNING: Method definition #backwardsModel(Array{Any, 1}, Main.#backwardsModel, Any) in module Main overwritten.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'backwardsModel :: Tuple{Any}' in module 'Main'.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "backwardsModel"
      ]
     },
     "execution_count": 613,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "o = g(z)    squashing tanh function, running from 0 to 1, is equal to 0.5 when input is 0.\n",
    "\"\"\"\n",
    "function g(z)\n",
    "    return 0.5*tanh.(z)+0.5\n",
    "end\n",
    "    \n",
    "\"\"\"\n",
    "z = g^-1(o)    inverse of squashing tanh function, input must be in (0, 1), output is zero when passed 0.5.\n",
    "\"\"\"\n",
    "function ginverse(z)\n",
    "    return 0.5*log.(z./(1-z))\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "forwardModel(startU; dt=0.01, tau=0.1, nsteps=100, input=[0.1, 0], noise=[], W=[0 -5;-5 0], \n",
    "    init_add=0, const_add=0, sigma=0, gleak=1, U_rest=0, \n",
    "    do_plot=false, nderivs=0, difforder=0, clearfig=true, fignum=1, dUdt_mag_only=false)\n",
    "\n",
    "Runs a tanh() style-network forwards in time, given its starting point, using simple Euler integration\n",
    "    tau dU/dt = -U + W*V + I\n",
    "    V = 0.5*tanh(U)+ 0.5\n",
    "\n",
    "**PARAMETERS:**\n",
    "\n",
    "startU     A column vector, nunits-by-1, indicating the values of U at time zero\n",
    "\n",
    "\n",
    "**OPTIONAL PARAMETERS**\n",
    "\n",
    "dt      Scalar, timestep size\n",
    "\n",
    "tau     Scalar, in seconds\n",
    "\n",
    "gleak   \n",
    "        dUdt will have a term equal to gleak*(U_rest - U)\n",
    "U_rest\n",
    "\n",
    "nsteps  Number of timesteps to run, including time=0.\n",
    "\n",
    "input   Either an nunits-by-1 vector, in which case inputs to each unit are constant\n",
    "        across time, or a matrix, nunits-by-nsteps, indicating input for each unit at each timepoint.\n",
    "\n",
    "W       Weight matrix, nunits-by-nunits\n",
    "\n",
    "init_add    Vector or scalar that gets added to U at very first timestep, U[:,1]\n",
    "\n",
    "const_add   Scalar that gets added to U after every timestep\n",
    "\n",
    "sigma       After each timestep, add sigma*sqrt(dt)*randn() to each element of U\n",
    "\n",
    "do_plot   Default false, if true, plots V of up to the first two dimensions\n",
    "\n",
    "fignum     Figure number on which to plot\n",
    "\n",
    "clrearfig  If true, the figure is first cleared, otherwise any plot ois overlaid\n",
    "\n",
    "nderivs, difforder     Required for making sure function can create its own arrays and \n",
    "                       still be differentiated\n",
    "\n",
    "dUdt_mag_only  If true, returns |dUdt|^2 from the first timestep only, then stops.\n",
    "\n",
    "** RETURNS:**\n",
    "\n",
    "Uend Vend       nunits-by-1 vectors representing the final values of U and V that were found.\n",
    "U, V            nunits-by-nsteps matrices containing the full trajectories\n",
    "\n",
    "\"\"\"\n",
    "function forwardModel(startU; dt=0.01, tau=0.1, nsteps=100, input=[], noise=[], W=[0 -5;-5 0], \n",
    "    init_add=0, const_add=0, do_plot=false, nderivs=0, difforder=0, clearfig=true, fignum=1,\n",
    "    dUdt_mag_only=false, sigma=0, g_leak=1, U_rest=0, theta=0, beta=1, other_unused_params...)\n",
    "\n",
    "    my_input = ForwardDiffZeros(size(input,1), size(input,2), nderivs=nderivs, difforder=difforder)\n",
    "    for i=1:prod(size(input)); my_input[i] = input[i]; end\n",
    "    input = my_input;\n",
    "    \n",
    "    nunits = length(startU)\n",
    "    if size(startU,2) > size(startU,1)\n",
    "        error(\"startU must be a column vector\")\n",
    "    end\n",
    "    \n",
    "    # --- formatting input ---\n",
    "    if ~(typeof(input)<:Array) || prod(size(input))==1  # was a scalar\n",
    "        input = input[1]*(1+ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    elseif length(input)==0 # was the empty matrix\n",
    "        input = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    elseif size(input,2)==1     # was a column vector\n",
    "        input = input*(1+ForwardDiffZeros(1, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    end    \n",
    "    # --- formatting noise ---\n",
    "    if ~(typeof(noise)<:Array) || prod(size(noise))==1  # was a scalar\n",
    "        noise = noise*(1+ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    elseif length(noise)==0 # was the empty matrix\n",
    "        noise = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    elseif size(noise,2)==1     # was a column vector\n",
    "        noise = noise*(1+ForwardDiffZeros(1, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    end    \n",
    "    \n",
    "    U = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    V = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    \n",
    "    if ~(typeof(W)<:Array); W = [W]; end\n",
    "\n",
    "    W     = reshape(W, nunits, nunits)\n",
    "    U     = reshape(U, nunits, nsteps)\n",
    "    V     = reshape(V, nunits, nsteps)\n",
    "    input = reshape(input, nunits, nsteps)\n",
    "    noise = reshape(noise, nunits, nsteps)\n",
    "\n",
    "    input[:,1] += init_add\n",
    "    input      += const_add\n",
    "\n",
    "    #@printf(\"size(U) is (%d,%d), and size(startU) is (%d,%d) and size(noise) is (%d,%d)\", \n",
    "    #    size(U,1), size(U,2), size(startU,1), size(startU,2), size(noise,1), size(noise,2))\n",
    "    # @printf(\"U[1]=%g, noise[1]=%g\\n\", startU, noise[1])\n",
    "    U[:,1] = startU + noise[:,1]; # @printf(\"Resulting U=%g\\n\", U[1])\n",
    "    V[:,1] = g((U[:,1]-theta)/beta); # @printf(\"Resulting V=%g\\n\", V[1])\n",
    "    \n",
    "    for i=2:nsteps\n",
    "        dUdt = g_leak*(U_rest -U[:,i-1]) + W*V[:,i-1] + input[:,i-1]\n",
    "        if dUdt_mag_only; return sum(dUdt.*dUdt); end;\n",
    "        # @printf(\"dUdt=%g\\n\", dUdt[1])\n",
    "        # @printf(\"i=%g\\n\", i)\n",
    "        # @printf(\"noise[2]=%g\\n\", noise[2])\n",
    "        U[:,i] = U[:,i-1] + (dt/tau)*dUdt + noise[:,i] + sigma*sqrt(dt)*randn(size(U,1),1)\n",
    "        # @printf(\"Resulting U[2]=%g\\n\", U[2])\n",
    "        V[:,i] = g((U[:,i]-theta)/beta)\n",
    "        # @printf(\"Resulting V[2]=%g\\n\", V[2])\n",
    "    end\n",
    "\n",
    "    if do_plot\n",
    "        figure(fignum)\n",
    "        if length(startU)==1\n",
    "            if clearfig; clf(); end;\n",
    "            t = (0:nsteps-1)*dt\n",
    "            plot(t, V[1,:], \"b-\")\n",
    "            plot(t[1], V[1,1], \"g.\")\n",
    "            plot(t[end], V[1,end], \"r.\")\n",
    "            xlabel(\"t\"); ylabel(\"V1\"); ylim([-0.01, 1.01])\n",
    "        elseif length(startU)>=2\n",
    "            if clearfig; clf(); end;\n",
    "            plot(V[1,:], V[2,:], \"b-\")\n",
    "            plot(V[1,1], V[2,1], \"g.\")\n",
    "            plot(V[1,end], V[2,end], \"r.\")\n",
    "            xlabel(\"V1\"); ylabel(\"V2\"); \n",
    "            xlim([-0.01, 1.01]); ylim([-0.01, 1.01])\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return U[:,end], V[:,end], U, V\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "backwardsModel(endU; dt=0.01, tau=0.1, nsteps=100, input=[0],noise=[],  W=[0 -5;-5 0], \n",
    "    do_plot=false, nderivs=0, difforder=0, clearfig=true, fignum=1, tol=1e-15, start_eta=10)\n",
    "\n",
    "Runs a tanh() style-network BACKWARDS in time, given its ending point, by making a backwards\n",
    "guess at each timepoint and then using Hessian minimization to find the backwards vector that correctly\n",
    "leads to the current timestep value.  Uses forwardModel() . The forwards equations are:\n",
    "\n",
    "    tau dU/dt = -U + W*V + I\n",
    "    V = 0.5*tanh(U)+ 0.5\n",
    "\n",
    "**PARAMETERS:**\n",
    "\n",
    "endU     A column vector, nunits-by-1, indicating the values of U at time=end\n",
    "\n",
    "\n",
    "**OPTIONAL PARAMETERS:**\n",
    "\n",
    "dt      Scalar, timestep size\n",
    "\n",
    "tau     Scalar, in seconds\n",
    "\n",
    "nsteps  Number of timesteps to run, including time=0.\n",
    "\n",
    "input   Either an nunits-by-1 vector, in which case inputs to each unit are constant\n",
    "        across time, or a matrix, nunits-by-nsteps, indicating input for each unit at each timepoint.\n",
    "\n",
    "W       Weight matrix, nunits-by-nunits\n",
    "\n",
    "do_plot   Default false, if true, plots V of up to the first two dimensions\n",
    "\n",
    "tol       Tolerance in the minimization procedure for finding each backwards timestep. Passed on\n",
    "          to trust_region_Hessian_minimization()\n",
    "\n",
    "start_eta   Passed on to trust_region_Hessian_minimization()\n",
    "\n",
    "fignum     Figure number on which to plot\n",
    "\n",
    "clrearfig  If true, the figure is first cleared, otherwise any plot ois overlaid\n",
    "\n",
    "nderivs, difforder     Required for making sure function can create its own arrays and \n",
    "                       still be differentiated\n",
    "\n",
    "\n",
    "\n",
    "** RETURNS:**\n",
    "\n",
    "Ustart Vstart   nunits-by-1 vectors representing the starting values of U and V that were found.\n",
    "U, V            nunits-by-nsteps matrices containing the full trajectories\n",
    "costs           1-by-nsteps vector with the final cost from the minimization procedure for each\n",
    "                timestep. This is the squared difference between the U[t+1] produced by the U[t] \n",
    "                guess and the actual U[t+1]\n",
    "\n",
    "\"\"\"\n",
    "function backwardsModel(endU; nsteps=100, start_eta=10, tol=1e-15, maxiter=400, \n",
    "    do_plot=false, init_add=0, input=[], noise=[], nderivs=0, difforder=0, clearfig=false, fignum=1, params...)    \n",
    "\n",
    "    nunits = length(endU)\n",
    "\n",
    "    # --- formatting input ---\n",
    "    if ~(typeof(input)<:Array) || prod(size(input))==1  # was a scalar\n",
    "        input = input[1]*(1+ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    elseif length(input)==0 # was the empty matrix\n",
    "        input = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    elseif size(input,2)==1     # was a column vector\n",
    "        input = input*(1+ForwardDiffZeros(1, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    end    \n",
    "    # --- formatting noise ---\n",
    "    if ~(typeof(noise)<:Array)  # was a scalar\n",
    "        noise = noise*(1+ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    elseif length(noise)==0 # was the empty matrix\n",
    "        noise = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    elseif size(noise,2)==1     # was a column vector\n",
    "        noise = noise*(1+ForwardDiffZeros(1, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    end    \n",
    "    \n",
    "    function J(U1, U2; nderivs=0, difforder=0, noise=[], inputs=[], pars...)\n",
    "        U2hat = forwardModel(U1; nsteps=2, noise=noise, input=input, nderivs=nderivs, difforder=difforder, pars...)[1]\n",
    "        U2hat = U2hat\n",
    "        DU = U2hat - U2\n",
    "    \n",
    "        return sum(DU.*DU)\n",
    "    end\n",
    "    \n",
    "    if length(noise)==0\n",
    "        noise = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    end\n",
    "\n",
    "    U = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    U = reshape(U, nunits, nsteps)\n",
    "    costs = ForwardDiffZeros(nsteps, 1, nderivs=nderivs, difforder=difforder)    \n",
    "    \n",
    "    U[:,end] = endU\n",
    "    for i=(nsteps-1):-1:1\n",
    "        if i==1\n",
    "            my_init_add = init_add\n",
    "        else\n",
    "            my_init_add = 0\n",
    "        end\n",
    "        \n",
    "        U[:,i], costs[i] = trust_region_Hessian_minimization(U[:,i+1], \n",
    "            (x) -> J(x, U[:,i+1]; nderivs=length(endU), difforder=2, \n",
    "            input=input[:,i:i+1], noise = noise[:,i:i+1], init_add=my_init_add, params...); \n",
    "            verbose=false, start_eta=start_eta, tol=tol, maxiter=maxiter)\n",
    "        U[:,i] += noise[:,i]\n",
    "    end\n",
    "    \n",
    "    \n",
    "    V = g(U)\n",
    "    \n",
    "    if do_plot\n",
    "        figure(fignum)        \n",
    "        if length(endU)==1\n",
    "            if clearfig; clf(); end;\n",
    "            t = (0:nsteps-1)*dt\n",
    "            plot(t, V[1,:], \"m-\")\n",
    "            plot(t[1], V[1,1], \"go\")\n",
    "            plot(t[end], V[1,end], \"ro\")            \n",
    "            ylim([-0.01, 1.01])\n",
    "        elseif length(endU)>=2\n",
    "            if clearfig; clf(); end;            \n",
    "            plot(V[1,:], V[2,:], \"m-\")\n",
    "            plot(V[1,1], V[2,1], \"go\")\n",
    "            plot(V[1,end], V[2,end], \"ro\")\n",
    "            xlim([-0.01, 1.01]); ylim([-0.01, 1.01])\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return U[:,1], V[:,1], U, V, costs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.593346 seconds (156.78 k allocations: 7.770 MB)\n",
      "Pro % correct = 100%\n",
      "Anti % correct = "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition make_input(Any) in module Main at In[752]:34 overwritten at In[771]:34.\n",
      "WARNING: Method definition #make_input(Array{Any, 1}, Main.#make_input, Any) in module Main overwritten.\n",
      "WARNING: Method definition run_ntrials(Any) in module Main at In[752]:58 overwritten at In[771]:58.\n",
      "WARNING: Method definition #run_ntrials(Array{Any, 1}, Main.#run_ntrials, Any) in module Main overwritten.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80% \n"
     ]
    }
   ],
   "source": [
    "model_params = Dict(\n",
    ":dt     =>  0.02, \n",
    ":tau    =>  0.1, \n",
    ":W      =>  [0.2 -1.7 0 -1.7; -1.7 0.2 -1.7 0; 0 -1.7 0.2 -1.7; -1.7 0 -1.7 0.2], \n",
    ":nsteps =>  2, \n",
    ":noise  =>  [], \n",
    ":sigma  =>  0.08, \n",
    ":input  =>  0, \n",
    ":g_leak =>  0.25, \n",
    ":U_rest =>  -1,\n",
    ":theta  =>  1, \n",
    ":beta   =>  1, \n",
    ":sw     =>  0.2,\n",
    ":hw     =>  -1.7,\n",
    ":vw     =>  -1.7,\n",
    ":constant_excitation      => 0.19, \n",
    ":anti_rule_strength       => 0.1,\n",
    ":pro_rule_strength        => 0.1, \n",
    ":target_period_excitation => 1,\n",
    ":right_light_excitation   => 0.5, \n",
    ":right_light_pro_extra    => 0,\n",
    ":const_add => 0, \n",
    ":init_add  => 0, \n",
    ":rule_and_delay_period    => 0.4,\n",
    ":target_period            => 0.1,\n",
    ":post_target_period       => 0.5,\n",
    ")\n",
    "\n",
    "\n",
    "function make_input(trial_type; dt=0.02, nderivs=0, difforder=0, constant_excitation=0.19, anti_rule_strength=0.1, \n",
    "    pro_rule_strength=0.1, target_period_excitation=1, right_light_excitation=0.5, right_light_pro_extra=0, \n",
    "    rule_and_delay_period=0.4, target_period=0.1, post_target_period=0.4, other_unused_params...)\n",
    "\n",
    "    T = rule_and_delay_period + target_period + post_target_period\n",
    "    t = 0:dt:T\n",
    "    nsteps = length(t)\n",
    "\n",
    "    input = constant_excitation + ForwardDiffZeros(4, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    if trial_type==\"Anti\"\n",
    "        input[2:3, t.<rule_and_delay_period] += anti_rule_strength\n",
    "    elseif trial_type==\"Pro\"\n",
    "        input[[1,4], t.<rule_and_delay_period] += pro_rule_strength\n",
    "    else\n",
    "        error(\"make_input: I don't recognize input type \\\"\" * trial_type * \"\\\"\")\n",
    "    end\n",
    "    \n",
    "    input[:,     (rule_and_delay_period.<=t) & (t.<rule_and_delay_period+target_period)] += target_period_excitation\n",
    "    input[1:2,   (rule_and_delay_period.<=t) & (t.<rule_and_delay_period+target_period)] += right_light_excitation\n",
    "    input[1,     (rule_and_delay_period.<=t) & (t.<rule_and_delay_period+target_period)] += right_light_pro_extra\n",
    "    \n",
    "    input[[1,4],:] += const_pro_bias\n",
    "    \n",
    "    return input, t, nsteps\n",
    "end\n",
    "\n",
    "\n",
    "function run_ntrials(ntrials; plot_list=[], nderivs=0, difforder=0, model_params...)\n",
    "    pro_input,  t, nsteps = make_input(\"Pro\" ; model_params...)\n",
    "    anti_input, t, nsteps = make_input(\"Anti\"; model_params...)\n",
    "\n",
    "    model_params = Dict(model_params)\n",
    "    sw = model_params[:sw]\n",
    "    hw = model_params[:hw]\n",
    "    vw = model_params[:vw]\n",
    "    model_params = make_dict([\"nsteps\", \"W\"], [nsteps, [sw vw 0 hw; vw sw hw 0; 0 hw sw vw; hw 0 vw sw]], model_params)\n",
    "    model_params = make_dict([\"nderivs\", \"difforder\"], [nderivs, difforder], model_params)\n",
    "    \n",
    "    proVs  = ForwardDiffZeros(4, ntrials, nderivs=nderivs, difforder=difforder)\n",
    "    antiVs = ForwardDiffZeros(4, ntrials, nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    # --- PRO ---\n",
    "    figure(1); clf();\n",
    "    model_params = make_dict([\"input\"], [pro_input], model_params)\n",
    "    for i=1:ntrials\n",
    "        startU = [-0.3, -0.7, -0.7, -0.3]\n",
    "        Uend, Vend, U, V = forwardModel(startU, do_plot=false; model_params...)\n",
    "        proVs[:,i] = Vend\n",
    "        if any(plot_list.==i) \n",
    "            plot_PA(t, U, V, fignum=1, clearfig=false)\n",
    "            subplot(3,1,1); title(\"PRO\")\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # --- ANTI ---\n",
    "    figure(2); clf();\n",
    "    model_params = make_dict([\"input\"], [anti_input], model_params)\n",
    "    for i=1:ntrials\n",
    "        startU = [-0.7, -0.3, -0.3, -0.7]\n",
    "        Uend, Vend, U, V = forwardModel(startU, do_plot=false; model_params...)\n",
    "        antiVs[:,i] = Vend\n",
    "        if any(plot_list.==i) \n",
    "            plot_PA(t, U, V, fignum=2, clearfig=false)\n",
    "            subplot(3,1,1); title(\"ANTI\")\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return proVs, antiVs\n",
    "end\n",
    "\n",
    "ntrials = 10\n",
    "proVs, antiVs = @time(run_ntrials(ntrials; plot_list=[1:5;], model_params...))\n",
    "\n",
    "@printf(\"Pro %% correct = %g%%\\n\", 100*length(find(proVs[1,:].>proVs[4,:]))/ntrials)\n",
    "@printf(\"Anti %% correct = %g%% \\n\", 100*length(find(antiVs[1,:].<antiVs[4,:]))/ntrials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition JJ(Any) in module Main at In[808]:5 overwritten at In[824]:5.\n",
      "WARNING: Method definition #JJ(Array{Any, 1}, Main.#JJ, Any) in module Main overwritten.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "JJ (generic function with 1 method)"
      ]
     },
     "execution_count": 824,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function JJ(ntrials; pro_target=0.9, anti_target=0.8, \n",
    "    theta1=0.025, theta2=0.035, cbeta=0.003, verbose=false, \n",
    "    do_plot=false, pre_string=\"\", zero_last_sigmas=0, seedrand=NaN, model_params...)\n",
    "\n",
    "    if ~isnan(seedrand); srand(seedrand); end\n",
    "    \n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    proVs, antiVs = run_ntrials(ntrials; model_params...)\n",
    "    \n",
    "    hitsP  = 0.5*(1 + tanh.((proVs[1,:]-proVs[4,:,])/theta1))\n",
    "    diffsP = tanh.((proVs[1,:,]-proVs[4,:])/theta2).^2\n",
    "    hitsA  = 0.5*(1 + tanh.((antiVs[4,:]-antiVs[1,:,])/theta1))\n",
    "    diffsA = tanh.((antiVs[4,:,]-antiVs[1,:])/theta2).^2\n",
    "    \n",
    "    cost1 = 0.5*(mean(hitsP) - pro_target).^2  + 0.5*(mean(hitsA) - anti_target).^2\n",
    "    cost2 = -cbeta*(0.5*mean(diffsP) + 0.5*mean(diffsA))\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"%s\", pre_string)\n",
    "        @printf(\"     -- cost=%g,   cost1=%g, cost2=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2))\n",
    "        @printf(\"     -- mean(hitsP)=%g, mean(diffsP)=%g mean(hitsA)=%g, mean(diffsA)=%g\\n\", \n",
    "            convert(Float64, mean(hitsP)), convert(Float64, mean(diffsP)),\n",
    "            convert(Float64, mean(hitsA)), convert(Float64, mean(diffsA)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Symbol,Any} with 25 entries:\n",
       "  :target_period            => 0.1\n",
       "  :right_light_pro_extra    => 0\n",
       "  :beta                     => 1\n",
       "  :sigma                    => 0.08\n",
       "  :anti_rule_strength       => 0.1\n",
       "  :init_add                 => 0\n",
       "  :pro_rule_strength        => 0.1\n",
       "  :vw                       => -1.7\n",
       "  :noise                    => Any[]\n",
       "  :tau                      => 0.1\n",
       "  :theta                    => 1\n",
       "  :W                        => [0.2 -1.7 0.0 -1.7; -1.7 0.2 -1.7 0.0; 0.0 -1.7 …\n",
       "  :right_light_excitation   => 0.5\n",
       "  :hw                       => -1.7\n",
       "  :sw                       => 0.2\n",
       "  :constant_excitation      => 0.19\n",
       "  :rule_and_delay_period    => 0.4\n",
       "  :const_add                => 0\n",
       "  :nsteps                   => 2\n",
       "  :post_target_period       => 0.5\n",
       "  :input                    => 0\n",
       "  :target_period_excitation => 1\n",
       "  :g_leak                   => 0.25\n",
       "  :dt                       => 0.02\n",
       "  :U_rest                   => -1"
      ]
     },
     "execution_count": 779,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: eta=0.01 ps=[0.200, -1.700, -1.700, 0.190, 0.150, 0.100, 0.100]\n",
      "     -- cost=0.0962041,   cost1=0.0962713, cost2=-6.7238e-05\n",
      "     -- mean(hitsP)=0.589085, mean(diffsP)=0.0914605 mean(hitsA)=0.490364, mean(diffsA)=0.0430155\n",
      "     -- cost=0.0935037,   cost1=0.0935753, cost2=-7.1632e-05\n",
      "     -- mean(hitsP)=0.597226, mean(diffsP)=0.0982496 mean(hitsA)=0.491004, mean(diffsA)=0.0450144\n",
      "1: eta=0.011 cost=0.0935 jtype=constrained costheta=-1.000 ps=[0.201, -1.700, -1.700, 0.197, 0.157, 0.101, 0.097]\n",
      "     -- cost=0.090329,   cost1=0.0904055, cost2=-7.65244e-05\n",
      "     -- mean(hitsP)=0.606745, mean(diffsP)=0.105981 mean(hitsA)=0.492084, mean(diffsA)=0.0470675\n",
      "2: eta=0.0121 cost=0.0903 jtype=constrained costheta=-0.999 ps=[0.202, -1.701, -1.700, 0.204, 0.164, 0.102, 0.093]\n",
      "     -- cost=0.0865702,   cost1=0.0866521, cost2=-8.19284e-05\n",
      "     -- mean(hitsP)=0.617972, mean(diffsP)=0.114782 mean(hitsA)=0.493791, mean(diffsA)=0.0490754\n",
      "3: eta=0.01331 cost=0.0866 jtype=constrained costheta=-0.999 ps=[0.204, -1.701, -1.699, 0.212, 0.171, 0.103, 0.088]\n",
      "     -- cost=0.0820858,   cost1=0.0821736, cost2=-8.78516e-05\n",
      "     -- mean(hitsP)=0.631354, mean(diffsP)=0.124824 mean(hitsA)=0.496394, mean(diffsA)=0.0508787\n",
      "4: eta=0.014641 cost=0.0821 jtype=constrained costheta=-0.999 ps=[0.205, -1.702, -1.699, 0.221, 0.179, 0.105, 0.083]\n",
      "     -- cost=0.0766941,   cost1=0.0767884, cost2=-9.4319e-05\n",
      "     -- mean(hitsP)=0.647513, mean(diffsP)=0.136399 mean(hitsA)=0.500288, mean(diffsA)=0.0522386\n",
      "5: eta=0.0161051 cost=0.0767 jtype=constrained costheta=-0.999 ps=[0.207, -1.702, -1.698, 0.231, 0.187, 0.106, 0.076]\n",
      "     -- cost=0.0701664,   cost1=0.0702678, cost2=-0.000101444\n",
      "     -- mean(hitsP)=0.667319, mean(diffsP)=0.150066 mean(hitsA)=0.506069, mean(diffsA)=0.0528224\n",
      "6: eta=0.0177156 cost=0.0702 jtype=constrained costheta=-0.999 ps=[0.209, -1.703, -1.698, 0.242, 0.196, 0.108, 0.069]\n",
      "     -- cost=0.0622326,   cost1=0.0623422, cost2=-0.000109609\n",
      "     -- mean(hitsP)=0.692, mean(diffsP)=0.166997 mean(hitsA)=0.514657, mean(diffsA)=0.0522215\n",
      "7: eta=0.0194872 cost=0.0622 jtype=constrained costheta=-0.999 ps=[0.212, -1.704, -1.697, 0.254, 0.205, 0.109, 0.060]\n",
      "     -- cost=0.0526251,   cost1=0.052745, cost2=-0.0001199\n",
      "     -- mean(hitsP)=0.723213, mean(diffsP)=0.189714 mean(hitsA)=0.527536, mean(diffsA)=0.0500864\n",
      "8: eta=0.0214359 cost=0.0526 jtype=constrained costheta=-0.999 ps=[0.214, -1.705, -1.697, 0.268, 0.214, 0.112, 0.050]\n",
      "     -- cost=0.0412234,   cost1=0.0413584, cost2=-0.00013508\n",
      "     -- mean(hitsP)=0.762899, mean(diffsP)=0.223535 mean(hitsA)=0.547176, mean(diffsA)=0.0466252\n",
      "9: eta=0.0235795 cost=0.0422 jtype=constrained costheta=-0.999 ps=[0.217, -1.706, -1.696, 0.283, 0.223, 0.114, 0.039]\n",
      "     -- cost=0.0292799,   cost1=0.0294581, cost2=-0.000178252\n",
      "     -- mean(hitsP)=0.804474, mean(diffsP)=0.292838 mean(hitsA)=0.576861, mean(diffsA)=0.0636652\n",
      "10: eta=0.0259374 cost=0.0311 jtype=constrained costheta=-0.999 ps=[0.222, -1.708, -1.695, 0.303, 0.233, 0.117, 0.035]\n",
      "     -- cost=0.0174461,   cost1=0.017688, cost2=-0.000241923\n",
      "     -- mean(hitsP)=0.849666, mean(diffsP)=0.387205 mean(hitsA)=0.618775, mean(diffsA)=0.0966423\n",
      "11: eta=0.0285312 cost=0.0201 jtype=constrained costheta=-0.999 ps=[0.226, -1.710, -1.693, 0.325, 0.244, 0.120, 0.032]\n",
      "     -- cost=0.00787042,   cost1=0.00820383, cost2=-0.000333415\n",
      "     -- mean(hitsP)=0.891255, mean(diffsP)=0.503876 mean(hitsA)=0.672206, mean(diffsA)=0.162954\n",
      "12: eta=0.0313843 cost=0.0109 jtype=constrained costheta=-0.999 ps=[0.232, -1.713, -1.691, 0.350, 0.257, 0.124, 0.031]\n",
      "     -- cost=0.00280504,   cost1=0.00325307, cost2=-0.000448039\n",
      "     -- mean(hitsP)=0.91298, mean(diffsP)=0.61079 mean(hitsA)=0.720391, mean(diffsA)=0.285287\n",
      "13: eta=0.0345227 cost=0.0049 jtype=constrained costheta=-0.995 ps=[0.239, -1.718, -1.687, 0.375, 0.272, 0.128, 0.034]\n",
      "     -- cost=0.000987203,   cost1=0.00154667, cost2=-0.000559469\n",
      "     -- mean(hitsP)=0.907655, mean(diffsP)=0.677973 mean(hitsA)=0.744912, mean(diffsA)=0.440965\n",
      "14: eta=0.037975 cost=0.0016 jtype=constrained costheta=-0.944 ps=[0.247, -1.725, -1.681, 0.397, 0.294, 0.133, 0.041]\n",
      "     -- cost=-0.000116594,   cost1=0.000540517, cost2=-0.000657111\n",
      "     -- mean(hitsP)=0.904043, mean(diffsP)=0.738973 mean(hitsA)=0.76737, mean(diffsA)=0.575249\n",
      "15: eta=0.0189875 cost=0.0016 jtype=constrained costheta=NaN ps=[0.247, -1.725, -1.681, 0.397, 0.294, 0.133, 0.041]\n",
      "     -- cost=0.000212769,   cost1=0.000830609, cost2=-0.000617839\n",
      "     -- mean(hitsP)=0.907118, mean(diffsP)=0.715043 mean(hitsA)=0.759868, mean(diffsA)=0.520636\n",
      "16: eta=0.00949375 cost=0.0016 jtype=constrained costheta=NaN ps=[0.247, -1.725, -1.681, 0.397, 0.294, 0.133, 0.041]\n",
      "     -- cost=0.000500294,   cost1=0.00109047, cost2=-0.000590178\n",
      "     -- mean(hitsP)=0.90919, mean(diffsP)=0.69889 mean(hitsA)=0.754213, mean(diffsA)=0.481466\n",
      "17: eta=0.00474687 cost=0.0016 jtype=constrained costheta=NaN ps=[0.247, -1.725, -1.681, 0.397, 0.294, 0.133, 0.041]\n",
      "     -- cost=0.000689463,   cost1=0.00126462, cost2=-0.000575158\n",
      "     -- mean(hitsP)=0.909753, mean(diffsP)=0.689804 mean(hitsA)=0.750663, mean(diffsA)=0.460512\n",
      "18: eta=0.00237344 cost=0.0016 jtype=constrained costheta=NaN ps=[0.247, -1.725, -1.681, 0.397, 0.294, 0.133, 0.041]\n",
      "     -- cost=0.000805468,   cost1=0.00137284, cost2=-0.000567376\n",
      "     -- mean(hitsP)=0.909617, mean(diffsP)=0.68474 mean(hitsA)=0.748491, mean(diffsA)=0.450012\n",
      "19: eta=0.00261078 cost=0.0015 jtype=constrained costheta=-0.955 ps=[0.248, -1.726, -1.680, 0.399, 0.295, 0.133, 0.041]\n",
      "     -- cost=0.000671932,   cost1=0.00124787, cost2=-0.000575937\n",
      "     -- mean(hitsP)=0.909986, mean(diffsP)=0.690498 mean(hitsA)=0.751051, mean(diffsA)=0.461377\n",
      "20: eta=0.00130539 cost=0.0015 jtype=constrained costheta=NaN ps=[0.248, -1.726, -1.680, 0.399, 0.295, 0.133, 0.041]\n",
      "     -- cost=0.000731473,   cost1=0.00130316, cost2=-0.000571688\n",
      "     -- mean(hitsP)=0.910014, mean(diffsP)=0.687826 mean(hitsA)=0.74994, mean(diffsA)=0.45555\n",
      "21: eta=0.00143593 cost=0.0014 jtype=constrained costheta=-0.996 ps=[0.248, -1.726, -1.680, 0.400, 0.296, 0.133, 0.041]\n",
      "     -- cost=0.000702944,   cost1=0.00127651, cost2=-0.000573565\n",
      "     -- mean(hitsP)=0.910488, mean(diffsP)=0.689543 mean(hitsA)=0.750573, mean(diffsA)=0.457587\n",
      "22: eta=0.00157952 cost=0.0014 jtype=constrained costheta=-0.545 ps=[0.248, -1.726, -1.680, 0.400, 0.297, 0.134, 0.041]\n",
      "     -- cost=0.000672845,   cost1=0.00124877, cost2=-0.000575925\n",
      "     -- mean(hitsP)=0.910794, mean(diffsP)=0.69142 mean(hitsA)=0.751204, mean(diffsA)=0.46043\n",
      "23: eta=0.00173747 cost=0.0013 jtype=constrained costheta=-0.997 ps=[0.249, -1.727, -1.679, 0.400, 0.298, 0.134, 0.041]\n",
      "     -- cost=0.00064202,   cost1=0.00122053, cost2=-0.000578512\n",
      "     -- mean(hitsP)=0.911057, mean(diffsP)=0.693411 mean(hitsA)=0.751846, mean(diffsA)=0.463614\n",
      "24: eta=0.00191122 cost=0.0012 jtype=constrained costheta=-1.000 ps=[0.249, -1.727, -1.679, 0.400, 0.300, 0.134, 0.041]\n",
      "     -- cost=0.000609295,   cost1=0.00119063, cost2=-0.000581339\n",
      "     -- mean(hitsP)=0.91132, mean(diffsP)=0.695561 mean(hitsA)=0.752533, mean(diffsA)=0.467116\n",
      "25: eta=0.00210234 cost=0.0012 jtype=constrained costheta=-1.000 ps=[0.250, -1.728, -1.679, 0.400, 0.301, 0.135, 0.042]\n",
      "     -- cost=0.000574143,   cost1=0.00115857, cost2=-0.000584427\n",
      "     -- mean(hitsP)=0.911601, mean(diffsP)=0.6979 mean(hitsA)=0.753282, mean(diffsA)=0.470953\n",
      "26: eta=0.00231258 cost=0.0011 jtype=constrained costheta=-1.000 ps=[0.251, -1.728, -1.678, 0.400, 0.303, 0.135, 0.042]\n",
      "     -- cost=0.000536314,   cost1=0.00112411, cost2=-0.000587798\n",
      "     -- mean(hitsP)=0.911903, mean(diffsP)=0.700446 mean(hitsA)=0.754103, mean(diffsA)=0.475149\n",
      "27: eta=0.00254384 cost=0.0010 jtype=constrained costheta=-1.000 ps=[0.252, -1.729, -1.678, 0.400, 0.305, 0.136, 0.042]\n",
      "     -- cost=0.000495656,   cost1=0.00108713, cost2=-0.000591475\n",
      "     -- mean(hitsP)=0.912232, mean(diffsP)=0.703216 mean(hitsA)=0.755004, mean(diffsA)=0.479734\n",
      "28: eta=0.00279822 cost=0.0009 jtype=constrained costheta=-1.000 ps=[0.253, -1.730, -1.677, 0.400, 0.307, 0.136, 0.043]\n",
      "     -- cost=0.00045206,   cost1=0.00104754, cost2=-0.000595484\n",
      "     -- mean(hitsP)=0.912588, mean(diffsP)=0.706227 mean(hitsA)=0.755993, mean(diffsA)=0.484741\n",
      "29: eta=0.00307804 cost=0.0008 jtype=constrained costheta=-1.000 ps=[0.254, -1.730, -1.677, 0.400, 0.309, 0.137, 0.043]\n",
      "     -- cost=0.000405447,   cost1=0.0010053, cost2=-0.000599849\n",
      "     -- mean(hitsP)=0.912973, mean(diffsP)=0.709495 mean(hitsA)=0.757078, mean(diffsA)=0.490203\n",
      "30: eta=0.00338585 cost=0.0008 jtype=constrained costheta=-1.000 ps=[0.255, -1.731, -1.676, 0.400, 0.312, 0.137, 0.043]\n",
      "     -- cost=0.000355771,   cost1=0.000960367, cost2=-0.000604596\n",
      "     -- mean(hitsP)=0.913388, mean(diffsP)=0.713036 mean(hitsA)=0.758269, mean(diffsA)=0.496157\n",
      "31: eta=0.00372443 cost=0.0007 jtype=constrained costheta=-1.000 ps=[0.256, -1.732, -1.675, 0.400, 0.315, 0.138, 0.044]\n",
      "     -- cost=0.000303027,   cost1=0.00091278, cost2=-0.000609754\n",
      "     -- mean(hitsP)=0.913833, mean(diffsP)=0.716867 mean(hitsA)=0.759575, mean(diffsA)=0.50264\n",
      "32: eta=0.00409687 cost=0.0006 jtype=constrained costheta=-1.000 ps=[0.257, -1.733, -1.674, 0.400, 0.318, 0.139, 0.044]\n",
      "     -- cost=0.000247261,   cost1=0.000862608, cost2=-0.000615347\n",
      "     -- mean(hitsP)=0.914309, mean(diffsP)=0.721002 mean(hitsA)=0.761007, mean(diffsA)=0.509692\n",
      "33: eta=0.00450656 cost=0.0005 jtype=constrained costheta=-1.000 ps=[0.259, -1.734, -1.673, 0.400, 0.321, 0.140, 0.045]\n",
      "     -- cost=0.000188581,   cost1=0.000809982, cost2=-0.000621401\n",
      "     -- mean(hitsP)=0.914813, mean(diffsP)=0.725451 mean(hitsA)=0.762576, mean(diffsA)=0.517352\n",
      "34: eta=0.00495722 cost=0.0004 jtype=constrained costheta=-1.000 ps=[0.261, -1.736, -1.672, 0.400, 0.324, 0.141, 0.045]\n",
      "     -- cost=0.000127168,   cost1=0.000755108, cost2=-0.000627939\n",
      "     -- mean(hitsP)=0.915341, mean(diffsP)=0.730221 mean(hitsA)=0.764295, mean(diffsA)=0.525657\n",
      "35: eta=0.00545294 cost=0.0003 jtype=constrained costheta=-1.000 ps=[0.263, -1.738, -1.671, 0.400, 0.328, 0.142, 0.046]\n",
      "     -- cost=6.32892e-05,   cost1=0.000698264, cost2=-0.000634975\n",
      "     -- mean(hitsP)=0.915885, mean(diffsP)=0.73531 mean(hitsA)=0.766174, mean(diffsA)=0.53464\n",
      "36: eta=0.00599823 cost=0.0001 jtype=constrained costheta=-0.999 ps=[0.265, -1.739, -1.669, 0.400, 0.332, 0.144, 0.047]\n",
      "     -- cost=-2.69888e-06,   cost1=0.000639811, cost2=-0.00064251\n",
      "     -- mean(hitsP)=0.916426, mean(diffsP)=0.740696 mean(hitsA)=0.768223, mean(diffsA)=0.544323\n",
      "37: eta=0.00659806 cost=0.0001 jtype=constrained costheta=-0.999 ps=[0.268, -1.742, -1.668, 0.400, 0.336, 0.145, 0.047]\n",
      "     -- cost=-7.03317e-05,   cost1=0.000580181, cost2=-0.000650513\n",
      "     -- mean(hitsP)=0.916937, mean(diffsP)=0.746328 mean(hitsA)=0.770445, mean(diffsA)=0.554698\n",
      "38: eta=0.00725786 cost=-0.0000 jtype=constrained costheta=-0.998 ps=[0.271, -1.744, -1.666, 0.400, 0.341, 0.148, 0.048]\n",
      "     -- cost=-0.000139002,   cost1=0.000519888, cost2=-0.00065889\n",
      "     -- mean(hitsP)=0.917363, mean(diffsP)=0.752093 mean(hitsA)=0.772828, mean(diffsA)=0.565688\n",
      "39: eta=0.00798365 cost=-0.0001 jtype=constrained costheta=-0.995 ps=[0.275, -1.747, -1.664, 0.400, 0.345, 0.150, 0.049]\n",
      "     -- cost=-0.000207834,   cost1=0.000459585, cost2=-0.000667419\n",
      "     -- mean(hitsP)=0.917617, mean(diffsP)=0.757769 mean(hitsA)=0.775326, mean(diffsA)=0.57707\n",
      "40: eta=0.00878201 cost=-0.0002 jtype=constrained costheta=-0.991 ps=[0.279, -1.751, -1.662, 0.400, 0.349, 0.153, 0.050]\n",
      "     -- cost=-0.000275465,   cost1=0.000400225, cost2=-0.000675689\n",
      "     -- mean(hitsP)=0.917578, mean(diffsP)=0.762996 mean(hitsA)=0.777831, mean(diffsA)=0.588383\n",
      "41: eta=0.00966021 cost=-0.0003 jtype=constrained costheta=-0.985 ps=[0.284, -1.756, -1.659, 0.400, 0.352, 0.157, 0.050]\n",
      "     -- cost=-0.000344405,   cost1=0.000340515, cost2=-0.000684921\n",
      "     -- mean(hitsP)=0.91553, mean(diffsP)=0.767547 mean(hitsA)=0.779027, mean(diffsA)=0.602294\n",
      "42: eta=0.0106262 cost=-0.0003 jtype=constrained costheta=-0.907 ps=[0.290, -1.760, -1.656, 0.400, 0.356, 0.161, 0.051]\n",
      "     -- cost=-0.000413663,   cost1=0.000281357, cost2=-0.00069502\n",
      "     -- mean(hitsP)=0.913923, mean(diffsP)=0.773084 mean(hitsA)=0.780794, mean(diffsA)=0.616956\n",
      "43: eta=0.0116889 cost=-0.0004 jtype=constrained costheta=-0.997 ps=[0.296, -1.765, -1.653, 0.400, 0.360, 0.165, 0.053]\n",
      "     -- cost=-0.000482291,   cost1=0.000223667, cost2=-0.000705958\n",
      "     -- mean(hitsP)=0.912229, mean(diffsP)=0.779237 mean(hitsA)=0.782744, mean(diffsA)=0.632678\n",
      "44: eta=0.0128577 cost=-0.0005 jtype=constrained costheta=-0.996 ps=[0.303, -1.771, -1.650, 0.400, 0.365, 0.170, 0.054]\n",
      "     -- cost=-0.000548928,   cost1=0.000168844, cost2=-0.000717772\n",
      "     -- mean(hitsP)=0.910451, mean(diffsP)=0.786052 mean(hitsA)=0.784885, mean(diffsA)=0.649492\n",
      "45: eta=0.0141435 cost=-0.0005 jtype=constrained costheta=-0.993 ps=[0.311, -1.777, -1.646, 0.400, 0.371, 0.175, 0.055]\n",
      "     -- cost=-0.000611963,   cost1=0.000118536, cost2=-0.000730499\n",
      "     -- mean(hitsP)=0.908592, mean(diffsP)=0.793573 mean(hitsA)=0.787223, mean(diffsA)=0.667425\n",
      "46: eta=0.0155579 cost=-0.0006 jtype=constrained costheta=-0.988 ps=[0.319, -1.783, -1.643, 0.400, 0.377, 0.180, 0.057]\n",
      "     -- cost=-0.000669545,   cost1=7.4636e-05, cost2=-0.000744181\n",
      "     -- mean(hitsP)=0.906663, mean(diffsP)=0.80185 mean(hitsA)=0.789759, mean(diffsA)=0.686513\n",
      "47: eta=0.0171137 cost=-0.0007 jtype=constrained costheta=-0.978 ps=[0.329, -1.789, -1.639, 0.400, 0.384, 0.186, 0.058]\n",
      "     -- cost=-0.000719652,   cost1=3.92275e-05, cost2=-0.000758879\n",
      "     -- mean(hitsP)=0.90468, mean(diffsP)=0.810949 mean(hitsA)=0.79248, mean(diffsA)=0.70681\n",
      "48: eta=0.018825 cost=-0.0007 jtype=constrained costheta=-0.957 ps=[0.340, -1.796, -1.634, 0.400, 0.392, 0.193, 0.060]\n",
      "     -- cost=-0.000760259,   cost1=1.44475e-05, cost2=-0.000774706\n",
      "     -- mean(hitsP)=0.902678, mean(diffsP)=0.820986 mean(hitsA)=0.795339, mean(diffsA)=0.728427\n",
      "49: eta=0.0207075 cost=-0.0008 jtype=constrained costheta=-0.909 ps=[0.352, -1.802, -1.630, 0.400, 0.403, 0.199, 0.062]\n",
      "     -- cost=-0.000789824,   cost1=1.96422e-06, cost2=-0.000791788\n",
      "     -- mean(hitsP)=0.900759, mean(diffsP)=0.832126 mean(hitsA)=0.798169, mean(diffsA)=0.75145\n",
      "50: eta=0.0227783 cost=-0.0008 jtype=constrained costheta=-0.787 ps=[0.365, -1.807, -1.627, 0.400, 0.416, 0.206, 0.065]\n",
      "     -- cost=-0.00080886,   cost1=3.51761e-07, cost2=-0.000809212\n",
      "     -- mean(hitsP)=0.899268, mean(diffsP)=0.843898 mean(hitsA)=0.80041, mean(diffsA)=0.774525\n",
      "51: eta=0.0250561 cost=-0.0008 jtype=constrained costheta=-0.526 ps=[0.378, -1.806, -1.626, 0.400, 0.434, 0.211, 0.068]\n",
      "     -- cost=-0.000822067,   cost1=2.22504e-06, cost2=-0.000824292\n",
      "     -- mean(hitsP)=0.898553, mean(diffsP)=0.854457 mean(hitsA)=0.801535, mean(diffsA)=0.794127\n",
      "52: eta=0.0275617 cost=-0.0008 jtype=constrained costheta=-0.287 ps=[0.387, -1.799, -1.629, 0.400, 0.456, 0.214, 0.071]\n",
      "     -- cost=-0.000833963,   cost1=3.13963e-06, cost2=-0.000837102\n",
      "     -- mean(hitsP)=0.898336, mean(diffsP)=0.863633 mean(hitsA)=0.801874, mean(diffsA)=0.810572\n",
      "53: eta=0.0303179 cost=-0.0008 jtype=constrained costheta=-0.223 ps=[0.393, -1.787, -1.633, 0.400, 0.479, 0.215, 0.074]\n",
      "     -- cost=-0.000845834,   cost1=3.11548e-06, cost2=-0.000848949\n",
      "     -- mean(hitsP)=0.898315, mean(diffsP)=0.872193 mean(hitsA)=0.801841, mean(diffsA)=0.825706\n",
      "54: eta=0.0333497 cost=-0.0008 jtype=constrained costheta=-0.206 ps=[0.399, -1.773, -1.638, 0.400, 0.504, 0.217, 0.077]\n",
      "     -- cost=-0.000857799,   cost1=2.72332e-06, cost2=-0.000860522\n",
      "     -- mean(hitsP)=0.898356, mean(diffsP)=0.880554 mean(hitsA)=0.801657, mean(diffsA)=0.840491\n",
      "55: eta=0.0366846 cost=-0.0009 jtype=constrained costheta=-0.182 ps=[0.406, -1.757, -1.644, 0.400, 0.532, 0.219, 0.080]\n",
      "     -- cost=-0.000869751,   cost1=2.26272e-06, cost2=-0.000872014\n",
      "     -- mean(hitsP)=0.898411, mean(diffsP)=0.888824 mean(hitsA)=0.801414, mean(diffsA)=0.855204\n",
      "56: eta=0.0403531 cost=-0.0009 jtype=constrained costheta=-0.153 ps=[0.414, -1.739, -1.651, 0.400, 0.562, 0.222, 0.083]\n",
      "     -- cost=-0.000881533,   cost1=1.85159e-06, cost2=-0.000883385\n",
      "     -- mean(hitsP)=0.898457, mean(diffsP)=0.896986 mean(hitsA)=0.80115, mean(diffsA)=0.869784\n",
      "57: eta=0.0443884 cost=-0.0009 jtype=constrained costheta=-0.125 ps=[0.424, -1.719, -1.657, 0.400, 0.594, 0.226, 0.087]\n",
      "     -- cost=-0.000892963,   cost1=1.53987e-06, cost2=-0.000894503\n",
      "     -- mean(hitsP)=0.898482, mean(diffsP)=0.904999 mean(hitsA)=0.800881, mean(diffsA)=0.884006\n",
      "58: eta=0.0488273 cost=-0.0009 jtype=constrained costheta=-0.099 ps=[0.438, -1.697, -1.664, 0.400, 0.629, 0.232, 0.090]\n",
      "     -- cost=-0.000864093,   cost1=3.96245e-05, cost2=-0.000903717\n",
      "     -- mean(hitsP)=0.895454, mean(diffsP)=0.91197 mean(hitsA)=0.807654, mean(diffsA)=0.895465\n",
      "59: eta=0.0244136 cost=-0.0009 jtype=constrained costheta=NaN ps=[0.438, -1.697, -1.664, 0.400, 0.629, 0.232, 0.090]\n",
      "     -- cost=-0.00088486,   cost1=1.27507e-05, cost2=-0.000897611\n",
      "     -- mean(hitsP)=0.897354, mean(diffsP)=0.907207 mean(hitsA)=0.804301, mean(diffsA)=0.888014\n",
      "60: eta=0.0122068 cost=-0.0009 jtype=constrained costheta=NaN ps=[0.438, -1.697, -1.664, 0.400, 0.629, 0.232, 0.090]\n",
      "     -- cost=-0.000889923,   cost1=6.4955e-08, cost2=-0.000889988\n",
      "     -- mean(hitsP)=0.899641, mean(diffsP)=0.901747 mean(hitsA)=0.800029, mean(diffsA)=0.878228\n",
      "61: eta=0.00610341 cost=-0.0009 jtype=constrained costheta=NaN ps=[0.438, -1.697, -1.664, 0.400, 0.629, 0.232, 0.090]\n",
      "     -- cost=-0.000891669,   cost1=9.36944e-07, cost2=-0.000892606\n",
      "     -- mean(hitsP)=0.899122, mean(diffsP)=0.903576 mean(hitsA)=0.80105, mean(diffsA)=0.881635\n",
      "62: eta=0.0030517 cost=-0.0009 jtype=constrained costheta=NaN ps=[0.438, -1.697, -1.664, 0.400, 0.629, 0.232, 0.090]\n",
      "     -- cost=-0.000892425,   cost1=1.09917e-06, cost2=-0.000893524\n",
      "     -- mean(hitsP)=0.899073, mean(diffsP)=0.904235 mean(hitsA)=0.801157, mean(diffsA)=0.882813\n",
      "  3.162807 seconds (6.39 M allocations: 382.107 MB, 3.05% gc time)\n",
      "Pro % correct = 89.9%\n",
      "Anti % correct = 81.6% \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0008061402268203909"
      ]
     },
     "execution_count": 843,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WORKS:\n",
    "# keyword_gradient((;params...) -> JJ(100; merge(model_params, Dict(params))...), [\"sw\", \"vw\", \"hw\"], [0.2, -1.7, -1.7])\n",
    "\n",
    "args = [\"sw\", \"vw\", \"hw\", \"constant_excitation\", \"right_light_excitation\", \"target_period_excitation\", \"sigma\"]\n",
    "seed = [0.2,   -1.7, -1.7,      0.19,                0.5,                       1,                       0.1]\n",
    "seed = [0.2,   0.17,  0.17,      0.19,                0.5,                       1,                       0.1]\n",
    "seed = [0.2,   -1.7, -1.7,      -0.19,                0.5,                       1,                       0.1]\n",
    "seed = [0.2,   -1.7, -1.7,      0.19,                0.15,                       0.1,                       0.1]\n",
    "\n",
    "bbox = [\n",
    "    -3    3 ;\n",
    "    -3    3 ; \n",
    "    -3    3 ;\n",
    "    0.1   0.4 ;\n",
    "    0.1   2.0 ;\n",
    "    0.1   2.0 ;\n",
    "    0.05  2.0 ;\n",
    "]\n",
    "\n",
    "pars = bbox_Hessian_keyword_minimization(seed, args, bbox, \n",
    "(;params...) -> JJ(100; seedrand=30, cbeta=0.001, verbose=true, merge(model_params, Dict(params))...),\n",
    "start_eta = 0.01, verbose=true)[1]\n",
    "\n",
    "# --------------------\n",
    "# NOW EVALUATE RESULTS\n",
    "# --------------------\n",
    "\n",
    "ntrials = 1000\n",
    "proVs, antiVs = @time(run_ntrials(ntrials; plot_list=[], make_dict(args, pars, model_params)...))\n",
    "\n",
    "@printf(\"Pro %% correct = %g%%\\n\", 100*length(find(proVs[1,:].>proVs[4,:]))/ntrials)\n",
    "@printf(\"Anti %% correct = %g%% \\n\", 100*length(find(antiVs[1,:].<antiVs[4,:]))/ntrials)\n",
    "\n",
    "\n",
    "figure(3); clf();\n",
    "ax1 = subplot(2,1,1)\n",
    "h = plt[:hist](proVs[1,:]-proVs[4,:],-1:0.02:0.1)\n",
    "title(\"PRO Vr - Vl\")\n",
    "remove_xtick_labels(ax1)\n",
    "vlines(0, ylim()[1], ylim()[2])\n",
    "\n",
    "ax2 = subplot(2,1,2)\n",
    "h = plt[:hist](antiVs[1,:]-antiVs[4,:],-1:0.02:1)\n",
    "title(\"ANTI Vr - Vl\")\n",
    "vlines(0, ylim()[1], ylim()[2])\n",
    "\n",
    "JJ(100; make_dict([args; \"plot_list\"], [pars; [[1:5;]]], model_params)...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7×2 Array{Any,2}:\n",
       " \"sw\"                        -0.658701 \n",
       " \"vw\"                        -0.646861 \n",
       " \"hw\"                         1.32357  \n",
       " \"constant_excitation\"        0.400064 \n",
       " \"right_light_excitation\"     2.00115  \n",
       " \"target_period_excitation\"   2.00022  \n",
       " \"sigma\"                      0.0451168"
      ]
     },
     "execution_count": 836,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[args pars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3.353905 seconds (6.39 M allocations: 382.107 MB, 3.00% gc time)\n",
      "Pro % correct = 100%\n",
      "Anti % correct = 73% \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.030415644997541208"
      ]
     },
     "execution_count": 839,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ntrials = 1000\n",
    "proVs, antiVs = @time(run_ntrials(ntrials; plot_list=[], make_dict(args, pars, model_params)...))\n",
    "\n",
    "@printf(\"Pro %% correct = %g%%\\n\", 100*length(find(proVs[1,:].>proVs[4,:]))/ntrials)\n",
    "@printf(\"Anti %% correct = %g%% \\n\", 100*length(find(antiVs[1,:].<antiVs[4,:]))/ntrials)\n",
    "\n",
    "\n",
    "figure(3); clf();\n",
    "ax1 = subplot(2,1,1)\n",
    "h = plt[:hist](proVs[1,:]-proVs[4,:],-1:0.02:0.1)\n",
    "title(\"PRO Vr - Vl\")\n",
    "remove_xtick_labels(ax1)\n",
    "vlines(0, ylim()[1], ylim()[2])\n",
    "\n",
    "ax2 = subplot(2,1,2)\n",
    "h = plt[:hist](antiVs[1,:]-antiVs[4,:],-1:0.02:1)\n",
    "title(\"ANTI Vr - Vl\")\n",
    "vlines(0, ylim()[1], ylim()[2])\n",
    "\n",
    "JJ(100; make_dict([args; \"plot_list\"; \"dt\"], [pars; [[1:5;]]; 0.01], model_params)...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0316676359730454"
      ]
     },
     "execution_count": 841,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "JJ(100; make_dict([args; \"plot_list\"; \"dt\"], [pars; [[1:5;]]; 0.005], model_params)...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.841215 seconds (676.39 k allocations: 498.280 MB, 14.03% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6×6 Array{Float64,2}:\n",
       " -0.439367   -0.235191    0.0598165    1.78209    0.0  -2.01213 \n",
       " -0.235191   -0.0192072   0.0417205    0.779757   0.0  -0.288305\n",
       "  0.0598165   0.0417205  -0.0117162   -0.699812   0.0   0.879395\n",
       "  1.78209     0.779757   -0.699812   -20.3828    -0.0  -0.76631 \n",
       "  0.0         0.0         0.0         -0.0        0.0   0.0     \n",
       " -2.01213    -0.288305    0.879395    -0.76631    0.0  -3.24674 "
      ]
     },
     "execution_count": 750,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time(ForwardDiff.hessian((x)->JJ(100; nderivs=length(x), difforder=2, \n",
    "make_dict([\"sw\", \"hw\", \"vw\", \"sigma\", \"gleak\", \"constant_excitation\", \"plot_list\"], \n",
    "[x[1], x[2], x[3], x[4], x[5], x[6], []], model_params)...), \n",
    "[0.2, -1.7, -1.7, 0.08, 0.25, 0.19]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.314015 seconds (639.88 k allocations: 38.362 MB, 3.50% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.39929640831305885"
      ]
     },
     "execution_count": 751,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time(JJ(100; make_dict([\"sw\", \"hw\", \"vw\", \"plot_list\"], [0.2, -1.7, -1.7, []], model_params)...))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.303153 seconds (639.70 k allocations: 38.326 MB, 1.57% gc time)\n",
      "Pro % correct = 95%\n",
      "Anti % correct = 71% \n"
     ]
    }
   ],
   "source": [
    "ntrials = 100\n",
    "proVs, antiVs = @time(run_ntrials(ntrials; plot_list=[], model_params...))\n",
    "\n",
    "@printf(\"Pro %% correct = %g%%\\n\", 100*length(find(proVs[1,:].>proVs[4,:]))/ntrials)\n",
    "@printf(\"Anti %% correct = %g%% \\n\", 100*length(find(antiVs[1,:].<antiVs[4,:]))/ntrials)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the distribution of VR - VL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.594181 seconds (3.19 M allocations: 190.965 MB, 2.67% gc time)\n",
      "Pro % correct = 97%\n",
      "Anti % correct = 78.6% \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PyObject <matplotlib.collections.LineCollection object at 0x33aa27a10>"
      ]
     },
     "execution_count": 689,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntrials = 500\n",
    "proVs, antiVs = @time(run_ntrials(ntrials; plot_list=[], model_params...))\n",
    "\n",
    "@printf(\"Pro %% correct = %g%%\\n\", 100*length(find(proVs[1,:].>proVs[4,:]))/ntrials)\n",
    "@printf(\"Anti %% correct = %g%% \\n\", 100*length(find(antiVs[1,:].<antiVs[4,:]))/ntrials)\n",
    "\n",
    "figure(1); clf();\n",
    "ax1 = subplot(2,1,1)\n",
    "h = plt[:hist](proVs[1,:]-proVs[4,:],-0.1:0.002:0.1)\n",
    "title(\"PRO Vr - Vl\")\n",
    "remove_xtick_labels(ax1)\n",
    "vlines(0, ylim()[1], ylim()[2])\n",
    "\n",
    "ax2 = subplot(2,1,2)\n",
    "h = plt[:hist](antiVs[1,:]-antiVs[4,:],-0.1:0.002:0.1)\n",
    "title(\"ANTI Vr - Vl\")\n",
    "vlines(0, ylim()[1], ylim()[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition plot_PA(Any, Any, Any) in module Main at In[222]:2 overwritten at In[228]:2.\n",
      "WARNING: Method definition #plot_PA(Array{Any, 1}, Main.#plot_PA, Any, Any, Any) in module Main overwritten.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "plot_PA (generic function with 2 methods)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function plot_PA(t, U, V; fignum=1, clearfig=true)\n",
    "    figure(fignum)\n",
    "    if clearfig; clf(); end\n",
    "    \n",
    "    ax1 = subplot(3,1,1)\n",
    "    h = plot(t, V'); \n",
    "    setp(h[1], color=[0, 0, 1])\n",
    "    setp(h[2], color=[1, 0, 0])\n",
    "    setp(h[3], color=[1, 0.5, 0.5])\n",
    "    setp(h[4], color=[0, 1, 1])\n",
    "    ylabel(\"V\")\n",
    "\n",
    "    ax = gca()\n",
    "    yl = [ylim()[1], ylim()[2]]\n",
    "    vlines([rule_and_delay_period, \n",
    "            rule_and_delay_period+target_period,\n",
    "            rule_and_delay_period+target_period+post_target_period], \n",
    "            -0.05, 1.05, linewidth=2)\n",
    "    if yl[1]<0.02\n",
    "        yl[1] = -0.02\n",
    "    end\n",
    "    if yl[2]>0.98\n",
    "        yl[2] = 1.02\n",
    "    end\n",
    "    ylim(yl)\n",
    "    grid(true)\n",
    "    remove_xtick_labels(ax1)\n",
    "        \n",
    "    ax2 = subplot(3,1,2)\n",
    "    hu = plot(t, U')\n",
    "    setp(hu[1], color=[0, 0, 1])\n",
    "    setp(hu[2], color=[1, 0, 0])\n",
    "    setp(hu[3], color=[1, 0.5, 0.5])\n",
    "    setp(hu[4], color=[0, 1, 1])\n",
    "    ylabel(\"U\"); ylim(minimum(U[:])-0.1, maximum(U[:])+0.1)\n",
    "    vlines([rule_and_delay_period, \n",
    "            rule_and_delay_period+target_period,\n",
    "            rule_and_delay_period+target_period+post_target_period], \n",
    "            ylim()[1], ylim()[2], linewidth=2)\n",
    "    remove_xtick_labels(ax2)\n",
    "\n",
    "    grid(true)\n",
    "    \n",
    "    subplot(3,1,3)\n",
    "    delta = V[1,:] - V[4,:]\n",
    "    hr = plot(t, delta)\n",
    "    ylim(minimum(delta[:])-0.1, maximum(delta[:])+0.1)\n",
    "    vlines([rule_and_delay_period, \n",
    "            rule_and_delay_period+target_period,\n",
    "            rule_and_delay_period+target_period+post_target_period], \n",
    "            ylim()[1], ylim()[2], linewidth=2)\n",
    "    xlabel(\"t\"); ylabel(\"Pro R - Pro L\")\n",
    "    grid(true)\n",
    "        \n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt = 0.0002\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "W = -4\n",
    "noise = 0\n",
    "sigma = 0.2\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    "    :noise=>0, :input=>0, :const_add=>0, :init_add=>0, :sigma=>sigma)\n",
    "\n",
    "clf()\n",
    "srand(20)\n",
    "startUs = randn(20,2)-3\n",
    "startUs = [-2 -4.1]\n",
    "\n",
    "for i=1:size(startUs,1)\n",
    "    forwardModel(startUs[i,:]; do_plot=true, clearfig=false, model_params...)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typeof(model_params[:input])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO-DOs\n",
    "\n",
    "1. ~~Be able to use W as an optimizable parameter (including configs like \"all horizontal weights are the same\")~~ DONE!\n",
    "2. ~~Check out what is going on with the weird trajectories in the function-based MGO example~~  DONE: it's just the strong, single-timestep initial_add\n",
    "3. ~~Check out whether reducing beta solves the sticking issue even without extra finalFluxPoint locations~~. It does. Reducing beta from 0.01 to 0.003 was enough.  (We also needed dto change the cost_limit to -0.00288, since the range of costs changes when beta changes.)\n",
    "3. Find the saddle points and use those as the finalFluxPoint locations\n",
    "4. ===\n",
    "5. ~~Run a ProAnti model with noise only in initial conditions, and thus with the framework as we have it~~ (skipped, went straight to next step)\n",
    "6. ~~Make a cost function with frozen noise, and figure out how frozen noise will interact with the backwards trajectory in the minimizations~~\n",
    "7. ~~Make a forwards and backwards model with Urest, etc., just like in ProAnti()~~\n",
    "6. ===\n",
    "7. Clean up examples of forward and backwards models and of 1-d use of fluxSense() function\n",
    "8. Fine a 2-d example where flux points are actually needed -- when beta=0, it is not so clear.\n",
    "9. Fix the walls issue in bbox_Hessian_minimization using tanh encoding.\n",
    "8. Measure gradient sensitivity to each of the endpoints in a set of trajectories, as a measure of whether fluxSense is needed or not.\n",
    "9. Optimize either an MGO or a ProAnti\n",
    "10. If fluxSense is needed in ProAnti, could try choosing the Anti unit endpoint values by maximizing the |dJ/dw|^2 over those values.\n",
    "11. Clean up the notebooks and write up what we've been doing!\n",
    "12. Try to combinee fluxSense with bbox_Hessian_minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beginning of attempt at finding saddle points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.069567 seconds (34.34 k allocations: 1.826 MB, 8.38% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.740907,-0.740907],1.7094696491279047e-7)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = 0.02\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "W = -4\n",
    "noise = 0\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    "    :noise=>noise, :noise=>noise, :const_add=>0, :init_add=>0)\n",
    "\n",
    "@time(trust_region_Hessian_minimization([-2.1, -2.1], \n",
    "    (x)->forwardModel(x; do_plot=false, nderivs=2, difforder=1, dUdt_mag_only=true, model_params...), \n",
    "verbose=false, start_eta=0.1, tol=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example doing a successful minimization of a 2d model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function JJ(initUs; theta1=0.15, theta2=0.2, beta=0.003, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, pre_string=\"\", zero_last_sigmas=0, seedrand=NaN, params...)\n",
    "\n",
    "    if ~isnan(seedrand); srand(seedrand); end\n",
    "    \n",
    "    Vend = ForwardDiffZeros(size(initUs,1), size(initUs,2), nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:size(initUs,1)\n",
    "        if i>size(initUs,1) - zero_last_sigmas\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; sigma=0, nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)            \n",
    "        else\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)\n",
    "        end\n",
    "        Vend[i,:] = Ve\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend[:,1]-Vend[:,2])/theta1))\n",
    "    diffs = tanh.((Vend[:,1]-Vend[:,2])/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"%s\", pre_string)\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1070,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- cost=0.0775991,   cost1=0.0775991, cost2=0 :  mean(hits)=0.471434, mean(diffs)=0.745863\n",
      "0: eta=0.01 ps=[0.001, 0.001, 0.000, -4.000]\n",
      "-- cost=0.0775991,   cost1=0.0775991, cost2=0 :  mean(hits)=0.471434, mean(diffs)=0.745863\n",
      "-- cost=0.0774,   cost1=0.0774, cost2=0 :  mean(hits)=0.471791, mean(diffs)=0.758408\n",
      "1: eta=0.011 cost=0.0774 jtype=constrained costheta=-0.997 ps=[0.005, -0.003, 0.008, -4.002]\n",
      "-- cost=0.0772065,   cost1=0.0772065, cost2=0 :  mean(hits)=0.472139, mean(diffs)=0.77071\n",
      "2: eta=0.0121 cost=0.0772 jtype=constrained costheta=-0.995 ps=[0.010, -0.008, 0.016, -4.004]\n",
      "-- cost=0.0770193,   cost1=0.0770193, cost2=0 :  mean(hits)=0.472476, mean(diffs)=0.782257\n",
      "3: eta=0.01331 cost=0.0770 jtype=constrained costheta=-0.992 ps=[0.016, -0.014, 0.024, -4.007]\n",
      "-- cost=0.076836,   cost1=0.076836, cost2=0 :  mean(hits)=0.472807, mean(diffs)=0.792585\n",
      "4: eta=0.014641 cost=0.0768 jtype=constrained costheta=-0.991 ps=[0.024, -0.022, 0.031, -4.009]\n",
      "-- cost=0.0766514,   cost1=0.0766514, cost2=0 :  mean(hits)=0.47314, mean(diffs)=0.801494\n",
      "5: eta=0.0161051 cost=0.0767 jtype=constrained costheta=-0.991 ps=[0.033, -0.031, 0.038, -4.011]\n",
      "-- cost=0.0764598,   cost1=0.0764598, cost2=0 :  mean(hits)=0.473486, mean(diffs)=0.809084\n",
      "6: eta=0.0177156 cost=0.0765 jtype=constrained costheta=-0.994 ps=[0.044, -0.041, 0.044, -4.014]\n",
      "-- cost=0.0762559,   cost1=0.0762559, cost2=0 :  mean(hits)=0.473855, mean(diffs)=0.815642\n",
      "7: eta=0.0194872 cost=0.0763 jtype=constrained costheta=-0.996 ps=[0.056, -0.053, 0.049, -4.016]\n",
      "-- cost=0.0760351,   cost1=0.0760351, cost2=0 :  mean(hits)=0.474255, mean(diffs)=0.821526\n",
      "8: eta=0.0214359 cost=0.0760 jtype=constrained costheta=-0.998 ps=[0.070, -0.066, 0.053, -4.018]\n",
      "-- cost=0.0757935,   cost1=0.0757935, cost2=0 :  mean(hits)=0.474694, mean(diffs)=0.827126\n",
      "9: eta=0.0235795 cost=0.0758 jtype=constrained costheta=-0.999 ps=[0.085, -0.080, 0.058, -4.021]\n",
      "-- cost=0.0755273,   cost1=0.0755273, cost2=0 :  mean(hits)=0.475178, mean(diffs)=0.832907\n",
      "10: eta=0.0259374 cost=0.0755 jtype=constrained costheta=-1.000 ps=[0.101, -0.096, 0.063, -4.023]\n",
      "-- cost=0.0752318,   cost1=0.0752318, cost2=0 :  mean(hits)=0.475716, mean(diffs)=0.839544\n",
      "11: eta=0.0285312 cost=0.0752 jtype=constrained costheta=-1.000 ps=[0.119, -0.114, 0.069, -4.026]\n",
      "-- cost=0.0748998,   cost1=0.0748998, cost2=0 :  mean(hits)=0.476322, mean(diffs)=0.848299\n",
      "12: eta=0.0313843 cost=0.0749 jtype=constrained costheta=-0.999 ps=[0.138, -0.133, 0.077, -4.030]\n",
      "-- cost=0.0745134,   cost1=0.0745134, cost2=0 :  mean(hits)=0.477029, mean(diffs)=0.86197\n",
      "13: eta=0.0345227 cost=0.0745 jtype=constrained costheta=-0.991 ps=[0.158, -0.152, 0.090, -4.036]\n",
      "-- cost=0.0740082,   cost1=0.0740082, cost2=0 :  mean(hits)=0.477956, mean(diffs)=0.883927\n",
      "14: eta=0.037975 cost=0.0740 jtype=constrained costheta=-0.959 ps=[0.176, -0.168, 0.114, -4.044]\n",
      "-- cost=0.0732873,   cost1=0.0732873, cost2=0 :  mean(hits)=0.479284, mean(diffs)=0.909826\n",
      "15: eta=0.0417725 cost=0.0733 jtype=constrained costheta=-0.969 ps=[0.188, -0.180, 0.146, -4.053]\n",
      "-- cost=0.0724796,   cost1=0.0724796, cost2=0 :  mean(hits)=0.48078, mean(diffs)=0.933167\n",
      "16: eta=0.0459497 cost=0.0725 jtype=constrained costheta=-0.997 ps=[0.201, -0.193, 0.183, -4.063]\n",
      "-- cost=0.0717411,   cost1=0.0717411, cost2=0 :  mean(hits)=0.482155, mean(diffs)=0.949866\n",
      "17: eta=0.0505447 cost=0.0717 jtype=constrained costheta=-0.997 ps=[0.220, -0.211, 0.219, -4.074]\n",
      "-- cost=0.0709163,   cost1=0.0709163, cost2=0 :  mean(hits)=0.483699, mean(diffs)=0.962218\n",
      "18: eta=0.0555992 cost=0.0709 jtype=constrained costheta=-0.999 ps=[0.238, -0.229, 0.261, -4.086]\n",
      "-- cost=0.0696455,   cost1=0.0696455, cost2=0 :  mean(hits)=0.486096, mean(diffs)=0.970828\n",
      "19: eta=0.0611591 cost=0.0696 jtype=constrained costheta=-0.995 ps=[0.253, -0.243, 0.311, -4.101]\n",
      "-- cost=0.0675741,   cost1=0.0675741, cost2=0 :  mean(hits)=0.49005, mean(diffs)=0.977266\n",
      "20: eta=0.067275 cost=0.0676 jtype=constrained costheta=-0.998 ps=[0.265, -0.254, 0.367, -4.117]\n",
      "-- cost=0.0639902,   cost1=0.0639902, cost2=0 :  mean(hits)=0.497037, mean(diffs)=0.983974\n",
      "21: eta=0.0740025 cost=0.0640 jtype=constrained costheta=-1.000 ps=[0.278, -0.266, 0.429, -4.136]\n",
      "-- cost=0.0625633,   cost1=0.0625633, cost2=0 :  mean(hits)=0.499873, mean(diffs)=0.994626\n",
      "22: eta=0.0814027 cost=0.0626 jtype=constrained costheta=-0.775 ps=[0.255, -0.245, 0.491, -4.163]\n",
      "-- cost=0.0625022,   cost1=0.0625022, cost2=0 :  mean(hits)=0.499996, mean(diffs)=0.997263\n",
      "23: eta=0.089543 cost=0.0625 jtype=constrained costheta=-1.000 ps=[0.268, -0.256, 0.566, -4.187]\n",
      "-- cost=0.0625008,   cost1=0.0625008, cost2=0 :  mean(hits)=0.499998, mean(diffs)=0.997419\n",
      "24: eta=0.0984973 cost=0.0625 jtype=constrained costheta=-0.345 ps=[0.331, -0.319, 0.578, -4.179]\n",
      "-- cost=0.0625006,   cost1=0.0625006, cost2=0 :  mean(hits)=0.499999, mean(diffs)=0.997986\n",
      "25: eta=0.108347 cost=0.0625 jtype=constrained costheta=-0.106 ps=[0.272, -0.263, 0.635, -4.172]\n",
      "-- cost=0.0625001,   cost1=0.0625001, cost2=0 :  mean(hits)=0.5, mean(diffs)=0.998634\n",
      "26: eta=0.119182 cost=0.0625 jtype=constrained costheta=-0.980 ps=[0.288, -0.278, 0.737, -4.205]\n",
      "-- cost=0.0625,   cost1=0.0625, cost2=0 :  mean(hits)=0.5, mean(diffs)=0.998629\n",
      "27: eta=0.1311 cost=0.0625 jtype=constrained costheta=-0.622 ps=[0.371, -0.361, 0.738, -4.189]\n",
      "-- cost=0.0624992,   cost1=0.0624992, cost2=0 :  mean(hits)=0.500002, mean(diffs)=0.998268\n",
      "28: eta=0.14421 cost=0.0625 jtype=constrained costheta=-0.955 ps=[0.453, -0.443, 0.681, -4.167]\n",
      "-- cost=0.0624893,   cost1=0.0624893, cost2=0 :  mean(hits)=0.500021, mean(diffs)=0.997383\n",
      "29: eta=0.158631 cost=0.0625 jtype=constrained costheta=-1.000 ps=[0.543, -0.532, 0.613, -4.148]\n",
      "-- cost=0.0618381,   cost1=0.0618381, cost2=0 :  mean(hits)=0.501327, mean(diffs)=0.989264\n",
      "30: eta=0.174494 cost=0.0618 jtype=constrained costheta=-0.984 ps=[0.654, -0.643, 0.596, -4.146]\n",
      "-- cost=0.0529052,   cost1=0.0529052, cost2=0 :  mean(hits)=0.519989, mean(diffs)=0.998189\n",
      "31: eta=0.191943 cost=0.0529 jtype=constrained costheta=-0.992 ps=[0.754, -0.742, 0.694, -4.182]\n",
      "-- cost=0.0529076,   cost1=0.0529076, cost2=0 :  mean(hits)=0.519983, mean(diffs)=0.997449\n",
      "32: eta=0.0959717 cost=0.0529 jtype=constrained costheta=NaN ps=[0.754, -0.742, 0.694, -4.182]\n",
      "-- cost=0.052904,   cost1=0.052904, cost2=0 :  mean(hits)=0.519991, mean(diffs)=0.998021\n",
      "33: eta=0.105569 cost=0.0529 jtype=constrained costheta=-0.162 ps=[0.817, -0.804, 0.658, -4.176]\n",
      "-- cost=0.0529021,   cost1=0.0529021, cost2=0 :  mean(hits)=0.519995, mean(diffs)=0.998478\n",
      "34: eta=0.116126 cost=0.0529 jtype=constrained costheta=-0.280 ps=[0.759, -0.747, 0.721, -4.199]\n",
      "-- cost=0.0529005,   cost1=0.0529005, cost2=0 :  mean(hits)=0.519999, mean(diffs)=0.998929\n",
      "35: eta=0.127738 cost=0.0529 jtype=constrained costheta=-0.996 ps=[0.804, -0.791, 0.813, -4.232]\n",
      "-- cost=0.0529027,   cost1=0.0529027, cost2=0 :  mean(hits)=0.519994, mean(diffs)=0.99831\n",
      "36: eta=0.0638692 cost=0.0529 jtype=constrained costheta=NaN ps=[0.804, -0.791, 0.813, -4.232]\n",
      "-- cost=0.0529004,   cost1=0.0529004, cost2=0 :  mean(hits)=0.519999, mean(diffs)=0.999106\n",
      "37: eta=0.0702561 cost=0.0529 jtype=constrained costheta=-0.957 ps=[0.803, -0.790, 0.876, -4.235]\n",
      "-- cost=0.0529003,   cost1=0.0529003, cost2=0 :  mean(hits)=0.519999, mean(diffs)=0.999252\n",
      "38: eta=0.0772817 cost=0.0529 jtype=constrained costheta=-0.987 ps=[0.806, -0.793, 0.945, -4.239]\n",
      "-- cost=0.0529002,   cost1=0.0529002, cost2=0 :  mean(hits)=0.52, mean(diffs)=0.999371\n",
      "39: eta=0.0850099 cost=0.0529 jtype=constrained costheta=-0.996 ps=[0.809, -0.796, 1.022, -4.246]\n",
      "-- cost=0.0529001,   cost1=0.0529001, cost2=0 :  mean(hits)=0.52, mean(diffs)=0.999467\n",
      "40: eta=0.0935109 cost=0.0529 jtype=constrained costheta=-0.999 ps=[0.815, -0.801, 1.106, -4.253]\n",
      "-- cost=0.0529001,   cost1=0.0529001, cost2=0 :  mean(hits)=0.52, mean(diffs)=0.999545\n",
      "41: eta=0.102862 cost=0.0529 jtype=constrained costheta=-1.000 ps=[0.822, -0.809, 1.198, -4.263]\n",
      "-- cost=0.0529001,   cost1=0.0529001, cost2=0 :  mean(hits)=0.52, mean(diffs)=0.999605\n",
      "42: eta=0.113148 cost=0.0529 jtype=constrained costheta=-0.998 ps=[0.836, -0.823, 1.297, -4.275]\n",
      "-- cost=0.0529,   cost1=0.0529, cost2=0 :  mean(hits)=0.52, mean(diffs)=0.999643\n",
      "43: eta=0.124463 cost=0.0529 jtype=constrained costheta=-0.932 ps=[0.880, -0.866, 1.390, -4.291]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999664\n",
      "44: eta=0.136909 cost=0.0484 jtype=constrained costheta=-1.000 ps=[0.961, -0.946, 1.435, -4.318]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999702\n",
      "45: eta=0.1506 cost=0.0484 jtype=constrained costheta=-0.978 ps=[0.951, -0.936, 1.568, -4.344]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999729\n",
      "46: eta=0.16566 cost=0.0484 jtype=constrained costheta=-0.993 ps=[0.949, -0.934, 1.711, -4.390]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999698\n",
      "47: eta=0.0828301 cost=0.0484 jtype=constrained costheta=NaN ps=[0.949, -0.934, 1.711, -4.390]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999741\n",
      "48: eta=0.0911131 cost=0.0484 jtype=constrained costheta=-0.998 ps=[0.951, -0.936, 1.788, -4.420]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.99975\n",
      "49: eta=0.100224 cost=0.0484 jtype=constrained costheta=-0.996 ps=[0.952, -0.937, 1.870, -4.459]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999759\n",
      "50: eta=0.110247 cost=0.0484 jtype=constrained costheta=-0.993 ps=[0.954, -0.939, 1.955, -4.511]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999767\n",
      "51: eta=0.121272 cost=0.0484 jtype=constrained costheta=-0.991 ps=[0.956, -0.940, 2.042, -4.578]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999758\n",
      "52: eta=0.0606358 cost=0.0484 jtype=constrained costheta=NaN ps=[0.956, -0.940, 2.042, -4.578]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999763\n",
      "53: eta=0.0303179 cost=0.0484 jtype=constrained costheta=NaN ps=[0.956, -0.940, 2.042, -4.578]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999768\n",
      "54: eta=0.0333497 cost=0.0484 jtype=constrained costheta=-0.999 ps=[0.956, -0.941, 2.065, -4.598]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.99977\n",
      "55: eta=0.0366846 cost=0.0484 jtype=constrained costheta=-0.999 ps=[0.956, -0.941, 2.090, -4.620]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999772\n",
      "56: eta=0.0403531 cost=0.0484 jtype=constrained costheta=-0.999 ps=[0.957, -0.942, 2.117, -4.645]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.99977\n",
      "57: eta=0.0201766 cost=0.0484 jtype=constrained costheta=NaN ps=[0.957, -0.942, 2.117, -4.645]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999773\n",
      "58: eta=0.0221942 cost=0.0484 jtype=constrained costheta=-1.000 ps=[0.957, -0.942, 2.131, -4.659]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999774\n",
      "59: eta=0.0244136 cost=0.0484 jtype=constrained costheta=-1.000 ps=[0.958, -0.942, 2.147, -4.675]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999776\n",
      "60: eta=0.026855 cost=0.0484 jtype=constrained costheta=-1.000 ps=[0.958, -0.942, 2.163, -4.693]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999777\n",
      "61: eta=0.0295405 cost=0.0484 jtype=constrained costheta=-1.000 ps=[0.958, -0.943, 2.181, -4.713]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999778\n",
      "62: eta=0.0324945 cost=0.0484 jtype=constrained costheta=-1.000 ps=[0.958, -0.943, 2.200, -4.736]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999777\n",
      "63: eta=0.0162473 cost=0.0484 jtype=constrained costheta=NaN ps=[0.958, -0.943, 2.200, -4.736]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999779\n",
      "64: eta=0.017872 cost=0.0484 jtype=constrained costheta=-1.000 ps=[0.959, -0.943, 2.210, -4.748]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.99978\n",
      "65: eta=0.0196592 cost=0.0484 jtype=constrained costheta=-1.000 ps=[0.959, -0.943, 2.222, -4.762]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.99978\n",
      "66: eta=0.0216251 cost=0.0484 jtype=constrained costheta=-1.000 ps=[0.959, -0.944, 2.234, -4.778]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999781\n",
      "67: eta=0.0237876 cost=0.0484 jtype=constrained costheta=-1.000 ps=[0.959, -0.944, 2.247, -4.795]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999782\n",
      "68: eta=0.0261664 cost=0.0484 jtype=constrained costheta=-1.000 ps=[0.960, -0.944, 2.261, -4.814]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999783\n",
      "69: eta=0.028783 cost=0.0484 jtype=constrained costheta=-1.000 ps=[0.960, -0.944, 2.276, -4.836]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999782\n",
      "70: eta=0.0143915 cost=0.0484 jtype=constrained costheta=NaN ps=[0.960, -0.944, 2.276, -4.836]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999784\n",
      "71: eta=0.0158307 cost=0.0484 jtype=constrained costheta=-1.000 ps=[0.960, -0.945, 2.284, -4.848]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999784\n",
      "72: eta=0.0174137 cost=0.0484 jtype=constrained costheta=-1.000 ps=[0.960, -0.945, 2.293, -4.861]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999785\n",
      "73: eta=0.0191551 cost=0.0484 jtype=constrained costheta=-1.000 ps=[0.960, -0.945, 2.303, -4.875]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999785\n",
      "74: eta=0.0210706 cost=0.0484 jtype=constrained costheta=-1.000 ps=[0.960, -0.945, 2.313, -4.891]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999786\n",
      "75: eta=0.0231777 cost=0.0484 jtype=constrained costheta=-1.000 ps=[0.961, -0.945, 2.324, -4.909]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999787\n",
      "76: eta=0.0254954 cost=0.0484 jtype=constrained costheta=-1.000 ps=[0.961, -0.945, 2.337, -4.928]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999786\n",
      "77: eta=0.0127477 cost=0.0484 jtype=constrained costheta=NaN ps=[0.961, -0.945, 2.337, -4.928]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999787\n",
      "78: eta=0.0140225 cost=0.0484 jtype=constrained costheta=-1.000 ps=[0.961, -0.946, 2.344, -4.939]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999788\n",
      "79: eta=0.0154247 cost=0.0484 jtype=constrained costheta=-1.000 ps=[0.961, -0.946, 2.351, -4.951]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999788\n",
      "80: eta=0.0169672 cost=0.0484 jtype=constrained costheta=-1.000 ps=[0.961, -0.946, 2.359, -4.964]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999789\n",
      "81: eta=0.0186639 cost=0.0484 jtype=constrained costheta=-1.000 ps=[0.961, -0.946, 2.368, -4.979]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999789\n",
      "82: eta=0.0205303 cost=0.0484 jtype=constrained costheta=-1.000 ps=[0.961, -0.946, 2.378, -4.995]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.99979\n",
      "83: eta=0.0225834 cost=0.0484 jtype=constrained costheta=-1.000 ps=[0.962, -0.946, 2.388, -5.012]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999789\n",
      "84: eta=0.0112917 cost=0.0484 jtype=constrained costheta=NaN ps=[0.962, -0.946, 2.388, -5.012]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.99979\n",
      "85: eta=0.0124209 cost=0.0484 jtype=constrained costheta=-1.000 ps=[0.962, -0.946, 2.394, -5.022]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999791\n",
      "86: eta=0.0136629 cost=0.0484 jtype=constrained costheta=-1.000 ps=[0.962, -0.946, 2.400, -5.033]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999791\n",
      "87: eta=0.0150292 cost=0.0484 jtype=constrained costheta=-1.000 ps=[0.962, -0.946, 2.407, -5.045]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999791\n",
      "88: eta=0.0165322 cost=0.0484 jtype=constrained costheta=-1.000 ps=[0.962, -0.947, 2.414, -5.058]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999792\n",
      "89: eta=0.0181854 cost=0.0484 jtype=constrained costheta=-1.000 ps=[0.962, -0.947, 2.423, -5.072]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999792\n",
      "90: eta=0.0200039 cost=0.0484 jtype=constrained costheta=-1.000 ps=[0.962, -0.947, 2.432, -5.088]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999792\n",
      "91: eta=0.010002 cost=0.0484 jtype=constrained costheta=NaN ps=[0.962, -0.947, 2.432, -5.088]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999793\n",
      "92: eta=0.0110021 cost=0.0484 jtype=constrained costheta=-1.000 ps=[0.962, -0.947, 2.437, -5.097]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999793\n",
      "93: eta=0.0121024 cost=0.0484 jtype=constrained costheta=-1.000 ps=[0.962, -0.947, 2.442, -5.106]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999793\n",
      "94: eta=0.0133126 cost=0.0484 jtype=constrained costheta=-1.000 ps=[0.962, -0.947, 2.448, -5.117]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999794\n",
      "95: eta=0.0146439 cost=0.0484 jtype=constrained costheta=-1.000 ps=[0.963, -0.947, 2.454, -5.128]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999794\n",
      "96: eta=0.0161082 cost=0.0484 jtype=constrained costheta=-1.000 ps=[0.963, -0.947, 2.461, -5.141]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999794\n",
      "97: eta=0.0177191 cost=0.0484 jtype=constrained costheta=-1.000 ps=[0.963, -0.947, 2.469, -5.155]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999794\n",
      "98: eta=0.00885954 cost=0.0484 jtype=constrained costheta=NaN ps=[0.963, -0.947, 2.469, -5.155]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999794\n",
      "99: eta=0.00974549 cost=0.0484 jtype=constrained costheta=-1.000 ps=[0.963, -0.947, 2.473, -5.163]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999795\n",
      "100: eta=0.01072 cost=0.0484 jtype=constrained costheta=-1.000 ps=[0.963, -0.947, 2.478, -5.172]\n",
      "-- cost=0.0484001,   cost1=0.0484001, cost2=0 :  mean(hits)=0.53, mean(diffs)=0.999795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0484000509064604"
      ]
     },
     "execution_count": 1070,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following sequence leads to a situation where having only [-0.8, -0.8] as the single finalFluxPoint \n",
    "# leads to the minimization getting stuck.  Adding further finalFluxPoints solves the problem\n",
    "#\n",
    "srand(12)\n",
    "startU=randn(100,2)-3\n",
    "startU=randn(100,2)-3\n",
    "sigma = 0\n",
    "\n",
    "# startU=0.1*randn(100,2)-3\n",
    "# startU=zeros(100,2)-3\n",
    "\n",
    "\n",
    "dt = 0.005\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "W = -4\n",
    "noise = 0\n",
    "input = 0\n",
    "sigma = 0.1\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    ":noise=>noise, :input=>input, :sigma=>sigma, :const_add=>0, :init_add=>0)\n",
    "\n",
    "\n",
    "# WORKING gradient:\n",
    "# ForwardDiff.gradient((x)->JJ(startU; do_plot=true, nderivs=length(x), difforder=1, \n",
    "#    make_dict([[\"init_add\" 2], \"const_add\"], x, model_params)...), [2.9, -2.9, 0.1])\n",
    "\n",
    "\n",
    "\n",
    "# The backward and costfunc functions should turn a single-scalar parameter W into the matrix W\n",
    "# backward always runs with no within-forward noise, i.e., sigma=0\n",
    "backward = (endpoint; do_plot=false, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;     \n",
    "    backwardsModel(endpoint; do_plot=do_plot, make_dict([\"sigma\"], [0], pars)...)[1]\n",
    "end\n",
    "\n",
    "\n",
    "beta = 0.0001;\n",
    "beta = 0.003;\n",
    "beta=0\n",
    "\n",
    "costfunc = (startpoints; do_plot=false, verbose=false, nderivs=0, difforder=0, sr=26, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;         \n",
    "    JJ(startpoints; seedrand=sr, beta=beta, \n",
    "        do_plot=do_plot, verbose=verbose, nderivs=nderivs, difforder=difforder, pars...)\n",
    "end\n",
    "\n",
    "if beta==0.003;     cost_limit = -0.00288\n",
    "elseif beta<0.001;  cost_limit = -0.0008\n",
    "elseif beta==0.001; cost_limit = -0.000935\n",
    "elseif beta==0.05;  cost_limit = -0.0485\n",
    "else\n",
    "    error(\"Don't know what cost limit goes with beta %g\\n\", beta)\n",
    "end\n",
    "\n",
    "fluxFinalPoint = [-0.8 -0.8; -0.6 -0.6 ; -0.4 -0.4; -0.2 -0.2; 0 0; 0.2 0.2]\n",
    "fluxFinalPoint = zeros(0,2);\n",
    "\n",
    "args = [[\"init_add\" 2], \"const_add\", \"W\"] #, \"sigma\"]\n",
    "seed = [0.001, 0.001, 0, -4]# , 0.1]\n",
    "# seed = [4.09031,  -3.87551,  3.1997,  -3.08479,  0.437131]\n",
    "# seed = [1.190, -1.178, 2.000]\n",
    "# seed = zparams[1:4]\n",
    "# seed = [0.001, 0.001, 0, -4]# , 0.1]\n",
    "\n",
    "# sticking_point_with_beta_equal_0 = [4.74063,  -4.68228,  2.73165,  -5.6783]\n",
    "# seed = sticking_point_with_beta_equal_0\n",
    "\n",
    "bbox = [\n",
    "    -15        15  ;\n",
    "    -15        15  ;\n",
    "    -15        15  ;\n",
    "    -20.5  20.5  ; \n",
    "#    -0.5        0.5  ;\n",
    "]\n",
    "\n",
    "# clf(); subplot(2,1,1)\n",
    "# for i in 1:size(startU,1)\n",
    "#    Uend, Vend, U, V = forwardModel(startU[i,:]; do_plot=true, clearfig=false, model_params...)\n",
    "# end\n",
    "\n",
    "# Ustarthat, Vstarthat, Uhatm, Vhat, costs = backwardsModel([-0.8, -0.8]; do_plot=true, clearfig=false, \n",
    "#    tol=1e-50, maxiter=800, make_dict([\"sigma\"], [0], model_params)...)\n",
    "\n",
    "# subplot(2,1,2)\n",
    "# plot(t, costs, \".-\")\n",
    "\n",
    "\n",
    "\n",
    "clf()\n",
    "costfunc(startU; do_plot=true, verbose=true, make_dict(args, seed, model_params)...)\n",
    "\n",
    "\n",
    "params, traj = bbox_Hessian_keyword_minimization(seed, args, bbox, \n",
    "(;params...) -> costfunc(startU; verbose=true, merge(model_params, Dict(params))...), \n",
    "verbose=true, start_eta=0.01, maxiter=100, tol=1e-15, hardbox=true )\n",
    "\n",
    "# params, cost, ptraj, gtraj = fluxSense(costfunc, backward, model_params, startU, fluxFinalPoint, args, seed; \n",
    "#    start_eta=0.01, tol=1e-15, maxiter=400, verbose=true, report_every=5, do_plot=false, cost_limit=cost_limit) # cost_limit=-0.000935) # for beta=0.01\n",
    "\n",
    "# And show the final position\n",
    "clf()\n",
    "costfunc(startU; do_plot=true, verbose=true, make_dict(args, params, model_params)...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       "  4.74063\n",
       " -4.68228\n",
       "  2.73165\n",
       " -5.6783 "
      ]
     },
     "execution_count": 1052,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zzparams = params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1015,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- cost=0.0608404,   cost1=0.0636952, cost2=-0.00285484 :  mean(hits)=0.497621, mean(diffs)=0.951613\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.060840398979963996"
      ]
     },
     "execution_count": 1015,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf()\n",
    "costfunc(startU; do_plot=true, verbose=true, \n",
    "merge(make_dict(args, params, model_params), Dict(:dt=>0.005, :nsteps=>201))...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1008,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Symbol,Any} with 9 entries:\n",
       "  :input     => 0\n",
       "  :noise     => 0\n",
       "  :tau       => 0.1\n",
       "  :const_add => 0\n",
       "  :W         => [0 -4; -4 0]\n",
       "  :nsteps    => 51\n",
       "  :sigma     => 0\n",
       "  :init_add  => 0\n",
       "  :dt        => 0.02"
      ]
     },
     "execution_count": 1008,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 994,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure(2);\n",
    "clf()\n",
    "subplot(2,1,1); plot(cost', \".\")\n",
    "subplot(2,1,2); \n",
    "guys = 4:5\n",
    "ng = sqrt(sum(gtraj[guys,:].*gtraj[guys,:],1))\n",
    "plot(sum(gtraj[guys,1:end-1].*gtraj[guys,2:end],1)'./(ng[1:end-1].*ng[2:end]), \".\")\n",
    "grid(true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hessian_fluxSense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "function bbox_Hessian_keyword_minimization(seed, args, bbox, func; wallwidth=NaN, start_eta=10, tol=1e-6, \n",
    "    maxiter=400, verbose=false)\n",
    "\n",
    "Like constrained_Hessian_minimization, but uses keyword_hessian!(). \n",
    "\n",
    "PARAMETERS:\n",
    "===========\n",
    "\n",
    "seed        column vector, representing the starting value of the parameters.\n",
    "\n",
    "args        List of strings identifying parameters for differentiation, e.g., [\"const_E\", \"w_self]\n",
    "\n",
    "bbox        An nargs-by-2 matrix indicating the range for each argument,\n",
    "            with the minima (first column) and maxima (second column).\n",
    "\n",
    "func        func must take only optional keyword args, and must \n",
    "            take nderivs=0, difforder=0  and declare any new matrices using ForwardDiffZeros() instead of zeros()\n",
    "\n",
    "\n",
    "OPTIONAL PARAMETERS:\n",
    "====================\n",
    "\n",
    "start_eta=10 Starting value of the radius.  It's good to start with somethibg biggish, if it is\n",
    "             too much, it'll quickly get cut down.\n",
    "\n",
    "tol=1e-6     Numerical tolerance. If a proposed jump produces a change in func that is less than\n",
    "             this, the minimization stops.\n",
    "\n",
    "maxiter=400  Maximum number of iterations to do before stopping\n",
    "\n",
    "verbose=false   If true, print out a report on each iteration of iteration number, radius size (eta),\n",
    "                what type jump was proposed (\"Newton\" means going straight to global min, \"constrained\" means jump has \n",
    "                norm eta, failed means that finding the minimum at a given radius somehow didn't work). Will also\n",
    "                print out the cosine of the angle between the proposed jump and the gradient.\n",
    "\n",
    "hardbox=false   If true, ignores wallwidth, and just rests parameter values to the bounding box if they go outside it.\n",
    "                If false, adds cost function \"walls\" to implement the bounding box.\n",
    "\n",
    "walldith=NaN     Used for putting up cost function \"walls\" that implement the bounding box limits. Can be NaN.\n",
    "                If it is NaN, then the wallwidth is a constant factor of the range width for each argument. If not NaN, must\n",
    "                be an nargs-long vector that indicates the actual wall widths.\n",
    "\n",
    "wallwidth_factor=0.18   Only relevant if wallwidth is NaN, otherwise ignored. For each arg, the wall width\n",
    "                is going to be wall_width_factor*(bbox[i,2] - bbox[i,1])\n",
    "\n",
    "\n",
    "RETURNS:\n",
    "========\n",
    "\n",
    "params       A vector the size of seed that has the last values of the minimizing parameters for func\n",
    "trajectory   A (2+length(params))-by-nsteps matrix. Each column corresponds to an iteration step, and contains\n",
    "                 the value of eta used, the cost, and the value of the parameters at that iteration\n",
    "cost         Final value of objective function\n",
    "\n",
    "\n",
    "EXAMPLE:\n",
    "========\n",
    "\n",
    "function tester(;x=5, y=10, z=20, nderivs=0, difforder=0)\n",
    "    return x^2*y + z/tanh(y)\n",
    "end\n",
    "\n",
    "params, trajectory = bbox_Hessian_keyword_minimization([0.5, 0.5], [\"x\", \"y\"], [1.1 2 ; 1.1 4], tester, \n",
    "    verbose=true, tol=1e-12, start_eta=1);\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# function bbox_Hessian_keyword_minimization(seed, args, bbox, func; \n",
    "    \n",
    "start_eta=10 \n",
    "tol=1e-6 \n",
    "maxiter=400\n",
    "verbose=false\n",
    "verbose_every=1 \n",
    "wallwidth=NaN \n",
    "wallwidth_factor=0.18\n",
    "hardbox=false\n",
    "\n",
    "\n",
    "    \n",
    "    traj_increment = 100\n",
    "    params = seed\n",
    "    eta = start_eta\n",
    "    trajectory = zeros(2+length(params), traj_increment)\n",
    "\n",
    "    if verbose\n",
    "        @printf \"%d: eta=%g ps=\" 0 eta \n",
    "        print_vector_g(params)\n",
    "        @printf \"\\n\"\n",
    "    end\n",
    "    \n",
    "    if hardbox\n",
    "        cost, grad, hess = keyword_vgh((;pars...) -> func(;pars...), args, params)\n",
    "    else\n",
    "        cost, grad, hess = keyword_vgh((;pars...) -> func(;pars...) + wall_cost(args, bbox; wallwidth=wallwidth, pars...),\n",
    "            args, params)        \n",
    "    end\n",
    "        \n",
    "    chessdelta = zeros(size(params))\n",
    "\n",
    "    for i=1:maxiter\n",
    "        if i > size(trajectory, 2)\n",
    "            trajectory = [trajectory zeros(2+length(params), traj_increment)]\n",
    "        end\n",
    "        trajectory[1:2, i]   = [eta;cost]\n",
    "        trajectory[3:end, i] = params\n",
    "        \n",
    "        hessdelta  = - inv(hess)*grad\n",
    "        try\n",
    "            chessdelta = constrained_parabolic_minimization(hess, grad'', eta)[1]\n",
    "            jumptype = \"not failed\"\n",
    "        catch y\n",
    "            jumptype = \"failed\"\n",
    "            if verbose\n",
    "                @printf \"Constrained parabolic minimization failed with error %s\\n\" y\n",
    "                @printf \"\\n\"\n",
    "                @printf \"eta was %g\\n\" eta\n",
    "                @printf \"grad was\\n\"\n",
    "                print_vector(grad)\n",
    "                @printf \"\\n\\nhess was\\n\"\n",
    "                for k in [1:length(grad);]\n",
    "                    print_vector(hess[k,:])\n",
    "                    @printf \"\\n\"\n",
    "                end\n",
    "                @printf \"\\n\"\n",
    "            end\n",
    "        end\n",
    "\n",
    "        if norm(hessdelta) <= eta\n",
    "            new_params = params + hessdelta\n",
    "            jumptype = \"Newton\"\n",
    "        elseif jumptype != \"failed\" \n",
    "            new_params = params + chessdelta\n",
    "            jumptype  = \"constrained\"\n",
    "        end\n",
    "\n",
    "        if jumptype != \"failed\"\n",
    "            if hardbox\n",
    "                for p in [1:length(new_params);]\n",
    "                    if new_params[p] < bbox[p,1]; new_params[p] = bbox[p,1]; end\n",
    "                    if bbox[p,2] < new_params[p]; new_params[p] = bbox[p,2]; end\n",
    "                 end        \n",
    "                \n",
    "                new_cost, new_grad, new_hess = keyword_vgh((;pars...) -> func(;pars...), args, new_params)\n",
    "            else\n",
    "                new_cost, new_grad, new_hess = keyword_vgh((;pars...) -> func(;pars...) + \n",
    "                        wall_cost(args, bbox; wallwidth=wallwidth, pars...),\n",
    "                    args, new_params)                \n",
    "            end\n",
    "            \n",
    "            if abs(new_cost - cost) < tol || eta < tol\n",
    "                trajectory = trajectory[:,1:i]\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "\n",
    "        if jumptype == \"failed\" || new_cost >= cost  \n",
    "            eta = eta/2\n",
    "            costheta = NaN\n",
    "            if eta < tol\n",
    "                trajectory = trajectory[:,1:i]\n",
    "                break\n",
    "            end\n",
    "        else\n",
    "            eta = eta*1.1\n",
    "            costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "\n",
    "            params = new_params\n",
    "            cost = new_cost\n",
    "            grad = new_grad\n",
    "            hess = new_hess\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            if rem(i, verbose_every)==0\n",
    "                @printf \"%d: eta=%g cost=%.4f jtype=%s costheta=%.3f ps=\" i eta cost jumptype costheta\n",
    "                print_vector(params)\n",
    "                @printf \"\\n\"\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return params, trajectory, cost\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 995,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wall_cost"
      ]
     },
     "execution_count": 995,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \"\"\"\n",
    "    Given args, a list of string representing the arguments of interest, a bounding box for each,\n",
    "    and a Symbol=>value dictionary with the corresponding parameters, computes and returns a high cost for \n",
    "    being outside the bounding box\n",
    "    \"\"\"\n",
    "    function wall_cost(args, bbox; wallwidth=NaN, nderivs=0, difforder=0, pars...) \n",
    "        myparams = ForwardDiffZeros(length(pars), 1, nderivs=nderivs, difforder=difforder)\n",
    "        pars2 = Dict()\n",
    "        for i in [1:length(pars);]\n",
    "            pars2[string(pars[i][1])] = pars[i][2]\n",
    "        end\n",
    "        for i in [1:length(args);]\n",
    "            myparams[i] = pars2[args[i]]\n",
    "        end\n",
    "        \n",
    "        if isnan(wallwidth)\n",
    "            # We know that we're going to be taking hessian for params, so declare zeros accordingly:\n",
    "            wallwidth = ForwardDiffZeros(length(myparams), 1, nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "            for i in [1:length(myparams);]\n",
    "                wallwidth[i] = wallwidth_factor*(bbox[i,2]-bbox[i,1])\n",
    "            end\n",
    "        end\n",
    "\n",
    "        retval = 0\n",
    "        for i in [1:length(myparams);]\n",
    "            if myparams[i]<bbox[i,1]\n",
    "                retval += cosh((bbox[i,1]-myparams[i])/wallwidth[i])-1.0\n",
    "            elseif bbox[i,2] < myparams[i]\n",
    "                retval += cosh((myparams[i]-bbox[i,2])/wallwidth[i])-1.0                \n",
    "            end\n",
    "        end\n",
    "\n",
    "        return 2*retval\n",
    "    end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- cost=-0.00275999,   cost1=2.64439e-05, cost2=-0.00278644 :  mean(hits)=0.755142, mean(diffs)=0.928813\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.0027599945390814123"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf()\n",
    "costfunc(startU; do_plot=true, verbose=true, make_dict([\"sigma\"], [0.1], make_dict(args, params, model_params))...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working on figuring out the weird trajectories. Probably a dt thing.  The fourth one is the weird one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(::#91) (generic function with 1 method)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward = (startpoint; do_plot=false, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;     \n",
    "    forwardModel(startpoint; do_plot=do_plot, pars...)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTION DEFINITION: fluxSense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The main fluxSense() function containing the main minimization loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 942,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition fluxSense(Any, Any, Any, Any, Any, Any, Any) in module Main at In[925]:4 overwritten at In[942]:4.\n",
      "WARNING: Method definition #fluxSense(Array{Any, 1}, Main.#fluxSense, Any, Any, Any, Any, Any, Any, Any) in module Main overwritten.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fluxSense (generic function with 1 method)"
      ]
     },
     "execution_count": 942,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function fluxSense(costfunc, backward, paramsDict, startUs, ends, args, seed; start_eta=0.01, tol=1e-15, \n",
    "    maxiter=400, verbose=true, do_plot=false, cost_limit=[], report_fluxless_grad=false, report_every=1)\n",
    "   \n",
    "    if do_plot; clf(); end;\n",
    "\n",
    "    params = seed\n",
    "    eta    = start_eta\n",
    "\n",
    "    if ~(typeof(ends)<:Array); ends = [ends]; end\n",
    "    U0 = zeros(size(ends))\n",
    "    for j in 1:size(ends,1)\n",
    "        # @printf(\"model params is \"); print(model_params); print(\"\\n\")\n",
    "        # @printf(\"ends[j,:] is \"); print(ends[j,:]); print(\"\\n\")        \n",
    "        U0[j,:] = backward(ends[j,:]; tol=1e-25, do_plot=false, make_dict(args, params, model_params)...)'\n",
    "        # @printf(\"U0[j,:] is \"); print(U0[j,:]); print(\"\\n\")        \n",
    "    end\n",
    "    \n",
    "    if length(ends)>0\n",
    "        @printf(\"U0[end,:] is \"); print_vector_g(U0[end,:]); @printf(\"\\n\")\n",
    "    end\n",
    "    \n",
    "    cost, grad, hess = \n",
    "        vgh((x)->costfunc([startUs;U0]; do_plot=do_plot, nderivs=length(x), difforder=2, \n",
    "            make_dict(args, x, model_params)...), params)\n",
    "    \n",
    "    if verbose && report_fluxless_grad\n",
    "        fcost, fgrad, fhess = \n",
    "        vgh((x)->costfunc(startUs; do_plot=false, nderivs=length(x), difforder=2, \n",
    "                make_dict(args, x, model_params)...), params)\n",
    "        @printf(\"      ### grad without flux track = \"); print_vector_g(fgrad); @printf(\"\\n\")    \n",
    "    end\n",
    "\n",
    "    if verbose\n",
    "        @printf(\"Initial cost, grad, hess:\\n\")\n",
    "        @printf(\"   cost = %g\\n\", cost)\n",
    "        @printf(\"   grad = \"); print_vector_g(grad); print(\"\\n\")\n",
    "        @printf(\"   hess = \"); print_vector_g(hess); print(\"\\n\")\n",
    "    end\n",
    "\n",
    "    delta_params=0\n",
    "    ptrajectory = zeros(length(seed), maxiter); \n",
    "    gtrajectory = zeros(length(seed), maxiter); \n",
    "    ctrajectory = zeros(1, maxiter);\n",
    "    \n",
    "    for i in [1:maxiter;]         \n",
    "        my_verbose = verbose && rem(i, report_every)==0\n",
    "\n",
    "        new_params = params - eta*grad/(sqrt(sum(grad.*grad)))\n",
    "        delta_params = new_params - params\n",
    "\n",
    "        new_cost, new_grad, new_hess = \n",
    "        vgh((x)->costfunc([startUs;U0]; do_plot=false, verbose=my_verbose, pre_string=\"   newpars>> \",\n",
    "                zero_last_sigmas=size(U0,1), nderivs=length(x), difforder=2, make_dict(args, x, model_params)...), \n",
    "                new_params)\n",
    "\n",
    "        if my_verbose\n",
    "            @printf(\"delta_params=\"); print_vector_g(delta_params); @printf(\"\\n\"); \n",
    "            @printf(\"new_cost=%g  cost=%g   delta_cost=%g\\n\", new_cost, cost, new_cost-cost)\n",
    "        end\n",
    "        \n",
    "        if abs(new_cost - cost) < tol\n",
    "            @printf(\"\\n===\\nChange in cost was less than the tolerance %g\\n===\\n\", tol)\n",
    "            ptrajectory=ptrajectory[:,1:i-1]; gtrajectory=gtrajectory[:,1:i-1]; ctrajectory=ctrajectory[1:i-1]\n",
    "            break\n",
    "        end\n",
    "        if (length(cost_limit)>0 && cost < cost_limit)\n",
    "            @printf(\"\\n===\\nCost was less than the cost limit %g\\n===\\n\", cost_limit)\n",
    "            ptrajectory=ptrajectory[:,1:i-1]; gtrajectory=gtrajectory[:,1:i-1]; ctrajectory=ctrajectory[1:i-1]\n",
    "            break\n",
    "        end\n",
    "        \n",
    "        if new_cost >= cost\n",
    "            eta = eta/2\n",
    "            costheta = NaN\n",
    "        else\n",
    "            eta = eta*1.2\n",
    "            costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "\n",
    "            params = new_params\n",
    "    \n",
    "            for j in 1:size(ends,1)\n",
    "                U0[j,:] = backward(ends[j,:]; do_plot=false, make_dict(args, params, model_params)...)'\n",
    "            end\n",
    "            if my_verbose && length(ends)>0\n",
    "                @printf(\"U0[end,:] is \"); print_vector_g(U0[end,:]); @printf(\"\\n\")\n",
    "            end\n",
    "            cost, grad, hess = \n",
    "                vgh((x)->costfunc([startUs;U0]; do_plot=do_plot, verbose=my_verbose, nderivs=length(x), difforder=2, \n",
    "                    zero_last_sigmas=size(U0,1), make_dict(args, x, model_params)...), params)\n",
    "\n",
    "        end\n",
    "        \n",
    "        ptrajectory[:,i] = params\n",
    "        gtrajectory[:,i] = grad\n",
    "        ctrajectory[i]   = cost\n",
    "\n",
    "        if my_verbose\n",
    "            @printf \"%d: eta=%g cost=%g costheta=%g ps=\" i eta cost  costheta\n",
    "            print_vector(params)\n",
    "            @printf \"\\n\"\n",
    "            @printf(\"grad=\"); print_vector_g(grad); @printf(\"\\n\")\n",
    "            if report_fluxless_grad\n",
    "                fcost, fgrad, fhess = \n",
    "                vgh((x)->costfunc(startUs; do_plot=false, verbose=false, nderivs=length(x), difforder=2, \n",
    "                        make_dict(args, x, model_params)...), params)\n",
    "            @printf(\"      ### grad without flux track = \"); print_vector_g(fgrad); @printf(\"\\n\")    \n",
    "            end            \n",
    "        end\n",
    "    end    \n",
    "\n",
    "    return params, ctrajectory, ptrajectory, gtrajectory\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORKING EXAMPLE:   1-D example of using fluxSense()\n",
    "\n",
    "## after defining fluxSense(), run the next two cells in order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the cost function. It passes most keyword params down to the forward and backward models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition J(Any) in module Main at In[845]:4 overwritten at In[851]:4.\n",
      "WARNING: Method definition #J(Array{Any, 1}, Main.#J, Any) in module Main overwritten.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "J (generic function with 1 method)"
      ]
     },
     "execution_count": 851,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function J(initUs; theta1=0.15, theta2=0.2, beta=0.01, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, params...)\n",
    "\n",
    "    Vend = ForwardDiffZeros(length(initUs), 1, nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:length(initUs)\n",
    "        Ue, Ve, U, V = forwardModel(initUs[i]; nderivs=nderivs, difforder=difforder, \n",
    "            do_plot=do_plot, clearfig=false, params...)\n",
    "        Vend[i] = Ve[1]\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend-0.5)/theta1))\n",
    "    diffs = tanh((Vend-0.5)/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now setup, run fluxSense(), and display results. Example is only 1-d so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "backward = (endpoint; do_plot=false, pars...) -> backwardsModel(endpoint; do_plot=do_plot, pars...)[1]\n",
    "\n",
    "costfunc = (startpoints; do_plot=false, verbose=false, nderivs=0, difforder=0, pars...) -> \n",
    "J(startpoints; do_plot=do_plot, verbose=verbose, beta=0.01, nderivs=nderivs, difforder=difforder, pars...)\n",
    "\n",
    "dt = 0.01\n",
    "t = 0:dt:2\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "noise = 0*randn(1, nsteps)\n",
    "noise = 0.02*sin(2*pi*3*t); noise=reshape(noise, 1, nsteps)\n",
    "\n",
    "W = 4.1\n",
    "const_add = -2\n",
    "init_add=0\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>W, :nsteps=>nsteps, \n",
    "    :noise=>noise, :noise=>noise, :const_add=>const_add, :init_add=>init_add)\n",
    "\n",
    "srand(20)  \n",
    "startUs = randn(20, 1)       # The starting values\n",
    "# startUs = [randn(10,1)+2;randn(10,1)-2]\n",
    "\n",
    "\n",
    "args = [\"init_add\", \"const_add\", \"W\"]\n",
    "seed = [0, -2, 4.1]\n",
    "\n",
    "# Do an initial run plotting to show the starting position\n",
    "clf()\n",
    "costfunc(startUs; do_plot=true, verbose=true, model_params...)\n",
    "\n",
    "\n",
    "fluxFinalPoint = convert(Float64, 0)  # The final value of the pinned output\n",
    "#\n",
    "# If you remove the fluxFinalPoint, by un-commenting the following line, it gets stuck. But\n",
    "# it is also true that if you make beta=0 (in the constfunc() definition in line 4 above) then t\n",
    "# that also solves the sticking problem.  If we had beta=0 until after our hits are what we want, \n",
    "# would we ever need fluxPoint?\n",
    "#\n",
    "fluxFinalPoint = [];\n",
    "\n",
    "params, cost = fluxSense(costfunc, backward, model_params, startUs, fluxFinalPoint, args, seed; \n",
    "start_eta=0.01, tol=1e-15, maxiter=400, verbose=true, report_fluxless_grad=false, do_plot=true, cost_limit=-0.00959)\n",
    "\n",
    "# And show the final position\n",
    "clf()\n",
    "costfunc(startUs; do_plot=true, verbose=true, make_dict(args, params, model_params)...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---  END --- 1d example of using fluxSense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of forwards and backwards models\n",
    "\n",
    "### Inverting time even through a sinusoidal noise, with added noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt = 0.01\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "noise = 0*randn(1, nsteps)\n",
    "noise = 0.2*sin(2*pi*3*t); noise=reshape(noise, 1, nsteps)\n",
    "W = [0.5]\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>W, :nsteps=>nsteps, \n",
    "    :noise=>noise, :noise=>noise, :const_add=>-0.15, :init_add=>0.3)\n",
    "clf();\n",
    "\n",
    "srand(10)\n",
    "\n",
    "startUs = -0.5\n",
    "Uend, Vend, U, V =forwardModel(startUs; do_plot=true, clearfig=false, model_params...)\n",
    "Ustart, Vstart = backwardsModel(Uend;  do_plot=true, clearfig=false, tol=1e-15, model_params...)\n",
    "\n",
    "[startUs Ustart]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD: scripts on the path to writing fluxSense() as a function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the cost function. It passes most keyword params down to the forward and backward models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function J(initUs; theta1=0.15, theta2=0.2, beta=0.01, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, params...)\n",
    "\n",
    "    Vend = ForwardDiffZeros(length(initUs), 1, nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:length(initUs)\n",
    "        Ue, Ve, U, V = forwardModel(initUs[i]; nderivs=nderivs, difforder=difforder, \n",
    "            do_plot=do_plot, clearfig=false, params...)\n",
    "        Vend[i] = Ve[1]\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend-0.5)/theta1))\n",
    "    diffs = tanh((Vend-0.5)/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An example of some code that does differentiation. This cell not necessary for running the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# An example of a standard setup which we'll try to modify to try to get 75% correct\n",
    "\n",
    "dt = 0.01\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "noise = 0*randn(1, nsteps)\n",
    "noise = 0.02*sin(2*pi*3*t); noise=reshape(noise, 1, nsteps)\n",
    "\n",
    "W = 4.1\n",
    "const_add = -2\n",
    "init_add=0\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>W, :nsteps=>nsteps, \n",
    ":noise=>noise, :noise=>noise, :const_add=>const_add, :init_add=>init_add)\n",
    "\n",
    "srand(10)\n",
    "startUs = randn(40, 1)\n",
    "J(startUs; do_plot=true, verbose=true, model_params...)\n",
    "\n",
    "# --- now while taking the derivative ---\n",
    "args = [\"init_add\", \"const_add\", \"W\"]\n",
    "seed = [init_add, const_add, W]\n",
    "\n",
    "ForwardDiff.gradient((x)->J(startUs; do_plot=true, nderivs=length(x), difforder=1, verbose=true, make_dict(args, x, model_params)...), seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main adaptive step with gradient and keywords loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################################\n",
    "#                                                   #\n",
    "#                                                   #\n",
    "#     ADAPTIVE GRADIENT FOR KEYWORD VERSION         #\n",
    "#                                                   #\n",
    "#                                                   #\n",
    "#####################################################\n",
    "\n",
    "\n",
    "# -----------------  FORWARD MODEL SETUP ---------------\n",
    "dt = 0.01\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "noise = 0*randn(1, nsteps)\n",
    "noise = 0.02*sin(2*pi*3*t); noise=reshape(noise, 1, nsteps)\n",
    "\n",
    "W = 4.1\n",
    "const_add = -2\n",
    "init_add=0\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>W, :nsteps=>nsteps, \n",
    ":noise=>noise, :noise=>noise, :const_add=>const_add, :init_add=>init_add)\n",
    "\n",
    "\n",
    "# ----------------  CRITICAL INDICATION OF PARAMETERS TO OPTIMIZE IS HERE: -----\n",
    "args = [\"init_add\", \"const_add\", \"W\"]\n",
    "seed = [0, -2, 4.1]\n",
    "\n",
    "fluxFinalPoint = convert(Float64, 0)  # The final value of the pinned output\n",
    "\n",
    "srand(10)  \n",
    "startUs = randn(200, 1)       # The starting values\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "clf()\n",
    "\n",
    "start_eta = 0.01\n",
    "tol = 1e-15\n",
    "maxiter = 400\n",
    "verbose = true\n",
    "do_plot=false\n",
    "\n",
    "# -------\n",
    "\n",
    "params = seed\n",
    "eta    = start_eta\n",
    "\n",
    "\n",
    "U0 = backwardsModel(fluxFinalPoint; do_plot=false, make_dict(args, params, model_params)...)[1]\n",
    "J([startUs;U0]; verbose=true, do_plot=true, make_dict(args, params, model_params)...)\n",
    "\n",
    "\n",
    "cost, grad, hess = \n",
    "    vgh((x)->J([startUs;U0]; do_plot=false, nderivs=length(x), difforder=2, make_dict(args, x, model_params)...), params)\n",
    "\n",
    "@printf(\"Initial cost, grad, hess:\\n\")\n",
    "print_vector_g(:cost)\n",
    "print_vector_g(:grad)\n",
    "print_vector_g(:hess)\n",
    "delta_params=0\n",
    "\n",
    "\n",
    "for i in 1:maxiter         \n",
    "        new_params = params - eta*grad/(sqrt(sum(grad.*grad)))\n",
    "        delta_params = new_params - params\n",
    "        print_vector_g(:delta_params)\n",
    "        new_cost, new_grad, new_hess = \n",
    "            vgh((x)->J([startUs;U0]; do_plot=false, verbose=false,\n",
    "                nderivs=length(x), difforder=2, make_dict(args, x, model_params)...), new_params)\n",
    "        @printf(\"new_cost=%g  cost=%g   delta_cost=%g\\n\", new_cost, cost, new_cost-cost)\n",
    "        if abs(new_cost - cost) < tol\n",
    "            break\n",
    "        end\n",
    "\n",
    "        if new_cost >= cost\n",
    "            eta = eta/2\n",
    "            costheta = NaN\n",
    "        else\n",
    "            eta = eta*1.2\n",
    "            costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "\n",
    "            params = new_params\n",
    "    \n",
    "            U0 = backwardsModel(fluxFinalPoint; do_plot=false, make_dict(args, params, model_params)...)[1]\n",
    "            cost, grad, hess = \n",
    "                vgh((x)->J([startUs;U0]; do_plot=do_plot, verbose=true,\n",
    "                    nderivs=length(x), difforder=2, make_dict(args, x, model_params)...), params)\n",
    "\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            @printf \"%d: eta=%g cost=%g costheta=%g ps=\" i eta cost  costheta\n",
    "            print_vector(params)\n",
    "            @printf \"\\n\"\n",
    "        end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A cell to plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# J(params; initUs=[startUs;U0], verbose=true, do_plot=true)\n",
    "clf()\n",
    "Ve = zeros(length(startUs),1)\n",
    "for i=1:length(startUs)\n",
    "    Ue, Vee, U, V = forwardModel(startUs[i]; do_plot=true, clearfig=false, make_dict(args, params, model_params)...)\n",
    "    Ve[i] = Vee[1]\n",
    "end\n",
    "\n",
    "@printf(\"\\n\\nFinal result produces %d hits out of %d trials for %.1f per cent correct\\n\\n\", length(find(Ve.>0.5)), \n",
    "    length(Ve), 100*length(find(Ve.>0.5))/length(Ve))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- END Complete keyword-driven adaptive gradient version of FluxSense minimizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete adaptive gradient version of FluxSense minimizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################################\n",
    "#                                                   #\n",
    "#                                                   #\n",
    "#     In this cell we define J(x::Vector[3])        #\n",
    "#     Not full keyword version yet.                 #\n",
    "#                                                   #\n",
    "#     Next cell has the adaptive gradient procedure #\n",
    "#                                                   #\n",
    "#     Run the third cell to see results             #\n",
    "#                                                   #\n",
    "#                                                   #\n",
    "#####################################################\n",
    "\n",
    "dt = 0.01\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "\n",
    "W = [4]\n",
    "k = -2\n",
    "init_k = 0\n",
    "\n",
    "noise = 0.2*sin(2*pi*3*t); noise = reshape(noise, 1, nsteps)\n",
    "\n",
    "mypars = Dict(:dt=>dt, :tau=>tau, :nsteps=>nsteps)\n",
    "\n",
    "srand(10)\n",
    "startUs = 2*randn(200,1)\n",
    "# for i=1:length(startUs)\n",
    "#    Uend, Vend, U, V = forwardModel(startUs[i]; noise=noise+k, W=W, do_plot=true, clearfig=false, params...)\n",
    "#end\n",
    "\n",
    "# backwardsModel([1.2*0]; do_plot=true, clearfig=false, params...)\n",
    "\n",
    "function J(x; initUs=startUs, theta1=0.15, theta2=0.2, beta=0.05, verbose=false,\n",
    "    nderivs=0, difforder=0, do_plot=true)\n",
    "    \n",
    "    Vend = ForwardDiffZeros(length(initUs), 1, nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    k = x[1]\n",
    "    W = x[2]\n",
    "    init_k = x[3]\n",
    "    \n",
    "    for i=1:length(initUs)\n",
    "        Ue, Ve, U, V = forwardModel(initUs[i]+init_k; noise=noise+k, W=[W], \n",
    "        nderivs=nderivs, difforder=difforder, do_plot=do_plot, clearfig=false, mypars...)\n",
    "        Vend[i] = Ve[1]\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend-0.5)/theta1))\n",
    "    diffs = tanh((Vend-0.5)/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "\n",
    "\n",
    "# WORKS:\n",
    "# ForwardDiff.gradient((x)->forwardModel(x[1]; noise=noise+k, W=[x[2]], \n",
    "#    do_plot=true, clearfig=true, nderivs=2, difforder=1, params...)[1][1], [-2.1, 4])\n",
    "\n",
    "ForwardDiff.gradient((x)->J(x; nderivs=3, difforder=1), [-2, 4.1, 0])\n",
    "# J([-2.1, 4])\n",
    "\n",
    "\n",
    "# ForwardDiff.derivative((x)->forwardModel(startUs[1]; noise=noise+k, W=[x], \n",
    "#    do_plot=true, clearfig=true, nderivs=1, difforder=1, params...)[1], 4.5995)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################################\n",
    "#                                                   #\n",
    "#                                                   #\n",
    "#     ADAPTIVE GRADIENT VERSION                     #\n",
    "#                                                   #\n",
    "#                                                   #\n",
    "#####################################################\n",
    "\n",
    "# This is all BEFORE makign J fully keyword-value driven\n",
    "\n",
    "seed = [-2, 4.1, 0]   # params are constant add, W, and init_add.\n",
    "start_eta = 0.01\n",
    "tol = 1e-15\n",
    "maxiter = 400\n",
    "verbose = true\n",
    "\n",
    "params = seed\n",
    "eta = start_eta\n",
    "\n",
    "U0 = backwardsModel([1.2*0]; noise=noise+params[1], W=[params[2]], do_plot=true, mypars...)[1] - params[3]\n",
    "\n",
    "J(params; initUs=[startUs;U0], verbose=true)\n",
    "\n",
    "cost, grad, hess = vgh((x)->J(x; initUs=[startUs;U0], nderivs=length(params), difforder=2, verbose=true), params)\n",
    "\n",
    "@printf(\"Initial cost, grad, hess:\\n\")\n",
    "print_vector_g(:cost)\n",
    "print_vector_g(:grad)\n",
    "print_vector_g(:hess)\n",
    "delta_params=0\n",
    "\n",
    "for i in 1:maxiter         \n",
    "        new_params = params - eta*grad/(sqrt(sum(grad.*grad)))\n",
    "        delta_params = new_params - params\n",
    "        print_vector_g(:delta_params)\n",
    "        new_cost, new_grad, new_hess = \n",
    "            vgh((x)->J(x; initUs=[startUs;U0], nderivs=length(params), difforder=2, verbose=false, do_plot=false), \n",
    "                new_params)        \n",
    "        @printf(\"new_cost=%g  cost=%g   delta_cost=%g\\n\", new_cost, cost, new_cost-cost)\n",
    "        if abs(new_cost - cost) < tol\n",
    "            break\n",
    "        end\n",
    "\n",
    "        if new_cost >= cost\n",
    "            eta = eta/2\n",
    "            costheta = NaN\n",
    "        else\n",
    "            eta = eta*1.2\n",
    "            costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "\n",
    "            params = new_params\n",
    "    \n",
    "            U0 = backwardsModel([1.2*0]; noise=noise+params[1], W=[params[2]], do_plot=false, mypars...)[1] - params[3]\n",
    "            cost, grad, hess = \n",
    "                vgh((x)->J(x; initUs=[startUs;U0], nderivs=length(params), difforder=2, verbose=true, do_plot=false), params)\n",
    "\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            @printf \"%d: eta=%g cost=%g costheta=%g ps=\" i eta cost  costheta\n",
    "            print_vector(params)\n",
    "            @printf \"\\n\"\n",
    "        end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# J(params; initUs=[startUs;U0], verbose=true, do_plot=true)\n",
    "clf()\n",
    "Ve = zeros(length(startUs),1)\n",
    "for i=1:length(startUs)\n",
    "    Ue, Vee, U, V = forwardModel(startUs[i]+params[3]; noise=params[1], W=[params[2]],     \n",
    "        do_plot=true, clearfig=false, tau=0.1, nsteps=201, dt=0.01)\n",
    "    Ve[i] = Vee[1]\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------END OF: complete adaptive gradient version of FluxSense minimizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf()\n",
    "J(params; initUs=[startUs;U0], verbose=true, do_plot=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "length(find(Ve.>0.5))/200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mypars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################################\n",
    "#                                                   #\n",
    "#                                                   #\n",
    "#     HESSIAN VERSION                               #\n",
    "#                                                   #\n",
    "#                                                   #\n",
    "#####################################################\n",
    "\n",
    "\n",
    "seed = [-2, 4.1]\n",
    "start_eta = 0.0000001\n",
    "tol = 1e-15\n",
    "maxiter = 400\n",
    "verbose = true\n",
    "\n",
    "params = seed\n",
    "eta = start_eta\n",
    "\n",
    "U0 = backwardsModel([1.2*0]; noise=noise+params[1], W=[params[2]], do_plot=true, mypars...)[1]\n",
    "\n",
    "J(params; initUs=[startUs;U0], verbose=true)\n",
    "\n",
    "cost, grad, hess = vgh((x)->J(x; initUs=[startUs;U0], nderivs=length(params), difforder=2, verbose=true), params)\n",
    "\n",
    "@printf(\"Initial cost, grad, hess:\\n\")\n",
    "print_vector_g(:cost)\n",
    "print_vector_g(:grad)\n",
    "print_vector_g(:hess)\n",
    "\n",
    "for i in 1:maxiter\n",
    "        hathess    = hess + eye(length(grad), length(grad))/eta        \n",
    "        new_params = params - inv(hathess)*grad\n",
    "        new_cost, new_grad, new_hess = \n",
    "            vgh((x)->J(x; initUs=[startUs;U0], nderivs=length(params), difforder=2, verbose=true), new_params)\n",
    "            \n",
    "        if abs(new_cost - cost) < tol\n",
    "            # break\n",
    "        end\n",
    "\n",
    "        if new_cost >= cost\n",
    "            eta = eta/2\n",
    "            costheta = NaN\n",
    "        else\n",
    "            eta = eta*1.1\n",
    "            costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "\n",
    "            params = new_params\n",
    "    \n",
    "            U0 = backwardsModel([1.2*0]; noise=noise+params[1], W=[params[2]], do_plot=false, mypars...)[1]\n",
    "            cost, grad, hess = \n",
    "                vgh((x)->J(x; initUs=[startUs;U0], nderivs=length(params), difforder=2, verbose=true), params)\n",
    "\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            @printf \"%d: eta=%g cost=%g costheta=%g ps=\" i eta cost  costheta\n",
    "            print_vector(params)\n",
    "            @printf \"\\n\"\n",
    "        end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = [-2, 4.1]\n",
    "start_eta = 10\n",
    "tol = 1e-15\n",
    "maxiter = 400\n",
    "\n",
    "params = seed\n",
    "eta = start_eta\n",
    "\n",
    "# backwardsModel([0.5]; noise=noise+params[1], W=params[2], params...)\n",
    "\n",
    "cost, grad, hess = vgh(func, params)\n",
    "\n",
    "\n",
    "    if verbose && verbose_level >= 2\n",
    "        @printf(\"Initial cost, grad, hess:\\n\")\n",
    "        print_vector_g(:cost)\n",
    "        print_vector_g(:grad)\n",
    "        print_vector_g(:hess)\n",
    "    end\n",
    "    \n",
    "\n",
    "    for i in [1:maxiter;]\n",
    "        hathess    = hess + eye(length(grad), length(grad))/eta        \n",
    "        new_params = params - inv(hathess)*grad\n",
    "        new_cost, new_grad, new_hess = vgh(func, new_params)\n",
    "            \n",
    "        if abs(new_cost - cost) < tol\n",
    "            break\n",
    "        end\n",
    "\n",
    "        if new_cost >= cost\n",
    "            eta = eta/2\n",
    "            costheta = NaN\n",
    "        else\n",
    "            eta = eta*1.1\n",
    "            costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "\n",
    "            params = new_params\n",
    "            cost = new_cost\n",
    "            grad = new_grad\n",
    "            hess = new_hess\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            @printf \"%d: eta=%.3f cost=%.4f costheta=%.3f ps=\" i eta cost  costheta\n",
    "            print_vector(params)\n",
    "            @printf \"\\n\"\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return params, cost\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trust_region_Hessian_minimization([-2, 4.1], (x)->J(x; nderivs=2, difforder=2, verbose=true), verbose=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt = 0.01\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "\n",
    "noise = 3.5*sin(2*pi*3*t); noise=reshape(noise, 1, nsteps)\n",
    "W = [0.5]\n",
    "\n",
    "params = Dict(:dt=>dt, :tau=>tau, :nsteps=>nsteps, :noise=>noise)\n",
    "\n",
    "function J(x; nderivs=0, difforder=0)\n",
    "    startU = x[1]\n",
    "    W = x[2]\n",
    "    \n",
    "    Uend, Vend, U, V = forwardModel(startU; do_plot=true, W=[W], nderivs=nderivs, difforder=difforder, params...)\n",
    "    \n",
    "    return (Vend[1]-0.5)^2\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trust_region_Hessian_minimization([-0.5, 0.5], (x) -> J(x;nderivs=2, difforder=2), verbose=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing forward and backwards models with only 1 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = Dict(:noise => [0.1], :W => [0])\n",
    "Uend = forwardModel([1.1]; do_plot=true, nsteps=100, params...)[1]\n",
    "backwardsModel(Uend; do_plot=true, nsteps=100, params...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing forward and backwards models now with 2 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = Dict(:noise => [0.1, 0], :W => [0 -5; -5 0])\n",
    "\n",
    "Uend, Vend, U, V              = forwardModel([0,0]; do_plot=true, nsteps=50, params...);\n",
    "Ustart, Vstart, bU, bV, costs = backwardsModel(Uend; do_plot=true, nsteps=50, params...)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD STUFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- BEGIN --- Old example that gets stuck: too large beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The following sequence leads to a situation where having only [-0.8, -0.8] as the single finalFluxPoint \n",
    "# leads to the minimization getting stuck.  Adding further finalFluxPoints solves the problem.\n",
    "#\n",
    "# Reducing beta in the cost function J() from 0.01 to 0.003 also eliminated the problem.  \n",
    "#\n",
    "srand(10)\n",
    "startU=randn(100,2)-3\n",
    "startU=randn(100,2)-3\n",
    "\n",
    "# startU=randn(100,2)-3\n",
    "\n",
    "dt = 0.02\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "W = -4\n",
    "noise = 0\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    "    :noise=>noise, :noise=>noise, :const_add=>0, :init_add=>0)\n",
    "\n",
    "clf(); subplot(2,1,1)\n",
    "for i in 1:size(startU,1)\n",
    "    Uend, Vend, U, V = forwardModel(startU[i,:]; do_plot=true, clearfig=false, model_params...)\n",
    "end\n",
    "\n",
    "Ustarthat, Vstarthat, Uhatm, Vhat, costs = backwardsModel([-0.8, -0.8]; do_plot=true, clearfig=false, \n",
    "tol=1e-50, maxiter=800, model_params...)\n",
    "\n",
    "subplot(2,1,2)\n",
    "plot(t, costs, \".-\")\n",
    "\n",
    "\n",
    "function JJ(initUs; theta1=0.15, theta2=0.2, beta=0.01, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, pre_string=\"\", params...)\n",
    "\n",
    "    Vend = ForwardDiffZeros(size(initUs,1), size(initUs,2), nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:size(initUs,1)\n",
    "        Ue, Ve, U, V = forwardModel(initUs[i,:]; nderivs=nderivs, difforder=difforder, \n",
    "            do_plot=do_plot, clearfig=false, params...)\n",
    "        Vend[i,:] = Ve\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend[:,1]-Vend[:,2])/theta1))\n",
    "    diffs = tanh.((Vend[:,1]-Vend[:,2])/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"%s\", pre_string)\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "  \n",
    "clf();\n",
    "JJ(startU; do_plot=true, model_params...)\n",
    "\n",
    "# WORKING gradient:\n",
    "# ForwardDiff.gradient((x)->JJ(startU; do_plot=true, nderivs=length(x), difforder=1, \n",
    "#    make_dict([[\"init_add\" 2], \"const_add\"], x, model_params)...), [2.9, -2.9, 0.1])\n",
    "\n",
    "\n",
    "\n",
    "# The backward and costfunc functions should turn a single-scalar parameter W into the matrix W\n",
    "backward = (endpoint; do_plot=false, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;     \n",
    "    backwardsModel(endpoint; do_plot=do_plot, pars...)[1]\n",
    "end\n",
    "\n",
    "costfunc = (startpoints; do_plot=false, verbose=false, nderivs=0, difforder=0, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;         \n",
    "    JJ(startpoints; do_plot=do_plot, verbose=verbose, nderivs=nderivs, difforder=difforder, pars...)\n",
    "end\n",
    "\n",
    "\n",
    "fluxFinalPoint = [-0.8 -0.8]  # ; -0.6 -0.6 ; -0.4 -0.4; -0.2 -0.2; 0 0; 0.2 0.2]\n",
    "\n",
    "args = [[\"init_add\" 2], \"const_add\"] # , \"W\"]\n",
    "seed = [0.001, 0.001, 0] # , -4]\n",
    "# seed = [1.190, -1.178, 2.000]\n",
    "\n",
    "params, cost = fluxSense(costfunc, backward, model_params, startU, fluxFinalPoint, args, seed; \n",
    "start_eta=0.01, tol=1e-15, maxiter=400, verbose=true, do_plot=false, cost_limit=-0.00935) # cost_limit=-0.000935) # for beta=0.01\n",
    "\n",
    "# And show the final position\n",
    "clf()\n",
    "costfunc(startU; do_plot=true, verbose=true, make_dict(args, params, model_params)...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- END --- Old example that gets stuck: too large beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of getting stuck without a flux point even with beta=0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function JJ(initUs; theta1=0.15, theta2=0.2, beta=0.003, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, pre_string=\"\", zero_last_sigmas=0, seedrand=NaN, params...)\n",
    "\n",
    "    if ~isnan(seedrand); srand(seedrand); end\n",
    "    \n",
    "    Vend = ForwardDiffZeros(size(initUs,1), size(initUs,2), nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:size(initUs,1)\n",
    "        if i>size(initUs,1) - zero_last_sigmas\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; sigma=0, nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)            \n",
    "        else\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)\n",
    "        end\n",
    "        Vend[i,:] = Ve\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend[:,1]-Vend[:,2])/theta1))\n",
    "    diffs = tanh.((Vend[:,1]-Vend[:,2])/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"%s\", pre_string)\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1067,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- cost=0.0081001,   cost1=0.0081001, cost2=0 :  mean(hits)=0.659999, mean(diffs)=0.999805\n",
      "U0[end,:] is [-53670.9, -53670.4]\n",
      "Initial cost, grad, hess:\n",
      "   cost = 0.0086241\n",
      "   grad = [-2.69241e-08, 2.72832e-08, 0.000142562, -4.48013e-05]\n",
      "   hess = [-4.22267e-11, 4.13727e-11, -2.21191e-08, 5.00305e-09, 4.13727e-11, -4.05324e-11, 2.20507e-08, -5.1266e-09, -2.21191e-08, 2.20507e-08, -8.34377e-05, -6.18722e-05, 5.00305e-09, -5.1266e-09, -6.18722e-05, -2.06283e-06]\n",
      "   newpars>> -- cost=0.0086226,   cost1=0.0086226, cost2=0 :  mean(hits)=0.657142, mean(diffs)=0.990281\n",
      "delta_params=[1.80171e-06, -1.82575e-06, -0.00954001, 0.00299803]\n",
      "new_cost=0.0086226  cost=0.0086241   delta_cost=-1.49637e-06\n",
      "U0[end,:] is [-53142.8, -53142.3]\n",
      "-- cost=0.00862622,   cost1=0.00862622, cost2=0 :  mean(hits)=0.657123, mean(diffs)=0.990293\n",
      "1: eta=0.012 cost=0.00862622 costheta=-1 ps=[4.741, -4.682, 2.722, -5.675]\n",
      "grad=[-2.73767e-08, 2.77355e-08, 0.00014245, -4.54363e-05]\n",
      "   newpars>> -- cost=0.00862442,   cost1=0.00862442, cost2=0 :  mean(hits)=0.657132, mean(diffs)=0.990287\n",
      "delta_params=[2.19716e-06, -2.22596e-06, -0.0114325, 0.00364657]\n",
      "new_cost=0.00862442  cost=0.00862622   delta_cost=-1.79725e-06\n",
      "U0[end,:] is [-52500.8, -52500.3]\n",
      "-- cost=0.00862881,   cost1=0.00862881, cost2=0 :  mean(hits)=0.657109, mean(diffs)=0.990302\n",
      "2: eta=0.0144 cost=0.00862881 costheta=-1 ps=[4.741, -4.682, 2.711, -5.672]\n",
      "grad=[-2.79486e-08, 2.83065e-08, 0.000142217, -4.62164e-05]\n",
      "   newpars>> -- cost=0.00862665,   cost1=0.00862665, cost2=0 :  mean(hits)=0.65712, mean(diffs)=0.990294\n",
      "delta_params=[2.69135e-06, -2.72582e-06, -0.013695, 0.00445047]\n",
      "new_cost=0.00862665  cost=0.00862881   delta_cost=-2.15788e-06\n",
      "U0[end,:] is [-51717.8, -51717.3]\n",
      "-- cost=0.00863198,   cost1=0.00863198, cost2=0 :  mean(hits)=0.657092, mean(diffs)=0.990312\n",
      "3: eta=0.01728 cost=0.00863198 costheta=-1 ps=[4.741, -4.682, 2.697, -5.667]\n",
      "grad=[-2.86801e-08, 2.90364e-08, 0.000141778, -4.71781e-05]\n",
      "   newpars>> -- cost=0.00862939,   cost1=0.00862939, cost2=0 :  mean(hits)=0.657105, mean(diffs)=0.990303\n",
      "delta_params=[3.31675e-06, -3.35795e-06, -0.0163961, 0.00545597]\n",
      "new_cost=0.00862939  cost=0.00863198   delta_cost=-2.58891e-06\n",
      "U0[end,:] is [-50758.9, -50758.4]\n",
      "-- cost=0.00863589,   cost1=0.00863589, cost2=0 :  mean(hits)=0.657071, mean(diffs)=0.990325\n",
      "4: eta=0.020736 cost=0.00863589 costheta=-1 ps=[4.741, -4.682, 2.681, -5.662]\n",
      "grad=[-2.96307e-08, 2.9984e-08, 0.000140974, -4.83678e-05]\n",
      "   newpars>> -- cost=0.00863279,   cost1=0.00863279, cost2=0 :  mean(hits)=0.657087, mean(diffs)=0.990314\n",
      "delta_params=[4.1225e-06, -4.17165e-06, -0.0196137, 0.00672939]\n",
      "new_cost=0.00863279  cost=0.00863589   delta_cost=-3.10124e-06\n",
      "U0[end,:] is [-49578.2, -49577.7]\n",
      "-- cost=0.0086407,   cost1=0.0086407, cost2=0 :  mean(hits)=0.657045, mean(diffs)=0.990342\n",
      "5: eta=0.0248832 cost=0.0086407 costheta=-1 ps=[4.741, -4.682, 2.661, -5.655]\n",
      "grad=[-3.08921e-08, 3.12398e-08, 0.000139523, -4.98428e-05]\n",
      "   newpars>> -- cost=0.008637,   cost1=0.008637, cost2=0 :  mean(hits)=0.657065, mean(diffs)=0.990328\n",
      "delta_params=[5.18832e-06, -5.24671e-06, -0.0234329, 0.00837106]\n",
      "new_cost=0.008637  cost=0.0086407   delta_cost=-3.70364e-06\n",
      "U0[end,:] is [-48113.3, -48112.8]\n",
      "-- cost=0.00864661,   cost1=0.00864661, cost2=0 :  mean(hits)=0.657013, mean(diffs)=0.990362\n",
      "6: eta=0.0298598 cost=0.00864661 costheta=-1 ps=[4.741, -4.682, 2.638, -5.647]\n",
      "grad=[-3.26136e-08, 3.29509e-08, 0.000136887, -5.16678e-05]\n",
      "   newpars>> -- cost=0.00864221,   cost1=0.00864221, cost2=0 :  mean(hits)=0.657036, mean(diffs)=0.990346\n",
      "delta_params=[6.65581e-06, -6.72465e-06, -0.0279361, 0.0105444]\n",
      "new_cost=0.00864221  cost=0.00864661   delta_cost=-4.39642e-06\n",
      "U0[end,:] is [-46278, -46277.5]\n",
      "-- cost=0.00865379,   cost1=0.00865379, cost2=0 :  mean(hits)=0.656974, mean(diffs)=0.990388\n",
      "7: eta=0.0358318 cost=0.00865379 costheta=-1 ps=[4.741, -4.682, 2.610, -5.636]\n",
      "grad=[-3.5055e-08, 3.53727e-08, 0.000132009, -5.3898e-05]\n",
      "   newpars>> -- cost=0.00864863,   cost1=0.00864863, cost2=0 :  mean(hits)=0.657002, mean(diffs)=0.990369\n",
      "delta_params=[8.80918e-06, -8.88903e-06, -0.0331733, 0.0135444]\n",
      "new_cost=0.00864863  cost=0.00865379   delta_cost=-5.15502e-06\n",
      "U0[end,:] is [-43947.7, -43947.2]\n",
      "-- cost=0.00866216,   cost1=0.00866216, cost2=0 :  mean(hits)=0.656929, mean(diffs)=0.990419\n",
      "8: eta=0.0429982 cost=0.00866216 costheta=-1 ps=[4.741, -4.682, 2.576, -5.623]\n",
      "grad=[-3.8712e-08, 3.89917e-08, 0.000122725, -5.6507e-05]\n",
      "   newpars>> -- cost=0.00865627,   cost1=0.00865627, cost2=0 :  mean(hits)=0.656961, mean(diffs)=0.990396\n",
      "delta_params=[1.232e-05, -1.2409e-05, -0.039057, 0.0179832]\n",
      "new_cost=0.00865627  cost=0.00866216   delta_cost=-5.88734e-06\n",
      "U0[end,:] is [-40941.2, -40940.7]\n",
      "-- cost=0.00867073,   cost1=0.00867073, cost2=0 :  mean(hits)=0.656883, mean(diffs)=0.990452\n",
      "9: eta=0.0515978 cost=0.00867073 costheta=-1 ps=[4.741, -4.682, 2.537, -5.605]\n",
      "grad=[-4.46408e-08, 4.48435e-08, 0.00010452, -5.91239e-05]\n",
      "   newpars>> -- cost=0.00866441,   cost1=0.00866441, cost2=0 :  mean(hits)=0.656917, mean(diffs)=0.990426\n",
      "delta_params=[1.91814e-05, -1.92685e-05, -0.0449104, 0.0254045]\n",
      "new_cost=0.00866441  cost=0.00867073   delta_cost=-6.32559e-06\n",
      "U0[end,:] is [-37041.5, -37041.1]\n",
      "-- cost=0.00867515,   cost1=0.00867515, cost2=0 :  mean(hits)=0.656859, mean(diffs)=0.990468\n",
      "10: eta=0.0619174 cost=0.00867515 costheta=-1 ps=[4.741, -4.682, 2.492, -5.579]\n",
      "grad=[-5.53348e-08, 5.53776e-08, 6.87192e-05, -6.00916e-05]\n",
      "   newpars>> -- cost=0.00866933,   cost1=0.00866933, cost2=0 :  mean(hits)=0.656891, mean(diffs)=0.990444\n",
      "delta_params=[3.7532e-05, -3.7561e-05, -0.0466102, 0.0407584]\n",
      "new_cost=0.00866933  cost=0.00867515   delta_cost=-5.8229e-06\n",
      "U0[end,:] is [-32718.1, -32717.6]\n",
      "-- cost=0.00866504,   cost1=0.00866504, cost2=0 :  mean(hits)=0.656914, mean(diffs)=0.990427\n",
      "11: eta=0.0743008 cost=0.00866504 costheta=-1 ps=[4.741, -4.682, 2.446, -5.538]\n",
      "grad=[-3.6379e-07, 3.57127e-07, 1.29592e-05, -5.51666e-05]\n",
      "   newpars>> -- cost=0.0106017,   cost1=0.0106017, cost2=0 :  mean(hits)=0.647036, mean(diffs)=0.99041\n",
      "delta_params=[0.000476965, -0.000468229, -0.0169908, 0.072329]\n",
      "new_cost=0.0106017  cost=0.00866504   delta_cost=0.00193661\n",
      "12: eta=0.0371504 cost=0.00866504 costheta=NaN ps=[4.741, -4.682, 2.446, -5.538]\n",
      "grad=[-3.6379e-07, 3.57127e-07, 1.29592e-05, -5.51666e-05]\n",
      "   newpars>> -- cost=0.010604,   cost1=0.010604, cost2=0 :  mean(hits)=0.647024, mean(diffs)=0.990418\n",
      "delta_params=[0.000238483, -0.000234114, -0.00849538, 0.0361645]\n",
      "new_cost=0.010604  cost=0.00866504   delta_cost=0.00193897\n",
      "13: eta=0.0185752 cost=0.00866504 costheta=NaN ps=[4.741, -4.682, 2.446, -5.538]\n",
      "grad=[-3.6379e-07, 3.57127e-07, 1.29592e-05, -5.51666e-05]\n",
      "   newpars>> -- cost=0.0106052,   cost1=0.0106052, cost2=0 :  mean(hits)=0.647019, mean(diffs)=0.990423\n",
      "delta_params=[0.000119241, -0.000117057, -0.00424769, 0.0180822]\n",
      "new_cost=0.0106052  cost=0.00866504   delta_cost=0.00194014\n",
      "14: eta=0.0092876 cost=0.00866504 costheta=NaN ps=[4.741, -4.682, 2.446, -5.538]\n",
      "grad=[-3.6379e-07, 3.57127e-07, 1.29592e-05, -5.51666e-05]\n",
      "   newpars>> -- cost=0.0106058,   cost1=0.0106058, cost2=0 :  mean(hits)=0.647016, mean(diffs)=0.990425\n",
      "delta_params=[5.96206e-05, -5.85286e-05, -0.00212384, 0.00904112]\n",
      "new_cost=0.0106058  cost=0.00866504   delta_cost=0.00194073\n",
      "15: eta=0.0046438 cost=0.00866504 costheta=NaN ps=[4.741, -4.682, 2.446, -5.538]\n",
      "grad=[-3.6379e-07, 3.57127e-07, 1.29592e-05, -5.51666e-05]\n",
      "   newpars>> -- cost=0.0106061,   cost1=0.0106061, cost2=0 :  mean(hits)=0.647014, mean(diffs)=0.990426\n",
      "delta_params=[2.98103e-05, -2.92643e-05, -0.00106192, 0.00452056]\n",
      "new_cost=0.0106061  cost=0.00866504   delta_cost=0.00194102\n",
      "16: eta=0.0023219 cost=0.00866504 costheta=NaN ps=[4.741, -4.682, 2.446, -5.538]\n",
      "grad=[-3.6379e-07, 3.57127e-07, 1.29592e-05, -5.51666e-05]\n",
      "   newpars>> -- cost=0.0106062,   cost1=0.0106062, cost2=0 :  mean(hits)=0.647014, mean(diffs)=0.990426\n",
      "delta_params=[1.49052e-05, -1.46322e-05, -0.000530961, 0.00226028]\n",
      "new_cost=0.0106062  cost=0.00866504   delta_cost=0.00194117\n",
      "17: eta=0.00116095 cost=0.00866504 costheta=NaN ps=[4.741, -4.682, 2.446, -5.538]\n",
      "grad=[-3.6379e-07, 3.57127e-07, 1.29592e-05, -5.51666e-05]\n",
      "   newpars>> -- cost=0.00866498,   cost1=0.00866498, cost2=0 :  mean(hits)=0.656914, mean(diffs)=0.990426\n",
      "delta_params=[7.45258e-06, -7.31608e-06, -0.00026548, 0.00113014]\n",
      "new_cost=0.00866498  cost=0.00866504   delta_cost=-6.55505e-08\n",
      "U0[end,:] is [-32748.3, -32747.8]\n",
      "-- cost=0.00866509,   cost1=0.00866509, cost2=0 :  mean(hits)=0.656914, mean(diffs)=0.990427\n",
      "18: eta=0.00139314 cost=0.00866509 costheta=-1 ps=[4.741, -4.682, 2.446, -5.537]\n",
      "grad=[-2.35938e-06, 2.3085e-06, 1.23029e-05, -5.46255e-05]\n",
      "   newpars>> -- cost=0.0106063,   cost1=0.0106063, cost2=0 :  mean(hits)=0.647013, mean(diffs)=0.990427\n",
      "delta_params=[5.86002e-05, -5.73365e-05, -0.000305569, 0.00135674]\n",
      "new_cost=0.0106063  cost=0.00866509   delta_cost=0.00194123\n",
      "19: eta=0.00069657 cost=0.00866509 costheta=NaN ps=[4.741, -4.682, 2.446, -5.537]\n",
      "grad=[-2.35938e-06, 2.3085e-06, 1.23029e-05, -5.46255e-05]\n",
      "   newpars>> -- cost=0.00866506,   cost1=0.00866506, cost2=0 :  mean(hits)=0.656914, mean(diffs)=0.990423\n",
      "delta_params=[2.93001e-05, -2.86682e-05, -0.000152785, 0.000678371]\n",
      "new_cost=0.00866506  cost=0.00866509   delta_cost=-2.83776e-08\n",
      "U0[end,:] is [-32767.5, -32767]\n",
      "-- cost=0.00866514,   cost1=0.00866514, cost2=0 :  mean(hits)=0.656913, mean(diffs)=0.990423\n",
      "20: eta=0.000835884 cost=0.00866514 costheta=-1 ps=[4.741, -4.682, 2.445, -5.537]\n",
      "grad=[-0.00077389, 0.000756745, -0.000457928, 0.000177706]\n",
      "   newpars>> -- cost=0.00866514,   cost1=0.00866514, cost2=0 :  mean(hits)=0.656913, mean(diffs)=0.990427\n",
      "delta_params=[0.000544224, -0.000532168, 0.00032203, -0.000124969]\n",
      "new_cost=0.00866514  cost=0.00866514   delta_cost=-3.84611e-10\n",
      "U0[end,:] is [-32811.7, -32811.2]\n",
      "-- cost=0.0086653,   cost1=0.0086653, cost2=0 :  mean(hits)=0.656912, mean(diffs)=0.990428\n",
      "21: eta=0.00100306 cost=0.0086653 costheta=-1 ps=[4.741, -4.683, 2.446, -5.537]\n",
      "grad=[-1.75734e-07, 1.7323e-07, 1.46553e-05, -5.54135e-05]\n",
      "   newpars>> -- cost=0.00866525,   cost1=0.00866525, cost2=0 :  mean(hits)=0.656913, mean(diffs)=0.990428\n",
      "delta_params=[3.07526e-06, -3.03145e-06, -0.000256461, 0.000969712]\n",
      "new_cost=0.00866525  cost=0.0086653   delta_cost=-5.74805e-08\n",
      "U0[end,:] is [-32832.7, -32832.2]\n",
      "-- cost=0.00866533,   cost1=0.00866533, cost2=0 :  mean(hits)=0.656912, mean(diffs)=0.990428\n",
      "22: eta=0.00120367 cost=0.00866533 costheta=-1 ps=[4.741, -4.683, 2.446, -5.536]\n",
      "grad=[-2.9071e-07, 2.85655e-07, 1.49875e-05, -5.54191e-05]\n",
      "   newpars>> -- cost=0.00866526,   cost1=0.00866526, cost2=0 :  mean(hits)=0.656913, mean(diffs)=0.990428\n",
      "delta_params=[6.09495e-06, -5.98898e-06, -0.000314225, 0.0011619]\n",
      "new_cost=0.00866526  cost=0.00866533   delta_cost=-6.89519e-08\n",
      "U0[end,:] is [-32856.7, -32856.2]\n",
      "-- cost=0.00866534,   cost1=0.00866534, cost2=0 :  mean(hits)=0.656912, mean(diffs)=0.990428\n",
      "23: eta=0.00144441 cost=0.00866534 costheta=-1 ps=[4.741, -4.683, 2.445, -5.535]\n",
      "grad=[-1.36866e-06, 1.33971e-06, 1.47918e-05, -5.51387e-05]\n",
      "   newpars>> -- cost=0.0106066,   cost1=0.0106066, cost2=0 :  mean(hits)=0.647012, mean(diffs)=0.990427\n",
      "delta_params=[3.46095e-05, -3.38773e-05, -0.000374041, 0.0013943]\n",
      "new_cost=0.0106066  cost=0.00866534   delta_cost=0.00194125\n",
      "24: eta=0.000722204 cost=0.00866534 costheta=NaN ps=[4.741, -4.683, 2.445, -5.535]\n",
      "grad=[-1.36866e-06, 1.33971e-06, 1.47918e-05, -5.51387e-05]\n",
      "   newpars>> -- cost=0.00866531,   cost1=0.00866531, cost2=0 :  mean(hits)=0.656912, mean(diffs)=0.990427\n",
      "delta_params=[1.73047e-05, -1.69386e-05, -0.000187021, 0.000697148]\n",
      "new_cost=0.00866531  cost=0.00866534   delta_cost=-3.88762e-08\n",
      "U0[end,:] is [-32871.3, -32870.8]\n",
      "-- cost=0.00866536,   cost1=0.00866536, cost2=0 :  mean(hits)=0.656912, mean(diffs)=0.990427\n",
      "25: eta=0.000866645 cost=0.00866536 costheta=-1 ps=[4.741, -4.683, 2.445, -5.534]\n",
      "grad=[-6.96149e-05, 6.80729e-05, -2.6582e-05, -3.45766e-05]\n",
      "   newpars>> -- cost=0.00866534,   cost1=0.00866534, cost2=0 :  mean(hits)=0.656912, mean(diffs)=0.990428\n",
      "delta_params=[0.000565495, -0.000552969, 0.00021593, 0.000280872]\n",
      "new_cost=0.00866534  cost=0.00866536   delta_cost=-1.53575e-08\n",
      "U0[end,:] is [-32923.9, -32923.4]\n",
      "-- cost=0.00866553,   cost1=0.00866553, cost2=0 :  mean(hits)=0.656911, mean(diffs)=0.990429\n",
      "26: eta=0.00103997 cost=0.00866553 costheta=-1 ps=[4.742, -4.684, 2.445, -5.534]\n",
      "grad=[-1.93717e-07, 1.90805e-07, 1.66133e-05, -5.56244e-05]\n",
      "   newpars>> -- cost=0.00866547,   cost1=0.00866547, cost2=0 :  mean(hits)=0.656911, mean(diffs)=0.990428\n",
      "delta_params=[3.47029e-06, -3.41813e-06, -0.000297614, 0.000996468]\n",
      "new_cost=0.00866547  cost=0.00866553   delta_cost=-6.0348e-08\n",
      "U0[end,:] is [-32939.8, -32939.3]\n",
      "-- cost=0.00866553,   cost1=0.00866553, cost2=0 :  mean(hits)=0.656911, mean(diffs)=0.990429\n",
      "27: eta=0.00124797 cost=0.00866553 costheta=-1 ps=[4.742, -4.684, 2.445, -5.533]\n",
      "grad=[-3.78651e-07, 3.71635e-07, 1.68214e-05, -5.55965e-05]\n",
      "   newpars>> -- cost=0.00866546,   cost1=0.00866546, cost2=0 :  mean(hits)=0.656912, mean(diffs)=0.990428\n",
      "delta_params=[8.13499e-06, -7.98424e-06, -0.000361394, 0.00119444]\n",
      "new_cost=0.00866546  cost=0.00866553   delta_cost=-7.19455e-08\n",
      "U0[end,:] is [-32958, -32957.5]\n",
      "-- cost=0.00866552,   cost1=0.00866552, cost2=0 :  mean(hits)=0.656911, mean(diffs)=0.990428\n",
      "28: eta=0.00149756 cost=0.00866552 costheta=-1 ps=[4.742, -4.684, 2.445, -5.531]\n",
      "grad=[-5.87699e-06, 5.74798e-06, 1.38345e-05, -5.39657e-05]\n",
      "   newpars>> -- cost=0.0106068,   cost1=0.0106068, cost2=0 :  mean(hits)=0.647011, mean(diffs)=0.990428\n",
      "delta_params=[0.000156287, -0.000152857, -0.000367902, 0.00143511]\n",
      "new_cost=0.0106068  cost=0.00866552   delta_cost=0.00194127\n",
      "29: eta=0.000748781 cost=0.00866552 costheta=NaN ps=[4.742, -4.684, 2.445, -5.531]\n",
      "grad=[-5.87699e-06, 5.74798e-06, 1.38345e-05, -5.39657e-05]\n",
      "   newpars>> -- cost=0.00866745,   cost1=0.00866745, cost2=0 :  mean(hits)=0.656901, mean(diffs)=0.990199\n",
      "delta_params=[7.81436e-05, -7.64283e-05, -0.000183951, 0.000717557]\n",
      "new_cost=0.00866745  cost=0.00866552   delta_cost=1.92879e-06\n",
      "30: eta=0.000374391 cost=0.00866552 costheta=NaN ps=[4.742, -4.684, 2.445, -5.531]\n",
      "grad=[-5.87699e-06, 5.74798e-06, 1.38345e-05, -5.39657e-05]\n",
      "   newpars>> -- cost=0.0086655,   cost1=0.0086655, cost2=0 :  mean(hits)=0.656911, mean(diffs)=0.990428\n",
      "delta_params=[3.90718e-05, -3.82141e-05, -9.19756e-05, 0.000358778]\n",
      "new_cost=0.0086655  cost=0.00866552   delta_cost=-2.04301e-08\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      " in _promote_op at ./promotion.jl:224 [inlined]",
      " in promote_op(::Function, ::Type{ForwardDiff.Dual{2,ForwardDiff.Dual{2,Float64}}}) at ./promotion.jl:242",
      " in broadcast(::Function, ::Array{ForwardDiff.Dual{2,ForwardDiff.Dual{2,Float64}},1}) at ./broadcast.jl:230",
      " in #forwardModel#958(::Float64, ::Float64, ::Int64, ::Array{Float64,2}, ::Array{Float64,2}, ::Array{Float64,2}, ::Int64, ::Float64, ::Bool, ::Int64, ::Int64, ::Bool, ::Int64, ::Bool, ::Int64, ::Int64, ::Int64, ::Int64, ::Int64, ::Array{Any,1}, ::#forwardModel, ::Array{ForwardDiff.Dual{2,ForwardDiff.Dual{2,Float64}},1}) at ./In[613]:118",
      " in (::#kw##forwardModel)(::Array{Any,1}, ::#forwardModel, ::Array{ForwardDiff.Dual{2,ForwardDiff.Dual{2,Float64}},1}) at ./<missing>:0",
      " in (::##J#960#963)(::Int64, ::Int64, ::Array{Float64,2}, ::Array{Any,1}, ::Array{Any,1}, ::Function, ::Array{ForwardDiff.Dual{2,ForwardDiff.Dual{2,Float64}},1}, ::Array{Float64,1}) at ./In[613]:232",
      " in (::#kw##J#962)(::Array{Any,1}, ::#J#962, ::Array{ForwardDiff.Dual{2,ForwardDiff.Dual{2,Float64}},1}, ::Array{Float64,1}) at ./<missing>:0",
      " in (::##961#964{Array{Any,1},Array{Float64,1}})(::Array{ForwardDiff.Dual{2,ForwardDiff.Dual{2,Float64}},1}) at ./In[613]:256",
      " in vector_mode_gradient! at /Users/carlos/.julia/v0.5/ForwardDiff/src/gradient.jl:66 [inlined]",
      " in gradient!(::DiffBase.DiffResult{1,ForwardDiff.Dual{2,Float64},Tuple{Array{ForwardDiff.Dual{2,Float64},1}}}, ::##961#964{Array{Any,1},Array{Float64,1}}, ::Array{ForwardDiff.Dual{2,Float64},1}, ::ForwardDiff.GradientConfig{2,ForwardDiff.Dual{2,Float64},Array{ForwardDiff.Dual{2,ForwardDiff.Dual{2,Float64}},1}}) at /Users/carlos/.julia/v0.5/ForwardDiff/src/gradient.jl:15",
      " in (::ForwardDiff.##41#42{DiffBase.DiffResult{2,Float64,Tuple{Array{Float64,1},Array{Float64,2}}},##961#964{Array{Any,1},Array{Float64,1}},ForwardDiff.HessianConfig{2,Float64,Tuple{Array{ForwardDiff.Dual{2,Float64},1},Array{ForwardDiff.Dual{2,Float64},1}},ForwardDiff.Dual{2,Float64},Array{ForwardDiff.Dual{2,ForwardDiff.Dual{2,Float64}},1}}})(::Array{ForwardDiff.Dual{2,Float64},1}, ::Array{ForwardDiff.Dual{2,Float64},1}) at /Users/carlos/.julia/v0.5/ForwardDiff/src/hessian.jl:19",
      " in vector_mode_dual_eval(::ForwardDiff.##41#42{DiffBase.DiffResult{2,Float64,Tuple{Array{Float64,1},Array{Float64,2}}},##961#964{Array{Any,1},Array{Float64,1}},ForwardDiff.HessianConfig{2,Float64,Tuple{Array{ForwardDiff.Dual{2,Float64},1},Array{ForwardDiff.Dual{2,Float64},1}},ForwardDiff.Dual{2,Float64},Array{ForwardDiff.Dual{2,ForwardDiff.Dual{2,Float64}},1}}}, ::Array{Float64,1}, ::Array{Float64,1}, ::ForwardDiff.JacobianConfig{2,Float64,Tuple{Array{ForwardDiff.Dual{2,Float64},1},Array{ForwardDiff.Dual{2,Float64},1}}}) at /Users/carlos/.julia/v0.5/ForwardDiff/src/api_utils.jl:49",
      " in vector_mode_jacobian!(::Array{Float64,2}, ::ForwardDiff.##41#42{DiffBase.DiffResult{2,Float64,Tuple{Array{Float64,1},Array{Float64,2}}},##961#964{Array{Any,1},Array{Float64,1}},ForwardDiff.HessianConfig{2,Float64,Tuple{Array{ForwardDiff.Dual{2,Float64},1},Array{ForwardDiff.Dual{2,Float64},1}},ForwardDiff.Dual{2,Float64},Array{ForwardDiff.Dual{2,ForwardDiff.Dual{2,Float64}},1}}}, ::Array{Float64,1}, ::Array{Float64,1}, ::ForwardDiff.JacobianConfig{2,Float64,Tuple{Array{ForwardDiff.Dual{2,Float64},1},Array{ForwardDiff.Dual{2,Float64},1}}}) at /Users/carlos/.julia/v0.5/ForwardDiff/src/jacobian.jl:99",
      " in jacobian!(::Array{Float64,2}, ::ForwardDiff.##41#42{DiffBase.DiffResult{2,Float64,Tuple{Array{Float64,1},Array{Float64,2}}},##961#964{Array{Any,1},Array{Float64,1}},ForwardDiff.HessianConfig{2,Float64,Tuple{Array{ForwardDiff.Dual{2,Float64},1},Array{ForwardDiff.Dual{2,Float64},1}},ForwardDiff.Dual{2,Float64},Array{ForwardDiff.Dual{2,ForwardDiff.Dual{2,Float64}},1}}}, ::Array{Float64,1}, ::Array{Float64,1}, ::ForwardDiff.JacobianConfig{2,Float64,Tuple{Array{ForwardDiff.Dual{2,Float64},1},Array{ForwardDiff.Dual{2,Float64},1}}}) at /Users/carlos/.julia/v0.5/ForwardDiff/src/jacobian.jl:32",
      " in hessian! at /Users/carlos/.julia/v0.5/ForwardDiff/src/hessian.jl:23 [inlined]",
      " in hessian!(::DiffBase.DiffResult{2,Float64,Tuple{Array{Float64,1},Array{Float64,2}}}, ::##961#964{Array{Any,1},Array{Float64,1}}, ::Array{Float64,1}) at /Users/carlos/.julia/v0.5/ForwardDiff/src/hessian.jl:17",
      " in vgh(::Function, ::Array{Float64,1}) at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:18",
      " in #trust_region_Hessian_minimization#9(::Int64, ::Float64, ::Int64, ::Bool, ::Int64, ::Function, ::Array{Float64,1}, ::Function) at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:491",
      " in (::#kw##trust_region_Hessian_minimization)(::Array{Any,1}, ::#trust_region_Hessian_minimization, ::Array{Float64,1}, ::Function) at ./<missing>:0",
      " in #backwardsModel#959(::Int64, ::Int64, ::Float64, ::Int64, ::Bool, ::Array{Float64,1}, ::Int64, ::Int64, ::Int64, ::Int64, ::Bool, ::Int64, ::Array{Any,1}, ::#backwardsModel, ::Array{Float64,1}) at ./In[613]:255",
      " in (::#kw##backwardsModel)(::Array{Any,1}, ::#backwardsModel, ::Array{Float64,1}) at ./<missing>:0",
      " in ##2253#2254(::Bool, ::Array{Any,1}, ::Function, ::Array{Float64,1}) at ./In[1067]:36",
      " in (::#kw###2253#2255)(::Array{Any,1}, ::##2253#2255, ::Array{Float64,1}) at ./<missing>:0",
      " in #fluxSense#1727(::Float64, ::Float64, ::Int64, ::Bool, ::Bool, ::Float64, ::Bool, ::Int64, ::#fluxSense, ::##2256#2258, ::##2253#2255, ::Dict{Symbol,Any}, ::Array{Float64,2}, ::Array{Float64,2}, ::Array{Any,1}, ::Array{Float64,1}) at ./In[942]:82",
      " in (::#kw##fluxSense)(::Array{Any,1}, ::#fluxSense, ::Function, ::Function, ::Dict{Symbol,Any}, ::Array{Float64,2}, ::Array{Float64,2}, ::Array{Any,1}, ::Array{Float64,1}) at ./<missing>:0"
     ]
    }
   ],
   "source": [
    "\n",
    "srand(11)\n",
    "startU=randn(100,2)-3\n",
    "startU=randn(100,2)-3\n",
    "\n",
    "\n",
    "# startU=0.1*randn(100,2)-3\n",
    "# startU=zeros(100,2)-3\n",
    "\n",
    "\n",
    "dt = 0.005\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "W = -4\n",
    "noise = 0\n",
    "input = 0\n",
    "sigma = 0.1\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    ":noise=>noise, :input=>input, :sigma=>sigma, :const_add=>0, :init_add=>0)\n",
    "\n",
    "\n",
    "\n",
    "# The backward and costfunc functions should turn a single-scalar parameter W into the matrix W\n",
    "# backward always runs with no within-forward noise, i.e., sigma=0\n",
    "backward = (endpoint; do_plot=false, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;     \n",
    "    backwardsModel(endpoint; do_plot=do_plot, make_dict([\"sigma\"], [0], pars)...)[1]\n",
    "end\n",
    "\n",
    "\n",
    "beta=0\n",
    "\n",
    "costfunc = (startpoints; do_plot=false, verbose=false, nderivs=0, difforder=0, sr=26, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;         \n",
    "    JJ(startpoints; seedrand=sr, beta=beta, \n",
    "        do_plot=do_plot, verbose=verbose, nderivs=nderivs, difforder=difforder, pars...)\n",
    "end\n",
    "\n",
    "if beta==0.003;     cost_limit = -0.00288\n",
    "elseif beta<0.001;  cost_limit = -0.0008\n",
    "elseif beta==0.001; cost_limit = -0.000935\n",
    "elseif beta==0.05;  cost_limit = -0.0485\n",
    "else\n",
    "    error(\"Don't know what cost limit goes with beta %g\\n\", beta)\n",
    "end\n",
    "\n",
    "fluxFinalPoint = zeros(0,2);\n",
    "\n",
    "args = [[\"init_add\" 2], \"const_add\", \"W\"]\n",
    "\n",
    "seed = [0.001, 0.001, 0, -4]\n",
    "\n",
    "\n",
    "# Alternatively, start right from the sticking point:\n",
    "seed = [4.74063,  -4.68228,  2.73165,  -5.6783]\n",
    "\n",
    "# Walls are big enough that we never hit them, so it is immaterial:\n",
    "bbox = [\n",
    "    -15        15  ;\n",
    "    -15        15  ;\n",
    "    -15        15  ;\n",
    "    -20.5  20.5  ; \n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clf()\n",
    "costfunc(startU; do_plot=true, verbose=true, make_dict(args, seed, model_params)...)\n",
    "\n",
    "\n",
    "# YOU CAN DO EITHER THIS:\n",
    "# params, traj = bbox_Hessian_keyword_minimization(seed, args, bbox, \n",
    "# (;params...) -> costfunc(startU; verbose=true, merge(model_params, Dict(params))...), \n",
    "# verbose=true, start_eta=0.01, tol=1e-10, hardbox=true )\n",
    "\n",
    "# OR THIS:  (both get stuck)\n",
    "fluxFinalPoint = [-0.1 -0.1]\n",
    "params, cost, ptraj, gtraj = fluxSense(costfunc, backward, model_params, startU, fluxFinalPoint, args, seed; \n",
    "start_eta=0.01, tol=1e-15, maxiter=400, verbose=true, report_every=1, do_plot=true, cost_limit=cost_limit) # cost_limit=-0.000935) # for beta=0.01\n",
    "\n",
    "# And show the final position\n",
    "clf()\n",
    "costfunc(startU; do_plot=true, verbose=true, make_dict(args, params, model_params)...)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.2",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
