{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORKBOOK: FIGURING OUT TANH SOFT WALLS FOR PARAMETER MINIMIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">TABLE OF CONTENTS</h1>\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "**Updates to the table of contents are periodic, but run the cell below to first start or force an update.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://sites.google.com/site/brodylabhome/files/make_table_of_contents.js')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "macro javascript_str(s) display(\"text/javascript\", s); end\n",
    "\n",
    "javascript\"\"\"\n",
    "$.getScript('https://sites.google.com/site/brodylabhome/files/make_table_of_contents.js')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORKBOOK: Improving and implementing constrained_parabolic_minimization and bbox_Hessian_keyword_minimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We define functions to convert Duals, the variable types used by ForwardDiff, \\nto Floats. This is useful if we want to print out the value of a variable \\n(since print doesn't know how to Duals). Note that after being converted to a Float, no\\ndifferentiation by ForwardDiff can happen!  e.g. after\\n    x = convert(Float64, y)\\nForwardDiff can still differentiate y, but it can't differentiate x\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#########################################\n",
    "#                                       #\n",
    "#                                       #\n",
    "#     PRELIMINARIES                     #\n",
    "#                                       #\n",
    "#                                       #\n",
    "#########################################\n",
    "\n",
    "\n",
    "using PyCall\n",
    "using PyPlot\n",
    "using ForwardDiff\n",
    "using DiffBase\n",
    "\n",
    "pygui(true)\n",
    "\n",
    "import Base.convert\n",
    "convert(::Type{Float64}, x::ForwardDiff.Dual) = Float64(x.value)\n",
    "function convert(::Array{Float64}, x::Array{ForwardDiff.Dual}) \n",
    "    y = zeros(size(x)); \n",
    "    for i in 1:prod(size(x)) \n",
    "        y[i] = convert(Float64, x[i]) \n",
    "    end\n",
    "    return y\n",
    "end\n",
    "\n",
    "include(\"general_utils.jl\")\n",
    "include(\"hessian_utils.jl\")\n",
    "\n",
    "\"\"\"\n",
    "We define functions to convert Duals, the variable types used by ForwardDiff, \n",
    "to Floats. This is useful if we want to print out the value of a variable \n",
    "(since print doesn't know how to Duals). Note that after being converted to a Float, no\n",
    "differentiation by ForwardDiff can happen!  e.g. after\n",
    "    x = convert(Float64, y)\n",
    "ForwardDiff can still differentiate y, but it can't differentiate x\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# basic_gradient_descent()\n",
    "\n",
    "Just here as a simpler minimization version than bbox_Hessian_keywork_minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition basic_gradient_descent(Any, Any, Any, Any) in module Main at In[23]:3 overwritten at In[25]:3.\n",
      "WARNING: Method definition #basic_gradient_descent(Array{Any, 1}, Main.#basic_gradient_descent, Any, Any, Any, Any) in module Main overwritten.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "basic_gradient_descent (generic function with 1 method)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function basic_gradient_descent(func, args, seed, bbox; start_eta=0.01, verbose=true, maxiter=100, report_every=1)\n",
    "\n",
    "    function wallwrap(bdict, pdict)\n",
    "        pdict = copy(pdict)\n",
    "        if typeof(pdict)<:Array\n",
    "            pdict = Dict(pdict)\n",
    "        end\n",
    "\n",
    "        allkeys = keys(bdict)\n",
    "\n",
    "        for k in allkeys\n",
    "            local bbox = bdict[k]\n",
    "            d = 0.5*(bbox[2] - bbox[1])\n",
    "            m = 0.5*(bbox[2] + bbox[1])\n",
    "\n",
    "            pdict[k] = bbox[1] + d*(tanh((pdict[k]-m)/d)+1)\n",
    "        end\n",
    "        return pdict\n",
    "    end\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Given bdict, a dictionary of symbols to [minval, maxval] vectors, args, an array of strings representing\n",
    "    symbols, and params, an array of values corresponding to the args list, puts each param that has an entry \n",
    "    in bdict through the tanh-walling mechanism, and returns the result. Does not modify the contents of the \n",
    "    original params vector (or bdict or args).\n",
    "    \"\"\"\n",
    "    function vector_wrap(bbox, args, eparams)        \n",
    "        params = copy(eparams)\n",
    "        pdict = wallwrap(bbox, make_dict(args, params))\n",
    "        i=1; j=1\n",
    "        for i=1:length(args)\n",
    "            if typeof(args[i])<:Array\n",
    "                params[j:j+args[i][2]-1] = pdict[Symbol(args[i][1])]\n",
    "            j += args[i][2]-1\n",
    "            else\n",
    "                params[j] = pdict[Symbol(args[i])]\n",
    "            end\n",
    "        j = j+1\n",
    "        end\n",
    "        return params\n",
    "    end\n",
    "\n",
    "    \n",
    "    params = seed\n",
    "    eta = start_eta\n",
    "    \n",
    "    cost, grad, hess = keyword_vgh((;pars...)->func(;wallwrap(bbox, pars)...), args, params)\n",
    "    \n",
    "    for i=1:maxiter\n",
    "        new_params = params - eta*grad/norm(grad)\n",
    "        \n",
    "        new_cost, new_grad, new_hess = keyword_vgh((;pars...)->func(;wallwrap(bbox, pars)...), args, new_params)\n",
    "        \n",
    "        if new_cost > cost\n",
    "            eta = eta/2\n",
    "        else\n",
    "            eta = eta*1.1\n",
    "            params = new_params\n",
    "            cost   = new_cost\n",
    "            grad   = new_grad\n",
    "            hess   = new_hess\n",
    "        end\n",
    "\n",
    "        if verbose && rem(i, report_every)==0\n",
    "            @printf(\"%d: eta=%g, cost=%g, ps=\", i, eta, cost); print_vector_g(params); print(\"\\n\")\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # Report the walled-in result:\n",
    "    return vector_wrap!(bbox, args, params)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# constrained_parabolic_minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition constrained_parabolic_minimization(Any, Any, Any) in module Main at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:161 overwritten at In[4]:47.\n",
      "WARNING: Method definition #constrained_parabolic_minimization(Array{Any, 1}, Main.#constrained_parabolic_minimization, Any, Any, Any) in module Main overwritten.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'constrained_parabolic_minimization :: Tuple{Any,Any,Any}' in module 'Main'.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "constrained_parabolic_minimization"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "function constrained_parabolic_minimization(H, G, r; tol=1e-6, min_only=true)\n",
    "\n",
    "Given a Hessian matrix, a gradient vector, and a desired radius from the origin, finds the vector \n",
    "that minimizes the parabola defined by the Hessian and the gradient, subject to the constraint that the\n",
    "vector's length equals the desired radius.\n",
    "\n",
    "PARAMETERS:\n",
    "===========\n",
    "\n",
    "H      A square symmetric matrix. It should have all positive eigenvalues.\n",
    "\n",
    "G      A vector, length equal to the size(H,2)\n",
    "\n",
    "r      desired radius\n",
    "\n",
    "OPTIONAL PARAMETERS:\n",
    "====================\n",
    "\n",
    "tol=1e-6        Numerical tolerance on the computations\n",
    "\n",
    "min_only=true   Return only the minimum, or, if false, all xs for all lambdas that match x'*x = r^2\n",
    "\n",
    "efactor=3       The initial exploration of lambdas will go from -efactor(max(absolute(eig(H)))) to +efactor(max(absolute(eig(H))))\n",
    "\n",
    "lambdastepsize=0.003    The step size for initial exploration of lambdas, un units of efactor. It sshould\n",
    "                probably scale with the smallest difference in the eigenvalues of H; that has not been implemented yet.\n",
    "\n",
    "RETURNS:\n",
    "========\n",
    "\n",
    "x        The vector that minimizes 0.5*x'*H*x + x'*G subject to x'*x = r\n",
    " \n",
    "J        0.5*x'*H*x + x'*G at the returned x\n",
    "\n",
    "lambda   value of the Lagrange multiplier at which the radius constraint is satisfied\n",
    "\n",
    "c        The squared difference between the length of x and r. Should be small, otherwise somthing went wrong!\n",
    "\n",
    "\"\"\"\n",
    "function constrained_parabolic_minimization(H, G, r; tol=1e-6, min_only=true, \n",
    "    do_plot=false, verbose=false, efactor=3.0, max_efactor_tries=10, \n",
    "    lambdastepsize=0.003, minimum_tol=1e-24, tol_delta=1e-3)\n",
    "\n",
    "    #  --- First a couple of helper functions ----\n",
    "    \n",
    "    \"\"\"\n",
    "    function x_of_lambda(lambda)\n",
    "\n",
    "    Given square matrix H, vector G, and passed scalar lambda, returns the vector x that minimizes\n",
    "    \n",
    "    0.5 x'*H*x + x'*G - lambda *x'*x\n",
    "\n",
    "    \"\"\"\n",
    "    function x_of_lambda(lambda)\n",
    "        return inv(H - lambda*eye(size(H,1)))*(-G)\n",
    "    end\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    function q(lambda, r)\n",
    "\n",
    "    Returns the squared difference between r and the norm of x_of_lambda(lambda).\n",
    "    \"\"\"\n",
    "    function q(lambda, r)\n",
    "        return (r - norm(x_of_lambda(lambda)))^2\n",
    "    end\n",
    "\n",
    "\n",
    "    # efactor is the factor that multiplies the biggest eigenvalue of H, to determine the range over which we'll\n",
    "    # look for a lambda that satisfies the norm(x)==r requirement. If we don't find a solution, we iteratively \n",
    "    # increase efactor to try to get there, for a maximum of max_efactor_tries\n",
    "    for m=1:max_efactor_tries\n",
    "        # First scan lambda to find good candidates for minimizing the parabolic \n",
    "        # surface under the x'*x = r^2 constraint\n",
    "        L = eig(H)[1]\n",
    "        L0 = maximum(abs(L))\n",
    "        lambdas = L0*efactor*[-1.0:lambdastepsize:1.0;]\n",
    "        costs = zeros(size(lambdas))\n",
    "        for i in [1:length(lambdas);]\n",
    "            try \n",
    "                costs[i] = q(lambdas[i], r)\n",
    "            catch\n",
    "                costs[i] = Inf\n",
    "            end\n",
    "        end\n",
    "\n",
    "        if do_plot\n",
    "            figure(2); clf();\n",
    "            plot(lambdas, costs, \"b.-\")\n",
    "            xlabel(\"lambda\")\n",
    "            ylabel(\"cost\")\n",
    "        end\n",
    "\n",
    "        # Take all candidates where the derivative of costs changes sign \n",
    "        # from negative to positive (those would be minima),\n",
    "        # plus the smallest and the largest lambdas tested, as candidates\n",
    "        g = append!(prepend!(find(diff(sign(diff(costs))) .> 0.99), [1]), [length(lambdas)])\n",
    "        if verbose\n",
    "            @printf(\"cpm: g (candidate indices) are : \");           print_vector_g(g);        print(\"\\n\")\n",
    "            @printf(\"cpm: and their corresponding costs are : \");   print_vector(costs[g]);   print(\"\\n\");\n",
    "            @printf(\"cpm: and their corresponding lambdas are : \"); print_vector(lambdas[g]); print(\"\\n\");\n",
    "        end\n",
    "        # found_it_flag = 0  # A flag for when we've found at least one lambda that satisfies the r constraint\n",
    "        mytol = tol\n",
    "\n",
    "        while mytol > minimum_tol\n",
    "            lambdas_out = zeros(size(g))\n",
    "            costs_out   = zeros(size(g))\n",
    "            for i in [1:length(g);]\n",
    "                lambdas_out[i], costs_out[i] = one_d_minimizer(lambdas[g[i]], x -> q(x[1], r), start_eta=1, tol=mytol)\n",
    "            end\n",
    "\n",
    "            # Eliminate any lambdas where x'*x doesn't match our desired value r\n",
    "            I = find(costs_out .< tol)\n",
    "            lambdas_out = lambdas_out[I]; costs_out = costs_out[I];\n",
    "\n",
    "            if length(I) > 0; break; end\n",
    "\n",
    "            mytol *= tol_delta\n",
    "        end\n",
    "        if verbose\n",
    "            @printf(\"%d : After searching for lambdas with efactor=%g, we found these : \", m, efactor)\n",
    "            print_vector_g(lambdas_out); print(\"\\n\")\n",
    "        end\n",
    "        if length(lambdas_out) > 0; break; end;\n",
    "        efactor = efactor*4\n",
    "    end\n",
    "    \n",
    "    # Eliminate any repeated lambdas, to within the specified numerical tolerance.\n",
    "    I = setdiff(1:length(lambdas_out), find(diff(lambdas_out) .< tol))\n",
    "    lambdas_out = lambdas_out[I]; costs_out = costs_out[I];\n",
    "    \n",
    "    # Find the parabolic estimate of the cost function at these points\n",
    "    J  = zeros(size(lambdas_out))\n",
    "    xs = zeros(length(G), length(lambdas_out))\n",
    "    for i in [1:length(J);]\n",
    "        xs[:,i] = x_of_lambda(lambdas_out[i])\n",
    "        J[i] = (0.5*xs[:,i]'*H*xs[:,i] + xs[:,i]'*G)[1]\n",
    "    end\n",
    "\n",
    "    # Find and return only the x that has the smallest J\n",
    "    if min_only\n",
    "        I = indmin(J)    \n",
    "    else\n",
    "        I = 1:length(J)\n",
    "    end\n",
    "    return xs[:,I], J[I], lambdas_out[I], costs_out[I]\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bbox_Hessian_keyword_minimization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition bbox_Hessian_keyword_minimization(Any, Any, Any, Any) in module Main at In[22]:84 overwritten at In[47]:84.\n",
      "WARNING: Method definition #bbox_Hessian_keyword_minimization(Array{Any, 1}, Main.#bbox_Hessian_keyword_minimization, Any, Any, Any, Any) in module Main overwritten.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'bbox_Hessian_keyword_minimization :: Tuple{Any,Any,Any,Any}' in module 'Main'.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bbox_Hessian_keyword_minimization"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "function bbox_Hessian_keyword_minimization(seed, args, bbox, func; wallwidth=NaN, start_eta=10, tol=1e-6, \n",
    "    maxiter=400, verbose=false)\n",
    "\n",
    "Like constrained_Hessian_minimization, but uses keyword_hessian!(). \n",
    "\n",
    "PARAMETERS:\n",
    "===========\n",
    "\n",
    "seed        column vector, representing the starting value of the parameters.\n",
    "\n",
    "args        List of strings identifying parameters for differentiation, e.g., [\"const_E\", \"w_self]\n",
    "\n",
    "bbox        If softbox=true (the default), should then be a Dict of Symbol=>[minval maxval] entries. An entry\n",
    "            in this Dict indicates that the corresponding parameter is to be bounded, as indicated by the associated \n",
    "            [minval maxval] vector. The bbox dictionary can have fewer entries than the number of parameters, and its\n",
    "            default value is Dict(), indicating an unbounded search.\n",
    "\n",
    "            If softbox=false, then bbox should be an nargs-by-2 matrix indicating the range for each argument,\n",
    "            with the minima (first column) and maxima (second column), and entries for ALL parameters.\n",
    "\n",
    "func        func must take only optional keyword args, and must \n",
    "            take nderivs=0, difforder=0  and declare any new matrices using ForwardDiffZeros() instead of zeros()\n",
    "\n",
    "\n",
    "OPTIONAL PARAMETERS:\n",
    "====================\n",
    "\n",
    "start_eta    Starting value of the radius.  It's good to start with somethibg biggish, if it is\n",
    "             too much, it'll quickly get cut down.\n",
    "\n",
    "tol=1e-6     Numerical tolerance. If a proposed jump produces a change in func that is less than\n",
    "             this, the minimization stops.\n",
    "\n",
    "maxiter=400  Maximum number of iterations to do before stopping\n",
    "\n",
    "verbose=false   If true, print out a report on each iteration of iteration number, radius size (eta),\n",
    "                what type jump was proposed (\"Newton\" means going straight to global min, \"constrained\" means jump has \n",
    "                norm eta, failed means that finding the minimum at a given radius somehow didn't work). Will also\n",
    "                print out the cosine of the angle between the proposed jump and the gradient.\n",
    "\n",
    "verbose_level   If less than 2, regular verbose output, if 2 or greater, very verbose, for debugging.\n",
    "\n",
    "softbox         If true, then bbox must be a Dict() and we use the tanh() mechanism for putting a fixed limit\n",
    "                on the parameters.\n",
    "\n",
    "hardbox=false   If true, ignores wallwidth, and just rests parameter values to the bounding box if they go outside it.\n",
    "                If false, adds cost function \"walls\" to implement the bounding box.\n",
    "\n",
    "walldith=NaN     Used for putting up cost function \"walls\" that implement the bounding box limits. Can be NaN.\n",
    "                If it is NaN, then the wallwidth is a constant factor of the range width for each argument. If not NaN, must\n",
    "                be an nargs-long vector that indicates the actual wall widths.\n",
    "\n",
    "wallwidth_factor=0.18   Only relevant if wallwidth is NaN, otherwise ignored. For each arg, the wall width\n",
    "                is going to be wall_width_factor*(bbox[i,2] - bbox[i,1])\n",
    "\n",
    "\n",
    "RETURNS:\n",
    "========\n",
    "\n",
    "params       A vector the size of seed that has the last values of the minimizing parameters for func\n",
    "trajectory   A (2+length(params))-by-nsteps matrix. Each column corresponds to an iteration step, and contains\n",
    "                 the value of eta used, the cost, and the value of the parameters at that iteration\n",
    "cost         Final value of objective function\n",
    "\n",
    "\n",
    "EXAMPLE:\n",
    "========\n",
    "\n",
    "function tester(;x=5, y=10, z=20, nderivs=0, difforder=0)\n",
    "    return x^2*y + z/tanh(y)\n",
    "end\n",
    "\n",
    "params, trajectory = bbox_Hessian_keyword_minimization([0.5, 0.5], [\"x\", \"y\"], [1.1 2 ; 1.1 4], tester, \n",
    "    verbose=true, tol=1e-12, start_eta=1);\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "function bbox_Hessian_keyword_minimization(seed, args, bbox, func; start_eta=0.1, tol=1e-6, maxiter=400,\n",
    "    verbose=false, verbose_level=1, verbose_every=1, \n",
    "    softbox=true, hardbox=false, wallwidth=NaN, wallwidth_factor=0.18)\n",
    "\n",
    "    \"\"\"\n",
    "    Given bdict, a dictionary of symbols to [minval, maxval] vectors, and pdict, a dictionary of symbols\n",
    "    to values (or, alternatively, an Array of (Symbol, value) tuples], goes through each of the symbols in \n",
    "    bdict and modifies the corresponding value in pdict putting it through a tanh so the final output lies \n",
    "    within the limits in bdict.  Returns the new pdict.  Makes a copy of pdict so as not to modify the original.\n",
    "    \"\"\"\n",
    "    function wallwrap(bdict, pdict)\n",
    "        pdict = copy(pdict)\n",
    "        if typeof(pdict)<:Array\n",
    "            pdict = Dict(pdict)\n",
    "        end\n",
    "\n",
    "        allkeys = keys(bdict)\n",
    "\n",
    "        for k in allkeys\n",
    "            local bbox = bdict[k]\n",
    "            d = 0.5*(bbox[2] - bbox[1])\n",
    "            m = 0.5*(bbox[2] + bbox[1])\n",
    "\n",
    "            pdict[k] = bbox[1] + d*(tanh((pdict[k]-m)/d)+1)\n",
    "        end\n",
    "        return pdict\n",
    "    end\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    Given bdict, a dictionary of symbols to [minval, maxval] vectors, args, an array of strings representing\n",
    "    symbols, and params, an array of values corresponding to the args list, puts each param that has an entry \n",
    "    in bdict through the tanh-walling mechanism, and returns the result. Does not modify the contents of the \n",
    "    original params vector (or bdict or args).\n",
    "    \"\"\"\n",
    "    function vector_wrap(bbox, args, eparams)        \n",
    "        params = copy(eparams)\n",
    "        pdict = wallwrap(bbox, make_dict(args, params))\n",
    "        i=1; j=1\n",
    "        for i=1:length(args)\n",
    "            if typeof(args[i])<:Array\n",
    "                params[j:j+args[i][2]-1] = pdict[Symbol(args[i][1])]\n",
    "            j += args[i][2]-1\n",
    "            else\n",
    "                params[j] = pdict[Symbol(args[i])]\n",
    "            end\n",
    "        j = j+1\n",
    "        end\n",
    "        return params\n",
    "    end\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Given args, a list of string representing the arguments of interest, a bounding box for each,\n",
    "    and a Symbol=>value dictionary with the corresponding parameters, computes and returns a high cost for \n",
    "    being outside the bounding box\n",
    "    \"\"\"\n",
    "    function wall_cost(args, bbox; wallwidth=NaN, nderivs=0, difforder=0, pars...) \n",
    "        myparams = ForwardDiffZeros(length(pars), 1, nderivs=nderivs, difforder=difforder)\n",
    "        pars2 = Dict()\n",
    "        for i in [1:length(pars);]\n",
    "            pars2[string(pars[i][1])] = pars[i][2]\n",
    "        end\n",
    "        for i in [1:length(args);]\n",
    "            myparams[i] = pars2[args[i]]\n",
    "        end\n",
    "        \n",
    "        if isnan(wallwidth)\n",
    "            # We know that we're going to be taking hessian for params, so declare zeros accordingly:\n",
    "            wallwidth = ForwardDiffZeros(length(myparams), 1, nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "            for i in [1:length(myparams);]\n",
    "                wallwidth[i] = wallwidth_factor*(bbox[i,2]-bbox[i,1])\n",
    "            end\n",
    "        end\n",
    "\n",
    "        retval = 0\n",
    "        for i in [1:length(myparams);]\n",
    "            if myparams[i]<bbox[i,1]\n",
    "                retval += cosh((bbox[i,1]-myparams[i])/wallwidth[i])-1.0\n",
    "            elseif bbox[i,2] < myparams[i]\n",
    "                retval += cosh((myparams[i]-bbox[i,2])/wallwidth[i])-1.0                \n",
    "            end\n",
    "        end\n",
    "\n",
    "        return 2*retval\n",
    "    end\n",
    "\n",
    "    traj_increment = 100\n",
    "    params = seed\n",
    "    eta = start_eta\n",
    "    trajectory = zeros(2+length(params), traj_increment)\n",
    "\n",
    "    if verbose\n",
    "        @printf \"%d: eta=%g ps=\" 0 eta \n",
    "        print_vector(params)\n",
    "        @printf \"\\n\"\n",
    "    end\n",
    "    \n",
    "    if softbox\n",
    "        if !(typeof(bbox)<:Dict); error(\"bhm: If softbox=true, then bbox must eb a Dict\"); end\n",
    "        cost, grad, hess = keyword_vgh((;pars...)->func(;wallwrap(bbox, pars)...), args, params)\n",
    "    elseif hardbox\n",
    "        cost, grad, hess = keyword_vgh((;pars...) -> func(;pars...), args, params)\n",
    "    else\n",
    "        cost, grad, hess = keyword_vgh((;pars...) -> func(;pars...) + wall_cost(args, bbox; wallwidth=wallwidth, pars...),\n",
    "            args, params)        \n",
    "    end\n",
    "        \n",
    "    chessdelta = zeros(size(params))\n",
    "\n",
    "    for i in [1:maxiter;]\n",
    "        if i > size(trajectory, 2)\n",
    "            trajectory = [trajectory zeros(2+length(params), traj_increment)]\n",
    "        end\n",
    "        trajectory[1:2, i]   = [eta;cost]\n",
    "        trajectory[3:end, i] = params\n",
    "        \n",
    "        hessdelta  = - inv(hess)*grad\n",
    "        try\n",
    "            if verbose && verbose_level >= 2\n",
    "                @printf(\"bhm: about to try cpm with grad : \"); print_vector_g(grad); print(\"\\n\")\n",
    "                @printf(\"bhm:   hess :\"); print_vector_g(hess[:]); print(\"\\n\");\n",
    "            end\n",
    "            if verbose && verbose_level >= 2\n",
    "                chessdelta = constrained_parabolic_minimization(hess, grad'', eta, do_plot=true, verbose=true)[1]\n",
    "            else\n",
    "                chessdelta = constrained_parabolic_minimization(hess, grad'', eta)[1]\n",
    "            end\n",
    "            jumptype = \"not failed\"\n",
    "        catch y\n",
    "            jumptype = \"failed\"\n",
    "            if verbose\n",
    "                @printf \"Constrained parabolic minimization failed with error %s\\n\" y\n",
    "                @printf \"\\n\"\n",
    "                @printf \"eta was %g\\n\" eta\n",
    "                @printf \"grad was\\n\"\n",
    "                print_vector(grad)\n",
    "                @printf \"\\n\\nhess was\\n\"\n",
    "                for k in [1:length(grad);]\n",
    "                    print_vector(hess[k,:])\n",
    "                    @printf \"\\n\"\n",
    "                end\n",
    "                @printf \"\\n\"\n",
    "            end\n",
    "        end\n",
    "\n",
    "        if norm(hessdelta) <= eta\n",
    "            new_params = params + hessdelta\n",
    "            jumptype = \"Newton\"\n",
    "        elseif jumptype != \"failed\" \n",
    "            new_params = params + chessdelta\n",
    "            jumptype  = \"constrained\"\n",
    "        end\n",
    "\n",
    "        if jumptype != \"failed\"\n",
    "            if softbox\n",
    "                new_cost, new_grad, new_hess = \n",
    "                    keyword_vgh((;pars...) -> func(;wallwrap(bbox, pars)...), args, new_params)\n",
    "                if verbose && verbose_level >=2\n",
    "                    @printf(\"bhm: had new_params = : \"); print_vector_g(params); print(\"\\n\");\n",
    "                    @printf(\"bhm: and my bbox was : \"); print(bbox); print(\"\\n\")\n",
    "                    @printf(\"bhm: and my wallwrap output was : \"); print(wallwrap(bbox, make_dict(args, new_params))); print(\"\\n\")\n",
    "                    @printf(\"bhm: and this produced new_grad : \"); print_vector_g(new_grad); print(\"\\n\")\n",
    "                    @printf(\"bhm:   new_hess :\"); print_vector_g(new_hess[:]); print(\"\\n\");                                        \n",
    "                end\n",
    "            elseif hardbox\n",
    "                for p in [1:length(new_params);]\n",
    "                    if new_params[p] < bbox[p,1]; new_params[p] = bbox[p,1]; end\n",
    "                    if bbox[p,2] < new_params[p]; new_params[p] = bbox[p,2]; end\n",
    "                 end        \n",
    "                \n",
    "                new_cost, new_grad, new_hess = keyword_vgh((;pars...) -> func(;pars...), args, new_params)\n",
    "            else\n",
    "                new_cost, new_grad, new_hess = keyword_vgh((;pars...) -> func(;pars...) + \n",
    "                        wall_cost(args, bbox; wallwidth=wallwidth, pars...),\n",
    "                    args, new_params)                \n",
    "            end\n",
    "            \n",
    "            if abs(new_cost - cost) < tol || eta < tol\n",
    "                trajectory = trajectory[:,1:i]\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "\n",
    "        if jumptype == \"failed\" || new_cost >= cost  \n",
    "            eta = eta/2\n",
    "            costheta = NaN\n",
    "            if eta < tol\n",
    "                trajectory = trajectory[:,1:i]\n",
    "                break\n",
    "            end\n",
    "        else\n",
    "            eta = eta*1.1\n",
    "            costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "\n",
    "            params = new_params\n",
    "            cost = new_cost\n",
    "            grad = new_grad\n",
    "            hess = new_hess\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            if rem(i, verbose_every)==0\n",
    "                @printf \"%d: eta=%g cost=%g jtype=%s costheta=%.3f ps=\" i eta cost jumptype costheta\n",
    "                print_vector_wrap(bbox, args, params)\n",
    "                @printf \"\\n\"\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vector_wrap(bbox, args, params), trajectory, cost\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACTUAL EXAMPLE: Basic tanh-fitting example (no dynamics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have args = String[\"baseline\",\"amplitude\",\"threshold\",\"slope\"]\n",
      "I have params = [8, 3.1, -0.5, 0.04]\n",
      "I have bbox = Dict{Any,Any}()\n",
      "I have wallwrap(bbox, make_dict(args, params)) = Dict{Any,Any}(Pair{Any,Any}(:amplitude,3.1),Pair{Any,Any}(:threshold,-0.5),Pair{Any,Any}(:slope,0.04),Pair{Any,Any}(:baseline,8.0))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition JJ(Any, Any) in module Main at In[6]:22 overwritten at In[7]:22.\n",
      "WARNING: Method definition #JJ(Array{Any, 1}, Main.#JJ, Any, Any) in module Main overwritten.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100: eta=0.0251123, cost=3795.66, ps=[1.05716, 4.94554, 0.498406, 0.655728]\n",
      "200: eta=2.34987e-05, cost=3794.26, ps=[1.01254, 5.05359, 0.504565, 0.693853]\n",
      "300: eta=2.18872e-14, cost=3794.26, ps=[1.01252, 5.05366, 0.50456, 0.693876]\n",
      "400: eta=5.11849e-17, cost=3794.26, ps=[1.01252, 5.05366, 0.50456, 0.693876]\n",
      "500: eta=6.46889e-17, cost=3794.26, ps=[1.01252, 5.05366, 0.50456, 0.693876]\n",
      "600: eta=6.80955e-17, cost=3794.26, ps=[1.01252, 5.05366, 0.50456, 0.693876]\n",
      "700: eta=7.29267e-17, cost=3794.26, ps=[1.01252, 5.05366, 0.50456, 0.693876]\n",
      "800: eta=7.57057e-17, cost=3794.26, ps=[1.01252, 5.05366, 0.50456, 0.693876]\n",
      "900: eta=8.97576e-17, cost=3794.26, ps=[1.01252, 5.05366, 0.50456, 0.693876]\n",
      "1000: eta=8.96092e-17, cost=3794.26, ps=[1.01252, 5.05366, 0.50456, 0.693876]\n",
      "  2.172437 seconds (874.35 k allocations: 1.557 GB, 7.40% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2×4 Array{Float64,2}:\n",
       " 1.01252  5.05366  0.50456  0.693876\n",
       " 1.0      5.0      0.5      0.8     "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npoints = 1000; srand(400)\n",
    "args = [\"baseline\", \"amplitude\", \"threshold\", \"slope\"]\n",
    "\n",
    "# Generating values for our four params:\n",
    "params = [1 5 0.5 0.8]\n",
    "\n",
    "# Make some points and plot them\n",
    "x = rand(npoints, 1)*6-3\n",
    "y = params[1] + params[2]*0.5*(tanh((x-params[3])/params[4])+1) + randn(npoints,1)*2\n",
    "figure(1); clf();\n",
    "plot(x, y, \".\")\n",
    "\n",
    "# Starting values for the four params. Plot the corresponding curve they generate\n",
    "seed = [8, 3.1, 0, 0.01]\n",
    "xx = -3:0.01:3\n",
    "plot(xx, seed[1] + seed[2]*0.5*(tanh((xx-seed[3])/seed[4])+1), \"g-\")\n",
    "\n",
    "# Cost function.  Note that it takes nderivs and difforder.\n",
    "function JJ(x, y; baseline=0, amplitude=1, threshold=0, slope=1, do_plot=false, fignum=1, clearfig=true,\n",
    "    nderivs=0, difforder=0)\n",
    "\n",
    "    if do_plot\n",
    "        figure(fignum);\n",
    "        if clearfig; clf(); end;\n",
    "        xx = -3:0.01:3; x2=ForwardDiffZeros(size(xx,1), size(xx,2), nderivs=nderivs, difforder=difforder)\n",
    "        for i=1:length(xx); x2[i]=xx[i]; end; xx= x2\n",
    "        \n",
    "        plot(x, y, \".\")\n",
    "        plot(xx, baseline + amplitude*0.5*(tanh((xx-threshold)/slope)+1), \"r-\")\n",
    "    end\n",
    "\n",
    "    yhat =  baseline + amplitude*0.5*(tanh((x-threshold)/slope)+1) \n",
    "    err = yhat - y\n",
    "    return sum(err.*err)\n",
    "end\n",
    "\n",
    "\n",
    "# Now choose between simple adaptive gradient minimization, or constrained Hessian minimization. \n",
    "# Both here bound the value of the baseline parameter in the Dict() below.  Play with those bounds at will.\n",
    "\n",
    "if true\n",
    "    opars = @time(basic_gradient_descent((;pars...) -> JJ(x, y; do_plot=false, pars...), \n",
    "    [\"baseline\", \"amplitude\", \"threshold\", \"slope\"], [8, 3.1, -0.5, 0.04], Dict(),\n",
    "    # Dict(:baseline=>[2, 10.1], :slope=>[0.001 0.02]), \n",
    "    verbose=true, report_every=100, start_eta=0.001, maxiter=1000))\n",
    "else\n",
    "    bbox = [\n",
    "        -20  20 ; \n",
    "        -20  20 ;\n",
    "        -20  20 ; \n",
    "        -20  20 ;\n",
    "    ]\n",
    "    opars = @time(bbox_Hessian_keyword_minimization(seed, args, Dict(:baseline=>[-2, 10], :slope=>[0.001 5]), \n",
    "        (;pars...) -> JJ(x, y; do_plot=false, pars...),\n",
    "    verbose=false, verbose_level=1, softbox=true, start_eta=1)[1])\n",
    "end\n",
    "\n",
    "# Plot the resulting curve, and report both final and generating params\n",
    "plot(xx, opars[1] + opars[2]*0.5*(tanh((xx-opars[3])/opars[4])+1), \"r-\")\n",
    "[opars' ; params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACTUAL EXAMPLE: Same basic tanh-fitting, but packing two params into a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100: eta=0.0114147, cost=4420.02, ps=[1.42829, 3.76419, 0.551434, 0.0490457]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition JJ(Any, Any) in module Main at In[34]:22 overwritten at In[35]:22.\n",
      "WARNING: Method definition #JJ(Array{Any, 1}, Main.#JJ, Any, Any) in module Main overwritten.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200: eta=0.00252822, cost=4404.96, ps=[1.31427, 3.93787, 0.557999, 0.050302]\n",
      "300: eta=0.00123194, cost=4403.68, ps=[1.27977, 3.98762, 0.559051, 0.0512706]\n",
      "400: eta=0.00027286, cost=4403.53, ps=[1.26832, 4.00398, 0.558817, 0.0520889]\n",
      "500: eta=0.000132958, cost=4403.51, ps=[1.26456, 4.00934, 0.558526, 0.0527807]\n",
      "600: eta=2.07486e-05, cost=4403.51, ps=[1.26329, 4.01115, 0.558581, 0.0533823]\n",
      "700: eta=3.4869e-05, cost=4403.51, ps=[1.26286, 4.01176, 0.558581, 0.0539201]\n",
      "800: eta=6.86217e-06, cost=4403.51, ps=[1.26271, 4.01197, 0.558594, 0.0543969]\n",
      "900: eta=1.35904e-05, cost=4403.51, ps=[1.26266, 4.01204, 0.558604, 0.0548577]\n",
      "1000: eta=1.78778e-05, cost=4403.51, ps=[1.26265, 4.01206, 0.558605, 0.0552563]\n",
      "  1.931105 seconds (633.27 k allocations: 1.541 GB, 10.35% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2×4 Array{Float64,2}:\n",
       " 1.44444  4.01206  0.558605  0.0199985\n",
       " 1.0      5.0      0.5       0.8      "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npoints = 1000; \n",
    "args = [[\"baseline\" 2], \"threshold\", \"slope\"]\n",
    "\n",
    "# Generating values for our four params:\n",
    "params = [1 5 0.5 0.8]\n",
    "\n",
    "# Make some points and plot them\n",
    "x = rand(npoints, 1)*6-3\n",
    "y = params[1] + params[2]*0.5*(tanh((x-params[3])/params[4])+1) + randn(npoints,1)*2\n",
    "figure(1); clf();\n",
    "plot(x, y, \".\")\n",
    "\n",
    "# Starting values for the four params. Plot the corresponding curve they generate\n",
    "seed = [8, 3.1, 0, 0.01]\n",
    "xx = -3:0.01:3\n",
    "plot(xx, seed[1] + seed[2]*0.5*(tanh((xx-seed[3])/seed[4])+1), \"g-\")\n",
    "\n",
    "# Cost function.  Note that it takes nderivs and difforder.\n",
    "function JJ(x, y; baseline=[0 1], threshold=0, slope=1, do_plot=false, fignum=1, clearfig=true,\n",
    "    nderivs=0, difforder=0)\n",
    "\n",
    "    if do_plot\n",
    "        figure(fignum);\n",
    "        if clearfig; clf(); end;\n",
    "        xx = -3:0.01:3; x2=ForwardDiffZeros(size(xx,1), size(xx,2), nderivs=nderivs, difforder=difforder)\n",
    "        for i=1:length(xx); x2[i]=xx[i]; end; xx= x2\n",
    "        \n",
    "        plot(x, y, \".\")\n",
    "        plot(xx, baseline[1] + baseline[2]*0.5*(tanh((xx-threshold)/slope)+1), \"r-\")\n",
    "    end\n",
    "\n",
    "    yhat =  baseline[1] + baseline[2]*0.5*(tanh((x-threshold)/slope)+1) \n",
    "    err = yhat - y\n",
    "    return sum(err.*err)\n",
    "end\n",
    "\n",
    "\n",
    "# Now choose between simple adaptive gradient minimization, or constrained Hessian minimization. \n",
    "# Both here bound the value of the baseline parameter in the Dict() below.  Play with those bounds at will.\n",
    "\n",
    "if true\n",
    "    opars = @time(basic_gradient_descent((;pars...) -> JJ(x, y; do_plot=false, pars...), \n",
    "    args, [8, 3.1, -0.5, 0.04], Dict(:baseline=>[-2, 10.1], :slope=>[0.001 0.02]), \n",
    "    verbose=true, report_every=100, start_eta=0.001, maxiter=1000))\n",
    "else\n",
    "    opars = @time(bbox_Hessian_keyword_minimization(seed, args, Dict(:baseline=>[-2, 10], :slope=>[0.001 5]), \n",
    "        (;pars...) -> JJ(x, y; do_plot=false, pars...),\n",
    "    verbose=false, verbose_level=1, softbox=true, start_eta=1)[1])\n",
    "end\n",
    "\n",
    "# Plot the resulting curve, and report both final and generating params\n",
    "plot(xx, opars[1] + opars[2]*0.5*(tanh((xx-opars[3])/opars[4])+1), \"r-\")\n",
    "[opars' ; params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SANDLOT from here on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Symbol,Any} with 2 entries:\n",
       "  :a => [13.6525 25.2305 51.0]\n",
       "  :b => -2000.1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    function wallwrap(bdict, pdict)\n",
    "        pdict = copy(pdict)\n",
    "        if typeof(pdict)<:Array\n",
    "            pdict = Dict(pdict)\n",
    "        end\n",
    "\n",
    "        allkeys = keys(bdict)\n",
    "\n",
    "        for k in allkeys\n",
    "            local bbox = bdict[k]\n",
    "            d = 0.5*(bbox[2] - bbox[1])\n",
    "            m = 0.5*(bbox[2] + bbox[1])\n",
    "\n",
    "            pdict[k] = bbox[1] + d*(tanh((pdict[k]-m)/d)+1)\n",
    "        end\n",
    "        return pdict\n",
    "    end\n",
    "\n",
    "\n",
    "pdict = Dict(:a=>[1.1 25 300], :b=>-2000.1)\n",
    "bdict = Dict(:a=>[12, 51])\n",
    "\n",
    "newdict = wallwrap(bdict, pdict)\n",
    "newdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Any,Any} with 0 entries"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition vector_wrap(Any, Any, Any) in module Main at In[36]:8 overwritten at In[37]:8.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'vector_wrap :: Tuple{Any,Any,Any}' in module 'Main'.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "vector_wrap"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \"\"\"\n",
    "    Given bdict, a dictionary of symbols to [minval, maxval] vectors, args, an array of strings representing\n",
    "    symbols, and params, an array of values corresponding to the args list, puts each param that has an entry \n",
    "    in bdict through the tanh-walling mechanism, and returns the result. Does not modify the contents of the \n",
    "    original params vector (or bdict or args).\n",
    "    \"\"\"\n",
    "    function vector_wrap(bbox, args, eparams)        \n",
    "        params = copy(eparams)\n",
    "        pdict = wallwrap(bbox, make_dict(args, params))\n",
    "        i=1; j=1\n",
    "        for i=1:length(args)\n",
    "            if typeof(args[i])<:Array\n",
    "                params[j:j+args[i][2]-1] = pdict[Symbol(args[i][1])]\n",
    "            j += args[i][2]-1\n",
    "            else\n",
    "                params[j] = pdict[Symbol(args[i])]\n",
    "            end\n",
    "        j = j+1\n",
    "        end\n",
    "        return params\n",
    "    end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×4 Array{Float64,2}:\n",
       " 100.1  25.0  300.0  -444.4"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = [100.1 25 300 -444.4]\n",
    "params = vector_wrap(bdict, [[\"a\" 3], \"b\"], params)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×4 Array{Float64,2}:\n",
       " 10.0368  10.1799  0.5  0.8"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdict = Dict(:baseline=>[10 20])\n",
    "args = [[\"baseline\" 2], \"threshold\", \"slope\"]\n",
    "params = [1 5 0.5 0.8]\n",
    "\n",
    "vector_wrap(bdict, args, params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Any,Any} with 3 entries:\n",
       "  :threshold => 0.5\n",
       "  :slope     => 0.8\n",
       "  :baseline  => [10.0368,10.1799]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wallwrap(bdict, make_dict(args, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,1)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = matread(\"Results.mat\")[\"results\"][2:end,:]\n",
    "figure(1); clf()\n",
    "plot(results[:,1], results[:,2], \".\")\n",
    "ylim(0, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.4",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
