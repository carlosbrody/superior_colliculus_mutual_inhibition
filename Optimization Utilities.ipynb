{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utility functions related to cost function minimizations**\n",
    "\n",
    "The main function defined here to be heavily used elsewhere is **bbox_hessian_keyword_minimization()**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">TABLE OF CONTENTS</h1>\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "**Updates to the table of contents are periodic, but run the cell below to first start or force an update.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('make_table_of_contents.js')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "macro javascript_str(s) display(\"text/javascript\", s); end\n",
    "\n",
    "javascript\"\"\"\n",
    "$.getScript('make_table_of_contents.js')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions for the soft tanh() wall method for putting bounds on parameter values\n",
    "\n",
    "These are functions that help with the soft tanh() wall method for implementing the parameter bounding box.\n",
    "\n",
    "The basic idea of the soft tanh() method is the following:  Let's say we have a scalar function $f(x)$, and we want to find its minimum subject to $x_0 <= x <= x_1$.\n",
    "\n",
    "Let's say that $m$ is the midpoint of the range, $m = (x_0+x_1)/2$, and $d$ is the width of the range, $d = x_1 - x_0$. We're going to use the function $g()$\n",
    "\n",
    "$$\n",
    "   g(x) = x_0 + (x_1-x_0)*\\frac{1}{1 + \\exp(-\\frac{x-m}{d})}\n",
    "$$\n",
    "\n",
    "Note that $g()$ ranges from a strict minimum of $x_0$ to a strict maximum of $x_1$, and it is monotonic in $x$, so for any value of $x$ that is within the range, we can obtain $g^{-1}(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "PyPlot.Figure(PyObject <matplotlib.figure.Figure object at 0x12eb57630>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using PyPlot\n",
    "\n",
    "function g(x, x0, x1)\n",
    "    m = 0.5*(x0+x1)\n",
    "    d = x1-x0\n",
    "    \n",
    "    return x0 + (x1-x0)./(1 + exp.(-(x-m)/d))\n",
    "end\n",
    "\n",
    "h = figure(1); clf(); h[:set_size_inches](3, 2); \n",
    "x0 = 3; x1 = 10; x = -60:0.1:60\n",
    "\n",
    "plot(x, g(x, x0, x1), \"-\"); \n",
    "\n",
    "xlabel(\"xi\"); ylabel(\"x = g(xi)\"); hlines([x0, x1], xlim()[1], xlim()[2]); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our strategy:**\n",
    "\n",
    "Now here is our strategy: instead of minimizing the function $f(x)$ subject to the range constraints, we will instead  minimize the function $f(g(\\xi))$ subject to *no* constraints on $\\xi$. \n",
    "\n",
    "To start the minimization, if we wanted to start at $x = x_{seed}$, we first compute $\\xi_{seed} = g^{-1}(x_{seed})$, and we start at that value of $\\xi_{seed}$. That will correspond to having started at $x_{seed}$, as desired.\n",
    "\n",
    "For every value of $\\xi$ there is a corresponding value of $x = g(\\xi)$. And no matter where $\\xi$ ranges, its corresponding value of $x$ will be strictly bounded within the desired limits.  Note that \n",
    "\n",
    "$$\n",
    "    \\frac{{\\rm d}f}{{\\rm d}x} = \\frac{{\\rm d}f}{{\\rm d}g} \\cdot \\frac{{\\rm d}g}{{\\rm d}\\xi}\n",
    "$$\n",
    "\n",
    "Therefore, as $x$ approaches the rangle limits (which corresponds to $|\\xi - m| >> d$), we will be approaching the flat part of $g()$ and therefore $\\frac{{\\rm d}g}{{\\rm d}\\xi}$ will approach zero and so will $\\frac{{\\rm d}f}{{\\rm d}x}$: this is what prevents the search from going beyond the bounds. Note also that in each iteration of  bbox_hessian_keyword_minimization() we ask for the step that would minimize the cost function *given a certain step magnitude*. This means we will avoid directions that have little impact on the cost function.\n",
    "\n",
    "\n",
    "\n",
    "**The helper functions are**:\n",
    "\n",
    "## wallwrap()\n",
    "\n",
    "Takes a dictionary of range limits, and a dictionary of current free range parameter values and returns a dictionary of the corresponding \"walled\" values, constrained to lie within the desired bounds. (I.e., puts values through tanh())\n",
    "\n",
    "## vector_wrap()\n",
    "\n",
    "does the same as pdict, except that instead of the dictionary of current free range parameter values, it takes a list of strings and a corresponding vector of values, and returns a vector.\n",
    "\n",
    "## inverse_wall()\n",
    "\n",
    "is the inverse of the above two operations: if passed a dictionary of ranges and a dictionary of constrained values, it returns a dictionary of the corresponding free range values. If passed a list of strings and a vector or the corresponding constrained values, returns a vector of the corresponding free-range, unconstrained values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inverse_wall"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@include_me optimization_utils.jl\n",
    "\n",
    "include(\"general_utils.jl\")\n",
    "include(\"constrained_parabolic_minimization.jl\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "pdict = wallwrap(bdict, pdict)\n",
    "Given bdict, a dictionary of symbols to [minval, maxval] vectors, and pdict, a dictionary of symbols\n",
    "to values (or, alternatively, an Array of (Symbol, value) tuples], goes through each of the symbols in \n",
    "bdict and modifies the corresponding value in pdict putting it through a tanh so the final output lies \n",
    "within the limits in bdict.  Returns the new pdict.  Makes a copy of pdict so as not to modify the original.\n",
    "\"\"\"\n",
    "function wallwrap(bdict, epdict)\n",
    "    local pdict = two_level_copy(epdict)  # Must be very careful here! I got bit by the bug of forgetting that without\n",
    "    # an explicit copy() call, Julia does not make copies of the contents of arrays or dictionaries, making it\n",
    "    # easy to inadvertently modify something one did not intend to perturb.  Note the two_level_copy() call, \n",
    "    # necessary to make sure we don't mess up the content of the caller's dictionary.\n",
    "    \n",
    "    if typeof(pdict)<:Array\n",
    "        pdict = Dict(pdict)\n",
    "    end\n",
    "\n",
    "    allkeys = keys(bdict)\n",
    "\n",
    "    for k in allkeys\n",
    "        local bbox = bdict[k]\n",
    "        d = 0.5*(bbox[2] - bbox[1])\n",
    "        m = 0.5*(bbox[2] + bbox[1])\n",
    "\n",
    "        pdict[k] = bbox[1] + d*(tanh((pdict[k]-m)/d)+1)\n",
    "    end\n",
    "    return pdict\n",
    "end\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "params = vector_wrap(bbox, args, eparams)\n",
    "Given bdict, a dictionary of symbols to [minval, maxval] vectors, args, an array of strings representing\n",
    "symbols, and params, an array of values corresponding to the args list, puts each param that has an entry \n",
    "in bdict through the tanh-walling mechanism, and returns the result. Does not modify the contents of the \n",
    "original params vector (or bdict or args).\n",
    "\"\"\"\n",
    "function vector_wrap(bbox, args, eparams)\n",
    "    local params = two_level_copy(eparams)\n",
    "    pdict = wallwrap(bbox, make_dict(args, params))\n",
    "    i=1; j=1\n",
    "    for i=1:length(args)\n",
    "        if typeof(args[i])<:Array\n",
    "            params[j:j+args[i][2]-1] = pdict[Symbol(args[i][1])]\n",
    "            j += args[i][2]-1\n",
    "        else\n",
    "            params[j] = pdict[Symbol(args[i])]\n",
    "        end\n",
    "    j = j+1\n",
    "    end\n",
    "    return params\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "params = inverse_wall(bdict, args, wparams)\n",
    "Given bdict, a dictionary of symbols to [minval, maxval] vectors, args, an array of strings representing\n",
    "symbols, and wparams, an array of values corresponding to the args list where each param that has an entry \n",
    "in bdict has alreadt been through the tanh-walling mechanism, UNwalls the ones that have a bdict entry and\n",
    "returns the result. Does not modify the contents of the original params vector (or bdict or args).\n",
    "\"\"\"\n",
    "function inverse_wall(bdict, args, wparams)\n",
    "    local params = two_level_copy(wparams)\n",
    "    pdict = inverse_wall(bdict, make_dict(args, params))\n",
    "    i=1; j=1\n",
    "    for i=1:length(args)\n",
    "        if typeof(args[i])<:Array\n",
    "            params[j:j+args[i][2]-1] = pdict[Symbol(args[i][1])]\n",
    "            j += args[i][2]-1\n",
    "        else\n",
    "            params[j] = pdict[Symbol(args[i])]\n",
    "        end\n",
    "        j = j+1\n",
    "    end\n",
    "    return params    \n",
    "end\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "pdict = inverse_wall(bdict, wdict)\n",
    "Given bdict, a dictionary of symbols to [minval, maxval] vectors, and wdict, a dictionary of symbols to values\n",
    "(or vectors of values)  UNwalls the ones that have a bdict entry and\n",
    "returns the result. Does not modify the contents of any dictionaries.\n",
    "\"\"\"\n",
    "function inverse_wall(bdict, wdict)\n",
    "    local pdict = two_level_copy(wdict)\n",
    "\n",
    "    allkeys = keys(bdict)\n",
    "    for k in allkeys\n",
    "        local bbox = bdict[k]\n",
    "        d = 0.5*(bbox[2] - bbox[1])\n",
    "        m = 0.5*(bbox[2] + bbox[1])\n",
    "\n",
    "        pdict[k] = m + d*0.5*log((pdict[k]-bbox[1])./(2*d - pdict[k] + bbox[1]))\n",
    "    end\n",
    "    return(pdict)\n",
    "end\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bbox_trust_region_hessian_keyword_minimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trust_region_Hessian_minimization"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@include_me optimization_utils.jl\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "function adaptive_gradient_minimization(seed, func; start_eta=0.1, tol=1e-6, maxiter=400, verbose=false)\n",
    "\n",
    "NEEDS DOCMENTING\n",
    "\"\"\"\n",
    "\n",
    "function adaptive_gradient_minimization(seed, func; start_eta=0.1, tol=1e-6, maxiter=400,\n",
    "    verbose=false)\n",
    "    \n",
    "    params = seed\n",
    "    eta = start_eta\n",
    "\n",
    "    out = DiffBase.GradientResult(params)\n",
    "    ForwardDiff.gradient!(out, func, params)\n",
    "    cost = DiffBase.value(out)\n",
    "    grad = DiffBase.gradient(out)\n",
    "\n",
    "    for i in [1:maxiter;]\n",
    "        new_params = params - eta*grad\n",
    "\n",
    "        ForwardDiff.gradient!(out, func, new_params)\n",
    "        new_cost = DiffBase.value(out)\n",
    "        new_grad = DiffBase.gradient(out)\n",
    "\n",
    "        if abs(new_cost - cost) < tol\n",
    "            break\n",
    "        end\n",
    "    \n",
    "        if new_cost >= cost\n",
    "            eta = eta/2\n",
    "        else\n",
    "            eta = eta*1.1\n",
    "            params = new_params\n",
    "            cost = new_cost\n",
    "            grad = new_grad\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            @printf \"%d: eta=%.3f cost=%.4f ps=[\" i eta cost \n",
    "            for p in [1:length(params);]\n",
    "                @printf \"%.3f\" params[p]\n",
    "                if p<length(params) @printf \", \"; end\n",
    "            end\n",
    "            @printf \"]\\n\"\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return params\n",
    "end\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "#                                                                           #\n",
    "#                   TRUST_REGION_HESSIAN_MINIMIZATION                       #\n",
    "#                                                                           #\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "function trust_region_Hessian_minimization(seed, func; start_eta=10, tol=1e-6, maxiter=400,\n",
    "    verbose=false)\n",
    "\n",
    "(below, x stands for delta_x, the step from the current x=x0 position at which the cost = const)\n",
    "\n",
    "cost = 0.5*x'*H*x + grad*x + const\n",
    "\n",
    "dcost/dx = H*x + grad  ;   dcost/dx = 0  ==> x =  - inv(H)*grad\n",
    "\n",
    "Trust-region says have a parameter lambda, and replace H with hat{H} = H +  I/eta.  \n",
    "When eta is very large, this is equivalent to a straight Newton method jump, \n",
    "because hat{H} ~= H.  But when eta is small, this is more like a small gradient\n",
    "descent step, because for small eta inv(hat{H}) ~= eta and therefore the delta x is like \n",
    "-eta*grad.  So, if the cost function is going down, make eta larger, and if it is going\n",
    "up, make eta a lot smaller. Just like we do in other adaptive methods\n",
    "\n",
    "PARAMETERS:\n",
    "===========\n",
    "\n",
    "seed        column vector, representing the starting value of the parameters.\n",
    "\n",
    "func        Function that takes a vector and returns a scalar.  If you want to\n",
    "            work with a function that tales more parameterrs and returns more than one \n",
    "            output, you can use something like\n",
    "\n",
    "                    x -> orig_func(x, other_params)[1]\n",
    "\n",
    "            You only need the \"[1]\" part if the orig_func returns more outputs than a scalar. \n",
    "\n",
    "OPTIONAL PARAMETERS:\n",
    "====================\n",
    "\n",
    "start_eta=10    Starting value of eta.  It's good to start with somethibg biggish, if it is\n",
    "                too much, it'll quickly get cut down.\n",
    "\n",
    "tol=1e-15       Numerical tolerance. If a proposed jump produces a change in func that is less than\n",
    "               this, the minimization stops.\n",
    "\n",
    "maxiter=400    Maximum number of iterations to do before stopping\n",
    "\n",
    "verbose=false   If true, print out a report on each iteration of iteration number, radius size (eta),\n",
    "                what type jump was proposed (\"Newton\" means going straight to global min, \"constrained\" means jump has \n",
    "                norm eta, failed means that finding the minimum at a given radius somehow didn't work). Will also\n",
    "                print out the cosine of the angle between the proposed jump and the gradient.\n",
    "\n",
    "RETURNS:\n",
    "========\n",
    "\n",
    "params       A vector the size of seed that has the last values of the minimizing parameters for func\n",
    "\n",
    "\"\"\"\n",
    "function trust_region_Hessian_minimization(seed, func; start_eta=10, tol=1e-15, maxiter=400,\n",
    "    verbose=false, verbose_level=1)\n",
    "\n",
    "    params = seed\n",
    "    eta = start_eta\n",
    "\n",
    "    cost, grad, hess = vgh(func, params)\n",
    "    if verbose && verbose_level >= 2\n",
    "        @printf(\"Initial cost, grad, hess:\\n\")\n",
    "        print_vector_g(:cost)\n",
    "        print_vector_g(:grad)\n",
    "        print_vector_g(:hess)\n",
    "    end\n",
    "    \n",
    "\n",
    "    for i in [1:maxiter;]\n",
    "        hathess    = hess + eye(length(grad), length(grad))/eta        \n",
    "        new_params = params - inv(hathess)*grad\n",
    "        new_cost, new_grad, new_hess = vgh(func, new_params)\n",
    "            \n",
    "        if abs(new_cost - cost) < tol\n",
    "            break\n",
    "        end\n",
    "\n",
    "        if new_cost >= cost\n",
    "            eta = eta/2\n",
    "            costheta = NaN\n",
    "        else\n",
    "            eta = eta*1.1\n",
    "            costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "\n",
    "            params = new_params\n",
    "            cost = new_cost\n",
    "            grad = new_grad\n",
    "            hess = new_hess\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            @printf \"%d: eta=%.3f cost=%.4f costheta=%.3f ps=\" i eta cost  costheta\n",
    "            print_vector(params)\n",
    "            @printf \"\\n\"\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return params, cost\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bbox_hessian_keyword_minimization() function()\n",
    "\n",
    "The main function here does parameter optimization, i.e. searching for the minimum of a scalar function of a vector-valued set of payameters. It uses ForwardDiff to compute and use information about gradients and Hessians, and uses the soft tanh() wall method to keep selected parameters strictly within desired limits during the search.\n",
    "\n",
    "The main search function, called bbox_hessian_keyword_minimization(), takes four obligatory parameters (and many optional ones, see its documentation below for details):\n",
    "\n",
    "* seed, a vector with the starting value of some parameters\n",
    "* args, a list of strings, same length as seed, indicating the parameter names associated with each value in seed. \n",
    "* func, a function that returns a scalar and takes only keyword-value argument pairs. All of the strings in args must indicate keyword names for func() that func() knows about.\n",
    "* bbox, a dictionary where each key is a Symbol indicating a parameter name, and the values are two-long vectors, whose elements indicate the desired minimum and maximum of the range for that parameter, respectively. Any key in this dictionary must also be, in string form, in args. But not all entries in args, need be in bbox, the missing ones are assumed to have no bounds. bbox could even be an empty dictionary, indicating no bounds on any parameter.\n",
    "\n",
    "bbox_hessian_keyword_minimization() will start from seed, will search for parameter values that minimize fun, and will return those, along with a variety of information that is diagnostic regarding the search.\n",
    "\n",
    "\n",
    "The function minimizes a scalar function $f()$ of keyword-value pairs, by searching for the parameter values that produce the smallest output. At each step of the minimization, uses ForwardDiff to compute the gradient $\\bf g$ and the Hessian $H$ at the current set of parameter values ${\\bf p_0}$; and then uses constrained_parabolic_minimization() to find the the parameter values ${\\bf p}$, such that $|{\\bf p} - {\\bf p_0}| <= \\eta$, that would minimize the parabolic approximation to the cost function\n",
    "\n",
    "$$\n",
    "J({\\bf p}) = J({\\bf p_o}) + {\\bf g}^T ({\\bf p} - {\\bf p_0}) + \\frac{1}{2} ({\\bf p} - {\\bf p_0})^T \\cdot H \\cdot ({\\bf p} - {\\bf p_0})\n",
    "$$\n",
    "\n",
    "If the cost $f({\\bf p}) < f({\\bf p_0})$, then the step is accepted, the current parameter values become ${\\bf p}$ and $\\eta$ is increased slightly. Otherwise the step is not accepted, current parameter values are not changed, and $\\eta$ is cut by a factor of 2.  Note than when $\\eta$ is very small, this becomes gradient descent. It is only when $\\eta$ is larger that the Hessian begins to play a role.\n",
    "\n",
    "A parameter step for which $|{\\bf p} - {\\bf p_0}| < \\eta$ is called a \"Newton\" jump (it is Newton's method). A step in which $|{\\bf p} - {\\bf p_0}| = \\eta$ is called a \"constrained\" jump. If the `verbose=true option` is selected, then the diagnostic information printed out at each step includes the cosine of the angle between the step taken and the gradient (when this is close to -1, we're doing gradient descent; far from -1, we're using the Hessian information).\n",
    "\n",
    "**FAILURE MODES:**  At each step of the search iteration, constrained_parabolic_minimization(), which itself involves a search, may fail. For diagnostics on this, a matrix called `cpm_traj` is returned. Its first row is the number of iterations run by constrained_parabolic_minimization()'s internal search, at each step of the overall search. If this number is equal to the maximum requested (currently hardcoded as 500 iterations), that means that constrained_parabolic_minimization() returned only because it ran into its maximum iteration limit, not because it was successful, and is therefore a sign of a step that may have been taken in a poor direction.\n",
    "\n",
    "**RETURNING MULTIPLE OUTPUTS IN $f()$:** In the implementation below, $f()$ must either return a scalar, or the first output it returns must be a scalar. That scalar is what will be minimized. The trajectory across the minimization of any further outputs that f() returns will be available in ftraj (see help documentation for `bbox_Hessian_keyword_minimization()` below). **Note that you might want to convert some of those outputs into\n",
    "Float64s, so they don't return as ForwardDiff Duals.**  See example in the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mWARNING: \u001b[39m\u001b[22m\u001b[33mreplacing docs for 'bbox_Hessian_keyword_minimization :: NTuple{4,Any}' in module 'Main'.\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bbox_Hessian_keyword_minimization"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@include_me optimization_utils.jl\n",
    "\n",
    "\n",
    "# Julia 0.5 returns an error if you run Pkg.installed on a package that has not been installed\n",
    "try\n",
    "    Pkg.installed(\"JLD\")\n",
    "catch\n",
    "    Pkg.add(\"JLD\")\n",
    "end\n",
    "\n",
    "# Julia 0.6 does not crash but returns a Void if you run Pkg.installed on a package that has not been installed\n",
    "if isa(Pkg.installed(\"JLD\"), Void)\n",
    "    Pkg.add(\"JLD\")\n",
    "end\n",
    "\n",
    "\n",
    "using JLD\n",
    "using MAT\n",
    "\n",
    "\n",
    "######################################################\n",
    "#                                                    #\n",
    "#         BBOX_HESSIAN_KEYWORD_MINIMIZATION          #\n",
    "#                                                    #\n",
    "######################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "function bbox_Hessian_keyword_minimization(seed, args, bbox, func; start_eta=10, tol=1e-6, \n",
    "maxiter=400, frac_cost_threshold = 0.5, stopping_function = nothing, verbose=false, report_file=\"\")\n",
    "\n",
    "Like constrained_Hessian_minimization, but uses keyword_hessian!(). \n",
    "\n",
    "# PARAMETERS:\n",
    "\n",
    "- seed        column vector, representing the starting value of the parameters.\n",
    "\n",
    "- args        List of strings identifying parameters for differentiation, e.g., [\"const_E\", \"w_self]\n",
    "\n",
    "- bbox        If softbox=true (the default), should then be a Dict of Symbol=>[minval maxval] entries. An entry\n",
    "            in this Dict indicates that the corresponding parameter is to be bounded, as indicated by the associated \n",
    "            [minval maxval] vector. The bbox dictionary can have fewer entries than the number of parameters, and its\n",
    "            default value is Dict(), indicating an unbounded search.\n",
    "                If softbox=false, then bbox should be an nargs-by-2 matrix indicating the range for each argument,\n",
    "            with the minima (first column) and maxima (second column), and entries for ALL parameters.\n",
    "\n",
    "- func      func must take only optional keyword args, and must \n",
    "            take nderivs=0, difforder=0  and declare any new matrices using ForwardDiffZeros() instead of zeros().\n",
    "            [THE ABOVE PART OF THE DOCUMENTATION MUST BE UPDATED FOR JULIA 0.6]\n",
    "            func must either return a scalar, or the first output it returns must be a scalar. \n",
    "            That scalar is what will be minimized. The trajectory across the minimization of \n",
    "            any further outputs that f() returns will be available in ftraj (see RETURNS below)\n",
    "\n",
    "\n",
    "# OPTIONAL PARAMETERS:\n",
    "\n",
    "- start_eta    Starting value of the radius.  It's good to start with somethibg biggish, if it is\n",
    "             too much, it'll quickly get cut down.\n",
    "\n",
    "- tol=1e-6     Numerical tolerance. If a proposed jump produces a change in func that is less than\n",
    "             this, the minimization stops.\n",
    "\n",
    "- maxiter=400  Maximum number of iterations to do before stopping\n",
    "\n",
    "- frac_cost_threshold   When the algorithm is going to take a step, it first checks whether this will reduce the cost.\n",
    "                If the answer is \"no\" then the step is not taken, and the step size is halved. \n",
    "                Small step sizes will make the algorithm gradient-descent-like, so if the step size keeps \n",
    "                getting smaller, eventualluy we get to simple gradient descent. If the cost will be reduced, \n",
    "                another check is done, comparing to the expected cost change, from the quadratic approximation. \n",
    "                If the ratio of the actual cost reduction / expected cost reduction is less than \"frac_cost_threshold\", \n",
    "                then the step is not taken and the step size is halved. Otherwise, the  step is taken, and step \n",
    "                size goes up by 1.1.  A frac_cost_threshold=0 makes both of these checks equivalent.\n",
    "\n",
    "- stopping_function   If present, this should be a function that returns a boolean, that can take as \n",
    "                keyword-value pairs the same args, pars that are given to func, and also takes keyword-value \"cost\"\n",
    "                and keyword_value \"func_out\". The value of cost will be the latest scalar value of func, and the\n",
    "                value of \"func_out\" will be a tuple with any further returns from func. `stopping_function()` is run \n",
    "                each iteration, and if it returns true, the minimization is stopped.\n",
    "\n",
    "- verbose=false   If true, print out a report on each iteration of iteration number, radius size (eta),\n",
    "                what type jump was proposed (\"Newton\" means going straight to global min, \"constrained\" means jump has \n",
    "                norm eta, failed means that finding the minimum at a given radius somehow didn't work). Will also\n",
    "                print out the cosine of the angle between the proposed jump and the gradient.\n",
    "\n",
    "- verbose_level   If less than 2, regular verbose output, if 2 or greater, very verbose, for debugging.\n",
    "\n",
    "- softbox       If true, then bbox must be a Dict() and we use the tanh() mechanism for putting a fixed limit\n",
    "                on the parameters. NO LONGER SUPPORTING ANYTHING OTHER THAN softbox=true (which is the default)\n",
    "\n",
    "- report_file   If non-empty, at each iteration timestep will write into this file outputs trajectory, \n",
    "                (which contains eta, cost, and parameters), cpm_traj, and ftraj (which contains gradient, hessian, \n",
    "                and further cost function outputs).  The file must be a JLD file, and so will end with a .jld extension.\n",
    "                To load the saved dictionary, simply do D = load(filename)  (we have already called \"using JLD\" for you.)\n",
    "\n",
    "\n",
    "\n",
    "# RETURNS:\n",
    "\n",
    "- params       A vector the size of seed that has the last values of the minimizing parameters for func\n",
    "- trajectory   A (2+length(params))-by-nsteps matrix. Each column corresponds to an iteration step, and contains\n",
    "                 the value of eta used, the cost, and the value of the parameters at that iteration\n",
    "- cost         Final value of objective function\n",
    "- cpm_traj     A 4-by-nsteps matrix, containing reports from the contrained parabolic minimization at each timestep.\n",
    "             The first row is niters (how many iterations cpm's 1-d minimization ran for) and the second row is\n",
    "            Dlambda, the last change in the parameter being minimized in cpm's internal search, \n",
    "            the third row is the squared difference between the returned and desired radius (should be very small),\n",
    "            and the fourth row is cost change expected under the quadratic approximation\n",
    "\n",
    "- ftraj     Further components for the trajectory, will be an Array{Any}(3, nsteps). First row is gradient,\n",
    "            second row is Hessian, third row is second-and-further outputs of func, each one at each step of\n",
    "            the minimization. **NOTE** that if these further outputs contain variables that are being minimized, \n",
    "            they'll come out as ForwardDiff Duals, which you might not want!  So, for example, you might want to\n",
    "            convert vectors and matrices into Float64s before returning them in those extra outputs. E.g.,\n",
    "            if you want to return sum(err.*err) as the scalar to be minimized, and also return err, in your \n",
    "            cost function you would write   \" return sum(err.*err), Array{Float64}(err) \".   That way the first,\n",
    "            scalar output can still be differentiated, for minimization, and the second one comes out in readable form.\n",
    "\n",
    "\n",
    "\n",
    "# EXAMPLE:  (see also a more complete example in Cost Function Minimization and Hessian Utilities.ipynb)\n",
    "\n",
    "```\n",
    "function tester(;x=5, y=10, z=20, nderivs=0, difforder=0)\n",
    "    return x^2*y + z/tanh(y)\n",
    "end\n",
    "\n",
    "params, trajectory = bbox_Hessian_keyword_minimization([0.5, 0.5], [\"x\", \"y\"], [1.1 2 ; 1.1 4], tester, \n",
    "    verbose=true, tol=1e-12, start_eta=1);\n",
    "```\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "function bbox_Hessian_keyword_minimization(seed, args, bbox, func; start_eta=0.1, tol=1e-6, maxiter=400,\n",
    "    frac_cost_threshold = 0.5, stopping_function = nothing,\n",
    "    verbose=false, verbose_level=1, verbose_every=1, softbox=true, hardbox=false, report_file=\"\")\n",
    "\n",
    "    # --- check that saving will be done to a .jld file ---\n",
    "    if length(report_file)>0 && splitext(report_file)[2] != \".jld\"\n",
    "        if splitext(report_file)[2] == \"\"\n",
    "            report_file = report_file * \".jld\"\n",
    "        else\n",
    "            error(\"Sorry, report_file can only write to JLD files, the extension has to be .jld\")\n",
    "        end\n",
    "    end\n",
    "\n",
    "    \n",
    "    # --------- Initializing the trajectory trace and function wrapper--------\n",
    " \n",
    "    traj_increment = 100\n",
    "    params = 0  # Make sure to have this here so that params stays defined beyond the try/catch\n",
    "    if ( !(typeof(bbox)<:Dict) ); error(\"Currently only supporting softbox=true, bbox must be a Dict\"); end;\n",
    "    try\n",
    "        params = copy(inverse_wall(bbox, args, seed))\n",
    "    catch\n",
    "        error(\"Were all initial param values within the indicated walls?\")\n",
    "    end\n",
    "    eta = start_eta\n",
    "    trajectory = zeros(2+length(params), traj_increment); cpm_traj = zeros(4, traj_increment)\n",
    "    \n",
    "    ftraj = Array{Any}(3,0)  # will hold gradient, hessian, and further_out,  per iteration\n",
    "\n",
    "    further_out =[];  # We define this variable here so it will be available for stashing further outputs from func\n",
    "    stopping_func_out = false;   # Default value of stopping_func()\n",
    "    \n",
    "    # Now we define a wrapper around func() to do three things: (a) wallwrap parameters using the softbox method;\n",
    "    # (b) return as the desired scalar the first output of func; (c) stash in further_out any further outputs of func\n",
    "    internal_func = (;pars...) -> begin\n",
    "        fresults = func(;wallwrap(bbox, pars)...)   # note use of bbox external to this begin...end\n",
    "        if typeof(fresults)<:Tuple\n",
    "            answer = fresults[1]\n",
    "            further_out = fresults[2:end]\n",
    "        else\n",
    "            answer = fresults\n",
    "        end\n",
    "        return answer  # we assume that the first output of func() will always be a scalar, and that's what we return for ForwardDiff\n",
    "    end\n",
    "\n",
    "    # --------- END Initializing the trajectory trace --------\n",
    "\n",
    "    if verbose\n",
    "        @printf \"%d: eta=%g ps=\" 0 eta \n",
    "        print_vector(vector_wrap(bbox, args, params))\n",
    "        @printf \"\\n\"\n",
    "    end\n",
    "    \n",
    "    if softbox\n",
    "        if !(typeof(bbox)<:Dict); error(\"bhm: If softbox=true, then bbox must eb a Dict\"); end\n",
    "        cost, grad, hess = keyword_vgh(internal_func, args, params)  # further_out will be mutated\n",
    "    elseif hardbox\n",
    "        error(\"Sorry, no longer supporting hardbox=true\")\n",
    "    else\n",
    "        error(\"Sorry, no longer supporting softbox=false\")\n",
    "    end\n",
    "        \n",
    "    chessdelta = zeros(size(params))\n",
    "    expected_cost_delta = 0   # this will be the quadratically expected cost change, either the Newton prediction or the constrained prediction\n",
    "    hess_cost_delta     = 0   # this will hold the Newtorn prediction\n",
    "    chess_cost_delta    = 0   # this will hold the constrained prediction\n",
    "    \n",
    "    i=0  # here so variable i is available outside the loop\n",
    "    for i in [1:maxiter;]\n",
    "        if i > size(trajectory, 2)\n",
    "            trajectory = [trajectory zeros(2+length(params), traj_increment)]\n",
    "            cpm_traj   = [cpm_traj   zeros(4, traj_increment)]\n",
    "        end\n",
    "        trajectory[1:2, i]   = [eta;cost]\n",
    "        trajectory[3:end, i] = vector_wrap(bbox, args, params)\n",
    "        ftraj = [ftraj [grad, hess, further_out]]\n",
    "\n",
    "        if length(report_file)>0\n",
    "            save(report_file, Dict(\"traj\"=>trajectory[:,1:i], \"cpm_traj\"=>cpm_traj[:,1:i], \"ftraj\"=>ftraj))\n",
    "        end\n",
    "        \n",
    "        hessdelta  = - inv(hess)*grad\n",
    "        hess_cost_delta = 0.5 * hessdelta' * hess * hessdelta + grad'*hessdelta   # Netwon prediction for how much cost should change    \n",
    "        try\n",
    "            if verbose && verbose_level >= 2\n",
    "                @printf(\"bhm: about to try cpm with grad : \"); print_vector_g(grad); print(\"\\n\")\n",
    "                @printf(\"bhm:   hess :\"); print_vector_g(hess[:]); print(\"\\n\");\n",
    "            end\n",
    "            if verbose && verbose_level >= 2\n",
    "                cpm_out = constrained_parabolic_minimization(hess, grad'', eta, \n",
    "                    maxiter=500, tol=1e-20, do_plot=true, verbose=true)                \n",
    "            else\n",
    "                cpm_out = constrained_parabolic_minimization(hess, grad'', eta, maxiter=500, tol=1e-20)\n",
    "            end\n",
    "            chess_cost_delta = cpm_out[2]\n",
    "            chessdelta = cpm_out[1]; \n",
    "            \n",
    "            cpm_traj[1,i] = cpm_out[5]; cpm_traj[2,i] = cpm_out[6]; #  niters and Dlambda\n",
    "            cpm_traj[3,i] = cpm_out[4]; cpm_traj[4,i] = cpm_out[2]; #  radius error and quadratically predicted change in J\n",
    "            jumptype = \"not failed\"\n",
    "        catch y\n",
    "            if isa(y, InterruptException); throw(InterruptException()); end  # External interrupts should not be catchable\n",
    "            jumptype = \"failed\"\n",
    "            if verbose\n",
    "                @printf \"Constrained parabolic minimization failed with error %s\\n\" y\n",
    "                @printf \"\\n\"\n",
    "                @printf \"eta was %g\\n\" eta\n",
    "                @printf \"grad was\\n\"\n",
    "                print_vector(grad)\n",
    "                @printf \"\\n\\nhess was\\n\"\n",
    "                for k in [1:length(grad);]\n",
    "                    print_vector(hess[k,:])\n",
    "                    @printf \"\\n\"\n",
    "                end\n",
    "                @printf \"\\n\"\n",
    "                matwrite(\"error_report.mat\", Dict(\"grad\"=>grad, \"hess\"=>hess, \"eta\"=>eta))\n",
    "            end\n",
    "            break\n",
    "        end\n",
    "\n",
    "        if norm(hessdelta) <= eta\n",
    "            new_params = params + hessdelta\n",
    "            jumptype = \"Newton\"\n",
    "            expected_cost_delta = hess_cost_delta\n",
    "        elseif jumptype != \"failed\" \n",
    "            new_params = params + chessdelta\n",
    "            jumptype  = \"constrained\"\n",
    "            expected_cost_delta = chess_cost_delta\n",
    "        end\n",
    "\n",
    "        if jumptype != \"failed\"\n",
    "            new_cost, new_grad, new_hess = keyword_vgh(internal_func, args, new_params)   # further_out may mutate\n",
    "            if stopping_function != nothing\n",
    "                stopping_func_out = stopping_function(; cost=cost, func_out=further_out, \n",
    "                    make_dict(args, new_params)...)\n",
    "            end\n",
    "            if verbose && verbose_level >=2\n",
    "                @printf(\"bhm: had new_params = : \"); print_vector_g(vector_wrap(bbox, args, params)); print(\"\\n\");\n",
    "                @printf(\"bhm: and my bbox was : \"); print(bbox); print(\"\\n\")\n",
    "                @printf(\"bhm: and my wallwrap output was : \"); print(wallwrap(bbox, make_dict(args, new_params))); print(\"\\n\")\n",
    "                @printf(\"bhm: and this produced new_grad : \"); print_vector_g(new_grad); print(\"\\n\")\n",
    "                @printf(\"bhm:   new_hess :\"); print_vector_g(new_hess[:]); print(\"\\n\");                                        \n",
    "            end\n",
    "            \n",
    "            if abs(new_cost - cost) < tol || eta < tol || stopping_func_out\n",
    "                if verbose\n",
    "                    @printf(\"About to break -- stop_func_out = %s, tol=%g, new_cost-cost=%g, eta=%g\\n\", \n",
    "                        stopping_func_out, tol, new_cost-cost, eta)\n",
    "                end\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "\n",
    "        if jumptype == \"failed\" || cost <= new_cost || (new_cost - cost)/expected_cost_delta <= frac_cost_threshold  \n",
    "            if verbose\n",
    "                @printf(\"eta going down: \")\n",
    "                if jumptype==\"failed\"; @printf(\"jtype=failed\");\n",
    "                else                   @printf(\"cost (new-old)/expect = %.3f\", (new_cost - cost)/expected_cost_delta)\n",
    "                end\n",
    "                @printf(\" new_cost-cost=%g and jumptype='%s'\\n\", new_cost-cost, jumptype)\n",
    "                if verbose_level >= 2\n",
    "                    nwp = vector_wrap(bbox, args, new_params); wp = vector_wrap(bbox, args, params)\n",
    "                    @printf(\"   vvv: proposed new params were : \"); print_vector_g(nwp); print(\"\\n\")\n",
    "                    @printf(\"   vvv: proposed delta params was : \"); print_vector_g(nwp-wp); print(\"\\n\")\n",
    "                    @printf(\"   vvv: grad was : \"); print_vector_g(grad); print(\"\\n\")\n",
    "                    costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "                    @printf(\"   vvv: costheta of proposed jump was %g\\n\", costheta)\n",
    "                end\n",
    "            end\n",
    "            eta = eta/2\n",
    "            costheta = NaN\n",
    "            if eta < tol || stopping_func_out\n",
    "                if verbose\n",
    "                    @printf(\"About to break -- stop_func_out = %s, tol=%g, new_cost-cost=%g, eta=%g\\n\", \n",
    "                        stopping_func_out, tol, new_cost-cost, eta)\n",
    "                end\n",
    "                break\n",
    "            end\n",
    "        else\n",
    "            eta = eta*1.1\n",
    "            costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "\n",
    "            params = new_params\n",
    "            cost = new_cost\n",
    "            grad = new_grad\n",
    "            hess = new_hess\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            if rem(i, verbose_every)==0\n",
    "                @printf \"%d: eta=%g cost=%g jtype=%s costheta=%.3f ps=\" i eta cost jumptype costheta\n",
    "                print_vector_g(vector_wrap(bbox, args, params))\n",
    "                @printf \"\\n\"\n",
    "                if verbose_level >= 3\n",
    "                    @printf \"    At this point, grad is =\"\n",
    "                    print_vector_g(grad)\n",
    "                    @printf \"\\n\"                \n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    trajectory = trajectory[:,1:i]; cpm_traj = cpm_traj[:,1:i]\n",
    "    if length(report_file)>0\n",
    "        save(report_file, Dict(\"traj\"=>trajectory, \"cpm_traj\"=>cpm_traj, \"ftraj\"=>ftraj))\n",
    "    end\n",
    "    \n",
    "    return vector_wrap(bbox, args, params), trajectory, cost, cpm_traj, ftraj\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic example of using bbox_Hessian_keyword_minimization()\n",
    "\n",
    "We'll fit a sigmoid to some data generated from a sigmoid with noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final costs were: [4574.67, 4574.67, 4529.32, 4495.3]\n"
     ]
    }
   ],
   "source": [
    "pygui(true)\n",
    "\n",
    "sr = Int64(round(time()*1000))\n",
    "# sr = 1510162002784 # For these values of sr\n",
    "# sr = 1509561656447 # when start_eta=1, the threshold quickly goes very positive and the minimization gets stuck\n",
    "#\n",
    "# sr = 1510164239381   # For this value, it gets stuck at a very small inverse slope param... don't know why\n",
    "srand(sr)\n",
    "\n",
    "\n",
    "npoints = 1000; # srand(400)\n",
    "args = [\"baseline\", \"amplitude\", \"threshold\", \"slope\"]\n",
    "\n",
    "# Generating values for our four params:\n",
    "params = [1 5 0.5 0.8]\n",
    "\n",
    "# Make some points and plot them\n",
    "x = rand(npoints, 1)*6-3\n",
    "y = params[1] + params[2]*0.5*(tanh.((x-params[3])/params[4])+1) + randn(npoints,1)*2\n",
    "figure(1); clf();\n",
    "plot(x, y, \".\")\n",
    "\n",
    "# Starting values for the four params. Plot the corresponding curve they generate\n",
    "seed = [8, 3.1, 0, 0.02]\n",
    "xx = -3:0.01:3\n",
    "plot(xx, seed[1] + seed[2]*0.5*(tanh.((xx-seed[3])/seed[4])+1), \"g-\")\n",
    "\n",
    "if FDversion() < 0.6\n",
    "    # If using ForwardDiff version < 0.6  (e.g., with Julia 0.5.2) --------\n",
    "    #\n",
    "    # Cost function.  Note that it takes nderivs and difforder as parameters. First output is the scalar\n",
    "    # that will be minimized, and we also returns a second output whose trajectory will be stashed \n",
    "    # by bbox in ftraj as a diagnostic during the minimization.\n",
    "    function JJ(x, y; baseline=0, amplitude=1, threshold=0, slope=1, do_plot=false, fignum=1, clearfig=true,\n",
    "        nderivs=0, difforder=0)\n",
    "\n",
    "        if do_plot\n",
    "            figure(fignum);\n",
    "            if clearfig; clf(); end;\n",
    "            xx = -3:0.01:3; x2=ForwardDiffZeros(size(xx,1), size(xx,2), nderivs=nderivs, difforder=difforder)\n",
    "            for i=1:length(xx); x2[i]=xx[i]; end; xx= x2\n",
    "\n",
    "            plot(x, y, \".\")\n",
    "            plot(xx, baseline + amplitude*0.5*(tanh((xx-threshold)/slope)+1), \"r-\")\n",
    "        end\n",
    "\n",
    "        yhat =  baseline + amplitude*0.5*(tanh((x-threshold)/slope)+1) \n",
    "        err = yhat - y\n",
    "        return sum(err.*err), Array{Float64}(err)    # Note first output, the scalar to be minimized,\n",
    "        # may be ForwardDiff Duals during the minimization, which is fine, so it can be differentiated.\n",
    "        # The second one we cast into regular Float64 so it comes out readable.\n",
    "    end\n",
    "else \n",
    "    # If using ForwardDiff version >= 0.6  (e.g., with Julia 0.6) --------\n",
    "    #\n",
    "    # Cost function.  Note that it takes nderivs and difforder as parameters. First output is the scalar\n",
    "    # that will be minimized, and we also returns a second output whose trajectory will be stashed \n",
    "    # by bbox in ftraj as a diagnostic during the minimization.\n",
    "    function JJ(x, y; baseline=0, amplitude=1, threshold=0, slope=1, do_plot=false, fignum=1, clearfig=true)\n",
    "\n",
    "        if do_plot\n",
    "            figure(fignum);\n",
    "            if clearfig; clf(); end;\n",
    "            xx = -3:0.01:3; x2=zeros(get_eltype((baseline,amplitude,threshold,slope)), size(xx,1), size(xx,2))\n",
    "            for i=1:length(xx); x2[i]=xx[i]; end; xx= x2\n",
    "\n",
    "            plot(x, y, \".\")\n",
    "            plot(xx, baseline + amplitude*0.5*(tanh.((xx-threshold)/slope)+1), \"r-\")\n",
    "        end\n",
    "\n",
    "        yhat =  baseline + amplitude*0.5*(tanh.((x-threshold)/slope)+1) \n",
    "        err = yhat - y\n",
    "        return sum(err.*err), get_value(err)    # Note first output, the scalar to be minimized,\n",
    "        # may be ForwardDiff Duals during the minimization, which is fine, so it can be differentiated.\n",
    "        # The second one we use get_value to turn into regular Float64 array so it comes out readable.\n",
    "    end\n",
    "\n",
    "end\n",
    "\n",
    "\n",
    "if ~isdir(\"Trash\"); mkdir(\"Trash\"); end;  # we're going to put the iteration-step by iteration-step report file there\n",
    "\n",
    "bbox = Dict(:baseline=>[-2, 10], :slope=>[0.001 5])\n",
    "func = (;pars...) -> JJ(x, y; do_plot=false, pars...)\n",
    "\n",
    "stopping_func = (;cost=0, func_out=[], pars...) -> return cost<1500;   # Make that a high number and it'll stop early\n",
    "\n",
    "opars, traj, cost, cpm_traj, ftraj = bbox_Hessian_keyword_minimization(seed, args, bbox, func, \n",
    "frac_cost_threshold = 0.5, stopping_function = stopping_func,\n",
    "verbose=false, verbose_level=2, softbox=true, start_eta=0.1, report_file=\"Trash/example_report.jld\")\n",
    "\n",
    "# Note that the gradient at step i of the minimization will be available as ftraj[1,i], the hessian will be \n",
    "# in ftraj[2,i], and the error vector, which is the first of the extra outputs of JJ(), will be in ftraj[3,i][1].\n",
    "# In our example JJ() produced only one extra output; a second extra output would be in ftraj[3,i][2], and so on.\n",
    "\n",
    "# Plot the resulting curve, and report both final and generating params\n",
    "figure(1);\n",
    "plot(xx, opars[1] + opars[2]*0.5*(tanh.((xx-opars[3])/opars[4])+1), \"r-\")\n",
    "[opars' ; params]\n",
    "xlabel(\"x\"); ylabel(\"y\"); title(\"green is sigmoid with starting params, red is end\")\n",
    "\n",
    "\n",
    "figure(2); clf();\n",
    "ax1 = subplot(2,1,1)\n",
    "plot(cpm_traj[4,:], \".-\")\n",
    "plot(traj[2,2:end] - traj[2,1:end-1], \".-\")\n",
    "grid(\"on\")\n",
    "remove_xtick_labels()\n",
    "legend([\"expected cost change\", \"actual cost change\"])\n",
    "\n",
    "subplot(2,1,2)\n",
    "plot((traj[2,2:end] - traj[2,1:end-1])./cpm_traj[4,1:end-1], \".-\")\n",
    "plot(traj[1,2:end]./traj[1,1:end-1], \".-\")\n",
    "grid(\"on\")\n",
    "\n",
    "legend([\"actual/expected cost change\", \"fractional change in eta\"])\n",
    "\n",
    "@printf(\"Final costs were: \"); print_vector_g(traj[2,end-3:end]); print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       " 4448.3 \n",
       " 4438.15\n",
       " 4437.78\n",
       " 4437.78"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traj[2,end-3:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run this cell if you want to overlay, step by step, the result of the fit at that step.\n",
    "\n",
    "figure(1); \n",
    "for i=1:size(traj,2)\n",
    "    tpars = traj[3:end,i]\n",
    "    plot(xx, tpars[1] + tpars[2]*0.5*(tanh.((xx-tpars[3])/tpars[4])+1), \"r-\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4527.677869931307, [5.07651e-8, 2.61774e-8, -6.0073e-5, 2.03845e-5], [2000.0 693.379 -2625.9 678.824; 693.379 692.583 -1023.2 91.1165; -2625.9 -1023.2 1.83051e6 -3.31238e5; 678.824 91.1165 -3.31238e5 2.68709e5])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value, grad, hess = keyword_vgh((;pars...) -> func(;pars...)[1], args, [opars[1], opars[2], opars[3], opars[4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([393.204, 2294.79, 2.01362e5, 1.89786e6], [0.396374 0.918088 0.000710389 0.00142884; -0.918089 0.396374 -0.000568522 0.00053861; -8.9677e-5 0.00140966 0.199244 -0.979949; -0.000801741 -0.000722198 0.97995 0.199243])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D,V = eig(hess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
