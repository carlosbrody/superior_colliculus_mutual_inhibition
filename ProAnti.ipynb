{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sun 2017-09-10 8pm:** Focusing on ProAnti, using multiple rule_and_delay_period values, as well as multiple post_target_period_values.  Seems to be working fine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">TABLE OF CONTENTS</h1>\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "**Updates to the table of contents are periodic, but run the cell below to first start or force an update.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://sites.google.com/site/brodylabhome/files/make_table_of_contents.js')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "macro javascript_str(s) display(\"text/javascript\", s); end\n",
    "\n",
    "javascript\"\"\"\n",
    "$.getScript('https://sites.google.com/site/brodylabhome/files/make_table_of_contents.js')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We define functions to convert Duals, the variable types used by ForwardDiff, \\nto Floats. This is useful if we want to print out the value of a variable \\n(since print doesn't know how to Duals). Note that after being converted to a Float, no\\ndifferentiation by ForwardDiff can happen!  e.g. after\\n    x = convert(Float64, y)\\nForwardDiff can still differentiate y, but it can't differentiate x\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "using PyCall\n",
    "using PyPlot\n",
    "using ForwardDiff\n",
    "using DiffBase\n",
    "using MAT\n",
    "\n",
    "pygui(true)\n",
    "\n",
    "import Base.convert\n",
    "convert(::Type{Float64}, x::ForwardDiff.Dual) = Float64(x.value)\n",
    "function convert(::Array{Float64}, x::Array{ForwardDiff.Dual}) \n",
    "    y = zeros(size(x)); \n",
    "    for i in 1:prod(size(x)) \n",
    "        y[i] = convert(Float64, x[i]) \n",
    "    end\n",
    "    return y\n",
    "end\n",
    "\n",
    "include(\"general_utils.jl\")\n",
    "include(\"hessian_utils.jl\")\n",
    "\n",
    "\"\"\"\n",
    "We define functions to convert Duals, the variable types used by ForwardDiff, \n",
    "to Floats. This is useful if we want to print out the value of a variable \n",
    "(since print doesn't know how to Duals). Note that after being converted to a Float, no\n",
    "differentiation by ForwardDiff can happen!  e.g. after\n",
    "    x = convert(Float64, y)\n",
    "ForwardDiff can still differentiate y, but it can't differentiate x\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup -- definitions of forwardModel() and backwardsModel()\n",
    "\n",
    "These are functions that run arbitrary 0.5 * (1+tanh(x)) - style rate networks, either forwards in time, or backwards in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backwardsModel"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "o = g(z)    squashing tanh function, running from 0 to 1, is equal to 0.5 when input is 0.\n",
    "\"\"\"\n",
    "function g(z)\n",
    "    return 0.5*tanh.(z)+0.5\n",
    "end\n",
    "    \n",
    "\"\"\"\n",
    "z = g^-1(o)    inverse of squashing tanh function, input must be in (0, 1), output is zero when passed 0.5.\n",
    "\"\"\"\n",
    "function ginverse(z)\n",
    "    return 0.5*log.(z./(1-z))\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "forwardModel(startU; dt=0.01, tau=0.1, nsteps=100, input=[0.1, 0], noise=[], W=[0 -5;-5 0], \n",
    "init_add=0, start_add=0, const_add=0, sigma=0, gleak=1, U_rest=0, \n",
    "    do_plot=false, nderivs=0, difforder=0, clearfig=true, fignum=1, dUdt_mag_only=false,\n",
    "    warn_if_unused_params=false)\n",
    "\n",
    "Runs a tanh() style-network forwards in time, given its starting point, using simple Euler integration\n",
    "    tau dU/dt = -U + W*V + I\n",
    "    V = 0.5*tanh(U)+ 0.5\n",
    "\n",
    "**PARAMETERS:**\n",
    "\n",
    "startU     A column vector, nunits-by-1, indicating the values of U at time zero\n",
    "\n",
    "\n",
    "**OPTIONAL PARAMETERS**\n",
    "\n",
    "dt      Scalar, timestep size\n",
    "\n",
    "tau     Scalar, in seconds\n",
    "\n",
    "gleak   \n",
    "        dUdt will have a term equal to gleak*(U_rest - U)\n",
    "U_rest\n",
    "\n",
    "nsteps  Number of timesteps to run, including time=0.\n",
    "\n",
    "input   Either an nunits-by-1 vector, in which case inputs to each unit are constant\n",
    "        across time, or a matrix, nunits-by-nsteps, indicating input for each unit at each timepoint.\n",
    "\n",
    "W       Weight matrix, nunits-by-nunits\n",
    "\n",
    "init_add    DEPRECATED: Vector or scalar that gets added to the input current at very first timestep.\n",
    "            Deprecated because this made it dt-dependent. Replaced by start_add.\n",
    "\n",
    "start_add   Vector or scalar that gets added, once, to the initial U[:,1], before the integration process begins.\n",
    "\n",
    "const_add   Scalar that gets added to U after every timestep\n",
    "\n",
    "sigma       After each timestep, add sigma*sqrt(dt)*randn() to each element of U\n",
    "\n",
    "do_plot   Default false, if true, plots V of up to the first two dimensions\n",
    "\n",
    "fignum     Figure number on which to plot\n",
    "\n",
    "clrearfig  If true, the figure is first cleared, otherwise any plot ois overlaid\n",
    "\n",
    "nderivs, difforder     Required for making sure function can create its own arrays and \n",
    "                       still be differentiated\n",
    "\n",
    "dUdt_mag_only  If true, returns |dUdt|^2 from the first timestep only, then stops.\n",
    "\n",
    "warn_if_unused_params     If true, pronts out a warning of some of the passed parameters are not used.\n",
    "\n",
    "** RETURNS:**\n",
    "\n",
    "Uend Vend       nunits-by-1 vectors representing the final values of U and V that were found.\n",
    "U, V            nunits-by-nsteps matrices containing the full trajectories\n",
    "\n",
    "\"\"\"\n",
    "function forwardModel(startU; dt=0.01, tau=0.1, nsteps=100, input=[], noise=[], W=[0 -5;-5 0], \n",
    "    init_add=0, start_add=0, const_add=0, do_plot=false, nderivs=0, difforder=0, clearfig=true, fignum=1,\n",
    "    dUdt_mag_only=false, sigma=0, g_leak=1, U_rest=0, theta=0, beta=1, \n",
    "    warn_if_unused_params=false, other_unused_params...)\n",
    "\n",
    "    if warn_if_unused_params && length(other_unused_params)>0\n",
    "        @printf(\"\\n\\n=== forwardModel warning, had unused params \")\n",
    "        for k in keys(Dict(other_unused_params))\n",
    "            @printf(\"%s, \", k)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    my_input = ForwardDiffZeros(size(input,1), size(input,2), nderivs=nderivs, difforder=difforder)\n",
    "    for i=1:prod(size(input)); my_input[i] = input[i]; end\n",
    "    input = my_input;\n",
    "    \n",
    "    nunits = length(startU)\n",
    "    if size(startU,2) > size(startU,1)\n",
    "        error(\"startU must be a column vector\")\n",
    "    end\n",
    "    \n",
    "    # --- formatting input ---\n",
    "    if ~(typeof(input)<:Array) || prod(size(input))==1  # was a scalar\n",
    "        input = input[1]*(1+ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    elseif length(input)==0 # was the empty matrix\n",
    "        input = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    elseif size(input,2)==1     # was a column vector\n",
    "        input = input*(1+ForwardDiffZeros(1, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    end    \n",
    "    # --- formatting noise ---\n",
    "    if ~(typeof(noise)<:Array) || prod(size(noise))==1  # was a scalar\n",
    "        noise = noise*(1+ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    elseif length(noise)==0 # was the empty matrix\n",
    "        noise = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    elseif size(noise,2)==1     # was a column vector\n",
    "        noise = noise*(1+ForwardDiffZeros(1, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    end    \n",
    "    \n",
    "    U = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    V = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    \n",
    "    if ~(typeof(W)<:Array); W = [W]; end\n",
    "\n",
    "    W     = reshape(W, nunits, nunits)\n",
    "    U     = reshape(U, nunits, nsteps)\n",
    "    V     = reshape(V, nunits, nsteps)\n",
    "    input = reshape(input, nunits, nsteps)\n",
    "    noise = reshape(noise, nunits, nsteps)\n",
    "\n",
    "    input[:,1] += init_add\n",
    "    input      += const_add\n",
    "\n",
    "    #@printf(\"size(U) is (%d,%d), and size(startU) is (%d,%d) and size(noise) is (%d,%d)\", \n",
    "    #    size(U,1), size(U,2), size(startU,1), size(startU,2), size(noise,1), size(noise,2))\n",
    "    # @printf(\"U[1]=%g, noise[1]=%g\\n\", startU, noise[1])\n",
    "    U[:,1] = startU + noise[:,1] + start_add; # @printf(\"Resulting U=%g\\n\", U[1])\n",
    "    V[:,1] = g((U[:,1]-theta)/beta); # @printf(\"Resulting V=%g\\n\", V[1])\n",
    "    \n",
    "    for i=2:nsteps\n",
    "        dUdt = g_leak*(U_rest -U[:,i-1]) + W*V[:,i-1] + input[:,i-1]\n",
    "        if dUdt_mag_only; return sum(dUdt.*dUdt); end;\n",
    "        # @printf(\"dUdt=%g\\n\", dUdt[1])\n",
    "        # @printf(\"i=%g\\n\", i)\n",
    "        # @printf(\"noise[2]=%g\\n\", noise[2])\n",
    "        U[:,i] = U[:,i-1] + (dt/tau)*dUdt + noise[:,i] + sigma*sqrt(dt)*randn(size(U,1),1)\n",
    "        # @printf(\"Resulting U[2]=%g\\n\", U[2])\n",
    "        V[:,i] = g((U[:,i]-theta)/beta)\n",
    "        # @printf(\"Resulting V[2]=%g\\n\", V[2])\n",
    "    end\n",
    "\n",
    "    if do_plot\n",
    "        figure(fignum)\n",
    "        if length(startU)==1\n",
    "            if clearfig; clf(); end;\n",
    "            t = (0:nsteps-1)*dt\n",
    "            plot(t, V[1,:], \"b-\")\n",
    "            plot(t[1], V[1,1], \"g.\")\n",
    "            plot(t[end], V[1,end], \"r.\")\n",
    "            xlabel(\"t\"); ylabel(\"V1\"); ylim([-0.01, 1.01])\n",
    "        elseif length(startU)>=2\n",
    "            if clearfig; clf(); end;\n",
    "            plot(V[1,:], V[2,:], \"b-\")\n",
    "            plot(V[1,1], V[2,1], \"g.\")\n",
    "            plot(V[1,end], V[2,end], \"r.\")\n",
    "            xlabel(\"V1\"); ylabel(\"V2\"); \n",
    "            xlim([-0.01, 1.01]); ylim([-0.01, 1.01])\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return U[:,end], V[:,end], U, V\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "backwardsModel(endU; dt=0.01, tau=0.1, nsteps=100, input=[0],noise=[],  W=[0 -5;-5 0], \n",
    "    do_plot=false, nderivs=0, difforder=0, clearfig=true, fignum=1, tol=1e-15, start_eta=10)\n",
    "\n",
    "Runs a tanh() style-network BACKWARDS in time, given its ending point, by making a backwards\n",
    "guess at each timepoint and then using Hessian minimization to find the backwards vector that correctly\n",
    "leads to the current timestep value.  Uses forwardModel() . The forwards equations are:\n",
    "\n",
    "    tau dU/dt = -U + W*V + I\n",
    "    V = 0.5*tanh(U)+ 0.5\n",
    "\n",
    "**PARAMETERS:**\n",
    "\n",
    "endU     A column vector, nunits-by-1, indicating the values of U at time=end\n",
    "\n",
    "\n",
    "**OPTIONAL PARAMETERS:**\n",
    "\n",
    "dt      Scalar, timestep size\n",
    "\n",
    "tau     Scalar, in seconds\n",
    "\n",
    "nsteps  Number of timesteps to run, including time=0.\n",
    "\n",
    "input   Either an nunits-by-1 vector, in which case inputs to each unit are constant\n",
    "        across time, or a matrix, nunits-by-nsteps, indicating input for each unit at each timepoint.\n",
    "\n",
    "W       Weight matrix, nunits-by-nunits\n",
    "\n",
    "do_plot   Default false, if true, plots V of up to the first two dimensions\n",
    "\n",
    "tol       Tolerance in the minimization procedure for finding each backwards timestep. Passed on\n",
    "          to trust_region_Hessian_minimization()\n",
    "\n",
    "start_eta   Passed on to trust_region_Hessian_minimization()\n",
    "\n",
    "fignum     Figure number on which to plot\n",
    "\n",
    "clrearfig  If true, the figure is first cleared, otherwise any plot ois overlaid\n",
    "\n",
    "nderivs, difforder     Required for making sure function can create its own arrays and \n",
    "                       still be differentiated\n",
    "\n",
    "\n",
    "\n",
    "** RETURNS:**\n",
    "\n",
    "Ustart Vstart   nunits-by-1 vectors representing the starting values of U and V that were found.\n",
    "U, V            nunits-by-nsteps matrices containing the full trajectories\n",
    "costs           1-by-nsteps vector with the final cost from the minimization procedure for each\n",
    "                timestep. This is the squared difference between the U[t+1] produced by the U[t] \n",
    "                guess and the actual U[t+1]\n",
    "\n",
    "\"\"\"\n",
    "function backwardsModel(endU; nsteps=100, start_eta=10, tol=1e-15, maxiter=400, \n",
    "    do_plot=false, init_add=0, start_add=0, dt=0.01, \n",
    "    input=[], noise=[], nderivs=0, difforder=0, clearfig=false, fignum=1, params...)    \n",
    "\n",
    "    nunits = length(endU)\n",
    "\n",
    "    # --- formatting input ---\n",
    "    if ~(typeof(input)<:Array) || prod(size(input))==1  # was a scalar\n",
    "        input = input[1]*(1+ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    elseif length(input)==0 # was the empty matrix\n",
    "        input = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    elseif size(input,2)==1     # was a column vector\n",
    "        input = input*(1+ForwardDiffZeros(1, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    end    \n",
    "    # --- formatting noise ---\n",
    "    if ~(typeof(noise)<:Array)  # was a scalar\n",
    "        noise = noise*(1+ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    elseif length(noise)==0 # was the empty matrix\n",
    "        noise = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    elseif size(noise,2)==1     # was a column vector\n",
    "        noise = noise*(1+ForwardDiffZeros(1, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    end    \n",
    "    \n",
    "    function J(U1, U2; nderivs=0, difforder=0, noise=[], inputs=[], pars...)\n",
    "        U2hat = forwardModel(U1; nsteps=2, noise=noise, input=input, nderivs=nderivs, difforder=difforder, pars...)[1]\n",
    "        U2hat = U2hat\n",
    "        DU = U2hat - U2\n",
    "    \n",
    "        return sum(DU.*DU)\n",
    "    end\n",
    "    \n",
    "    if length(noise)==0\n",
    "        noise = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    end\n",
    "\n",
    "    U = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    U = reshape(U, nunits, nsteps)\n",
    "    costs = ForwardDiffZeros(nsteps, 1, nderivs=nderivs, difforder=difforder)    \n",
    "    \n",
    "    U[:,end] = endU\n",
    "    for i=(nsteps-1):-1:1\n",
    "        if i==1\n",
    "            my_init_add = init_add\n",
    "            my_start_add = start_add\n",
    "        else\n",
    "            my_init_add = 0\n",
    "            my_start_add = 0\n",
    "        end\n",
    "                \n",
    "        U[:,i], costs[i] = trust_region_Hessian_minimization(U[:,i+1], \n",
    "            (x) -> J(x, U[:,i+1]; nderivs=length(endU), difforder=2, \n",
    "            input=input[:,i:i+1], noise = noise[:,i:i+1], \n",
    "            init_add=my_init_add, start_add=my_start_add, params...); \n",
    "            verbose=false, start_eta=start_eta, tol=tol, maxiter=maxiter)\n",
    "        if i>1; U[:,i] += noise[:,i]; end\n",
    "    end\n",
    "    \n",
    "    \n",
    "    V = g(U)\n",
    "    \n",
    "    if do_plot\n",
    "        figure(fignum)   \n",
    "        if typeof(params)<:Array; params = Dict(params); end;\n",
    "        if haskey(params, :dt);     dt     = params[:dt];     end\n",
    "        if haskey(params, :nsteps); nsteps = params[:nsteps]; end\n",
    "        if length(endU)==1\n",
    "            if clearfig; clf(); end;\n",
    "            t = (0:nsteps-1)*dt\n",
    "            plot(t, V[1,:], \"m-\")\n",
    "            plot(t[1], V[1,1], \"go\")\n",
    "            plot(t[end], V[1,end], \"ro\")            \n",
    "            ylim([-0.01, 1.01])\n",
    "        elseif length(endU)>=2\n",
    "            if clearfig; clf(); end;            \n",
    "            plot(V[1,:], V[2,:], \"m-\")\n",
    "            plot(V[1,1], V[2,1], \"go\")\n",
    "            plot(V[1,end], V[2,end], \"ro\")\n",
    "            xlim([-0.01, 1.01]); ylim([-0.01, 1.01])\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return U[:,1], V[:,1], U, V, costs\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing forward and backwards models with only 1 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ustart came back as 1.1\n"
     ]
    }
   ],
   "source": [
    "figure(1); clf();\n",
    "params = Dict(:noise => [0.1], :W => [-2], :nsteps=>10, :start_add=>-1.9)\n",
    "Uend = forwardModel([1.1]; do_plot=true, params...)[1]\n",
    "Ustart = backwardsModel(Uend; do_plot=true, tol=1e-30, params...)[1]\n",
    "@printf(\"Ustart came back as %g\\n\", Ustart[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing forward and backwards models now with 2 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ustart came back as : [0.1, 0.1]\n"
     ]
    }
   ],
   "source": [
    "nsteps=50\n",
    "params = Dict(:noise =>0.03*randn(2,nsteps) + [0.1,0]*ones(1,nsteps), :W => [0 -5; -5 0], :nsteps=>nsteps)\n",
    "\n",
    "Uend, Vend, U, V              = forwardModel([0.1,0.1]; do_plot=true, params...);\n",
    "Ustart, Vstart, bU, bV, costs = backwardsModel(Uend; do_plot=true, tol=1e-30, params...)\n",
    "\n",
    "@printf(\"Ustart came back as : \"); print_vector_g(Ustart); print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring dt-dependence of gradients and hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×5 Array{Float64,2}:\n",
       " 0.940327  -1.07319   1.23881  -1.25331    0.852507\n",
       " 0.903854  -0.977375  0.85317  -0.874248  -0.212408"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# srand(111)\n",
    "startU=randn(100,2)-3\n",
    "startU=randn(100,2)-3\n",
    "sigma = 0\n",
    "\n",
    "# startU=0.1*randn(100,2)-3\n",
    "# startU=zeros(100,2)-3\n",
    "\n",
    "\n",
    "dt = 0.02\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "W = -4\n",
    "noise = 0\n",
    "input = 0\n",
    "sigma = 0\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    ":noise=>noise, :input=>input, :sigma=>sigma, :const_add=>0, :init_add=>0)\n",
    "\n",
    "forward = (startU; pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;     \n",
    "    forwardModel(startU; pars...)\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "# clf();\n",
    "# func = (;pars...) -> forward([-0.2, 0.3]; do_plot=true, merge(model_params, Dict(pars))...)\n",
    "# func(;W=-4)\n",
    "\n",
    "args = [\"W\", \"const_add\", [\"start_add\" 2], \"sigma\"]\n",
    "params = [-4.01, 0.5, 0.2, -0.2, 0.01]\n",
    "\n",
    "figure(1); clf();\n",
    "value1, grad1, hess1 = keyword_vgh((;pars...)->forward([-0.2, 0.3]; do_plot=true, merge(model_params, Dict(pars))...)[1][1], args, params)\n",
    "title(@sprintf(\"Running with dt=%g\", dt))\n",
    "\n",
    "dt = 0.005\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    ":noise=>noise, :input=>input, :sigma=>sigma, :const_add=>0, :init_add=>0)\n",
    "\n",
    "figure(2); clf();\n",
    "value2, grad2, hess2 = keyword_vgh((;pars...)->forward([-0.2, 0.3]; do_plot=true, fignum=2, merge(model_params, Dict(pars))...)[1][1], args, params)\n",
    "title(@sprintf(\"Running with dt=%g\", dt))\n",
    "\n",
    "[grad1[:]' ; grad2[:]']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO-DOs\n",
    "\n",
    "\n",
    "1. ===\n",
    "2. ~~Optimize either an MGO or a ProAnti~~ DONE with MGO. Now on to ProAnti\n",
    "3. ~~Set up so we can easily change task period durations in JJ as we run the model to evaluate the results of model-fitting~~ DONE\n",
    "4. Have different task period durations while model-fitting\n",
    "5. Set up to do searches over parameter space\n",
    "6. Incorporate RT into fits?\n",
    "10. If fluxSense is needed in ProAnti, could try choosing the Anti unit endpoint values by maximizing the |dJ/dw|^2 over those values.\n",
    "11. Clean up the notebooks and write up what we've been doing!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProAnti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition plot_PA(Any, Any, Any) in module Main at In[180]:9 overwritten at In[182]:9.\n",
      "WARNING: Method definition #plot_PA(Array{Any, 1}, Main.#plot_PA, Any, Any, Any) in module Main overwritten.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'plot_PA :: Tuple{Any,Any,Any}' in module 'Main'.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "plot_PA"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    plot_PA(t, U, V; fignum=1, clearfig=true, rule_and_delay_period=1, target_period=1, post_target_period=1,\n",
    "        other_unused_params...)\n",
    "\n",
    "Helper function for plotting ProAnti results\n",
    "\"\"\"\n",
    "function plot_PA(t, U, V; fignum=1, clearfig=true, rule_and_delay_period=1, target_period=1, post_target_period=1,\n",
    "    other_unused_params...)\n",
    "    figure(fignum)\n",
    "    if clearfig; clf(); end\n",
    "    \n",
    "    ax1 = subplot(3,1,1)\n",
    "    h = plot(t, V'); \n",
    "    setp(h[1], color=[0, 0, 1])\n",
    "    setp(h[2], color=[1, 0, 0])\n",
    "    setp(h[3], color=[1, 0.5, 0.5])\n",
    "    setp(h[4], color=[0, 1, 1])\n",
    "    ylabel(\"V\")\n",
    "\n",
    "    ax = gca()\n",
    "    yl = [ylim()[1], ylim()[2]]\n",
    "    vlines([rule_and_delay_period, \n",
    "            rule_and_delay_period+target_period,\n",
    "            rule_and_delay_period+target_period+post_target_period], \n",
    "            -0.05, 1.05, linewidth=2)\n",
    "    if yl[1]<0.02\n",
    "        yl[1] = -0.02\n",
    "    end\n",
    "    if yl[2]>0.98\n",
    "        yl[2] = 1.02\n",
    "    end\n",
    "    ylim(yl)\n",
    "    grid(true)\n",
    "    remove_xtick_labels(ax1)\n",
    "        \n",
    "    ax2 = subplot(3,1,2)\n",
    "    hu = plot(t, U')\n",
    "    setp(hu[1], color=[0, 0, 1])\n",
    "    setp(hu[2], color=[1, 0, 0])\n",
    "    setp(hu[3], color=[1, 0.5, 0.5])\n",
    "    setp(hu[4], color=[0, 1, 1])\n",
    "    ylabel(\"U\"); ylim(minimum(U[:])-0.1, maximum(U[:])+0.1)\n",
    "    vlines([rule_and_delay_period, \n",
    "            rule_and_delay_period+target_period,\n",
    "            rule_and_delay_period+target_period+post_target_period], \n",
    "            ylim()[1], ylim()[2], linewidth=2)\n",
    "    remove_xtick_labels(ax2)\n",
    "\n",
    "    grid(true)\n",
    "    \n",
    "    subplot(3,1,3)\n",
    "    delta = V[1,:] - V[4,:]\n",
    "    hr = plot(t, delta)\n",
    "    oldlims = [ylim()[1]+0.1, ylim()[2]-0.1]\n",
    "    ylim(minimum([delta[:];oldlims[1]])-0.1, maximum([delta[:];oldlims[2]])+0.1)\n",
    "    vlines([rule_and_delay_period, \n",
    "            rule_and_delay_period+target_period,\n",
    "            rule_and_delay_period+target_period+post_target_period], \n",
    "            ylim()[1], ylim()[2], linewidth=2)\n",
    "    xlabel(\"t\"); ylabel(\"Pro R - Pro L\")\n",
    "    grid(true)\n",
    "        \n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.366172 seconds (196.73 k allocations: 9.023 MB)\n",
      "Pro % correct = 90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition make_input(Any) in module Main at In[143]:39 overwritten at In[145]:39.\n",
      "WARNING: Method definition #make_input(Array{Any, 1}, Main.#make_input, Any) in module Main overwritten.\n",
      "WARNING: Method definition run_ntrials(Any, Any) in module Main at In[143]:63 overwritten at In[145]:63.\n",
      "WARNING: Method definition #run_ntrials(Array{Any, 1}, Main.#run_ntrials, Any, Any) in module Main overwritten.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anti % correct = 40% \n"
     ]
    }
   ],
   "source": [
    "model_params = Dict(\n",
    ":dt     =>  0.02, \n",
    ":tau    =>  0.1, \n",
    ":vW     =>  -1.7,\n",
    ":hW     =>  -1.7,\n",
    ":sW     =>  0.2,\n",
    ":dW     =>  0,\n",
    ":nsteps =>  2, \n",
    ":noise  =>  [], \n",
    ":sigma  =>  0.08, \n",
    ":input  =>  0, \n",
    ":g_leak =>  0.25, \n",
    ":U_rest =>  -1,\n",
    ":theta  =>  1, \n",
    ":beta   =>  1, \n",
    ":sw     =>  0.2,\n",
    ":hw     =>  -1.7,\n",
    ":vw     =>  -1.7,\n",
    ":constant_excitation      => 0.19, \n",
    ":anti_rule_strength       => 0.1,\n",
    ":pro_rule_strength        => 0.1, \n",
    ":target_period_excitation => 1,\n",
    ":right_light_excitation   => 0.5, \n",
    ":right_light_pro_extra    => 0,\n",
    ":const_add => 0, \n",
    ":init_add  => 0, \n",
    ":rule_and_delay_period    => 0.4,\n",
    ":target_period            => 0.1,\n",
    ":post_target_period       => 0.5,\n",
    ":const_pro_bias           => 0,\n",
    ")\n",
    "\n",
    "\n",
    "function make_input(trial_type; dt=0.02, nderivs=0, difforder=0, constant_excitation=0.19, anti_rule_strength=0.1, \n",
    "    pro_rule_strength=0.1, target_period_excitation=1, right_light_excitation=0.5, right_light_pro_extra=0, \n",
    "    rule_and_delay_period=0.4, target_period=0.1, post_target_period=0.4, const_pro_bias=0,\n",
    "    other_unused_params...)\n",
    "\n",
    "    T = rule_and_delay_period + target_period + post_target_period\n",
    "    t = 0:dt:T\n",
    "    nsteps = length(t)\n",
    "\n",
    "    input = constant_excitation + ForwardDiffZeros(4, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    if trial_type==\"Anti\"\n",
    "        input[2:3, t.<rule_and_delay_period] += anti_rule_strength\n",
    "    elseif trial_type==\"Pro\"\n",
    "        input[[1,4], t.<rule_and_delay_period] += pro_rule_strength\n",
    "    else\n",
    "        error(\"make_input: I don't recognize input type \\\"\" * trial_type * \"\\\"\")\n",
    "    end\n",
    "    \n",
    "    input[:,     (rule_and_delay_period.<=t) & (t.<rule_and_delay_period+target_period)] += target_period_excitation\n",
    "    input[1:2,   (rule_and_delay_period.<=t) & (t.<rule_and_delay_period+target_period)] += right_light_excitation\n",
    "    input[1,     (rule_and_delay_period.<=t) & (t.<rule_and_delay_period+target_period)] += right_light_pro_extra\n",
    "    \n",
    "    input[[1,4],:] += const_pro_bias\n",
    "    \n",
    "    return input, t, nsteps\n",
    "end\n",
    "\n",
    "\n",
    "function run_ntrials(nPro, nAnti; plot_list=[], nderivs=0, difforder=0, model_params...)\n",
    "    pro_input,  t, nsteps = make_input(\"Pro\" ; model_params...)\n",
    "    anti_input, t, nsteps = make_input(\"Anti\"; model_params...)\n",
    "\n",
    "    model_params = Dict(model_params)\n",
    "    sW = model_params[:sW]\n",
    "    hW = model_params[:hW]\n",
    "    vW = model_params[:vW]\n",
    "    dW = model_params[:dW]\n",
    "    model_params = make_dict([\"nsteps\", \"W\"], [nsteps, [sW vW dW hW; vW sW hW dW; dW hW sW vW; hW dW vW sW]], \n",
    "        model_params)\n",
    "    model_params = make_dict([\"nderivs\", \"difforder\"], [nderivs, difforder], model_params)\n",
    "    \n",
    "    proVs  = ForwardDiffZeros(4, nPro, nderivs=nderivs, difforder=difforder)\n",
    "    antiVs = ForwardDiffZeros(4, nAnti, nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    # --- PRO ---\n",
    "    figure(1); clf();\n",
    "    model_params = make_dict([\"input\"], [pro_input], model_params)\n",
    "    for i=1:nPro\n",
    "        startU = [-0.3, -0.7, -0.7, -0.3]\n",
    "        Uend, Vend, U, V = forwardModel(startU, do_plot=false; model_params...)\n",
    "        proVs[:,i] = Vend\n",
    "        if any(plot_list.==i) \n",
    "            plot_PA(t, U, V; fignum=1, clearfig=false, model_params...)\n",
    "            subplot(3,1,1); title(\"PRO\")\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # --- ANTI ---\n",
    "    figure(2); clf();\n",
    "    model_params = make_dict([\"input\"], [anti_input], model_params)\n",
    "    for i=1:nAnti\n",
    "        startU = [-0.7, -0.3, -0.3, -0.7]\n",
    "        Uend, Vend, U, V = forwardModel(startU, do_plot=false; model_params...)\n",
    "        antiVs[:,i] = Vend\n",
    "        if any(plot_list.==i) \n",
    "            plot_PA(t, U, V; fignum=2, clearfig=false, model_params...)\n",
    "            subplot(3,1,1); title(\"ANTI\")\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return proVs, antiVs\n",
    "end\n",
    "\n",
    "nPro = 10; nAnti = 5;\n",
    "proVs, antiVs = @time(run_ntrials(nPro, nAnti; plot_list=[1:5;], model_params...))\n",
    "\n",
    "@printf(\"Pro %% correct = %g%%\\n\", 100*length(find(proVs[1,:].>proVs[4,:]))/ntrials)\n",
    "@printf(\"Anti %% correct = %g%% \\n\", 100*length(find(antiVs[1,:].<antiVs[4,:]))/ntrials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition JJ(Any, Any) in module Main at In[250]:7 overwritten at In[251]:7.\n",
      "WARNING: Method definition #JJ(Array{Any, 1}, Main.#JJ, Any, Any) in module Main overwritten.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     -- cost=0.0145009,   cost1=0.0150714, cost2=-0.000570531\n",
      "     -- mean(hitsP)=0.649618, mean(diffsP)=0.116621 mean(hitsA)=0.625519, mean(diffsA)=0.204888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.014500908183137585"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function JJ(nPro, nAnti; pro_target=0.9, anti_target=0.7, \n",
    "    theta1=0.025, theta2=0.035, cbeta=0.003, verbose=false, \n",
    "    pre_string=\"\", zero_last_sigmas=0, seedrand=NaN, \n",
    "    rule_and_delay_periods = [0.4], target_periods = [0.1], post_target_periods = [0.5],\n",
    "    nderivs=0, difforder=0, model_params...)\n",
    "\n",
    "    nruns = length(rule_and_delay_periods)*length(target_periods)*length(post_target_periods)\n",
    "    \n",
    "    cost1s = ForwardDiffZeros(1, nruns, nderivs=nderivs, difforder=difforder)\n",
    "    cost2s = ForwardDiffZeros(1, nruns, nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if ~isnan(seedrand); srand(seedrand); end\n",
    "    \n",
    "    n = totHitsP = totHitsA = totDiffsP = totDiffsA = 0\n",
    "    for i in rule_and_delay_periods\n",
    "        for j in target_periods\n",
    "            for k = post_target_periods\n",
    "                n += 1\n",
    "                \n",
    "                my_params = make_dict([\"rule_and_delay_period\", \"target_period\", \"post_target_period\"],\n",
    "                [i, j, k], Dict(model_params))\n",
    "    \n",
    "                # print(\"model params is \" ); print(model_params); print(\"\\n\")\n",
    "                proVs, antiVs = run_ntrials(nPro, nAnti; nderivs=nderivs, difforder=difforder, my_params...)\n",
    "\n",
    "                hitsP  = 0.5*(1 + tanh.((proVs[1,:]-proVs[4,:,])/theta1))\n",
    "                diffsP = tanh.((proVs[1,:,]-proVs[4,:])/theta2).^2\n",
    "                hitsA  = 0.5*(1 + tanh.((antiVs[4,:]-antiVs[1,:,])/theta1))\n",
    "                diffsA = tanh.((antiVs[4,:,]-antiVs[1,:])/theta2).^2\n",
    "\n",
    "                if nPro>0 && nAnti>0\n",
    "                    cost1s[n] = (nPro*(mean(hitsP) - pro_target).^2  + nAnti*(mean(hitsA) - anti_target).^2)/(nPro+nAnti)\n",
    "                    cost2s[n] = -cbeta*(nPro*mean(diffsP) + nAnti*mean(diffsA))/(nPro+nAnti)\n",
    "                elseif nPro>0\n",
    "                    cost1s[n] = (mean(hitsP) - pro_target).^2\n",
    "                    cost2s[n] = -cbeta*mean(diffsP)\n",
    "                else\n",
    "                    cost1s[n] = (mean(hitsA) - anti_target).^2\n",
    "                    cost2s[n] = -cbeta*mean(diffsA)\n",
    "                end\n",
    "\n",
    "                totHitsP  += mean(hitsP);  totHitsA  += mean(hitsA); \n",
    "                totDiffsP += mean(diffsP); totDiffsA += mean(diffsA);\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    cost1 = mean(cost1s)\n",
    "    cost2 = mean(cost2s)\n",
    "\n",
    "    hitsP = totHitsP/n; hitsA = totHitsA/n; diffsP = totDiffsP/n; diffsA = totDiffsA/n\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"%s\", pre_string)\n",
    "        @printf(\"     -- cost=%g,   cost1=%g, cost2=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2))\n",
    "        if nPro>0 && nAnti>0\n",
    "            @printf(\"     -- mean(hitsP)=%g, mean(diffsP)=%g mean(hitsA)=%g, mean(diffsA)=%g\\n\", \n",
    "                convert(Float64, mean(hitsP)), convert(Float64, mean(diffsP)),\n",
    "                convert(Float64, mean(hitsA)), convert(Float64, mean(diffsA)))\n",
    "        elseif nPro>0\n",
    "            @printf(\"     -- mean(hitsP)=%g, mean(diffsP)=%g (nAnti=0)\\n\", \n",
    "                convert(Float64, mean(hitsP)), convert(Float64, mean(diffsP)))\n",
    "        else\n",
    "            @printf(\"     -- (nPro=0) mean(hitsA)=%g, mean(diffsA)=%g\\n\", \n",
    "                convert(Float64, mean(hitsA)), convert(Float64, mean(diffsA)))\n",
    "        end        \n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "JJ(2, 10; plot_list=1:5, verbose=true, model_params...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.10535699311851135,[0.02585,-0.0078272,-0.0213399,0.342989,-0.367236,0.0262403,0.650505],\n",
       "[-0.14336 0.0472937 … -0.0471095 -0.260981; 0.0472937 -0.0329522 … 0.0179526 0.0201735; … ; -0.0471095 0.0179526 … -0.00211476 -0.0168496; -0.260981 0.0201735 … -0.0168496 -10.5855])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func = (;params...) -> JJ(100, 0; rule_and_delay_periods = [0.4, 0.8], seedrand=30, cbeta=0.01, \n",
    "plot_list = 1:5, verbose=false, merge(model_params, Dict(params))...)\n",
    "\n",
    "cost, grad, hess = keyword_vgh(func, args, seed)\n",
    "\n",
    "# func(;make_dict(args, seed+ [1,0.2,0,0,0,0,0])...) - func(;make_dict(args, seed)...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: eta=0.01 ps=[0.200, -1.700, -1.700, 0.390, 0.150, 0.100, 0.100]\n",
      "     -- cost=0.0609354,   cost1=0.0702293, cost2=-0.0092939\n",
      "     -- mean(hitsP)=0.574693, mean(diffsP)=0.952638 mean(hitsA)=0.522901, mean(diffsA)=0.906143\n",
      "     -- cost=0.0594435,   cost1=0.0686243, cost2=-0.00918085\n",
      "     -- mean(hitsP)=0.57821, mean(diffsP)=0.941545 mean(hitsA)=0.525837, mean(diffsA)=0.894625\n",
      "1: eta=0.011 cost=0.0594435 jtype=constrained costheta=-0.985 ps=[0.199944, -1.70014, -1.69931, 0.382009, 0.150148, 0.0999991, 0.0947458]\n",
      "     -- cost=0.0586714,   cost1=0.067794, cost2=-0.0091226\n",
      "     -- mean(hitsP)=0.583192, mean(diffsP)=0.934959 mean(hitsA)=0.522194, mean(diffsA)=0.889561\n",
      "2: eta=0.0121 cost=0.0586714 jtype=constrained costheta=-0.428 ps=[0.200019, -1.70054, -1.69829, 0.373021, 0.150443, 0.0999975, 0.0994822]\n",
      "     -- cost=0.05822,   cost1=0.0672907, cost2=-0.00907067\n",
      "     -- mean(hitsP)=0.589098, mean(diffsP)=0.93218 mean(hitsA)=0.515129, mean(diffsA)=0.881954\n",
      "3: eta=0.01331 cost=0.05822 jtype=constrained costheta=-0.922 ps=[0.200186, -1.70123, -1.69672, 0.364247, 0.150863, 0.0999967, 0.105732]\n",
      "     -- cost=0.0577886,   cost1=0.0668117, cost2=-0.00902312\n",
      "     -- mean(hitsP)=0.587356, mean(diffsP)=0.92718 mean(hitsA)=0.521075, mean(diffsA)=0.877444\n",
      "4: eta=0.014641 cost=0.0577886 jtype=constrained costheta=-0.741 ps=[0.200991, -1.70386, -1.69409, 0.364611, 0.15185, 0.0999958, 0.099493]\n",
      "     -- cost=0.0562053,   cost1=0.0649936, cost2=-0.00878835\n",
      "     -- mean(hitsP)=0.591659, mean(diffsP)=0.910092 mean(hitsA)=0.523509, mean(diffsA)=0.847578\n",
      "5: eta=0.0161051 cost=0.0562053 jtype=constrained costheta=-0.808 ps=[0.201054, -1.70485, -1.69264, 0.352479, 0.152291, 0.099995, 0.0939078]\n",
      "     -- cost=0.0542893,   cost1=0.0627915, cost2=-0.0085022\n",
      "     -- mean(hitsP)=0.596231, mean(diffsP)=0.888019 mean(hitsA)=0.527329, mean(diffsA)=0.812421\n",
      "6: eta=0.0177156 cost=0.0542893 jtype=constrained costheta=-0.946 ps=[0.201112, -1.70606, -1.69014, 0.340528, 0.15284, 0.0999991, 0.0860907]\n",
      "     -- cost=0.0514922,   cost1=0.059548, cost2=-0.00805579\n",
      "     -- mean(hitsP)=0.60507, mean(diffsP)=0.84793 mean(hitsA)=0.529794, mean(diffsA)=0.763227\n",
      "7: eta=0.0194872 cost=0.0514922 jtype=constrained costheta=-0.975 ps=[0.20096, -1.70616, -1.68907, 0.325164, 0.152998, 0.0999991, 0.0784282]\n",
      "     -- cost=0.0500142,   cost1=0.0577104, cost2=-0.00769618\n",
      "     -- mean(hitsP)=0.610282, mean(diffsP)=0.816119 mean(hitsA)=0.531386, mean(diffsA)=0.723117\n",
      "8: eta=0.0214359 cost=0.0500142 jtype=constrained costheta=-0.772 ps=[0.20142, -1.70911, -1.68481, 0.314637, 0.154349, 0.100007, 0.0732291]\n",
      "     -- cost=0.0482316,   cost1=0.0551179, cost2=-0.00688629\n",
      "     -- mean(hitsP)=0.615989, mean(diffsP)=0.739217 mean(hitsA)=0.536287, mean(diffsA)=0.638042\n",
      "9: eta=0.0235795 cost=0.0482316 jtype=constrained costheta=-0.995 ps=[0.201365, -1.7099, -1.68284, 0.297695, 0.154873, 0.100009, 0.0621529]\n",
      "     -- cost=0.0471565,   cost1=0.0535924, cost2=-0.00643591\n",
      "     -- mean(hitsP)=0.62549, mean(diffsP)=0.697353 mean(hitsA)=0.529384, mean(diffsA)=0.58983\n",
      "10: eta=0.0259374 cost=0.0471565 jtype=constrained costheta=-0.862 ps=[0.201772, -1.71276, -1.67737, 0.283359, 0.15652, 0.100024, 0.0648333]\n",
      "     -- cost=0.0462898,   cost1=0.0525102, cost2=-0.00622037\n",
      "     -- mean(hitsP)=0.633864, mean(diffsP)=0.678969 mean(hitsA)=0.522557, mean(diffsA)=0.565105\n",
      "11: eta=0.0285312 cost=0.0462898 jtype=constrained costheta=-0.946 ps=[0.202394, -1.71672, -1.67017, 0.274286, 0.158693, 0.100046, 0.0689509]\n",
      "     -- cost=0.0454422,   cost1=0.0516201, cost2=-0.00617788\n",
      "     -- mean(hitsP)=0.638691, mean(diffsP)=0.676141 mean(hitsA)=0.520496, mean(diffsA)=0.559435\n",
      "12: eta=0.0313843 cost=0.0454422 jtype=constrained costheta=-0.980 ps=[0.203196, -1.72159, -1.66152, 0.271739, 0.161314, 0.100074, 0.0704131]\n",
      "     -- cost=0.0445102,   cost1=0.0506894, cost2=-0.00617924\n",
      "     -- mean(hitsP)=0.642204, mean(diffsP)=0.676909 mean(hitsA)=0.520735, mean(diffsA)=0.55894\n",
      "13: eta=0.0345227 cost=0.0445102 jtype=constrained costheta=-0.997 ps=[0.204112, -1.72699, -1.65186, 0.271489, 0.164277, 0.100106, 0.070395]\n",
      "     -- cost=0.0434651,   cost1=0.0496582, cost2=-0.00619306\n",
      "     -- mean(hitsP)=0.645484, mean(diffsP)=0.678678 mean(hitsA)=0.521964, mean(diffsA)=0.559934\n",
      "14: eta=0.037975 cost=0.0434651 jtype=constrained costheta=-1.000 ps=[0.205136, -1.7329, -1.64111, 0.272033, 0.167618, 0.10014, 0.0697537]\n",
      "     -- cost=0.0422893,   cost1=0.0485025, cost2=-0.00621321\n",
      "     -- mean(hitsP)=0.648952, mean(diffsP)=0.681036 mean(hitsA)=0.523723, mean(diffsA)=0.561607\n",
      "15: eta=0.0417725 cost=0.0422893 jtype=constrained costheta=-1.000 ps=[0.206279, -1.73932, -1.62913, 0.272926, 0.171395, 0.100177, 0.0688352]\n",
      "     -- cost=0.0409639,   cost1=0.0472024, cost2=-0.00623856\n",
      "     -- mean(hitsP)=0.652786, mean(diffsP)=0.683942 mean(hitsA)=0.5259, mean(diffsA)=0.56377\n",
      "16: eta=0.0459497 cost=0.0409639 jtype=constrained costheta=-1.000 ps=[0.207557, -1.7463, -1.61576, 0.274038, 0.175678, 0.100218, 0.067749]\n",
      "     -- cost=0.0394676,   cost1=0.045737, cost2=-0.00626941\n",
      "     -- mean(hitsP)=0.657084, mean(diffsP)=0.68746 mean(hitsA)=0.528521, mean(diffsA)=0.566422\n",
      "17: eta=0.0505447 cost=0.0394676 jtype=constrained costheta=-1.000 ps=[0.208988, -1.75387, -1.60082, 0.275353, 0.180547, 0.100263, 0.0665171]\n",
      "     -- cost=0.0377759,   cost1=0.0440826, cost2=-0.00630665\n",
      "     -- mean(hitsP)=0.661927, mean(diffsP)=0.691701 mean(hitsA)=0.531674, mean(diffsA)=0.569629\n",
      "18: eta=0.0555992 cost=0.0377759 jtype=constrained costheta=-1.000 ps=[0.210596, -1.76207, -1.58408, 0.276889, 0.1861, 0.100311, 0.065131]\n",
      "     -- cost=0.0358612,   cost1=0.0422127, cost2=-0.00635149\n",
      "     -- mean(hitsP)=0.667393, mean(diffsP)=0.696807 mean(hitsA)=0.535487, mean(diffsA)=0.573492\n",
      "19: eta=0.0611591 cost=0.0358612 jtype=constrained costheta=-1.000 ps=[0.212406, -1.77094, -1.56529, 0.27868, 0.192451, 0.100364, 0.0635724]\n",
      "     -- cost=0.0336923,   cost1=0.0400978, cost2=-0.00640551\n",
      "     -- mean(hitsP)=0.673578, mean(diffsP)=0.702959 mean(hitsA)=0.54013, mean(diffsA)=0.578142\n",
      "20: eta=0.067275 cost=0.0336923 jtype=constrained costheta=-1.000 ps=[0.214453, -1.78053, -1.54416, 0.280771, 0.199741, 0.100421, 0.0618224]\n",
      "     -- cost=0.0312357,   cost1=0.0377066, cost2=-0.00647087\n",
      "     -- mean(hitsP)=0.6806, mean(diffsP)=0.710404 mean(hitsA)=0.545803, mean(diffsA)=0.58377\n",
      "21: eta=0.0740025 cost=0.0312357 jtype=constrained costheta=-1.000 ps=[0.216774, -1.79087, -1.52031, 0.283206, 0.208136, 0.100482, 0.0598721]\n",
      "     -- cost=0.0284575,   cost1=0.0350083, cost2=-0.00655072\n",
      "     -- mean(hitsP)=0.68863, mean(diffsP)=0.719491 mean(hitsA)=0.552727, mean(diffsA)=0.590653\n",
      "22: eta=0.0814027 cost=0.0284575 jtype=constrained costheta=-1.000 ps=[0.219412, -1.802, -1.49332, 0.286027, 0.217838, 0.100548, 0.057743]\n",
      "     -- cost=0.0253283,   cost1=0.0319778, cost2=-0.00664945\n",
      "     -- mean(hitsP)=0.697923, mean(diffsP)=0.730709 mean(hitsA)=0.561099, mean(diffsA)=0.59918\n",
      "23: eta=0.089543 cost=0.0253283 jtype=constrained costheta=-1.000 ps=[0.222411, -1.81396, -1.46264, 0.289263, 0.229092, 0.100619, 0.055509]\n",
      "     -- cost=0.0218332,   cost1=0.0286046, cost2=-0.00677145\n",
      "     -- mean(hitsP)=0.708818, mean(diffsP)=0.744582 mean(hitsA)=0.571108, mean(diffsA)=0.609709\n",
      "24: eta=0.0984973 cost=0.0218332 jtype=constrained costheta=-1.000 ps=[0.225807, -1.82677, -1.42756, 0.29291, 0.242189, 0.100695, 0.0532735]\n",
      "     -- cost=0.0179877,   cost1=0.0249047, cost2=-0.00691704\n",
      "     -- mean(hitsP)=0.721609, mean(diffsP)=0.761285 mean(hitsA)=0.583118, mean(diffsA)=0.622124\n",
      "25: eta=0.108347 cost=0.0179877 jtype=constrained costheta=-1.000 ps=[0.229621, -1.84045, -1.38719, 0.296927, 0.257476, 0.100776, 0.0510433]\n",
      "     -- cost=0.0138569,   cost1=0.0209352, cost2=-0.00707832\n",
      "     -- mean(hitsP)=0.736401, mean(diffsP)=0.780279 mean(hitsA)=0.597919, mean(diffsA)=0.635385\n",
      "26: eta=0.119182 cost=0.0138569 jtype=constrained costheta=-1.000 ps=[0.233851, -1.85492, -1.34044, 0.301181, 0.275375, 0.100861, 0.0486268]\n",
      "     -- cost=0.00958037,   cost1=0.0168337, cost2=-0.00725335\n",
      "     -- mean(hitsP)=0.753209, mean(diffsP)=0.801542 mean(hitsA)=0.616505, mean(diffsA)=0.649128\n",
      "27: eta=0.1311 cost=0.00958037 jtype=constrained costheta=-1.000 ps=[0.238463, -1.87006, -1.28604, 0.305622, 0.296414, 0.100948, 0.0459271]\n",
      "     -- cost=0.00539569,   cost1=0.0128508, cost2=-0.00745513\n",
      "     -- mean(hitsP)=0.77264, mean(diffsP)=0.826271 mean(hitsA)=0.638777, mean(diffsA)=0.664755\n",
      "28: eta=0.14421 cost=0.00539569 jtype=constrained costheta=-0.999 ps=[0.243295, -1.88565, -1.22274, 0.310105, 0.321316, 0.101034, 0.043575]\n",
      "     -- cost=0.00150182,   cost1=0.00912849, cost2=-0.00762667\n",
      "     -- mean(hitsP)=0.798501, mean(diffsP)=0.85075 mean(hitsA)=0.658867, mean(diffsA)=0.674584\n",
      "29: eta=0.158631 cost=0.00150182 jtype=constrained costheta=-0.968 ps=[0.247664, -1.90177, -1.14772, 0.310831, 0.35067, 0.10112, 0.0441158]\n",
      "     -- cost=-0.00200285,   cost1=0.00522869, cost2=-0.00723154\n",
      "     -- mean(hitsP)=0.840649, mean(diffsP)=0.836904 mean(hitsA)=0.666135, mean(diffsA)=0.609404\n",
      "30: eta=0.174494 cost=-0.00200285 jtype=constrained costheta=-0.847 ps=[0.251135, -1.91867, -1.06159, 0.289926, 0.38545, 0.101249, 0.0476522]\n",
      "     -- cost=-0.00464681,   cost1=0.00321302, cost2=-0.00785983\n",
      "     -- mean(hitsP)=0.872273, mean(diffsP)=0.891122 mean(hitsA)=0.691104, mean(diffsA)=0.680843\n",
      "31: eta=0.191943 cost=-0.00464681 jtype=constrained costheta=-0.723 ps=[0.251201, -1.93568, -0.933554, 0.304367, 0.415443, 0.101362, 0.0543018]\n",
      "     -- cost=-0.00662375,   cost1=0.00203528, cost2=-0.00865903\n",
      "     -- mean(hitsP)=0.913901, mean(diffsP)=0.94802 mean(hitsA)=0.687489, mean(diffsA)=0.783787\n",
      "32: eta=0.211138 cost=-0.00662375 jtype=constrained costheta=-0.434 ps=[0.248672, -1.95381, -0.780385, 0.328251, 0.444378, 0.101443, 0.0749949]\n",
      "     -- cost=-0.00741861,   cost1=0.00158745, cost2=-0.00900605\n",
      "     -- mean(hitsP)=0.925545, mean(diffsP)=0.957664 mean(hitsA)=0.695482, mean(diffsA)=0.843547\n",
      "33: eta=0.232252 cost=-0.00741861 jtype=constrained costheta=-0.252 ps=[0.241041, -1.95471, -0.606836, 0.360614, 0.411492, 0.101328, 0.0864238]\n",
      "     -- cost=-0.00793573,   cost1=0.00113705, cost2=-0.00907278\n",
      "     -- mean(hitsP)=0.925389, mean(diffsP)=0.950064 mean(hitsA)=0.695924, mean(diffsA)=0.864492\n",
      "34: eta=0.255477 cost=-0.00793573 jtype=constrained costheta=-0.344 ps=[0.22985, -1.93176, -0.481803, 0.387116, 0.355151, 0.101254, 0.0913501]\n",
      "     -- cost=-0.00822581,   cost1=0.000855663, cost2=-0.00908148\n",
      "     -- mean(hitsP)=0.918392, mean(diffsP)=0.932211 mean(hitsA)=0.695062, mean(diffsA)=0.884084\n",
      "35: eta=0.281024 cost=-0.00822581 jtype=constrained costheta=-0.233 ps=[0.233444, -1.81757, -0.38867, 0.42007, 0.317438, 0.101183, 0.0935607]\n",
      "     -- cost=-0.00835962,   cost1=0.000686424, cost2=-0.00904605\n",
      "     -- mean(hitsP)=0.906756, mean(diffsP)=0.914815 mean(hitsA)=0.691834, mean(diffsA)=0.894394\n",
      "36: eta=0.309127 cost=-0.00835962 jtype=constrained costheta=-0.137 ps=[0.197063, -1.65883, -0.369865, 0.451357, 0.307688, 0.10116, 0.0880435]\n",
      "     -- cost=0.0193773,   cost1=0.0226079, cost2=-0.00323062\n",
      "     -- mean(hitsP)=0.75877, mean(diffsP)=0.33568 mean(hitsA)=0.624182, mean(diffsA)=0.310443\n",
      "eta going down: new_cost-cost=0.0277369 and jumptype='constrained'\n",
      "37: eta=0.154563 cost=-0.00835962 jtype=constrained costheta=NaN ps=[0.197063, -1.65883, -0.369865, 0.451357, 0.307688, 0.10116, 0.0880435]\n",
      "     -- cost=-0.00825202,   cost1=0.00091341, cost2=-0.00916543\n",
      "     -- mean(hitsP)=0.913658, mean(diffsP)=0.938501 mean(hitsA)=0.696693, mean(diffsA)=0.894585\n",
      "eta going down: new_cost-cost=0.000107605 and jumptype='constrained'\n",
      "38: eta=0.0772817 cost=-0.00835962 jtype=constrained costheta=NaN ps=[0.197063, -1.65883, -0.369865, 0.451357, 0.307688, 0.10116, 0.0880435]\n",
      "     -- cost=-0.00833144,   cost1=0.000768549, cost2=-0.00909999\n",
      "     -- mean(hitsP)=0.912202, mean(diffsP)=0.925342 mean(hitsA)=0.694187, mean(diffsA)=0.894655\n",
      "eta going down: new_cost-cost=2.81866e-05 and jumptype='constrained'\n",
      "39: eta=0.0386409 cost=-0.00835962 jtype=constrained costheta=NaN ps=[0.197063, -1.65883, -0.369865, 0.451357, 0.307688, 0.10116, 0.0880435]\n",
      "     -- cost=-0.00835418,   cost1=0.000710358, cost2=-0.00906454\n",
      "     -- mean(hitsP)=0.912379, mean(diffsP)=0.918457 mean(hitsA)=0.693433, mean(diffsA)=0.894451\n",
      "eta going down: new_cost-cost=5.44301e-06 and jumptype='constrained'\n",
      "40: eta=0.0193204 cost=-0.00835962 jtype=constrained costheta=NaN ps=[0.197063, -1.65883, -0.369865, 0.451357, 0.307688, 0.10116, 0.0880435]\n",
      "     -- cost=-0.00838045,   cost1=0.000645199, cost2=-0.00902564\n",
      "     -- mean(hitsP)=0.91108, mean(diffsP)=0.911341 mean(hitsA)=0.69247, mean(diffsA)=0.893788\n",
      "41: eta=0.0212525 cost=-0.00838045 jtype=constrained costheta=-0.583 ps=[0.195759, -1.6511, -0.355765, 0.454499, 0.307143, 0.101154, 0.0889379]\n",
      "     -- cost=-0.00838941,   cost1=0.000631523, cost2=-0.00902094\n",
      "     -- mean(hitsP)=0.910739, mean(diffsP)=0.909915 mean(hitsA)=0.692351, mean(diffsA)=0.894272\n",
      "42: eta=0.0233777 cost=-0.00838941 jtype=constrained costheta=-0.987 ps=[0.194237, -1.63729, -0.352334, 0.456777, 0.306618, 0.101143, 0.0886839]\n",
      "     -- cost=-0.00839885,   cost1=0.00062055, cost2=-0.00901939\n",
      "     -- mean(hitsP)=0.910367, mean(diffsP)=0.908862 mean(hitsA)=0.692294, mean(diffsA)=0.895017\n",
      "43: eta=0.0257155 cost=-0.00839885 jtype=constrained costheta=-0.990 ps=[0.192686, -1.62165, -0.349784, 0.459165, 0.306653, 0.10113, 0.0882969]\n",
      "     -- cost=-0.00840898,   cost1=0.000611572, cost2=-0.00902055\n",
      "     -- mean(hitsP)=0.910039, mean(diffsP)=0.908088 mean(hitsA)=0.692292, mean(diffsA)=0.896022\n",
      "44: eta=0.028287 cost=-0.00840898 jtype=constrained costheta=-0.990 ps=[0.191113, -1.60422, -0.347707, 0.46178, 0.307173, 0.101116, 0.0878144]\n",
      "     -- cost=-0.00841998,   cost1=0.000603798, cost2=-0.00902378\n",
      "     -- mean(hitsP)=0.909742, mean(diffsP)=0.907505 mean(hitsA)=0.692331, mean(diffsA)=0.89725\n",
      "45: eta=0.0311157 cost=-0.00841998 jtype=constrained costheta=-0.987 ps=[0.189512, -1.58487, -0.345868, 0.464666, 0.308094, 0.101102, 0.0872482]\n",
      "     -- cost=-0.00843196,   cost1=0.000596573, cost2=-0.00902854\n",
      "     -- mean(hitsP)=0.909453, mean(diffsP)=0.907033 mean(hitsA)=0.692395, mean(diffsA)=0.898674\n",
      "46: eta=0.0342273 cost=-0.00843196 jtype=constrained costheta=-0.982 ps=[0.187876, -1.5634, -0.344085, 0.467869, 0.309361, 0.101088, 0.0866021]\n",
      "     -- cost=-0.00844503,   cost1=0.000589391, cost2=-0.00903442\n",
      "     -- mean(hitsP)=0.909155, mean(diffsP)=0.906608 mean(hitsA)=0.692476, mean(diffsA)=0.900277\n",
      "47: eta=0.03765 cost=-0.00844503 jtype=constrained costheta=-0.973 ps=[0.186196, -1.53956, -0.342218, 0.471446, 0.310956, 0.101074, 0.0858743]\n",
      "     -- cost=-0.00845927,   cost1=0.000581859, cost2=-0.00904113\n",
      "     -- mean(hitsP)=0.908826, mean(diffsP)=0.906178 mean(hitsA)=0.692567, mean(diffsA)=0.902049\n",
      "48: eta=0.041415 cost=-0.00845927 jtype=constrained costheta=-0.960 ps=[0.184462, -1.51307, -0.34016, 0.475466, 0.312891, 0.101062, 0.0850583]\n",
      "     -- cost=-0.00847477,   cost1=0.000573641, cost2=-0.00904841\n",
      "     -- mean(hitsP)=0.90845, mean(diffsP)=0.905698 mean(hitsA)=0.692664, mean(diffsA)=0.903983\n",
      "49: eta=0.0455566 cost=-0.00847477 jtype=constrained costheta=-0.939 ps=[0.182659, -1.48364, -0.337829, 0.480012, 0.315209, 0.101052, 0.0841431]\n",
      "     -- cost=-0.0084916,   cost1=0.000564426, cost2=-0.00905602\n",
      "     -- mean(hitsP)=0.908005, mean(diffsP)=0.905128 mean(hitsA)=0.692764, mean(diffsA)=0.906076\n",
      "50: eta=0.0501122 cost=-0.0084916 jtype=constrained costheta=-0.908 ps=[0.180762, -1.45096, -0.335158, 0.485193, 0.317991, 0.101046, 0.0831128]\n",
      "     -- cost=-0.00850982,   cost1=0.000553917, cost2=-0.00906374\n",
      "     -- mean(hitsP)=0.907464, mean(diffsP)=0.904428 mean(hitsA)=0.692866, mean(diffsA)=0.90832\n",
      "51: eta=0.0551234 cost=-0.00850982 jtype=constrained costheta=-0.862 ps=[0.178738, -1.41472, -0.332098, 0.49114, 0.321362, 0.101045, 0.0819464]\n",
      "     -- cost=-0.00852947,   cost1=0.000541943, cost2=-0.00907141\n",
      "     -- mean(hitsP)=0.906799, mean(diffsP)=0.903578 mean(hitsA)=0.692972, mean(diffsA)=0.910704\n",
      "52: eta=0.0606358 cost=-0.00852947 jtype=constrained costheta=-0.800 ps=[0.176549, -1.37465, -0.328629, 0.498014, 0.325523, 0.101052, 0.0806182]\n",
      "     -- cost=-0.00855042,   cost1=0.000528841, cost2=-0.00907926\n",
      "     -- mean(hitsP)=0.905992, mean(diffsP)=0.902631 mean(hitsA)=0.693089, mean(diffsA)=0.913221\n",
      "53: eta=0.0666993 cost=-0.00855042 jtype=constrained costheta=-0.723 ps=[0.17419, -1.33055, -0.324795, 0.505982, 0.330808, 0.10107, 0.0791013]\n",
      "     -- cost=-0.00857216,   cost1=0.000516212, cost2=-0.00908837\n",
      "     -- mean(hitsP)=0.905073, mean(diffsP)=0.901801 mean(hitsA)=0.693228, mean(diffsA)=0.915874\n",
      "54: eta=0.0733693 cost=-0.00857216 jtype=constrained costheta=-0.637 ps=[0.171782, -1.2824, -0.320753, 0.515158, 0.337771, 0.101106, 0.0773806]\n",
      "     -- cost=-0.00859335,   cost1=0.000507433, cost2=-0.00910078\n",
      "     -- mean(hitsP)=0.90417, mean(diffsP)=0.901488 mean(hitsA)=0.693405, mean(diffsA)=0.918668\n",
      "55: eta=0.0807062 cost=-0.00859335 jtype=constrained costheta=-0.544 ps=[0.169698, -1.23051, -0.316732, 0.525511, 0.347203, 0.101173, 0.0754728]\n",
      "     -- cost=-0.00861135,   cost1=0.000506768, cost2=-0.00911811\n",
      "     -- mean(hitsP)=0.90345, mean(diffsP)=0.902045 mean(hitsA)=0.69363, mean(diffsA)=0.921577\n",
      "56: eta=0.0887768 cost=-0.00861135 jtype=constrained costheta=-0.419 ps=[0.168695, -1.1755, -0.312838, 0.536837, 0.359947, 0.101323, 0.0734326]\n",
      "     -- cost=-0.00862227,   cost1=0.000517539, cost2=-0.00913981\n",
      "     -- mean(hitsP)=0.903075, mean(diffsP)=0.903532 mean(hitsA)=0.693908, mean(diffsA)=0.924431\n",
      "57: eta=0.0976545 cost=-0.00862227 jtype=constrained costheta=-0.241 ps=[0.171065, -1.11862, -0.308772, 0.54802, 0.37604, 0.102049, 0.0714139]\n",
      "     -- cost=-0.00862543,   cost1=0.000526402, cost2=-0.00915183\n",
      "     -- mean(hitsP)=0.903728, mean(diffsP)=0.904751 mean(hitsA)=0.694461, mean(diffsA)=0.925615\n",
      "58: eta=0.10742 cost=-0.00862543 jtype=constrained costheta=-0.043 ps=[0.174508, -1.09753, -0.307258, 0.551589, 0.384293, 0.106895, 0.0706154]\n",
      "     -- cost=-0.00862698,   cost1=0.000528575, cost2=-0.00915555\n",
      "     -- mean(hitsP)=0.903749, mean(diffsP)=0.905152 mean(hitsA)=0.694587, mean(diffsA)=0.925959\n",
      "59: eta=0.118162 cost=-0.00862698 jtype=constrained costheta=-0.055 ps=[0.174406, -1.09239, -0.307263, 0.55279, 0.387637, 0.113284, 0.0702709]\n",
      "     -- cost=-0.00862878,   cost1=0.000528586, cost2=-0.00915737\n",
      "     -- mean(hitsP)=0.903714, mean(diffsP)=0.905408 mean(hitsA)=0.694616, mean(diffsA)=0.926066\n",
      "60: eta=0.129978 cost=-0.00862878 jtype=constrained costheta=-0.124 ps=[0.174252, -1.09083, -0.307453, 0.552895, 0.389596, 0.12117, 0.0700842]\n",
      "     -- cost=-0.00863101,   cost1=0.000528263, cost2=-0.00915927\n",
      "     -- mean(hitsP)=0.903652, mean(diffsP)=0.905697 mean(hitsA)=0.694631, mean(diffsA)=0.926158\n",
      "61: eta=0.142976 cost=-0.00863101 jtype=constrained costheta=-0.130 ps=[0.17408, -1.08944, -0.307711, 0.552854, 0.391786, 0.130961, 0.0698807]\n",
      "     -- cost=-0.0086338,   cost1=0.000527867, cost2=-0.00916167\n",
      "     -- mean(hitsP)=0.903567, mean(diffsP)=0.906071 mean(hitsA)=0.694643, mean(diffsA)=0.926262\n",
      "62: eta=0.157274 cost=-0.0086338 jtype=constrained costheta=-0.108 ps=[0.173862, -1.08779, -0.308058, 0.552792, 0.394583, 0.143249, 0.0696253]\n",
      "     -- cost=-0.00863732,   cost1=0.000527462, cost2=-0.00916479\n",
      "     -- mean(hitsP)=0.903446, mean(diffsP)=0.906571 mean(hitsA)=0.69465, mean(diffsA)=0.926386\n",
      "63: eta=0.173001 cost=-0.00863732 jtype=constrained costheta=-0.088 ps=[0.173565, -1.08576, -0.308537, 0.552734, 0.398277, 0.158857, 0.0692945]\n",
      "     -- cost=-0.00864184,   cost1=0.000527164, cost2=-0.009169\n",
      "     -- mean(hitsP)=0.903267, mean(diffsP)=0.907267 mean(hitsA)=0.694647, mean(diffsA)=0.926534\n",
      "64: eta=0.190301 cost=-0.00864184 jtype=constrained costheta=-0.071 ps=[0.173136, -1.08321, -0.309232, 0.552708, 0.403296, 0.178926, 0.0688555]\n",
      "     -- cost=-0.00864772,   cost1=0.000527292, cost2=-0.00917501\n",
      "     -- mean(hitsP)=0.902984, mean(diffsP)=0.908289 mean(hitsA)=0.694625, mean(diffsA)=0.926713\n",
      "65: eta=0.209331 cost=-0.00864772 jtype=constrained costheta=-0.056 ps=[0.172469, -1.08002, -0.310301, 0.552775, 0.410364, 0.205057, 0.0682554]\n",
      "     -- cost=-0.00865553,   cost1=0.000528816, cost2=-0.00918435\n",
      "     -- mean(hitsP)=0.9025, mean(diffsP)=0.909929 mean(hitsA)=0.694573, mean(diffsA)=0.926941\n",
      "66: eta=0.230264 cost=-0.00865553 jtype=constrained costheta=-0.045 ps=[0.171335, -1.07599, -0.312094, 0.553105, 0.420838, 0.239459, 0.0673977]\n",
      "     -- cost=-0.00866636,   cost1=0.000534583, cost2=-0.00920095\n",
      "     -- mean(hitsP)=0.901594, mean(diffsP)=0.912926 mean(hitsA)=0.694487, mean(diffsA)=0.927264\n",
      "67: eta=0.253291 cost=-0.00866636 jtype=constrained costheta=-0.035 ps=[0.169224, -1.07081, -0.315461, 0.554113, 0.437547, 0.285021, 0.0660907]\n",
      "     -- cost=-0.00865387,   cost1=0.00053053, cost2=-0.0091844\n",
      "     -- mean(hitsP)=0.902002, mean(diffsP)=0.909578 mean(hitsA)=0.694269, mean(diffsA)=0.927302\n",
      "eta going down: new_cost-cost=1.24895e-05 and jumptype='constrained'\n",
      "68: eta=0.126645 cost=-0.00866636 jtype=constrained costheta=NaN ps=[0.169224, -1.07081, -0.315461, 0.554113, 0.437547, 0.285021, 0.0660907]\n",
      "     -- cost=-0.00867462,   cost1=0.000541705, cost2=-0.00921632\n",
      "     -- mean(hitsP)=0.901543, mean(diffsP)=0.915945 mean(hitsA)=0.694962, mean(diffsA)=0.92732\n",
      "69: eta=0.13931 cost=-0.00867462 jtype=constrained costheta=-0.031 ps=[0.1677, -1.06714, -0.318452, 0.555081, 0.452073, 0.31329, 0.0650384]\n",
      "     -- cost=-0.00868438,   cost1=0.000540099, cost2=-0.00922448\n",
      "     -- mean(hitsP)=0.901077, mean(diffsP)=0.9177 mean(hitsA)=0.694781, mean(diffsA)=0.927195\n",
      "70: eta=0.153241 cost=-0.00868438 jtype=constrained costheta=-0.092 ps=[0.166237, -1.06203, -0.320171, 0.556495, 0.468491, 0.348049, 0.063935]\n",
      "     -- cost=-0.0082391,   cost1=0.000637738, cost2=-0.00887684\n",
      "     -- mean(hitsP)=0.889565, mean(diffsP)=0.871375 mean(hitsA)=0.678131, mean(diffsA)=0.903993\n",
      "eta going down: new_cost-cost=0.000445277 and jumptype='constrained'\n",
      "71: eta=0.0766204 cost=-0.00868438 jtype=constrained costheta=NaN ps=[0.166237, -1.06203, -0.320171, 0.556495, 0.468491, 0.348049, 0.063935]\n",
      "     -- cost=-0.00859043,   cost1=0.000463853, cost2=-0.00905428\n",
      "     -- mean(hitsP)=0.898284, mean(diffsP)=0.896115 mean(hitsA)=0.688542, mean(diffsA)=0.914741\n",
      "eta going down: new_cost-cost=9.39494e-05 and jumptype='constrained'\n",
      "72: eta=0.0383102 cost=-0.00868438 jtype=constrained costheta=NaN ps=[0.166237, -1.06203, -0.320171, 0.556495, 0.468491, 0.348049, 0.063935]\n",
      "     -- cost=-0.00865644,   cost1=0.0006144, cost2=-0.00927084\n",
      "     -- mean(hitsP)=0.902911, mean(diffsP)=0.925794 mean(hitsA)=0.697306, mean(diffsA)=0.928374\n",
      "eta going down: new_cost-cost=2.7933e-05 and jumptype='constrained'\n",
      "73: eta=0.0191551 cost=-0.00868438 jtype=constrained costheta=NaN ps=[0.166237, -1.06203, -0.320171, 0.556495, 0.468491, 0.348049, 0.063935]\n",
      "     -- cost=-0.00868613,   cost1=0.000538523, cost2=-0.00922465\n",
      "     -- mean(hitsP)=0.901576, mean(diffsP)=0.917957 mean(hitsA)=0.695103, mean(diffsA)=0.926973\n",
      "74: eta=0.0210706 cost=-0.00868613 jtype=constrained costheta=-0.073 ps=[0.166135, -1.06191, -0.320224, 0.556455, 0.471474, 0.352959, 0.0637711]\n",
      "     -- cost=-0.00868779,   cost1=0.000538162, cost2=-0.00922595\n",
      "     -- mean(hitsP)=0.901538, mean(diffsP)=0.91826 mean(hitsA)=0.695091, mean(diffsA)=0.92693\n",
      "75: eta=0.0231777 cost=-0.00868779 jtype=constrained costheta=-0.943 ps=[0.165955, -1.06096, -0.320486, 0.556726, 0.474525, 0.358514, 0.0635775]\n",
      "     -- cost=-0.00868966,   cost1=0.000537522, cost2=-0.00922718\n",
      "     -- mean(hitsP)=0.901503, mean(diffsP)=0.918572 mean(hitsA)=0.695074, mean(diffsA)=0.926864\n",
      "76: eta=0.0254954 cost=-0.00868966 jtype=constrained costheta=-0.888 ps=[0.165751, -1.06008, -0.320766, 0.556948, 0.477823, 0.36476, 0.0633738]\n",
      "     -- cost=-0.00869176,   cost1=0.000536641, cost2=-0.0092284\n",
      "     -- mean(hitsP)=0.901468, mean(diffsP)=0.918903 mean(hitsA)=0.695052, mean(diffsA)=0.926778\n",
      "77: eta=0.028045 cost=-0.00869176 jtype=constrained costheta=-0.851 ps=[0.165528, -1.05927, -0.32108, 0.557136, 0.481432, 0.371779, 0.0631556]\n",
      "     -- cost=-0.00869413,   cost1=0.000535497, cost2=-0.00922963\n",
      "     -- mean(hitsP)=0.901433, mean(diffsP)=0.919257 mean(hitsA)=0.695022, mean(diffsA)=0.926668\n",
      "78: eta=0.0308495 cost=-0.00869413 jtype=constrained costheta=-0.803 ps=[0.165287, -1.05854, -0.321436, 0.557292, 0.485423, 0.379671, 0.0629198]\n",
      "     -- cost=-0.00869681,   cost1=0.000534059, cost2=-0.00923087\n",
      "     -- mean(hitsP)=0.901398, mean(diffsP)=0.919641 mean(hitsA)=0.694985, mean(diffsA)=0.926533\n",
      "79: eta=0.0339344 cost=-0.00869681 jtype=constrained costheta=-0.744 ps=[0.165028, -1.05787, -0.321837, 0.557413, 0.489875, 0.388548, 0.0626628]\n",
      "     -- cost=-0.00869984,   cost1=0.000532287, cost2=-0.00923213\n",
      "     -- mean(hitsP)=0.901364, mean(diffsP)=0.92006 mean(hitsA)=0.694939, mean(diffsA)=0.926366\n",
      "80: eta=0.0373279 cost=-0.00869984 jtype=constrained costheta=-0.675 ps=[0.164752, -1.05727, -0.322293, 0.557496, 0.494886, 0.398544, 0.0623806]\n",
      "     -- cost=-0.00870327,   cost1=0.000530136, cost2=-0.00923341\n",
      "     -- mean(hitsP)=0.901331, mean(diffsP)=0.920522 mean(hitsA)=0.694882, mean(diffsA)=0.92616\n",
      "81: eta=0.0410607 cost=-0.00870327 jtype=constrained costheta=-0.600 ps=[0.16446, -1.05675, -0.322812, 0.557536, 0.500567, 0.40981, 0.0620689]\n",
      "     -- cost=-0.00870717,   cost1=0.000527551, cost2=-0.00923472\n",
      "     -- mean(hitsP)=0.901301, mean(diffsP)=0.921035 mean(hitsA)=0.694811, mean(diffsA)=0.925909\n",
      "82: eta=0.0451667 cost=-0.00870717 jtype=constrained costheta=-0.522 ps=[0.164151, -1.05633, -0.323411, 0.557522, 0.507055, 0.422525, 0.0617229]\n",
      "     -- cost=-0.00871159,   cost1=0.000524473, cost2=-0.00923606\n",
      "     -- mean(hitsP)=0.901274, mean(diffsP)=0.92161 mean(hitsA)=0.694724, mean(diffsA)=0.925603\n",
      "83: eta=0.0496834 cost=-0.00871159 jtype=constrained costheta=-0.445 ps=[0.163824, -1.05605, -0.324108, 0.557439, 0.51451, 0.436894, 0.0613372]\n",
      "     -- cost=-0.00871661,   cost1=0.00052084, cost2=-0.00923745\n",
      "     -- mean(hitsP)=0.901254, mean(diffsP)=0.922262 mean(hitsA)=0.694619, mean(diffsA)=0.925228\n",
      "84: eta=0.0546518 cost=-0.00871661 jtype=constrained costheta=-0.374 ps=[0.163476, -1.05596, -0.32493, 0.557269, 0.523132, 0.453153, 0.0609058]\n",
      "     -- cost=-0.00872229,   cost1=0.000516597, cost2=-0.00923889\n",
      "     -- mean(hitsP)=0.901241, mean(diffsP)=0.923007 mean(hitsA)=0.69449, mean(diffsA)=0.92477\n",
      "85: eta=0.0601169 cost=-0.00872229 jtype=constrained costheta=-0.309 ps=[0.1631, -1.05617, -0.325915, 0.556982, 0.53316, 0.471574, 0.060422]\n",
      "     -- cost=-0.0087287,   cost1=0.000511703, cost2=-0.0092404\n",
      "     -- mean(hitsP)=0.901238, mean(diffsP)=0.923869 mean(hitsA)=0.694334, mean(diffsA)=0.924212\n",
      "86: eta=0.0661286 cost=-0.0087287 jtype=constrained costheta=-0.253 ps=[0.162684, -1.05679, -0.327116, 0.556542, 0.544892, 0.492467, 0.0598783]\n",
      "     -- cost=-0.00873588,   cost1=0.000506157, cost2=-0.00924204\n",
      "     -- mean(hitsP)=0.901248, mean(diffsP)=0.924878 mean(hitsA)=0.694148, mean(diffsA)=0.923531\n",
      "87: eta=0.0727415 cost=-0.00873588 jtype=constrained costheta=-0.205 ps=[0.162208, -1.058, -0.328606, 0.555904, 0.558703, 0.51618, 0.0592662]\n",
      "     -- cost=-0.00874385,   cost1=0.000500031, cost2=-0.00924388\n",
      "     -- mean(hitsP)=0.901274, mean(diffsP)=0.926072 mean(hitsA)=0.693928, mean(diffsA)=0.922703\n",
      "88: eta=0.0800156 cost=-0.00874385 jtype=constrained costheta=-0.164 ps=[0.161633, -1.06005, -0.330485, 0.555012, 0.575064, 0.543092, 0.058576]\n",
      "     -- cost=-0.00875252,   cost1=0.000493517, cost2=-0.00924603\n",
      "     -- mean(hitsP)=0.901318, mean(diffsP)=0.927502 mean(hitsA)=0.693671, mean(diffsA)=0.921705\n",
      "89: eta=0.0880172 cost=-0.00875252 jtype=constrained costheta=-0.130 ps=[0.160895, -1.06327, -0.33289, 0.553807, 0.594588, 0.5736, 0.0577966]\n",
      "     -- cost=-0.00876171,   cost1=0.000486988, cost2=-0.0092487\n",
      "     -- mean(hitsP)=0.901386, mean(diffsP)=0.929226 mean(hitsA)=0.693378, mean(diffsA)=0.920514\n",
      "90: eta=0.0968189 cost=-0.00876171 jtype=constrained costheta=-0.101 ps=[0.159884, -1.06806, -0.336001, 0.552242, 0.618091, 0.608067, 0.0569154]\n",
      "     -- cost=-0.00877111,   cost1=0.000481043, cost2=-0.00925215\n",
      "     -- mean(hitsP)=0.901482, mean(diffsP)=0.93131 mean(hitsA)=0.693055, mean(diffsA)=0.91912\n",
      "91: eta=0.106501 cost=-0.00877111 jtype=constrained costheta=-0.077 ps=[0.158417, -1.07495, -0.340042, 0.550303, 0.646702, 0.646716, 0.0559179]\n",
      "     -- cost=-0.00878021,   cost1=0.000476488, cost2=-0.0092567\n",
      "     -- mean(hitsP)=0.901616, mean(diffsP)=0.933804 mean(hitsA)=0.692713, mean(diffsA)=0.917535\n",
      "92: eta=0.117151 cost=-0.00878021 jtype=constrained costheta=-0.058 ps=[0.15619, -1.08451, -0.345264, 0.548079, 0.682041, 0.68938, 0.0547864]\n",
      "     -- cost=-0.0087884,   cost1=0.000474239, cost2=-0.00926264\n",
      "     -- mean(hitsP)=0.901782, mean(diffsP)=0.936707 mean(hitsA)=0.692371, mean(diffsA)=0.915821\n",
      "93: eta=0.128866 cost=-0.0087884 jtype=constrained costheta=-0.041 ps=[0.152592, -1.09721, -0.351876, 0.545952, 0.726543, 0.734822, 0.0534878]\n",
      "     -- cost=-0.00879547,   cost1=0.000475438, cost2=-0.00927091\n",
      "     -- mean(hitsP)=0.901869, mean(diffsP)=0.939938 mean(hitsA)=0.692097, mean(diffsA)=0.914244\n",
      "94: eta=0.141753 cost=-0.00879547 jtype=constrained costheta=-0.028 ps=[0.145876, -1.11287, -0.359826, 0.545359, 0.783978, 0.777792, 0.0519421]\n",
      "     -- cost=-0.00880466,   cost1=0.000482583, cost2=-0.00928724\n",
      "     -- mean(hitsP)=0.901597, mean(diffsP)=0.944287 mean(hitsA)=0.691955, mean(diffsA)=0.913162\n",
      "95: eta=0.155928 cost=-0.00880466 jtype=constrained costheta=-0.022 ps=[0.136797, -1.12792, -0.368885, 0.547113, 0.859187, 0.810209, 0.0501208]\n",
      "     -- cost=-0.00881963,   cost1=0.000492044, cost2=-0.00931167\n",
      "     -- mean(hitsP)=0.900862, mean(diffsP)=0.950355 mean(hitsA)=0.691701, mean(diffsA)=0.911979\n",
      "96: eta=0.171521 cost=-0.00881963 jtype=constrained costheta=-0.032 ps=[0.129881, -1.14478, -0.379999, 0.547744, 0.954495, 0.841314, 0.0482654]\n",
      "     -- cost=-0.00883358,   cost1=0.000484316, cost2=-0.00931789\n",
      "     -- mean(hitsP)=0.900407, mean(diffsP)=0.95304 mean(hitsA)=0.691529, mean(diffsA)=0.910539\n",
      "97: eta=0.188673 cost=-0.00883358 jtype=constrained costheta=-0.026 ps=[0.115093, -1.17132, -0.389009, 0.550708, 1.05192, 0.868019, 0.0466511]\n",
      "     -- cost=-0.0088409,   cost1=0.000476404, cost2=-0.00931731\n",
      "     -- mean(hitsP)=0.90069, mean(diffsP)=0.953692 mean(hitsA)=0.691953, mean(diffsA)=0.909769\n",
      "98: eta=0.20754 cost=-0.0088409 jtype=Newton costheta=-0.017 ps=[0.0997248, -1.19305, -0.395077, 0.553976, 1.12593, 0.880166, 0.0456609]\n",
      "     -- cost=-0.00884303,   cost1=0.000471691, cost2=-0.00931472\n",
      "     -- mean(hitsP)=0.900651, mean(diffsP)=0.953967 mean(hitsA)=0.691985, mean(diffsA)=0.908977\n",
      "99: eta=0.228294 cost=-0.00884303 jtype=Newton costheta=-0.015 ps=[0.0898776, -1.2079, -0.399666, 0.555332, 1.18249, 0.889359, 0.0449695]\n",
      "     -- cost=-0.00884339,   cost1=0.000470208, cost2=-0.0093136\n",
      "     -- mean(hitsP)=0.900756, mean(diffsP)=0.953993 mean(hitsA)=0.692157, mean(diffsA)=0.908728\n",
      "100: eta=0.251123 cost=-0.00884339 jtype=Newton costheta=-0.010 ps=[0.0850058, -1.21456, -0.40164, 0.556126, 1.20712, 0.89199, 0.0446968]\n",
      "     -- cost=-0.00884341,   cost1=0.00046976, cost2=-0.00931317\n",
      "     -- mean(hitsP)=0.90076, mean(diffsP)=0.953983 mean(hitsA)=0.692179, mean(diffsA)=0.908651\n",
      "101: eta=0.276236 cost=-0.00884341 jtype=Newton costheta=-0.010 ps=[0.0835418, -1.21651, -0.402224, 0.556376, 1.21453, 0.892727, 0.0446154]\n",
      "     -- cost=-0.00884341,   cost1=0.000469734, cost2=-0.00931314\n",
      "     -- mean(hitsP)=0.900761, mean(diffsP)=0.953982 mean(hitsA)=0.692182, mean(diffsA)=0.908647\n",
      "102: eta=0.303859 cost=-0.00884341 jtype=Newton costheta=-0.007 ps=[0.0834454, -1.21662, -0.402257, 0.556394, 1.21496, 0.892756, 0.0446109]\n",
      "     -- cost=-0.00884341,   cost1=0.000469734, cost2=-0.00931314\n",
      "     -- mean(hitsP)=0.900761, mean(diffsP)=0.953982 mean(hitsA)=0.692182, mean(diffsA)=0.908647\n",
      "About to break -- tol=1e-12, new_cost-cost=-1.46064e-15, eta=0.303859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.08344536755256632"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = [\"sW\", \"vW\", \"hW\", \"constant_excitation\", \"right_light_excitation\", \"target_period_excitation\", \"sigma\"]\n",
    "seed = [0.2,   -1.7, -1.7,      0.19,                0.5,                       1,                       0.1]\n",
    "seed = [0.2,   0.17,  0.17,      0.19,                0.5,                       1,                       0.1]\n",
    "seed = [0.2,   -1.7, -1.7,      -0.19,                0.5,                       1,                       0.1]\n",
    "seed = [0.2,   -1.7, -1.7,      0.39,                0.15,                       0.1,                       0.1]\n",
    "bbox = Dict(:sW=>[0 3], :vW=>[-3 3], :hW=>[-3 3], :constant_excitation=>[-2 2],\n",
    ":right_light_excitation=>[0.05 4], :target_period_excitation=>[0.05 4], :sigma=>[0.05 1])\n",
    "model_params = merge(model_params, Dict(:post_target_period=>0.5))\n",
    "\n",
    "\n",
    "# Now with constant_pro_bias and a fixed sigma=0.1\n",
    "args = [\"sW\", \"vW\", \"hW\", \"constant_excitation\", \"right_light_excitation\", \"target_period_excitation\", \"const_pro_bias\"]\n",
    "seed = [0.2,   -1.7, -1.7,      0.39,                0.15,                       0.1,                       0.1]\n",
    "model_params = merge(model_params, Dict(:post_target_period=>0.5, :sigma=>0.1))\n",
    "bbox = Dict(:sW=>[0 3], :vW=>[-3 3], :hW=>[-3 3], :constant_excitation=>[-2 2],\n",
    ":right_light_excitation=>[0.05 4], :target_period_excitation=>[0.05 4], :const_pro_bias=>[-2 2])\n",
    "\n",
    "# ==========\n",
    "\n",
    "nPro=100; nAnti=100\n",
    "\n",
    "rule_and_delay_periods = [0.4, 0.8]\n",
    "post_target_periods    = [0.5, 1]\n",
    "\n",
    "pars, traj, cost, cpm_traj, Dlambda = bbox_Hessian_keyword_minimization(seed, args, bbox, \n",
    "(;params...) -> JJ(nPro, nAnti; rule_and_delay_periods=rule_and_delay_periods,\n",
    "post_target_periods=post_target_periods,\n",
    "seedrand=31, cbeta=0.01, verbose=true, merge(model_params, Dict(params))...),\n",
    "start_eta = 0.01, tol=1e-12, verbose=true)[1]\n",
    "\n",
    "pars'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     -- cost=-0.00884341,   cost1=0.000469729, cost2=-0.00931314\n",
      "     -- mean(hitsP)=0.90076, mean(diffsP)=0.953981 mean(hitsA)=0.69218, mean(diffsA)=0.908647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.008843411106774281"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func = (;params...) -> JJ(nPro, nAnti; rule_and_delay_periods=rule_and_delay_periods,\n",
    "post_target_periods=post_target_periods,\n",
    "seedrand=31, cbeta=0.01, verbose=true, merge(model_params, Dict(params))...)\n",
    "\n",
    "func(;make_dict(args, pars)...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8.740874 seconds (13.19 M allocations: 798.834 MB, 1.98% gc time)\n",
      "Pro % correct = 90.7%\n",
      "Anti % correct = 67.1% \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.0016377281161004287"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------\n",
    "# NOW EVALUATE RESULTS HERE\n",
    "# --------------------\n",
    "\n",
    "my_params = make_dict([args; \"plot_list\"; \"post_target_period\"; \"rule_and_delay_period\"; \"dt\"], \n",
    "[pars; [[1:10;]]; 1.5; 0.5; 0.02], model_params)\n",
    "\n",
    "run_factor = 10\n",
    "\n",
    "proVs, antiVs = @time(run_ntrials(nPro*run_factor, nAnti*run_factor; plot_list=[], my_params...))\n",
    "\n",
    "if nPro>0;  @printf(\"Pro %% correct = %g%%\\n\", 100*length(find(proVs[1,:].>proVs[4,:]))/(nPro*run_factor)); end;\n",
    "if nAnti>0; @printf(\"Anti %% correct = %g%% \\n\", 100*length(find(antiVs[1,:].<antiVs[4,:]))/(nAnti*run_factor)); end;\n",
    "\n",
    "\n",
    "figure(3); clf();\n",
    "ax1 = subplot(2,1,1)\n",
    "h = plt[:hist](proVs[1,:]-proVs[4,:],-1:0.02:1)\n",
    "title(\"PRO Vr - Vl\")\n",
    "remove_xtick_labels(ax1)\n",
    "vlines(0, ylim()[1], ylim()[2])\n",
    "\n",
    "ax2 = subplot(2,1,2)\n",
    "h = plt[:hist](antiVs[1,:]-antiVs[4,:],-1:0.02:1)\n",
    "title(\"ANTI Vr - Vl\")\n",
    "vlines(0, ylim()[1], ylim()[2])\n",
    "\n",
    "figure(1); clf(); figure(2); clf();\n",
    "\n",
    "JJ(nPro, nAnti; rule_and_delay_periods=my_params[:rule_and_delay_period], \n",
    "post_target_periods=my_params[:post_target_period], my_params...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Symbol,Any} with 29 entries:\n",
       "  :target_period          => 0.1\n",
       "  :right_light_pro_extra  => 0\n",
       "  :const_pro_bias         => 0\n",
       "  :beta                   => 1\n",
       "  :sigma                  => 0.08\n",
       "  :dW                     => 0\n",
       "  :anti_rule_strength     => 0.1\n",
       "  :init_add               => 0\n",
       "  :pro_rule_strength      => 0.1\n",
       "  :vw                     => -1.7\n",
       "  :sW                     => 0.2\n",
       "  :vW                     => -1.7\n",
       "  :noise                  => Any[]\n",
       "  :tau                    => 0.1\n",
       "  :theta                  => 1\n",
       "  :right_light_excitation => 0.5\n",
       "  :hW                     => -1.7\n",
       "  :hw                     => -1.7\n",
       "  :sw                     => 0.2\n",
       "  :constant_excitation    => 0.19\n",
       "  :rule_and_delay_period  => 0.4\n",
       "  :const_add              => 0\n",
       "  :nsteps                 => 2\n",
       "  :post_target_period     => 0.5\n",
       "  :input                  => 0\n",
       "  ⋮                       => ⋮"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition wallwrap(Any, Any) in module Main at In[90]:18 overwritten at In[92]:18.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'wallwrap :: Tuple{Any,Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition vector_wrap(Any, Any, Any) in module Main at In[90]:44 overwritten at In[92]:44.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'vector_wrap :: Tuple{Any,Any,Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition inverse_wall(Any, Any, Any) in module Main at In[90]:68 overwritten at In[92]:68.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'inverse_wall :: Tuple{Any,Any,Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition inverse_wall(Any, Any) in module Main at In[90]:91 overwritten at In[92]:91.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'inverse_wall :: Tuple{Any,Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition bbox_Hessian_keyword_minimization(Any, Any, Any, Any) in module Main at In[90]:193 overwritten at In[92]:193.\n",
      "WARNING: Method definition #bbox_Hessian_keyword_minimization(Array{Any, 1}, Main.#bbox_Hessian_keyword_minimization, Any, Any, Any, Any) in module Main overwritten.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'bbox_Hessian_keyword_minimization :: Tuple{Any,Any,Any,Any}' in module 'Main'.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bbox_Hessian_keyword_minimization"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################################\n",
    "#                                                    #\n",
    "#         BBOX_HESSIAN_KEYWORD_MINIMIZATION          #\n",
    "#                                                    #\n",
    "######################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "pdict = wallwrap(bdict, pdict)\n",
    "Given bdict, a dictionary of symbols to [minval, maxval] vectors, and pdict, a dictionary of symbols\n",
    "to values (or, alternatively, an Array of (Symbol, value) tuples], goes through each of the symbols in \n",
    "bdict and modifies the corresponding value in pdict putting it through a tanh so the final output lies \n",
    "within the limits in bdict.  Returns the new pdict.  Makes a copy of pdict so as not to modify the original.\n",
    "\"\"\"\n",
    "function wallwrap(bdict, epdict)\n",
    "    local pdict = two_level_copy(epdict)\n",
    "    if typeof(pdict)<:Array\n",
    "        pdict = Dict(pdict)\n",
    "    end\n",
    "\n",
    "    allkeys = keys(bdict)\n",
    "\n",
    "    for k in allkeys\n",
    "        local bbox = bdict[k]\n",
    "        d = 0.5*(bbox[2] - bbox[1])\n",
    "        m = 0.5*(bbox[2] + bbox[1])\n",
    "\n",
    "        pdict[k] = bbox[1] + d*(tanh((pdict[k]-m)/d)+1)\n",
    "    end\n",
    "    return pdict\n",
    "end\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "params = vector_wrap(bbox, args, eparams)\n",
    "Given bdict, a dictionary of symbols to [minval, maxval] vectors, args, an array of strings representing\n",
    "symbols, and params, an array of values corresponding to the args list, puts each param that has an entry \n",
    "in bdict through the tanh-walling mechanism, and returns the result. Does not modify the contents of the \n",
    "original params vector (or bdict or args).\n",
    "\"\"\"\n",
    "function vector_wrap(bbox, args, eparams)\n",
    "    local params = two_level_copy(eparams)\n",
    "    pdict = wallwrap(bbox, make_dict(args, params))\n",
    "    i=1; j=1\n",
    "    for i=1:length(args)\n",
    "        if typeof(args[i])<:Array\n",
    "            params[j:j+args[i][2]-1] = pdict[Symbol(args[i][1])]\n",
    "            j += args[i][2]-1\n",
    "        else\n",
    "            params[j] = pdict[Symbol(args[i])]\n",
    "        end\n",
    "    j = j+1\n",
    "    end\n",
    "    return params\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "params = inverse_wall(bdict, args, wparams)\n",
    "Given bdict, a dictionary of symbols to [minval, maxval] vectors, args, an array of strings representing\n",
    "symbols, and wparams, an array of values corresponding to the args list where each param that has an entry \n",
    "in bdict has alreadt been through the tanh-walling mechanism, UNwalls the ones that have a bdict entry and\n",
    "returns the result. Does not modify the contents of the original params vector (or bdict or args).\n",
    "\"\"\"\n",
    "function inverse_wall(bdict, args, wparams)\n",
    "    local params = two_level_copy(wparams)\n",
    "    pdict = inverse_wall(bdict, make_dict(args, params))\n",
    "    i=1; j=1\n",
    "    for i=1:length(args)\n",
    "        if typeof(args[i])<:Array\n",
    "            params[j:j+args[i][2]-1] = pdict[Symbol(args[i][1])]\n",
    "            j += args[i][2]-1\n",
    "        else\n",
    "            params[j] = pdict[Symbol(args[i])]\n",
    "        end\n",
    "        j = j+1\n",
    "    end\n",
    "    return params    \n",
    "end\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "pdict = inverse_wall(bdict, wdict)\n",
    "Given bdict, a dictionary of symbols to [minval, maxval] vectors, and wdict, a dictionary of symbols to values\n",
    "(or vectors of values)  UNwalls the ones that have a bdict entry and\n",
    "returns the result. Does not modify the contents of any dictionaries.\n",
    "\"\"\"\n",
    "function inverse_wall(bdict, wdict)\n",
    "    local pdict = two_level_copy(wdict)\n",
    "\n",
    "    allkeys = keys(bdict)\n",
    "    for k in allkeys\n",
    "        local bbox = bdict[k]\n",
    "        d = 0.5*(bbox[2] - bbox[1])\n",
    "        m = 0.5*(bbox[2] + bbox[1])\n",
    "\n",
    "        pdict[k] = m + d*0.5*log((pdict[k]-bbox[1])./(2*d - pdict[k] + bbox[1]))\n",
    "    end\n",
    "    return(pdict)\n",
    "end\n",
    "  \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "function bbox_Hessian_keyword_minimization(seed, args, bbox, func; wallwidth=NaN, start_eta=10, tol=1e-6, \n",
    "    maxiter=400, verbose=false)\n",
    "\n",
    "Like constrained_Hessian_minimization, but uses keyword_hessian!(). \n",
    "\n",
    "PARAMETERS:\n",
    "===========\n",
    "\n",
    "seed        column vector, representing the starting value of the parameters.\n",
    "\n",
    "args        List of strings identifying parameters for differentiation, e.g., [\"const_E\", \"w_self]\n",
    "\n",
    "bbox        If softbox=true (the default), should then be a Dict of Symbol=>[minval maxval] entries. An entry\n",
    "            in this Dict indicates that the corresponding parameter is to be bounded, as indicated by the associated \n",
    "            [minval maxval] vector. The bbox dictionary can have fewer entries than the number of parameters, and its\n",
    "            default value is Dict(), indicating an unbounded search.\n",
    "\n",
    "            If softbox=false, then bbox should be an nargs-by-2 matrix indicating the range for each argument,\n",
    "            with the minima (first column) and maxima (second column), and entries for ALL parameters.\n",
    "\n",
    "func        func must take only optional keyword args, and must \n",
    "            take nderivs=0, difforder=0  and declare any new matrices using ForwardDiffZeros() instead of zeros()\n",
    "\n",
    "\n",
    "OPTIONAL PARAMETERS:\n",
    "====================\n",
    "\n",
    "start_eta    Starting value of the radius.  It's good to start with somethibg biggish, if it is\n",
    "             too much, it'll quickly get cut down.\n",
    "\n",
    "tol=1e-6     Numerical tolerance. If a proposed jump produces a change in func that is less than\n",
    "             this, the minimization stops.\n",
    "\n",
    "maxiter=400  Maximum number of iterations to do before stopping\n",
    "\n",
    "verbose=false   If true, print out a report on each iteration of iteration number, radius size (eta),\n",
    "                what type jump was proposed (\"Newton\" means going straight to global min, \"constrained\" means jump has \n",
    "                norm eta, failed means that finding the minimum at a given radius somehow didn't work). Will also\n",
    "                print out the cosine of the angle between the proposed jump and the gradient.\n",
    "\n",
    "verbose_level   If less than 2, regular verbose output, if 2 or greater, very verbose, for debugging.\n",
    "\n",
    "softbox         If true, then bbox must be a Dict() and we use the tanh() mechanism for putting a fixed limit\n",
    "                on the parameters.\n",
    "\n",
    "hardbox=false   If true, ignores wallwidth, and just rests parameter values to the bounding box if they go outside it.\n",
    "                If false, adds cost function \"walls\" to implement the bounding box.\n",
    "\n",
    "walldith=NaN     Used for putting up cost function \"walls\" that implement the bounding box limits. Can be NaN.\n",
    "                If it is NaN, then the wallwidth is a constant factor of the range width for each argument. If not NaN, must\n",
    "                be an nargs-long vector that indicates the actual wall widths.\n",
    "\n",
    "wallwidth_factor=0.18   Only relevant if wallwidth is NaN, otherwise ignored. For each arg, the wall width\n",
    "                is going to be wall_width_factor*(bbox[i,2] - bbox[i,1])\n",
    "\n",
    "\n",
    "RETURNS:\n",
    "========\n",
    "\n",
    "params       A vector the size of seed that has the last values of the minimizing parameters for func\n",
    "trajectory   A (2+length(params))-by-nsteps matrix. Each column corresponds to an iteration step, and contains\n",
    "                 the value of eta used, the cost, and the value of the parameters at that iteration\n",
    "cost         Final value of objective function\n",
    "cpm_traj     A 2-by-nsteps matrix, containing reports from the contrained parabolic minimization at each timestep.\n",
    "             The first row is niters (how many iterations cpm's 1-d minimization ran for) and the second row is\n",
    "             Dlambda, the last change in the parameter being minimized in cpm's internal search\n",
    "\n",
    "\n",
    "EXAMPLE:\n",
    "========\n",
    "\n",
    "function tester(;x=5, y=10, z=20, nderivs=0, difforder=0)\n",
    "    return x^2*y + z/tanh(y)\n",
    "end\n",
    "\n",
    "params, trajectory = bbox_Hessian_keyword_minimization([0.5, 0.5], [\"x\", \"y\"], [1.1 2 ; 1.1 4], tester, \n",
    "    verbose=true, tol=1e-12, start_eta=1);\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "function bbox_Hessian_keyword_minimization(seed, args, bbox, func; start_eta=0.1, tol=1e-6, maxiter=400,\n",
    "    verbose=false, verbose_level=1, verbose_every=1, \n",
    "    softbox=true, hardbox=false, wallwidth=NaN, wallwidth_factor=0.18)\n",
    "\n",
    "      \n",
    "    \"\"\"\n",
    "    Given args, a list of string representing the arguments of interest, a bounding box for each,\n",
    "    and a Symbol=>value dictionary with the corresponding parameters, computes and returns a high cost for \n",
    "    being outside the bounding box\n",
    "    \"\"\"\n",
    "    function wall_cost(args, bbox; wallwidth=NaN, nderivs=0, difforder=0, pars...) \n",
    "        myparams = ForwardDiffZeros(length(pars), 1, nderivs=nderivs, difforder=difforder)\n",
    "        pars2 = Dict()\n",
    "        for i in [1:length(pars);]\n",
    "            pars2[string(pars[i][1])] = pars[i][2]\n",
    "        end\n",
    "        for i in [1:length(args);]\n",
    "            myparams[i] = pars2[args[i]]\n",
    "        end\n",
    "        \n",
    "        if isnan(wallwidth)\n",
    "            # We know that we're going to be taking hessian for params, so declare zeros accordingly:\n",
    "            wallwidth = ForwardDiffZeros(length(myparams), 1, nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "            for i in [1:length(myparams);]\n",
    "                wallwidth[i] = wallwidth_factor*(bbox[i,2]-bbox[i,1])\n",
    "            end\n",
    "        end\n",
    "\n",
    "        retval = 0\n",
    "        for i in [1:length(myparams);]\n",
    "            if myparams[i]<bbox[i,1]\n",
    "                retval += cosh((bbox[i,1]-myparams[i])/wallwidth[i])-1.0\n",
    "            elseif bbox[i,2] < myparams[i]\n",
    "                retval += cosh((myparams[i]-bbox[i,2])/wallwidth[i])-1.0                \n",
    "            end\n",
    "        end\n",
    "\n",
    "        return 2*retval\n",
    "    end\n",
    "\n",
    "    traj_increment = 100\n",
    "    params = 0  # Make sure to have this here so that params stays defined beyond the try/catch\n",
    "    if ( !(typeof(bbox)<:Dict) ); error(\"Currently only supporting softbox=true, bbox must be a Dict\"); end;\n",
    "    try\n",
    "        params = copy(inverse_wall(bbox, args, seed))\n",
    "    catch\n",
    "        error(\"Were all initial param values within the indicated walls?\")\n",
    "    end\n",
    "    eta = start_eta\n",
    "    trajectory = zeros(2+length(params), traj_increment); cpm_traj = zeros(2, traj_increment)\n",
    "\n",
    "    if verbose\n",
    "        @printf \"%d: eta=%g ps=\" 0 eta \n",
    "        print_vector(vector_wrap(bbox, args, params))\n",
    "        @printf \"\\n\"\n",
    "    end\n",
    "    \n",
    "    if softbox\n",
    "        if !(typeof(bbox)<:Dict); error(\"bhm: If softbox=true, then bbox must eb a Dict\"); end\n",
    "        cost, grad, hess = keyword_vgh((;pars...)->func(;wallwrap(bbox, pars)...), args, params)\n",
    "    elseif hardbox\n",
    "        cost, grad, hess = keyword_vgh((;pars...) -> func(;pars...), args, params)\n",
    "    else\n",
    "        cost, grad, hess = keyword_vgh((;pars...) -> func(;pars...) + wall_cost(args, bbox; wallwidth=wallwidth, pars...),\n",
    "            args, params)        \n",
    "    end\n",
    "        \n",
    "    chessdelta = zeros(size(params))\n",
    "\n",
    "    for i in [1:maxiter;]\n",
    "        if i > size(trajectory, 2)\n",
    "            trajectory = [trajectory zeros(2+length(params), traj_increment)]\n",
    "            cpm_traj   = [cpm_traj   zeros(2, traj_increment)]\n",
    "        end\n",
    "        trajectory[1:2, i]   = [eta;cost]\n",
    "        trajectory[3:end, i] = vector_wrap(bbox, args, params)\n",
    "        \n",
    "        hessdelta  = - inv(hess)*grad\n",
    "        try\n",
    "            if verbose && verbose_level >= 2\n",
    "                @printf(\"bhm: about to try cpm with grad : \"); print_vector_g(grad); print(\"\\n\")\n",
    "                @printf(\"bhm:   hess :\"); print_vector_g(hess[:]); print(\"\\n\");\n",
    "            end\n",
    "            if verbose && verbose_level >= 2\n",
    "                cpm_out = constrained_parabolic_minimization(hess, grad'', eta, \n",
    "                    maxiter=500, tol=1e-20, do_plot=true, verbose=true)                \n",
    "            else\n",
    "                cpm_out = constrained_parabolic_minimization(hess, grad'', eta, maxiter=500, tol=1e-20)\n",
    "            end\n",
    "            chessdelta = cpm_out[1]; cpm_traj[1,i] = cpm_out[5]; cpm_traj[2,i] = cpm_out[6]\n",
    "            jumptype = \"not failed\"\n",
    "        catch y\n",
    "            jumptype = \"failed\"\n",
    "            if verbose\n",
    "                @printf \"Constrained parabolic minimization failed with error %s\\n\" y\n",
    "                @printf \"\\n\"\n",
    "                @printf \"eta was %g\\n\" eta\n",
    "                @printf \"grad was\\n\"\n",
    "                print_vector(grad)\n",
    "                @printf \"\\n\\nhess was\\n\"\n",
    "                for k in [1:length(grad);]\n",
    "                    print_vector(hess[k,:])\n",
    "                    @printf \"\\n\"\n",
    "                end\n",
    "                @printf \"\\n\"\n",
    "                matwrite(\"error_report.mat\", Dict(\"grad\"=>grad, \"hess\"=>hess, \"eta\"=>eta))\n",
    "            end\n",
    "            break\n",
    "        end\n",
    "\n",
    "        if norm(hessdelta) <= eta\n",
    "            new_params = params + hessdelta\n",
    "            jumptype = \"Newton\"\n",
    "        elseif jumptype != \"failed\" \n",
    "            new_params = params + chessdelta\n",
    "            jumptype  = \"constrained\"\n",
    "        end\n",
    "\n",
    "        if jumptype != \"failed\"\n",
    "            if softbox\n",
    "                new_cost, new_grad, new_hess = \n",
    "                    keyword_vgh((;pars...) -> func(;wallwrap(bbox, pars)...), args, new_params)\n",
    "                if verbose && verbose_level >=2\n",
    "                    @printf(\"bhm: had new_params = : \"); print_vector_g(vector_wrap(bbox, args, params)); print(\"\\n\");\n",
    "                    @printf(\"bhm: and my bbox was : \"); print(bbox); print(\"\\n\")\n",
    "                    @printf(\"bhm: and my wallwrap output was : \"); print(wallwrap(bbox, make_dict(args, new_params))); print(\"\\n\")\n",
    "                    @printf(\"bhm: and this produced new_grad : \"); print_vector_g(new_grad); print(\"\\n\")\n",
    "                    @printf(\"bhm:   new_hess :\"); print_vector_g(new_hess[:]); print(\"\\n\");                                        \n",
    "                end\n",
    "            elseif hardbox\n",
    "                for p in [1:length(new_params);]\n",
    "                    if new_params[p] < bbox[p,1]; new_params[p] = bbox[p,1]; end\n",
    "                    if bbox[p,2] < new_params[p]; new_params[p] = bbox[p,2]; end\n",
    "                 end        \n",
    "                \n",
    "                new_cost, new_grad, new_hess = keyword_vgh((;pars...) -> func(;pars...), args, new_params)\n",
    "            else\n",
    "                new_cost, new_grad, new_hess = keyword_vgh((;pars...) -> func(;pars...) + \n",
    "                        wall_cost(args, bbox; wallwidth=wallwidth, pars...),\n",
    "                    args, new_params)                \n",
    "            end\n",
    "            \n",
    "            if abs(new_cost - cost) < tol || eta < tol\n",
    "                trajectory = trajectory[:,1:i]; cpm_traj = cpm_traj[:,1:i]\n",
    "                if verbose\n",
    "                    @printf(\"About to break -- tol=%g, new_cost-cost=%g, eta=%g\\n\", tol, new_cost-cost, eta)\n",
    "                end\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "\n",
    "        if jumptype == \"failed\" || new_cost >= cost  \n",
    "            if verbose\n",
    "                @printf(\"eta going down: new_cost-cost=%g and jumptype='%s'\\n\", new_cost-cost, jumptype)\n",
    "                if verbose_level >= 2\n",
    "                    nwp = vector_wrap(bbox, args, new_params); wp = vector_wrap(bbox, args, params)\n",
    "                    @printf(\"   vvv: proposed new params were : \"); print_vector_g(nwp); print(\"\\n\")\n",
    "                    @printf(\"   vvv: proposed delta params was : \"); print_vector_g(nwp-wp); print(\"\\n\")\n",
    "                    @printf(\"   vvv: grad was : \"); print_vector_g(grad); print(\"\\n\")\n",
    "                    costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "                    @printf(\"   vvv: costheta of proposed jump was %g\\n\", costheta)\n",
    "                end\n",
    "            end\n",
    "            eta = eta/2\n",
    "            costheta = NaN\n",
    "            if eta < tol\n",
    "                trajectory = trajectory[:,1:i]; cpm_traj = cpm_traj[:,1:i]\n",
    "                if verbose\n",
    "                    @printf(\"About to break -- tol=%g, new_cost-cost=%g, eta=%g\\n\", tol, new_cost-cost, eta)\n",
    "                end\n",
    "                break\n",
    "            end\n",
    "        else\n",
    "            eta = eta*1.1\n",
    "            costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "\n",
    "            params = new_params\n",
    "            cost = new_cost\n",
    "            grad = new_grad\n",
    "            hess = new_hess\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            if rem(i, verbose_every)==0\n",
    "                @printf \"%d: eta=%g cost=%g jtype=%s costheta=%.3f ps=\" i eta cost jumptype costheta\n",
    "                print_vector_g(vector_wrap(bbox, args, params))\n",
    "                @printf \"\\n\"\n",
    "                if verbose_level >= 3\n",
    "                    @printf \"    At this point, grad is =\"\n",
    "                    print_vector_g(grad)\n",
    "                    @printf \"\\n\"                \n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vector_wrap(bbox, args, params), trajectory, cost, cpm_traj\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the distribution of VR - VL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ntrials = 500\n",
    "proVs, antiVs = @time(run_ntrials(ntrials; plot_list=[], model_params...))\n",
    "\n",
    "@printf(\"Pro %% correct = %g%%\\n\", 100*length(find(proVs[1,:].>proVs[4,:]))/ntrials)\n",
    "@printf(\"Anti %% correct = %g%% \\n\", 100*length(find(antiVs[1,:].<antiVs[4,:]))/ntrials)\n",
    "\n",
    "figure(1); clf();\n",
    "ax1 = subplot(2,1,1)\n",
    "h = plt[:hist](proVs[1,:]-proVs[4,:],-0.1:0.002:0.1)\n",
    "title(\"PRO Vr - Vl\")\n",
    "remove_xtick_labels(ax1)\n",
    "vlines(0, ylim()[1], ylim()[2])\n",
    "\n",
    "ax2 = subplot(2,1,2)\n",
    "h = plt[:hist](antiVs[1,:]-antiVs[4,:],-0.1:0.002:0.1)\n",
    "title(\"ANTI Vr - Vl\")\n",
    "vlines(0, ylim()[1], ylim()[2])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.2",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
