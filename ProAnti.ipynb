{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sun 2017-09-10 8pm:** Focusing on ProAnti, using multiple rule_and_delay_period values, as well as multiple post_target_period_values.  Seems to be working fine. Should to set up a parameter space search.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">TABLE OF CONTENTS</h1>\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "**Updates to the table of contents are periodic, but run the cell below to first start or force an update.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://sites.google.com/site/brodylabhome/files/make_table_of_contents.js')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition @javascript_str(ANY<:Any) in module Main at In[9]:1 overwritten at In[33]:1.\n"
     ]
    }
   ],
   "source": [
    "macro javascript_str(s) display(\"text/javascript\", s); end\n",
    "\n",
    "javascript\"\"\"\n",
    "$.getScript('https://sites.google.com/site/brodylabhome/files/make_table_of_contents.js')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition convert(Type{Float64}, ForwardDiff.Dual) in module Main at In[2]:11 overwritten at In[10]:11.\n",
      "WARNING: Method definition convert(Array{Float64, N<:Any}, Array{ForwardDiff.Dual, N<:Any}) in module Main at In[2]:13 overwritten at In[10]:13.\n",
      "WARNING: Method definition remove_xtick_labels(Any) in module Main at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/general_utils.jl:10 overwritten at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/general_utils.jl:10.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'remove_xtick_labels :: Tuple{Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition print_vector(Any) in module Main at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/general_utils.jl:40 overwritten at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/general_utils.jl:40.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'print_vector :: Tuple{Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition print_vector_g(Any) in module Main at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/general_utils.jl:67 overwritten at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/general_utils.jl:67.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'print_vector_g :: Tuple{Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition two_level_copy(Any) in module Main at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/general_utils.jl:111 overwritten at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/general_utils.jl:111.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'two_level_copy :: Tuple{Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition vgh(Any, Any) in module Main at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:17 overwritten at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:17.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'vgh :: Tuple{Any,Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition one_d_minimizer(Any, Any) in module Main at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:72 overwritten at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:72.\n",
      "WARNING: Method definition #one_d_minimizer(Array{Any, 1}, Main.#one_d_minimizer, Any, Any) in module Main overwritten.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'one_d_minimizer :: Tuple{Any,Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition constrained_parabolic_minimization(Any, Any, Any) in module Main at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:195 overwritten at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:195.\n",
      "WARNING: Method definition #constrained_parabolic_minimization(Array{Any, 1}, Main.#constrained_parabolic_minimization, Any, Any, Any) in module Main overwritten.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'constrained_parabolic_minimization :: Tuple{Any,Any,Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition constrained_Hessian_minimization(Any, Any) in module Main at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:362 overwritten at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:362.\n",
      "WARNING: Method definition #constrained_Hessian_minimization(Array{Any, 1}, Main.#constrained_Hessian_minimization, Any, Any) in module Main overwritten.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'constrained_Hessian_minimization :: Tuple{Any,Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition adaptive_gradient_minimization(Any, Any) in module Main at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:429 overwritten at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:429.\n",
      "WARNING: Method definition #adaptive_gradient_minimization(Array{Any, 1}, Main.#adaptive_gradient_minimization, Any, Any) in module Main overwritten.\n",
      "WARNING: Method definition trust_region_Hessian_minimization(Any, Any) in module Main at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:534 overwritten at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:534.\n",
      "WARNING: Method definition #trust_region_Hessian_minimization(Array{Any, 1}, Main.#trust_region_Hessian_minimization, Any, Any) in module Main overwritten.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'trust_region_Hessian_minimization :: Tuple{Any,Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition make_dict(Any, Any) in module Main at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:631 overwritten at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:631.\n",
      "WARNING: Method definition make_dict(Any, Any, Any) in module Main at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:631 overwritten at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:631.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'make_dict :: Union{Tuple{Any,Any,Any},Tuple{Any,Any}}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition ForwardDiffZeros(Any, Any) in module Main at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:684 overwritten at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:684.\n",
      "WARNING: Method definition #ForwardDiffZeros(Array{Any, 1}, Main.#ForwardDiffZeros, Any, Any) in module Main overwritten.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'ForwardDiffZeros :: Tuple{Any,Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition keyword_gradient(Any, Any, Any) in module Main at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:742 overwritten at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:742.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'keyword_gradient :: Tuple{Any,Any,Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition keyword_gradient!(Any, Any, Any, Any) in module Main at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:776 overwritten at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:776.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'keyword_gradient! :: Tuple{Any,Any,Any,Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition keyword_hessian(Any, Any, Any) in module Main at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:832 overwritten at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:832.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'keyword_hessian :: Tuple{Any,Any,Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition keyword_hessian!(Any, Any, Any, Any) in module Main at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:870 overwritten at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:870.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'keyword_hessian! :: Tuple{Any,Any,Any,Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition keyword_vgh(Any, Any, Any) in module Main at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:909 overwritten at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:909.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'keyword_vgh :: Tuple{Any,Any,Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition wallwrap(Any, Any) in module Main at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:939 overwritten at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:939.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'wallwrap :: Tuple{Any,Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition vector_wrap(Any, Any, Any) in module Main at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:965 overwritten at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:965.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'vector_wrap :: Tuple{Any,Any,Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition inverse_wall(Any, Any, Any) in module Main at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:989 overwritten at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:989.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'inverse_wall :: Tuple{Any,Any,Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition inverse_wall(Any, Any) in module Main at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:1012 overwritten at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:1012.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'inverse_wall :: Tuple{Any,Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition bbox_Hessian_keyword_minimization(Any, Any, Any, Any) in module Main at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:1114 overwritten at /Users/carlos/Papers/MarinoPagan/ProAnti/Carlos/superior_colliculus_mutual_inhibition/hessian_utils.jl:1114.\n",
      "WARNING: Method definition #bbox_Hessian_keyword_minimization(Array{Any, 1}, Main.#bbox_Hessian_keyword_minimization, Any, Any, Any, Any) in module Main overwritten.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'bbox_Hessian_keyword_minimization :: Tuple{Any,Any,Any,Any}' in module 'Main'.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"We define functions to convert Duals, the variable types used by ForwardDiff, \\nto Floats. This is useful if we want to print out the value of a variable \\n(since print doesn't know how to Duals). Note that after being converted to a Float, no\\ndifferentiation by ForwardDiff can happen!  e.g. after\\n    x = convert(Float64, y)\\nForwardDiff can still differentiate y, but it can't differentiate x\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "using PyCall\n",
    "using PyPlot\n",
    "using ForwardDiff\n",
    "using DiffBase\n",
    "using MAT\n",
    "\n",
    "pygui(true)\n",
    "\n",
    "import Base.convert\n",
    "convert(::Type{Float64}, x::ForwardDiff.Dual) = Float64(x.value)\n",
    "function convert(::Array{Float64}, x::Array{ForwardDiff.Dual}) \n",
    "    y = zeros(size(x)); \n",
    "    for i in 1:prod(size(x)) \n",
    "        y[i] = convert(Float64, x[i]) \n",
    "    end\n",
    "    return y\n",
    "end\n",
    "\n",
    "include(\"general_utils.jl\")\n",
    "include(\"hessian_utils.jl\")\n",
    "\n",
    "\"\"\"\n",
    "We define functions to convert Duals, the variable types used by ForwardDiff, \n",
    "to Floats. This is useful if we want to print out the value of a variable \n",
    "(since print doesn't know how to Duals). Note that after being converted to a Float, no\n",
    "differentiation by ForwardDiff can happen!  e.g. after\n",
    "    x = convert(Float64, y)\n",
    "ForwardDiff can still differentiate y, but it can't differentiate x\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup -- definitions of forwardModel() and backwardsModel()\n",
    "\n",
    "These are functions that run arbitrary 0.5 * (1+tanh(x)) - style rate networks, either forwards in time, or backwards in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition g(Any) in module Main at In[3]:5 overwritten at In[11]:5.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'g :: Tuple{Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition ginverse(Any) in module Main at In[3]:12 overwritten at In[11]:12.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'ginverse :: Tuple{Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition forwardModel(Any) in module Main at In[3]:81 overwritten at In[11]:81.\n",
      "WARNING: Method definition #forwardModel(Array{Any, 1}, Main.#forwardModel, Any) in module Main overwritten.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'forwardModel :: Tuple{Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition backwardsModel(Any) in module Main at In[3]:227 overwritten at In[11]:227.\n",
      "WARNING: Method definition #backwardsModel(Array{Any, 1}, Main.#backwardsModel, Any) in module Main overwritten.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'backwardsModel :: Tuple{Any}' in module 'Main'.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "backwardsModel"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "o = g(z)    squashing tanh function, running from 0 to 1, is equal to 0.5 when input is 0.\n",
    "\"\"\"\n",
    "function g(z)\n",
    "    return 0.5*tanh.(z)+0.5\n",
    "end\n",
    "    \n",
    "\"\"\"\n",
    "z = g^-1(o)    inverse of squashing tanh function, input must be in (0, 1), output is zero when passed 0.5.\n",
    "\"\"\"\n",
    "function ginverse(z)\n",
    "    return 0.5*log.(z./(1-z))\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "forwardModel(startU; dt=0.01, tau=0.1, nsteps=100, input=[0.1, 0], noise=[], W=[0 -5;-5 0], \n",
    "init_add=0, start_add=0, const_add=0, sigma=0, gleak=1, U_rest=0, \n",
    "    do_plot=false, nderivs=0, difforder=0, clearfig=true, fignum=1, dUdt_mag_only=false,\n",
    "    warn_if_unused_params=false)\n",
    "\n",
    "Runs a tanh() style-network forwards in time, given its starting point, using simple Euler integration\n",
    "    tau dU/dt = -U + W*V + I\n",
    "    V = 0.5*tanh(U)+ 0.5\n",
    "\n",
    "**PARAMETERS:**\n",
    "\n",
    "startU     A column vector, nunits-by-1, indicating the values of U at time zero\n",
    "\n",
    "\n",
    "**OPTIONAL PARAMETERS**\n",
    "\n",
    "dt      Scalar, timestep size\n",
    "\n",
    "tau     Scalar, in seconds\n",
    "\n",
    "gleak   \n",
    "        dUdt will have a term equal to gleak*(U_rest - U)\n",
    "U_rest\n",
    "\n",
    "nsteps  Number of timesteps to run, including time=0.\n",
    "\n",
    "input   Either an nunits-by-1 vector, in which case inputs to each unit are constant\n",
    "        across time, or a matrix, nunits-by-nsteps, indicating input for each unit at each timepoint.\n",
    "\n",
    "W       Weight matrix, nunits-by-nunits\n",
    "\n",
    "init_add    DEPRECATED: Vector or scalar that gets added to the input current at very first timestep.\n",
    "            Deprecated because this made it dt-dependent. Replaced by start_add.\n",
    "\n",
    "start_add   Vector or scalar that gets added, once, to the initial U[:,1], before the integration process begins.\n",
    "\n",
    "const_add   Scalar that gets added to U after every timestep\n",
    "\n",
    "sigma       After each timestep, add sigma*sqrt(dt)*randn() to each element of U\n",
    "\n",
    "do_plot   Default false, if true, plots V of up to the first two dimensions\n",
    "\n",
    "fignum     Figure number on which to plot\n",
    "\n",
    "clrearfig  If true, the figure is first cleared, otherwise any plot ois overlaid\n",
    "\n",
    "nderivs, difforder     Required for making sure function can create its own arrays and \n",
    "                       still be differentiated\n",
    "\n",
    "dUdt_mag_only  If true, returns |dUdt|^2 from the first timestep only, then stops.\n",
    "\n",
    "warn_if_unused_params     If true, pronts out a warning of some of the passed parameters are not used.\n",
    "\n",
    "** RETURNS:**\n",
    "\n",
    "Uend Vend       nunits-by-1 vectors representing the final values of U and V that were found.\n",
    "U, V            nunits-by-nsteps matrices containing the full trajectories\n",
    "\n",
    "\"\"\"\n",
    "function forwardModel(startU; dt=0.01, tau=0.1, nsteps=100, input=[], noise=[], W=[0 -5;-5 0], \n",
    "    init_add=0, start_add=0, const_add=0, do_plot=false, nderivs=0, difforder=0, clearfig=true, fignum=1,\n",
    "    dUdt_mag_only=false, sigma=0, g_leak=1, U_rest=0, theta=0, beta=1, \n",
    "    warn_if_unused_params=false, other_unused_params...)\n",
    "\n",
    "    if warn_if_unused_params && length(other_unused_params)>0\n",
    "        @printf(\"\\n\\n=== forwardModel warning, had unused params \")\n",
    "        for k in keys(Dict(other_unused_params))\n",
    "            @printf(\"%s, \", k)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    my_input = ForwardDiffZeros(size(input,1), size(input,2), nderivs=nderivs, difforder=difforder)\n",
    "    for i=1:prod(size(input)); my_input[i] = input[i]; end\n",
    "    input = my_input;\n",
    "    \n",
    "    nunits = length(startU)\n",
    "    if size(startU,2) > size(startU,1)\n",
    "        error(\"startU must be a column vector\")\n",
    "    end\n",
    "    \n",
    "    # --- formatting input ---\n",
    "    if ~(typeof(input)<:Array) || prod(size(input))==1  # was a scalar\n",
    "        input = input[1]*(1+ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    elseif length(input)==0 # was the empty matrix\n",
    "        input = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    elseif size(input,2)==1     # was a column vector\n",
    "        input = input*(1+ForwardDiffZeros(1, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    end    \n",
    "    # --- formatting noise ---\n",
    "    if ~(typeof(noise)<:Array) || prod(size(noise))==1  # was a scalar\n",
    "        noise = noise*(1+ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    elseif length(noise)==0 # was the empty matrix\n",
    "        noise = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    elseif size(noise,2)==1     # was a column vector\n",
    "        noise = noise*(1+ForwardDiffZeros(1, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    end    \n",
    "    \n",
    "    U = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    V = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    \n",
    "    if ~(typeof(W)<:Array); W = [W]; end\n",
    "\n",
    "    W     = reshape(W, nunits, nunits)\n",
    "    U     = reshape(U, nunits, nsteps)\n",
    "    V     = reshape(V, nunits, nsteps)\n",
    "    input = reshape(input, nunits, nsteps)\n",
    "    noise = reshape(noise, nunits, nsteps)\n",
    "\n",
    "    input[:,1] += init_add\n",
    "    input      += const_add\n",
    "\n",
    "    #@printf(\"size(U) is (%d,%d), and size(startU) is (%d,%d) and size(noise) is (%d,%d)\", \n",
    "    #    size(U,1), size(U,2), size(startU,1), size(startU,2), size(noise,1), size(noise,2))\n",
    "    # @printf(\"U[1]=%g, noise[1]=%g\\n\", startU, noise[1])\n",
    "    U[:,1] = startU + noise[:,1] + start_add; # @printf(\"Resulting U=%g\\n\", U[1])\n",
    "    V[:,1] = g((U[:,1]-theta)/beta); # @printf(\"Resulting V=%g\\n\", V[1])\n",
    "    \n",
    "    for i=2:nsteps\n",
    "        dUdt = g_leak*(U_rest -U[:,i-1]) + W*V[:,i-1] + input[:,i-1]\n",
    "        if dUdt_mag_only; return sum(dUdt.*dUdt); end;\n",
    "        # @printf(\"dUdt=%g\\n\", dUdt[1])\n",
    "        # @printf(\"i=%g\\n\", i)\n",
    "        # @printf(\"noise[2]=%g\\n\", noise[2])\n",
    "        U[:,i] = U[:,i-1] + (dt/tau)*dUdt + noise[:,i] + sigma*sqrt(dt)*randn(size(U,1),1)\n",
    "        # @printf(\"Resulting U[2]=%g\\n\", U[2])\n",
    "        V[:,i] = g((U[:,i]-theta)/beta)\n",
    "        # @printf(\"Resulting V[2]=%g\\n\", V[2])\n",
    "    end\n",
    "\n",
    "    if do_plot\n",
    "        figure(fignum)\n",
    "        if length(startU)==1\n",
    "            if clearfig; clf(); end;\n",
    "            t = (0:nsteps-1)*dt\n",
    "            plot(t, V[1,:], \"b-\")\n",
    "            plot(t[1], V[1,1], \"g.\")\n",
    "            plot(t[end], V[1,end], \"r.\")\n",
    "            xlabel(\"t\"); ylabel(\"V1\"); ylim([-0.01, 1.01])\n",
    "        elseif length(startU)>=2\n",
    "            if clearfig; clf(); end;\n",
    "            plot(V[1,:], V[2,:], \"b-\")\n",
    "            plot(V[1,1], V[2,1], \"g.\")\n",
    "            plot(V[1,end], V[2,end], \"r.\")\n",
    "            xlabel(\"V1\"); ylabel(\"V2\"); \n",
    "            xlim([-0.01, 1.01]); ylim([-0.01, 1.01])\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return U[:,end], V[:,end], U, V\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "backwardsModel(endU; dt=0.01, tau=0.1, nsteps=100, input=[0],noise=[],  W=[0 -5;-5 0], \n",
    "    do_plot=false, nderivs=0, difforder=0, clearfig=true, fignum=1, tol=1e-15, start_eta=10)\n",
    "\n",
    "Runs a tanh() style-network BACKWARDS in time, given its ending point, by making a backwards\n",
    "guess at each timepoint and then using Hessian minimization to find the backwards vector that correctly\n",
    "leads to the current timestep value.  Uses forwardModel() . The forwards equations are:\n",
    "\n",
    "    tau dU/dt = -U + W*V + I\n",
    "    V = 0.5*tanh(U)+ 0.5\n",
    "\n",
    "**PARAMETERS:**\n",
    "\n",
    "endU     A column vector, nunits-by-1, indicating the values of U at time=end\n",
    "\n",
    "\n",
    "**OPTIONAL PARAMETERS:**\n",
    "\n",
    "dt      Scalar, timestep size\n",
    "\n",
    "tau     Scalar, in seconds\n",
    "\n",
    "nsteps  Number of timesteps to run, including time=0.\n",
    "\n",
    "input   Either an nunits-by-1 vector, in which case inputs to each unit are constant\n",
    "        across time, or a matrix, nunits-by-nsteps, indicating input for each unit at each timepoint.\n",
    "\n",
    "W       Weight matrix, nunits-by-nunits\n",
    "\n",
    "do_plot   Default false, if true, plots V of up to the first two dimensions\n",
    "\n",
    "tol       Tolerance in the minimization procedure for finding each backwards timestep. Passed on\n",
    "          to trust_region_Hessian_minimization()\n",
    "\n",
    "start_eta   Passed on to trust_region_Hessian_minimization()\n",
    "\n",
    "fignum     Figure number on which to plot\n",
    "\n",
    "clrearfig  If true, the figure is first cleared, otherwise any plot ois overlaid\n",
    "\n",
    "nderivs, difforder     Required for making sure function can create its own arrays and \n",
    "                       still be differentiated\n",
    "\n",
    "\n",
    "\n",
    "** RETURNS:**\n",
    "\n",
    "Ustart Vstart   nunits-by-1 vectors representing the starting values of U and V that were found.\n",
    "U, V            nunits-by-nsteps matrices containing the full trajectories\n",
    "costs           1-by-nsteps vector with the final cost from the minimization procedure for each\n",
    "                timestep. This is the squared difference between the U[t+1] produced by the U[t] \n",
    "                guess and the actual U[t+1]\n",
    "\n",
    "\"\"\"\n",
    "function backwardsModel(endU; nsteps=100, start_eta=10, tol=1e-15, maxiter=400, \n",
    "    do_plot=false, init_add=0, start_add=0, dt=0.01, \n",
    "    input=[], noise=[], nderivs=0, difforder=0, clearfig=false, fignum=1, params...)    \n",
    "\n",
    "    nunits = length(endU)\n",
    "\n",
    "    # --- formatting input ---\n",
    "    if ~(typeof(input)<:Array) || prod(size(input))==1  # was a scalar\n",
    "        input = input[1]*(1+ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    elseif length(input)==0 # was the empty matrix\n",
    "        input = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    elseif size(input,2)==1     # was a column vector\n",
    "        input = input*(1+ForwardDiffZeros(1, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    end    \n",
    "    # --- formatting noise ---\n",
    "    if ~(typeof(noise)<:Array)  # was a scalar\n",
    "        noise = noise*(1+ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    elseif length(noise)==0 # was the empty matrix\n",
    "        noise = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    elseif size(noise,2)==1     # was a column vector\n",
    "        noise = noise*(1+ForwardDiffZeros(1, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    end    \n",
    "    \n",
    "    function J(U1, U2; nderivs=0, difforder=0, noise=[], inputs=[], pars...)\n",
    "        U2hat = forwardModel(U1; nsteps=2, noise=noise, input=input, nderivs=nderivs, difforder=difforder, pars...)[1]\n",
    "        U2hat = U2hat\n",
    "        DU = U2hat - U2\n",
    "    \n",
    "        return sum(DU.*DU)\n",
    "    end\n",
    "    \n",
    "    if length(noise)==0\n",
    "        noise = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    end\n",
    "\n",
    "    U = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    U = reshape(U, nunits, nsteps)\n",
    "    costs = ForwardDiffZeros(nsteps, 1, nderivs=nderivs, difforder=difforder)    \n",
    "    \n",
    "    U[:,end] = endU\n",
    "    for i=(nsteps-1):-1:1\n",
    "        if i==1\n",
    "            my_init_add = init_add\n",
    "            my_start_add = start_add\n",
    "        else\n",
    "            my_init_add = 0\n",
    "            my_start_add = 0\n",
    "        end\n",
    "                \n",
    "        U[:,i], costs[i] = trust_region_Hessian_minimization(U[:,i+1], \n",
    "            (x) -> J(x, U[:,i+1]; nderivs=length(endU), difforder=2, \n",
    "            input=input[:,i:i+1], noise = noise[:,i:i+1], \n",
    "            init_add=my_init_add, start_add=my_start_add, params...); \n",
    "            verbose=false, start_eta=start_eta, tol=tol, maxiter=maxiter)\n",
    "        if i>1; U[:,i] += noise[:,i]; end\n",
    "    end\n",
    "    \n",
    "    \n",
    "    V = g(U)\n",
    "    \n",
    "    if do_plot\n",
    "        figure(fignum)   \n",
    "        if typeof(params)<:Array; params = Dict(params); end;\n",
    "        if haskey(params, :dt);     dt     = params[:dt];     end\n",
    "        if haskey(params, :nsteps); nsteps = params[:nsteps]; end\n",
    "        if length(endU)==1\n",
    "            if clearfig; clf(); end;\n",
    "            t = (0:nsteps-1)*dt\n",
    "            plot(t, V[1,:], \"m-\")\n",
    "            plot(t[1], V[1,1], \"go\")\n",
    "            plot(t[end], V[1,end], \"ro\")            \n",
    "            ylim([-0.01, 1.01])\n",
    "        elseif length(endU)>=2\n",
    "            if clearfig; clf(); end;            \n",
    "            plot(V[1,:], V[2,:], \"m-\")\n",
    "            plot(V[1,1], V[2,1], \"go\")\n",
    "            plot(V[1,end], V[2,end], \"ro\")\n",
    "            xlim([-0.01, 1.01]); ylim([-0.01, 1.01])\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return U[:,1], V[:,1], U, V, costs\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing forward and backwards models with only 1 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ustart came back as 1.1\n"
     ]
    }
   ],
   "source": [
    "figure(1); clf();\n",
    "params = Dict(:noise => [0.1], :W => [-2], :nsteps=>10, :start_add=>-1.9)\n",
    "Uend = forwardModel([1.1]; do_plot=true, params...)[1]\n",
    "Ustart = backwardsModel(Uend; do_plot=true, tol=1e-30, params...)[1]\n",
    "@printf(\"Ustart came back as %g\\n\", Ustart[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing forward and backwards models now with 2 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ustart came back as : [0.1, 0.1]\n"
     ]
    }
   ],
   "source": [
    "nsteps=50\n",
    "params = Dict(:noise =>0.03*randn(2,nsteps) + [0.1,0]*ones(1,nsteps), :W => [0 -5; -5 0], :nsteps=>nsteps)\n",
    "\n",
    "Uend, Vend, U, V              = forwardModel([0.1,0.1]; do_plot=true, params...);\n",
    "Ustart, Vstart, bU, bV, costs = backwardsModel(Uend; do_plot=true, tol=1e-30, params...)\n",
    "\n",
    "@printf(\"Ustart came back as : \"); print_vector_g(Ustart); print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring dt-dependence of gradients and hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×5 Array{Float64,2}:\n",
       " 0.919879  -1.0215   1.0337    -1.04592   -0.270006\n",
       " 0.915848  -1.00756  0.968136  -0.992003   0.571474"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# srand(111)\n",
    "startU=randn(100,2)-3\n",
    "startU=randn(100,2)-3\n",
    "sigma = 0\n",
    "\n",
    "# startU=0.1*randn(100,2)-3\n",
    "# startU=zeros(100,2)-3\n",
    "\n",
    "\n",
    "dt = 0.02\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "W = -4\n",
    "noise = 0\n",
    "input = 0\n",
    "sigma = 0\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    ":noise=>noise, :input=>input, :sigma=>sigma, :const_add=>0, :init_add=>0)\n",
    "\n",
    "forward = (startU; pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;     \n",
    "    forwardModel(startU; pars...)\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "# clf();\n",
    "# func = (;pars...) -> forward([-0.2, 0.3]; do_plot=true, merge(model_params, Dict(pars))...)\n",
    "# func(;W=-4)\n",
    "\n",
    "args = [\"W\", \"const_add\", [\"start_add\" 2], \"sigma\"]\n",
    "params = [-4.01, 0.5, 0.2, -0.2, 0.01]\n",
    "\n",
    "figure(1); clf();\n",
    "value1, grad1, hess1 = keyword_vgh((;pars...)->forward([-0.2, 0.3]; do_plot=true, merge(model_params, Dict(pars))...)[1][1], args, params)\n",
    "title(@sprintf(\"Running with dt=%g\", dt))\n",
    "\n",
    "dt = 0.005\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    ":noise=>noise, :input=>input, :sigma=>sigma, :const_add=>0, :init_add=>0)\n",
    "\n",
    "figure(2); clf();\n",
    "value2, grad2, hess2 = keyword_vgh((;pars...)->forward([-0.2, 0.3]; do_plot=true, fignum=2, merge(model_params, Dict(pars))...)[1][1], args, params)\n",
    "title(@sprintf(\"Running with dt=%g\", dt))\n",
    "\n",
    "[grad1[:]' ; grad2[:]']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO-DOs\n",
    "\n",
    "\n",
    "1. ===\n",
    "2. ~~Optimize either an MGO or a ProAnti~~ DONE with MGO. Now on to ProAnti\n",
    "3. ~~Set up so we can easily change task period durations in JJ as we run the model to evaluate the results of model-fitting~~ DONE\n",
    "4. Have different task period durations while model-fitting\n",
    "5. Set up to do searches over parameter space\n",
    "6. Incorporate RT into fits?\n",
    "10. If fluxSense is needed in ProAnti, could try choosing the Anti unit endpoint values by maximizing the |dJ/dw|^2 over those values.\n",
    "11. Clean up the notebooks and write up what we've been doing!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProAnti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition plot_PA(Any, Any, Any) in module Main at In[43]:9 overwritten at In[44]:9.\n",
      "WARNING: Method definition #plot_PA(Array{Any, 1}, Main.#plot_PA, Any, Any, Any) in module Main overwritten.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'plot_PA :: Tuple{Any,Any,Any}' in module 'Main'.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "plot_PA"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    plot_PA(t, U, V; fignum=1, clearfig=true, rule_and_delay_period=1, target_period=1, post_target_period=1,\n",
    "        other_unused_params...)\n",
    "\n",
    "Helper function for plotting ProAnti results\n",
    "\"\"\"\n",
    "function plot_PA(t, U, V; fignum=1, clearfig=true, rule_and_delay_period=1, target_period=1, post_target_period=1,\n",
    "    other_unused_params...)\n",
    "    figure(fignum)\n",
    "    if clearfig; clf(); end\n",
    "    \n",
    "    ax1 = subplot(3,1,1)\n",
    "    h = plot(t, V'); \n",
    "    setp(h[1], color=[0, 0, 1])\n",
    "    setp(h[2], color=[1, 0, 0])\n",
    "    setp(h[3], color=[1, 0.5, 0.5])\n",
    "    setp(h[4], color=[0, 1, 1])\n",
    "    ylabel(\"V\")\n",
    "\n",
    "    ax = gca()\n",
    "    oldlims = [ylim()[1]+0.1, ylim()[2]-0.1]\n",
    "    ylim(minimum([V[:];oldlims[1]])-0.1, maximum([V[:];oldlims[2]])+0.1)\n",
    "    yl = [ylim()[1], ylim()[2]]\n",
    "    vlines([rule_and_delay_period, \n",
    "            rule_and_delay_period+target_period,\n",
    "            rule_and_delay_period+target_period+post_target_period], \n",
    "            -0.05, 1.05, linewidth=2)\n",
    "    if yl[1]<0.02\n",
    "        yl[1] = -0.02\n",
    "    end\n",
    "    if yl[2]>0.98\n",
    "        yl[2] = 1.02\n",
    "    end\n",
    "    ylim(yl)\n",
    "    grid(true)\n",
    "    remove_xtick_labels(ax1)\n",
    "        \n",
    "    ax2 = subplot(3,1,2)\n",
    "    hu = plot(t, U')\n",
    "    oldlims = [ylim()[1]+0.1, ylim()[2]-0.1]\n",
    "    ylim(minimum([U[:];oldlims[1]])-0.1, maximum([U[:];oldlims[2]])+0.1)\n",
    "    setp(hu[1], color=[0, 0, 1])\n",
    "    setp(hu[2], color=[1, 0, 0])\n",
    "    setp(hu[3], color=[1, 0.5, 0.5])\n",
    "    setp(hu[4], color=[0, 1, 1])\n",
    "    ylabel(\"U\"); \n",
    "    vlines([rule_and_delay_period, \n",
    "            rule_and_delay_period+target_period,\n",
    "            rule_and_delay_period+target_period+post_target_period], \n",
    "            ylim()[1], ylim()[2], linewidth=2)\n",
    "    remove_xtick_labels(ax2)\n",
    "\n",
    "    grid(true)\n",
    "    \n",
    "    subplot(3,1,3)\n",
    "    delta = V[1,:] - V[4,:]\n",
    "    hr = plot(t, delta)\n",
    "    oldlims = [ylim()[1]+0.1, ylim()[2]-0.1]\n",
    "    ylim(minimum([delta[:];oldlims[1]])-0.1, maximum([delta[:];oldlims[2]])+0.1)\n",
    "    vlines([rule_and_delay_period, \n",
    "            rule_and_delay_period+target_period,\n",
    "            rule_and_delay_period+target_period+post_target_period], \n",
    "            ylim()[1], ylim()[2], linewidth=2)\n",
    "    xlabel(\"t\"); ylabel(\"Pro R - Pro L\")\n",
    "    grid(true)\n",
    "        \n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.346095 seconds (167.06 k allocations: 7.927 MB, 1.17% gc time)\n",
      "Pro % correct = 100%\n",
      "Anti % correct = "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition make_input(Any) in module Main at In[16]:39 overwritten at In[89]:39.\n",
      "WARNING: Method definition #make_input(Array{Any, 1}, Main.#make_input, Any) in module Main overwritten.\n",
      "WARNING: Method definition run_ntrials(Any, Any) in module Main at In[16]:63 overwritten at In[89]:63.\n",
      "WARNING: Method definition #run_ntrials(Array{Any, 1}, Main.#run_ntrials, Any, Any) in module Main overwritten.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80% \n"
     ]
    }
   ],
   "source": [
    "model_params = Dict(\n",
    ":dt     =>  0.02, \n",
    ":tau    =>  0.1, \n",
    ":vW     =>  -1.7,\n",
    ":hW     =>  -1.7,\n",
    ":sW     =>  0.2,\n",
    ":dW     =>  0,\n",
    ":nsteps =>  2, \n",
    ":noise  =>  [], \n",
    ":sigma  =>  0.08, \n",
    ":input  =>  0, \n",
    ":g_leak =>  0.25, \n",
    ":U_rest =>  -1,\n",
    ":theta  =>  1, \n",
    ":beta   =>  1, \n",
    ":sw     =>  0.2,\n",
    ":hw     =>  -1.7,\n",
    ":vw     =>  -1.7,\n",
    ":constant_excitation      => 0.19, \n",
    ":anti_rule_strength       => 0.1,\n",
    ":pro_rule_strength        => 0.1, \n",
    ":target_period_excitation => 1,\n",
    ":right_light_excitation   => 0.5, \n",
    ":right_light_pro_extra    => 0,\n",
    ":const_add => 0, \n",
    ":init_add  => 0, \n",
    ":rule_and_delay_period    => 0.4,\n",
    ":target_period            => 0.1,\n",
    ":post_target_period       => 0.5,\n",
    ":const_pro_bias           => 0,\n",
    ")\n",
    "\n",
    "\n",
    "function make_input(trial_type; dt=0.02, nderivs=0, difforder=0, constant_excitation=0.19, anti_rule_strength=0.1, \n",
    "    pro_rule_strength=0.1, target_period_excitation=1, right_light_excitation=0.5, right_light_pro_extra=0, \n",
    "    rule_and_delay_period=0.4, target_period=0.1, post_target_period=0.4, const_pro_bias=0,\n",
    "    other_unused_params...)\n",
    "\n",
    "    T = rule_and_delay_period + target_period + post_target_period\n",
    "    t = 0:dt:T\n",
    "    nsteps = length(t)\n",
    "\n",
    "    input = constant_excitation + ForwardDiffZeros(4, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    if trial_type==\"Anti\"\n",
    "        input[2:3, t.<rule_and_delay_period] += anti_rule_strength\n",
    "    elseif trial_type==\"Pro\"\n",
    "        input[[1,4], t.<rule_and_delay_period] += pro_rule_strength\n",
    "    else\n",
    "        error(\"make_input: I don't recognize input type \\\"\" * trial_type * \"\\\"\")\n",
    "    end\n",
    "    \n",
    "    input[:,     (rule_and_delay_period.<=t) & (t.<rule_and_delay_period+target_period)] += target_period_excitation\n",
    "    input[1:2,   (rule_and_delay_period.<=t) & (t.<rule_and_delay_period+target_period)] += right_light_excitation\n",
    "    input[1,     (rule_and_delay_period.<=t) & (t.<rule_and_delay_period+target_period)] += right_light_pro_extra\n",
    "    \n",
    "    input[[1,4],:] += const_pro_bias\n",
    "    \n",
    "    return input, t, nsteps\n",
    "end\n",
    "\n",
    "\n",
    "function run_ntrials(nPro, nAnti; plot_list=[], nderivs=0, difforder=0, model_params...)\n",
    "    pro_input,  t, nsteps = make_input(\"Pro\" ; model_params...)\n",
    "    anti_input, t, nsteps = make_input(\"Anti\"; model_params...)\n",
    "\n",
    "    model_params = Dict(model_params)\n",
    "    sW = model_params[:sW]\n",
    "    hW = model_params[:hW]\n",
    "    vW = model_params[:vW]\n",
    "    dW = model_params[:dW]\n",
    "    model_params = make_dict([\"nsteps\", \"W\"], [nsteps, [sW vW dW hW; vW sW hW dW; dW hW sW vW; hW dW vW sW]], \n",
    "        model_params)\n",
    "    model_params = make_dict([\"nderivs\", \"difforder\"], [nderivs, difforder], model_params)\n",
    "    \n",
    "    proVs  = ForwardDiffZeros(4, nPro, nderivs=nderivs, difforder=difforder)\n",
    "    antiVs = ForwardDiffZeros(4, nAnti, nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    # --- PRO ---\n",
    "    if length(plot_list)>0; figure(1); clf(); end\n",
    "    model_params = make_dict([\"input\"], [pro_input], model_params)\n",
    "    for i=1:nPro\n",
    "        startU = [-0.3, -0.7, -0.7, -0.3]\n",
    "        Uend, Vend, U, V = forwardModel(startU, do_plot=false; model_params...)\n",
    "        proVs[:,i] = Vend\n",
    "        if any(plot_list.==i) \n",
    "            plot_PA(t, U, V; fignum=1, clearfig=false, model_params...)\n",
    "            subplot(3,1,1); title(\"PRO\")\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # --- ANTI ---\n",
    "    if length(plot_list)>0; figure(2); clf(); end\n",
    "    model_params = make_dict([\"input\"], [anti_input], model_params)\n",
    "    for i=1:nAnti\n",
    "        startU = [-0.7, -0.3, -0.3, -0.7]\n",
    "        Uend, Vend, U, V = forwardModel(startU, do_plot=false; model_params...)\n",
    "        antiVs[:,i] = Vend\n",
    "        if any(plot_list.==i) \n",
    "            plot_PA(t, U, V; fignum=2, clearfig=false, model_params...)\n",
    "            subplot(3,1,1); title(\"ANTI\")\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return proVs, antiVs\n",
    "end\n",
    "\n",
    "nPro = 10; nAnti = 5;\n",
    "proVs, antiVs = @time(run_ntrials(nPro, nAnti; plot_list=[1:5;], model_params...))\n",
    "\n",
    "@printf(\"Pro %% correct = %g%%\\n\", 100*length(find(proVs[1,:].>proVs[4,:]))/nPro)\n",
    "@printf(\"Anti %% correct = %g%% \\n\", 100*length(find(antiVs[1,:].<antiVs[4,:]))/nAnti)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     -- cost=-0.000412731,   cost1=0.000445974, cost2=-0.000858706\n",
      "     -- mean(hitsP)=0.854628, mean(diffsP)=0.314139 mean(hitsA)=0.68889, mean(diffsA)=0.280654\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.0004127313597031488"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function JJ(nPro, nAnti; pro_target=0.9, anti_target=0.7, \n",
    "    theta1=0.025, theta2=0.035, cbeta=0.003, verbose=false, \n",
    "    pre_string=\"\", zero_last_sigmas=0, seedrand=NaN, \n",
    "    rule_and_delay_periods = [0.4], target_periods = [0.1], post_target_periods = [0.5],\n",
    "    nderivs=0, difforder=0, model_params...)\n",
    "\n",
    "    nruns = length(rule_and_delay_periods)*length(target_periods)*length(post_target_periods)\n",
    "    \n",
    "    cost1s = ForwardDiffZeros(1, nruns, nderivs=nderivs, difforder=difforder)\n",
    "    cost2s = ForwardDiffZeros(1, nruns, nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if ~isnan(seedrand); srand(seedrand); end\n",
    "    \n",
    "    n = totHitsP = totHitsA = totDiffsP = totDiffsA = 0\n",
    "    for i in rule_and_delay_periods\n",
    "        for j in target_periods\n",
    "            for k = post_target_periods\n",
    "                n += 1\n",
    "                \n",
    "                my_params = make_dict([\"rule_and_delay_period\", \"target_period\", \"post_target_period\"],\n",
    "                [i, j, k], Dict(model_params))\n",
    "    \n",
    "                # print(\"model params is \" ); print(model_params); print(\"\\n\")\n",
    "                proVs, antiVs = run_ntrials(nPro, nAnti; nderivs=nderivs, difforder=difforder, my_params...)\n",
    "\n",
    "                hitsP  = 0.5*(1 + tanh.((proVs[1,:]-proVs[4,:,])/theta1))\n",
    "                diffsP = tanh.((proVs[1,:,]-proVs[4,:])/theta2).^2\n",
    "                hitsA  = 0.5*(1 + tanh.((antiVs[4,:]-antiVs[1,:,])/theta1))\n",
    "                diffsA = tanh.((antiVs[4,:,]-antiVs[1,:])/theta2).^2\n",
    "\n",
    "                if nPro>0 && nAnti>0\n",
    "                    cost1s[n] = (nPro*(mean(hitsP) - pro_target).^2  + nAnti*(mean(hitsA) - anti_target).^2)/(nPro+nAnti)\n",
    "                    cost2s[n] = -cbeta*(nPro*mean(diffsP) + nAnti*mean(diffsA))/(nPro+nAnti)\n",
    "                elseif nPro>0\n",
    "                    cost1s[n] = (mean(hitsP) - pro_target).^2\n",
    "                    cost2s[n] = -cbeta*mean(diffsP)\n",
    "                else\n",
    "                    cost1s[n] = (mean(hitsA) - anti_target).^2\n",
    "                    cost2s[n] = -cbeta*mean(diffsA)\n",
    "                end\n",
    "\n",
    "                totHitsP  += mean(hitsP);  totHitsA  += mean(hitsA); \n",
    "                totDiffsP += mean(diffsP); totDiffsA += mean(diffsA);\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    cost1 = mean(cost1s)\n",
    "    cost2 = mean(cost2s)\n",
    "\n",
    "    hitsP = totHitsP/n; hitsA = totHitsA/n; diffsP = totDiffsP/n; diffsA = totDiffsA/n\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"%s\", pre_string)\n",
    "        @printf(\"     -- cost=%g,   cost1=%g, cost2=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2))\n",
    "        if nPro>0 && nAnti>0\n",
    "            @printf(\"     -- mean(hitsP)=%g, mean(diffsP)=%g mean(hitsA)=%g, mean(diffsA)=%g\\n\", \n",
    "                convert(Float64, mean(hitsP)), convert(Float64, mean(diffsP)),\n",
    "                convert(Float64, mean(hitsA)), convert(Float64, mean(diffsA)))\n",
    "        elseif nPro>0\n",
    "            @printf(\"     -- mean(hitsP)=%g, mean(diffsP)=%g (nAnti=0)\\n\", \n",
    "                convert(Float64, mean(hitsP)), convert(Float64, mean(diffsP)))\n",
    "        else\n",
    "            @printf(\"     -- (nPro=0) mean(hitsA)=%g, mean(diffsA)=%g\\n\", \n",
    "                convert(Float64, mean(hitsA)), convert(Float64, mean(diffsA)))\n",
    "        end        \n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "JJ(2, 10; plot_list=1:5, verbose=true, model_params...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of taking the gradient and Hessian of the cost function JJ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.07535583378383748,[0.0570604,-0.00251589,-0.0368942,0.288332,-0.48954,0.0181813,0.00435835],\n",
       "[0.283457 -0.086324 … 0.00977107 1.02778; -0.086324 0.00153948 … -0.000470085 -0.191414; … ; 0.00977107 -0.000470085 … 0.00702631 0.108539; 1.02778 -0.191414 … 0.108539 3.61222])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = [\"sW\", \"vW\", \"hW\", \"constant_excitation\", \"right_light_excitation\", \"target_period_excitation\", \"const_pro_bias\"]\n",
    "seed = [0.2,   -1.7, -1.7,      0.39,                0.15,                       0.1,                       0.1]\n",
    "\n",
    "func = (;params...) -> JJ(100, 0; rule_and_delay_periods = [0.4, 0.8], seedrand=30, cbeta=0.01, \n",
    "plot_list = 1:5, verbose=false, merge(model_params, Dict(params))...)\n",
    "\n",
    "cost, grad, hess = keyword_vgh(func, args, seed)\n",
    "\n",
    "# func(;make_dict(args, seed+ [1,0.2,0,0,0,0,0])...) - func(;make_dict(args, seed)...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sandbox\n",
    "\n",
    "A place to play around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Symbol,Any} with 9 entries:\n",
       "  :constant_excitation      => [-2 2]\n",
       "  :vW                       => [-3 3]\n",
       "  :target_period_excitation => [0.05 4.0]\n",
       "  :const_pro_bias           => [-2 2]\n",
       "  :right_light_excitation   => [0.05 4.0]\n",
       "  :hW                       => [-3 3]\n",
       "  :dW                       => [-3 3]\n",
       "  :sigma                    => [0.01 0.2]\n",
       "  :sW                       => [0 3]"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# ======= ARGUMENTS AND SEED VALUES:\n",
    "args = [\"sW\", \"vW\", \"hW\", \"dW\", \"constant_excitation\", \"right_light_excitation\", \"target_period_excitation\"]\n",
    "seed = [0.2,   1,   0.2,  1,    0.39,                0.15,                       0.1]\n",
    "\n",
    "args = [args ; [\"const_pro_bias\", \"sigma\"]]\n",
    "seed = [seed ; [0.1,               0.1]]\n",
    "\n",
    "\n",
    "# ======= BOUNDING BOX:\n",
    "bbox = Dict(:sW=>[0 3], :vW=>[-3 3], :hW=>[-3 3], :dW=>[-3 3], :constant_excitation=>[-2 2],\n",
    ":right_light_excitation=>[0.05 4], :target_period_excitation=>[0.05 4], :const_pro_bias=>[-2 2],\n",
    ":sigma=>[0.01 0.2])\n",
    "\n",
    "model_params = merge(model_params, Dict(:post_target_period=>0.5))\n",
    "# seed = [0.0840597,  -1.32677,  -0.437334,  -0.324835,  0.567997, 0.712216,  0.0500075,  0.0858569,  0.25]\n",
    "\n",
    "\n",
    "# ======== SEARCH ZONE:\n",
    "\n",
    "sbox = Dict(:sW=>[0.001 0.5], :vW=>[-0.5 0.5], :hW=>[-0.5 0.5], :dW=>[-0.5 0.5],\n",
    ":constant_excitation=>[-0.5 0.5], :right_light_excitation=>[0.1 0.5], :target_period_excitation=>[0.1 0.5],\n",
    ":const_pro_bias=>[0 0.2], :sigma=>[0.02 0.19])\n",
    "\n",
    "cbetas = [0.002, 0.01]\n",
    "\n",
    "basename = \"farm_TEST_\"\n",
    "\n",
    "while true\n",
    "    myseed = seed;\n",
    "    sr = convert(Int64, round(time()))\n",
    "\n",
    "    myseed = copy(seed);\n",
    "    for i=1:length(args)\n",
    "        sym = Symbol(args[i])\n",
    "        if haskey(sbox, sym)\n",
    "            myseed[i] = sbox[sym][1] + diff(sbox[sym],2)[1]*rand()\n",
    "        end\n",
    "    end\n",
    "    nPro=100; nAnti=100\n",
    "\n",
    "    rule_and_delay_periods = [0.4, 1.2]\n",
    "    post_target_periods    = [0.5, 1.5]\n",
    "\n",
    "    for cb in cbetas\n",
    "        @printf(\"Going with seed = \"); print_vector_g(myseed); print(\"\\n\")\n",
    "        pars, traj, cost, cpm_traj = bbox_Hessian_keyword_minimization(myseed, args, bbox, \n",
    "            (;params...) -> JJ(nPro, nAnti; rule_and_delay_periods=rule_and_delay_periods,\n",
    "            post_target_periods=post_target_periods,\n",
    "            seedrand=sr, cbeta=cb, verbose=false, merge(model_params, Dict(params))...),\n",
    "            start_eta = 0.01, tol=1e-12, verbose=true, maxiter=2)\n",
    "        @printf(\"Came out with cost %g and pars = \", cost); print_vector_g(pars); print(\"\\n\\n\")\n",
    "\n",
    "        matwrite(next_file(basename, 4), Dict(\"args\"=>args, \"myseed\"=>myseed, \"pars\"=>pars, \"traj\"=>traj,\n",
    "        \"cost\"=>cost, \"cpm_traj\"=>cpm_traj, \"nPro\"=>nPro, \"nAnti\"=>nAnti, \"sr\"=>sr, \"cb\"=>cb, \n",
    "        \"model_params\"=>ascii_key_ize(model_params), \"bbox\"=>ascii_key_ize(bbox), \"sbox\"=>ascii_key_ize(sbox)))\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition standard_cost(Any, Any, Any, Any) in module Main at In[311]:7 overwritten at In[313]:7.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'standard_cost :: Tuple{Any,Any,Any,Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition standard_cost(Any) in module Main at In[311]:27 overwritten at In[313]:27.\n",
      "WARNING: Method definition #standard_cost(Array{Any, 1}, Main.#standard_cost, Any) in module Main overwritten.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'standard_cost :: Tuple{Any}' in module 'Main'.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "standard_cost"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "cost = standard_cost(args, pars, sr, model_params)\n",
    "\n",
    "Computes the cost as if cb had been 0.01\n",
    "\"\"\"\n",
    "function standard_cost(args, pars, sr, model_params)\n",
    "    cb = 0.01\n",
    "    nPro=100; nAnti=100\n",
    "\n",
    "    rule_and_delay_periods = [0.4, 1.2]\n",
    "    post_target_periods    = [0.5, 1.5]\n",
    "\n",
    "    func = (;params...) -> JJ(nPro, nAnti; rule_and_delay_periods=rule_and_delay_periods,\n",
    "            post_target_periods=post_target_periods,\n",
    "            seedrand=sr, cbeta=cb, verbose=false, merge(model_params, Dict(params))...)\n",
    "    \n",
    "    return func(;make_dict(args, pars)...)\n",
    "end\n",
    "    \n",
    "\"\"\"\n",
    "cost = standard_cost(filename)\n",
    "\n",
    "Returns the standard cost (at cb=0.01) and inserts it into the file with key \"scost\" if it wasn't there\n",
    "already\n",
    "\"\"\"\n",
    "function standard_cost(filename; verbose=false)\n",
    "    A = matread(filename)\n",
    "    if !haskey(A, \"scost\")\n",
    "        get!(A, \"scost\", standard_cost(A[\"args\"], A[\"pars\"], A[\"sr\"], symbol_key_ize(A[\"model_params\"])))\n",
    "        if verbose\n",
    "            @printf(\"File %s did not have scost, adding its value %g\\n\", filename, A[\"scost\"])\n",
    "        end\n",
    "        matwrite(filename, A)\n",
    "    end\n",
    "    return A[\"scost\"]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.007350536801978829"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_cost(\"farm_C_0104\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String,Any} with 14 entries:\n",
       "  \"traj\"         => [0.01 0.011 … 0.0240117 0.0264129; 0.0916161 0.0906175 … -0…\n",
       "  \"scost\"        => -0.00634099\n",
       "  \"cpm_traj\"     => [56.0 5.0 … 101.0 97.0; 0.0 0.0 … 0.0 0.0]\n",
       "  \"model_params\" => Dict{String,Any}(Pair{String,Any}(\"const_pro_bias\",0),Pair{…\n",
       "  \"sr\"           => 1505150372\n",
       "  \"args\"         => Any[\"sW\",\"vW\",\"hW\",\"dW\",\"constant_excitation\",\"right_light_…\n",
       "  \"nAnti\"        => 100\n",
       "  \"nPro\"         => 100\n",
       "  \"bbox\"         => Dict{String,Any}(Pair{String,Any}(\"sigma\",[0.01 0.2]),Pair{…\n",
       "  \"cost\"         => -0.00118598\n",
       "  \"myseed\"       => [0.287614,0.0213833,-0.464603,0.305447,-0.0605855,0.470735,…\n",
       "  \"pars\"         => [0.0342661,-0.927829,0.70743,1.32046,0.0516115,0.441745,0.4…\n",
       "  \"cb\"           => 0.002\n",
       "  \"sbox\"         => Dict{String,Any}(Pair{String,Any}(\"sigma\",[0.02 0.19]),Pair…"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At cb=0.002, cost was -0.00118598. At cb=0.01, cost was -0.00634099\n"
     ]
    }
   ],
   "source": [
    "A = matread(\"farm_A_0001\")\n",
    "@printf(\"At cb=0.002, cost was %g. At cb=0.01, cost was %g\\n\", A[\"cost\"], standard_cost(\"farm_A_0001\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost went from 0.0846739 to -0.0160859 ; max iters on cpm was 13\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dict{String,Any} with 18 entries:\n",
       "  \"traj\"                   => [0.01 0.011 … 9.19729e-6 4.59865e-6; 0.0846739 0.…\n",
       "  \"scost\"                  => -0.00853743\n",
       "  \"cpm_traj\"               => [14.0 10.0 … 89.0 83.0; 0.0 0.0 … 0.0 0.0]\n",
       "  \"post_target_periods\"    => [0.5,1.5]\n",
       "  \"rule_and_delay_periods\" => [0.4,1.2]\n",
       "  \"model_params\"           => Dict{String,Any}(Pair{String,Any}(\"const_pro_bias…\n",
       "  \"sr\"                     => 1505318661\n",
       "  \"args\"                   => Any[\"sW\",\"vW\",\"hW\",\"dW\",\"constant_excitation\",\"ri…\n",
       "  \"nAnti\"                  => 100\n",
       "  \"theta2\"                 => 0.25\n",
       "  \"nPro\"                   => 100\n",
       "  \"theta1\"                 => 0.15\n",
       "  \"bbox\"                   => Dict{String,Any}(Pair{String,Any}(\"sigma\",[0.01 0…\n",
       "  \"cost\"                   => -0.0160859\n",
       "  \"myseed\"                 => [0.253053,-0.476459,-0.0127227,-0.322372,0.015171…\n",
       "  \"pars\"                   => [0.121312,-2.26178,-0.222131,0.472211,0.401695,3.…\n",
       "  \"cb\"                     => 0.02\n",
       "  \"sbox\"                   => Dict{String,Any}(Pair{String,Any}(\"sigma\",[0.02 0…"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = matread(\"farm_E_0005\")\n",
    "model_params = symbol_key_ize(A[\"model_params\"])\n",
    "args         = A[\"args\"];\n",
    "nAnti        = A[\"nAnti\"]\n",
    "nPro         = A[\"nPro\"]\n",
    "pars         = A[\"pars\"]\n",
    "sr           = A[\"sr\"]\n",
    "traj         = A[\"traj\"]\n",
    "@printf(\"Cost went from %g to %g ; max iters on cpm was %d\\n\\n\", traj[2,1], traj[2,end], maximum(cpm_traj[1,:]))\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9×3 Array{Any,2}:\n",
       " \"sW\"                         0.121312    0.0840597\n",
       " \"vW\"                        -2.26178    -1.32677  \n",
       " \"hW\"                        -0.222131   -0.437334 \n",
       " \"dW\"                         0.472211   -0.324835 \n",
       " \"constant_excitation\"        0.401695    0.567997 \n",
       " \"right_light_excitation\"     3.99987     0.712216 \n",
       " \"target_period_excitation\"   0.0501671   0.0500075\n",
       " \"const_pro_bias\"             0.0382744   0.0858569\n",
       " \"sigma\"                      0.2         0.25     "
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_seed = [0.0840597,  -1.32677,  -0.437334,  -0.324835,  0.567997, 0.712216,  0.0500075,  0.0858569,  0.25]\n",
    "\n",
    "[args pars good_seed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4.884860 seconds (4.66 M allocations: 286.025 MB, 1.52% gc time)\n",
      "Pro % correct = 95%\n",
      "Anti % correct = 70.9% \n",
      "     -- cost=-0.000401259,   cost1=0.0023092, cost2=-0.00271046\n",
      "     -- mean(hitsP)=0.965516, mean(diffsP)=0.984829 mean(hitsA)=0.681942, mean(diffsA)=0.822143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.00040125882805575096"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------\n",
    "# NOW EVALUATE RESULTS HERE\n",
    "# --------------------\n",
    "\n",
    "my_params = make_dict([args; \"plot_list\"; \"post_target_period\"; \"rule_and_delay_period\"; \"dt\"], \n",
    "[pars; [[1:10;]]; 0.2; 0.4; 0.02], model_params)\n",
    "\n",
    "run_factor = 10\n",
    "\n",
    "proVs, antiVs = @time(run_ntrials(nPro*run_factor, nAnti*run_factor; plot_list=[], my_params...))\n",
    "\n",
    "if nPro>0;  @printf(\"Pro %% correct = %g%%\\n\", 100*length(find(proVs[1,:].>proVs[4,:]))/(nPro*run_factor)); end;\n",
    "if nAnti>0; @printf(\"Anti %% correct = %g%% \\n\", 100*length(find(antiVs[1,:].<antiVs[4,:]))/(nAnti*run_factor)); end;\n",
    "\n",
    "\n",
    "figure(3); clf();\n",
    "ax1 = subplot(2,1,1)\n",
    "h = plt[:hist](proVs[1,:]-proVs[4,:],-1:0.02:1)\n",
    "title(\"PRO Vr - Vl\")\n",
    "remove_xtick_labels(ax1)\n",
    "vlines(0, ylim()[1], ylim()[2])\n",
    "\n",
    "ax2 = subplot(2,1,2)\n",
    "h = plt[:hist](antiVs[1,:]-antiVs[4,:],-1:0.02:1)\n",
    "title(\"ANTI Vr - Vl\")\n",
    "vlines(0, ylim()[1], ylim()[2])\n",
    "\n",
    "figure(1); clf(); figure(2); clf();\n",
    "\n",
    "JJ(nPro, nAnti; verbose=true, seedrand=sr, rule_and_delay_periods=my_params[:rule_and_delay_period], \n",
    "post_target_periods=my_params[:post_target_period], my_params...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the results of the farm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 0.02\n",
       " 0.04"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basename = \"farm_E\"\n",
    "fnames = readdir()\n",
    "matched_filenames = Array{Bool}(length(fnames))\n",
    "for i=1:length(fnames)\n",
    "    matched_filenames[i] = ismatch(Regex(@sprintf(\"^%s\", basename)), fnames[i])\n",
    "end\n",
    "myguys = fnames[find(matched_filenames)]\n",
    "costs = zeros(size(myguys))\n",
    "cbs   = zeros(size(myguys))\n",
    "for i=1:length(myguys)\n",
    "    A = matread(myguys[i])\n",
    "    costs[i] = standard_cost(myguys[i]; verbose=true)\n",
    "    cbs[i]   = A[\"cb\"]\n",
    "end\n",
    "\n",
    "cbset = unique(cbs)\n",
    "\n",
    "figure(3); clf();\n",
    "for i=1:length(cbset)    \n",
    "    ax1 = subplot(length(cbset),1,i)\n",
    "    h1 = plt[:hist](costs[find(cbs.==cbset[i])], -0.02:0.001:0.008);\n",
    "    ylabel(\"# of runs\")\n",
    "    title(@sprintf(\"cb=%g  (%d total)\", cbset[i], length(find(cbs.==cbset[i]))))\n",
    "    if i < length(cbset)\n",
    "        remove_xtick_labels(ax1)\n",
    "    end\n",
    "    if i==length(cbset)\n",
    "        xlabel(\"final cost\")\n",
    "    end\n",
    "end\n",
    "        \n",
    "unique(cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21-element Array{String,1}:\n",
       " \"farm_E_0005\"\n",
       " \"farm_E_0012\"\n",
       " \"farm_E_0018\"\n",
       " \"farm_E_0027\"\n",
       " \"farm_E_0031\"\n",
       " \"farm_E_0036\"\n",
       " \"farm_E_0038\"\n",
       " \"farm_E_0039\"\n",
       " \"farm_E_0041\"\n",
       " \"farm_E_0043\"\n",
       " \"farm_E_0047\"\n",
       " \"farm_E_0049\"\n",
       " \"farm_E_0053\"\n",
       " \"farm_E_0054\"\n",
       " \"farm_E_0065\"\n",
       " \"farm_E_0067\"\n",
       " \"farm_E_0086\"\n",
       " \"farm_E_0110\"\n",
       " \"farm_E_0126\"\n",
       " \"farm_E_0135\"\n",
       " \"farm_E_0136\""
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myguys[find(costs.<=-0.0075)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of 9-param, including diagonal weight and sigma, and robust across 3x in rule/delay period and post-target period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: eta=0.01 ps=[0.200, -1.700, -1.700, 0.000, 0.390, 0.150, 0.100, 0.100, 0.100]\n",
      "     -- cost=0.0569741,   cost1=0.0664577, cost2=-0.00948358\n",
      "     -- mean(hitsP)=0.585719, mean(diffsP)=0.964404 mean(hitsA)=0.521092, mean(diffsA)=0.932312\n",
      "     -- cost=0.0546503,   cost1=0.0640934, cost2=-0.00944312\n",
      "     -- mean(hitsP)=0.59576, mean(diffsP)=0.960612 mean(hitsA)=0.518412, mean(diffsA)=0.928012\n",
      "1: eta=0.011 cost=0.0546503 jtype=constrained costheta=-0.929 ps=[0.200008, -1.70008, -1.69985, -0.000825069, 0.388531, 0.150065, 0.0999995, 0.102269, 0.0912348]\n",
      "     -- cost=0.0517394,   cost1=0.0611855, cost2=-0.00944606\n",
      "     -- mean(hitsP)=0.606351, mean(diffsP)=0.958814 mean(hitsA)=0.51702, mean(diffsA)=0.930398\n",
      "2: eta=0.0121 cost=0.0517394 jtype=constrained costheta=-0.590 ps=[0.200133, -1.70019, -1.70003, -0.00241266, 0.388902, 0.150087, 0.0999993, 0.112622, 0.0884086]\n",
      "     -- cost=0.049832,   cost1=0.059234, cost2=-0.00940196\n",
      "     -- mean(hitsP)=0.617304, mean(diffsP)=0.959881 mean(hitsA)=0.51318, mean(diffsA)=0.920512\n",
      "3: eta=0.01331 cost=0.049832 jtype=constrained costheta=-0.625 ps=[0.200114, -1.70018, -1.69972, -0.00592267, 0.381847, 0.150163, 0.0999916, 0.117622, 0.0820602]\n",
      "     -- cost=0.047824,   cost1=0.0572145, cost2=-0.00939049\n",
      "     -- mean(hitsP)=0.627085, mean(diffsP)=0.957958 mean(hitsA)=0.511378, mean(diffsA)=0.920141\n",
      "4: eta=0.014641 cost=0.047824 jtype=constrained costheta=-0.677 ps=[0.20037, -1.70055, -1.70042, -0.00538898, 0.390222, 0.15023, 0.0999924, 0.115062, 0.0742751]\n",
      "     -- cost=0.0468348,   cost1=0.0561615, cost2=-0.0093267\n",
      "     -- mean(hitsP)=0.634794, mean(diffsP)=0.950977 mean(hitsA)=0.507784, mean(diffsA)=0.914363\n",
      "5: eta=0.0161051 cost=0.0468348 jtype=constrained costheta=-0.529 ps=[0.200359, -1.7012, -1.69988, -0.00316715, 0.395437, 0.150324, 0.0999956, 0.105811, 0.067057]\n",
      "     -- cost=0.0434031,   cost1=0.0526064, cost2=-0.00920333\n",
      "     -- mean(hitsP)=0.658094, mean(diffsP)=0.944164 mean(hitsA)=0.504926, mean(diffsA)=0.896501\n",
      "6: eta=0.0177156 cost=0.0434031 jtype=constrained costheta=-0.317 ps=[0.200308, -1.70124, -1.69967, -0.00292751, 0.394301, 0.150417, 0.0999962, 0.10198, 0.0565507]\n",
      "     -- cost=0.0422591,   cost1=0.0513472, cost2=-0.0090881\n",
      "     -- mean(hitsP)=0.662828, mean(diffsP)=0.9349 mean(hitsA)=0.508913, mean(diffsA)=0.88272\n",
      "7: eta=0.0194872 cost=0.0422591 jtype=constrained costheta=-0.359 ps=[0.201359, -1.70272, -1.69982, -0.0178273, 0.394857, 0.150484, 0.100007, 0.0955907, 0.0533642]\n",
      "     -- cost=0.0435781,   cost1=0.0528704, cost2=-0.00929227\n",
      "     -- mean(hitsP)=0.666089, mean(diffsP)=0.954202 mean(hitsA)=0.50026, mean(diffsA)=0.904252\n",
      "eta going down: new_cost-cost=0.00131897 and jumptype='constrained'\n",
      "8: eta=0.00974359 cost=0.0422591 jtype=constrained costheta=NaN ps=[0.201359, -1.70272, -1.69982, -0.0178273, 0.394857, 0.150484, 0.100007, 0.0955907, 0.0533642]\n",
      "     -- cost=0.0420395,   cost1=0.0512304, cost2=-0.00919093\n",
      "     -- mean(hitsP)=0.669932, mean(diffsP)=0.945403 mean(hitsA)=0.50231, mean(diffsA)=0.892783\n",
      "9: eta=0.0107179 cost=0.0420395 jtype=constrained costheta=-0.605 ps=[0.20157, -1.70311, -1.70004, -0.0181592, 0.400835, 0.150516, 0.100008, 0.102253, 0.0515013]\n",
      "     -- cost=0.0416581,   cost1=0.0507368, cost2=-0.00907873\n",
      "     -- mean(hitsP)=0.670347, mean(diffsP)=0.93747 mean(hitsA)=0.507519, mean(diffsA)=0.878277\n",
      "10: eta=0.0117897 cost=0.0416581 jtype=constrained costheta=-0.879 ps=[0.20169, -1.70381, -1.69956, -0.0185618, 0.40433, 0.150569, 0.100011, 0.0944165, 0.0481062]\n",
      "     -- cost=0.0390419,   cost1=0.0480014, cost2=-0.00895948\n",
      "     -- mean(hitsP)=0.692826, mean(diffsP)=0.928786 mean(hitsA)=0.505818, mean(diffsA)=0.863111\n",
      "11: eta=0.0129687 cost=0.0390419 jtype=constrained costheta=-0.453 ps=[0.20168, -1.70402, -1.69923, -0.0190984, 0.403303, 0.150642, 0.100011, 0.092704, 0.0423258]\n",
      "     -- cost=0.0369274,   cost1=0.0457462, cost2=-0.00881882\n",
      "     -- mean(hitsP)=0.711368, mean(diffsP)=0.925024 mean(hitsA)=0.507101, mean(diffsA)=0.83874\n",
      "12: eta=0.0142656 cost=0.0369274 jtype=constrained costheta=-0.180 ps=[0.201691, -1.70471, -1.69853, -0.0198086, 0.402968, 0.150729, 0.100013, 0.0879233, 0.0371666]\n",
      "     -- cost=0.0326186,   cost1=0.0412526, cost2=-0.00863401\n",
      "     -- mean(hitsP)=0.714031, mean(diffsP)=0.909416 mean(hitsA)=0.527996, mean(diffsA)=0.817385\n",
      "13: eta=0.0156921 cost=0.0326186 jtype=constrained costheta=-0.140 ps=[0.201661, -1.70469, -1.6982, -0.0223644, 0.396488, 0.150743, 0.100013, 0.0757588, 0.0365168]\n",
      "     -- cost=0.0326413,   cost1=0.0411854, cost2=-0.00854407\n",
      "     -- mean(hitsP)=0.733673, mean(diffsP)=0.906255 mean(hitsA)=0.509393, mean(diffsA)=0.80256\n",
      "eta going down: new_cost-cost=2.27528e-05 and jumptype='constrained'\n",
      "14: eta=0.00784607 cost=0.0326186 jtype=constrained costheta=NaN ps=[0.201661, -1.70469, -1.6982, -0.0223644, 0.396488, 0.150743, 0.100013, 0.0757588, 0.0365168]\n",
      "     -- cost=0.0324481,   cost1=0.0410281, cost2=-0.00858001\n",
      "     -- mean(hitsP)=0.723849, mean(diffsP)=0.909699 mean(hitsA)=0.518136, mean(diffsA)=0.806303\n",
      "15: eta=0.00863068 cost=0.0324481 jtype=constrained costheta=-0.162 ps=[0.201762, -1.7049, -1.69786, -0.0261299, 0.391491, 0.150769, 0.100014, 0.0792475, 0.0354544]\n",
      "     -- cost=0.0299171,   cost1=0.0383975, cost2=-0.00848042\n",
      "     -- mean(hitsP)=0.722299, mean(diffsP)=0.899996 mean(hitsA)=0.53286, mean(diffsA)=0.796088\n",
      "16: eta=0.00949375 cost=0.0299171 jtype=constrained costheta=-0.974 ps=[0.201775, -1.70499, -1.69779, -0.0267672, 0.39042, 0.150769, 0.100014, 0.0707323, 0.0355736]\n",
      "     -- cost=0.0278188,   cost1=0.0361344, cost2=-0.00831557\n",
      "     -- mean(hitsP)=0.731805, mean(diffsP)=0.890191 mean(hitsA)=0.541881, mean(diffsA)=0.772923\n",
      "17: eta=0.0104431 cost=0.0278188 jtype=constrained costheta=-0.805 ps=[0.201847, -1.70538, -1.69708, -0.0288277, 0.389005, 0.150839, 0.100016, 0.064822, 0.0330847]\n",
      "     -- cost=0.0260949,   cost1=0.0342405, cost2=-0.0081456\n",
      "     -- mean(hitsP)=0.740902, mean(diffsP)=0.881393 mean(hitsA)=0.553683, mean(diffsA)=0.747727\n",
      "18: eta=0.0114874 cost=0.0260949 jtype=constrained costheta=-0.976 ps=[0.20187, -1.70575, -1.69645, -0.0304635, 0.386846, 0.150908, 0.100018, 0.0580291, 0.0306641]\n",
      "     -- cost=0.0240436,   cost1=0.031958, cost2=-0.00791437\n",
      "     -- mean(hitsP)=0.744809, mean(diffsP)=0.863966 mean(hitsA)=0.573831, mean(diffsA)=0.718908\n",
      "19: eta=0.0126362 cost=0.0240436 jtype=constrained costheta=-0.997 ps=[0.201854, -1.70602, -1.69583, -0.0321661, 0.383119, 0.15097, 0.100019, 0.0497317, 0.0286748]\n",
      "     -- cost=0.022035,   cost1=0.0297651, cost2=-0.00773006\n",
      "     -- mean(hitsP)=0.753865, mean(diffsP)=0.846019 mean(hitsA)=0.588224, mean(diffsA)=0.699993\n",
      "20: eta=0.0138998 cost=0.022035 jtype=constrained costheta=-0.918 ps=[0.201718, -1.70639, -1.69497, -0.0320566, 0.380272, 0.151082, 0.100022, 0.0434819, 0.0258978]\n",
      "     -- cost=0.0209973,   cost1=0.0286464, cost2=-0.00764913\n",
      "     -- mean(hitsP)=0.761849, mean(diffsP)=0.840751 mean(hitsA)=0.600453, mean(diffsA)=0.689075\n",
      "21: eta=0.0152898 cost=0.0209973 jtype=constrained costheta=-0.752 ps=[0.20187, -1.70783, -1.6924, -0.0359605, 0.382944, 0.151357, 0.100033, 0.0390972, 0.023351]\n",
      "     -- cost=0.0198771,   cost1=0.0273342, cost2=-0.00745708\n",
      "     -- mean(hitsP)=0.771881, mean(diffsP)=0.824738 mean(hitsA)=0.611629, mean(diffsA)=0.666678\n",
      "22: eta=0.0168187 cost=0.0198771 jtype=constrained costheta=-0.932 ps=[0.201805, -1.70907, -1.69023, -0.0380097, 0.380484, 0.151617, 0.100041, 0.0358703, 0.0207195]\n",
      "     -- cost=0.0179693,   cost1=0.0250751, cost2=-0.00710574\n",
      "     -- mean(hitsP)=0.786096, mean(diffsP)=0.801194 mean(hitsA)=0.60986, mean(diffsA)=0.619953\n",
      "23: eta=0.0185006 cost=0.0179693 jtype=constrained costheta=-0.985 ps=[0.201676, -1.7092, -1.6895, -0.0416948, 0.367664, 0.15171, 0.100042, 0.0375219, 0.0192293]\n",
      "     -- cost=0.0165466,   cost1=0.0232835, cost2=-0.00673697\n",
      "     -- mean(hitsP)=0.800932, mean(diffsP)=0.779165 mean(hitsA)=0.607358, mean(diffsA)=0.56823\n",
      "24: eta=0.0203507 cost=0.0165466 jtype=constrained costheta=-0.996 ps=[0.20159, -1.70979, -1.68813, -0.0459224, 0.356124, 0.151885, 0.100045, 0.0396397, 0.0174847]\n",
      "     -- cost=0.015463,   cost1=0.0215528, cost2=-0.00608978\n",
      "     -- mean(hitsP)=0.808264, mean(diffsP)=0.724427 mean(hitsA)=0.614219, mean(diffsA)=0.493529\n",
      "25: eta=0.0223858 cost=0.015463 jtype=constrained costheta=-0.914 ps=[0.201499, -1.71214, -1.68386, -0.0527589, 0.343194, 0.152271, 0.100063, 0.0358901, 0.0163188]\n",
      "     -- cost=0.014276,   cost1=0.0196311, cost2=-0.00535508\n",
      "     -- mean(hitsP)=0.816163, mean(diffsP)=0.663865 mean(hitsA)=0.617387, mean(diffsA)=0.407151\n",
      "26: eta=0.0246243 cost=0.014276 jtype=constrained costheta=-0.964 ps=[0.201512, -1.7149, -1.67873, -0.0611033, 0.330933, 0.152782, 0.100085, 0.0336752, 0.0151146]\n",
      "     -- cost=0.0127182,   cost1=0.0168045, cost2=-0.00408627\n",
      "     -- mean(hitsP)=0.814366, mean(diffsP)=0.536867 mean(hitsA)=0.618956, mean(diffsA)=0.280388\n",
      "27: eta=0.0270868 cost=0.0127182 jtype=constrained costheta=-0.999 ps=[0.201455, -1.71585, -1.67629, -0.0679773, 0.312968, 0.153086, 0.100093, 0.0286904, 0.0141317]\n",
      "     -- cost=0.0115976,   cost1=0.0154836, cost2=-0.00388608\n",
      "     -- mean(hitsP)=0.8269, mean(diffsP)=0.528178 mean(hitsA)=0.617466, mean(diffsA)=0.249039\n",
      "28: eta=0.0297954 cost=0.0115976 jtype=constrained costheta=-0.928 ps=[0.201826, -1.71946, -1.66999, -0.079386, 0.310744, 0.153938, 0.100122, 0.0302871, 0.012962]\n",
      "     -- cost=0.010678,   cost1=0.0145257, cost2=-0.00384767\n",
      "     -- mean(hitsP)=0.836519, mean(diffsP)=0.530922 mean(hitsA)=0.622565, mean(diffsA)=0.238611\n",
      "29: eta=0.032775 cost=0.010678 jtype=constrained costheta=-0.980 ps=[0.202369, -1.72461, -1.66129, -0.0934956, 0.312194, 0.155039, 0.100162, 0.0303021, 0.0121759]\n",
      "     -- cost=0.00986234,   cost1=0.0136834, cost2=-0.00382109\n",
      "     -- mean(hitsP)=0.844959, mean(diffsP)=0.533983 mean(hitsA)=0.627863, mean(diffsA)=0.230236\n",
      "30: eta=0.0360525 cost=0.00986234 jtype=constrained costheta=-0.982 ps=[0.203026, -1.73116, -1.65018, -0.110241, 0.313702, 0.156341, 0.100212, 0.0301959, 0.0116537]\n",
      "     -- cost=0.00910743,   cost1=0.0128976, cost2=-0.00379021\n",
      "     -- mean(hitsP)=0.852778, mean(diffsP)=0.53628 mean(hitsA)=0.632934, mean(diffsA)=0.221761\n",
      "31: eta=0.0396577 cost=0.00910743 jtype=constrained costheta=-0.986 ps=[0.203782, -1.73913, -1.63651, -0.129254, 0.314967, 0.157813, 0.100274, 0.0300923, 0.0113046]\n",
      "     -- cost=0.00839053,   cost1=0.0121472, cost2=-0.00375665\n",
      "     -- mean(hitsP)=0.860427, mean(diffsP)=0.538328 mean(hitsA)=0.63781, mean(diffsA)=0.213002\n",
      "32: eta=0.0436235 cost=0.00839053 jtype=constrained costheta=-0.990 ps=[0.204628, -1.7486, -1.62, -0.15011, 0.316001, 0.159421, 0.100347, 0.0300477, 0.0110694]\n",
      "     -- cost=0.00770567,   cost1=0.0114336, cost2=-0.00372792\n",
      "     -- mean(hitsP)=0.86822, mean(diffsP)=0.541171 mean(hitsA)=0.642622, mean(diffsA)=0.204413\n",
      "33: eta=0.0479859 cost=0.00770567 jtype=constrained costheta=-0.992 ps=[0.205557, -1.75974, -1.60021, -0.172276, 0.31687, 0.161112, 0.100433, 0.030109, 0.010914]\n",
      "     -- cost=0.00705836,   cost1=0.010765, cost2=-0.00370664\n",
      "     -- mean(hitsP)=0.876156, mean(diffsP)=0.545138 mean(hitsA)=0.647429, mean(diffsA)=0.196191\n",
      "34: eta=0.0527844 cost=0.00705836 jtype=constrained costheta=-0.992 ps=[0.20656, -1.77284, -1.57647, -0.194926, 0.317532, 0.162802, 0.100536, 0.0302966, 0.0108225]\n",
      "     -- cost=0.00645762,   cost1=0.0101418, cost2=-0.00368415\n",
      "     -- mean(hitsP)=0.883761, mean(diffsP)=0.54899 mean(hitsA)=0.652025, mean(diffsA)=0.187841\n",
      "35: eta=0.0580629 cost=0.00645762 jtype=constrained costheta=-0.989 ps=[0.207625, -1.78817, -1.54802, -0.216781, 0.317735, 0.164352, 0.10066, 0.0305912, 0.0107957]\n",
      "     -- cost=0.0059039,   cost1=0.00954709, cost2=-0.00364319\n",
      "     -- mean(hitsP)=0.890045, mean(diffsP)=0.550043 mean(hitsA)=0.655928, mean(diffsA)=0.178594\n",
      "36: eta=0.0638692 cost=0.0059039 jtype=constrained costheta=-0.981 ps=[0.208743, -1.80567, -1.51443, -0.236407, 0.317135, 0.165614, 0.100811, 0.0309084, 0.010859]\n",
      "     -- cost=0.00537849,   cost1=0.00895557, cost2=-0.00357707\n",
      "     -- mean(hitsP)=0.893943, mean(diffsP)=0.546492 mean(hitsA)=0.658766, mean(diffsA)=0.168923\n",
      "37: eta=0.0702561 cost=0.00537849 jtype=constrained costheta=-0.967 ps=[0.209907, -1.82446, -1.47624, -0.253524, 0.315721, 0.166587, 0.100989, 0.0311192, 0.0110951]\n",
      "     -- cost=0.00484059,   cost1=0.00838081, cost2=-0.00354022\n",
      "     -- mean(hitsP)=0.895229, mean(diffsP)=0.544063 mean(hitsA)=0.660822, mean(diffsA)=0.163982\n",
      "38: eta=0.0772817 cost=0.00484059 jtype=constrained costheta=-0.966 ps=[0.21105, -1.84289, -1.43541, -0.269311, 0.314472, 0.167476, 0.101181, 0.0313552, 0.0117848]\n",
      "     -- cost=0.00421808,   cost1=0.00791488, cost2=-0.0036968\n",
      "     -- mean(hitsP)=0.892413, mean(diffsP)=0.560657 mean(hitsA)=0.662082, mean(diffsA)=0.178703\n",
      "39: eta=0.0850099 cost=0.00421808 jtype=constrained costheta=-0.967 ps=[0.211921, -1.85912, -1.39542, -0.284064, 0.315537, 0.168447, 0.101358, 0.0324163, 0.0139491]\n",
      "     -- cost=0.00350525,   cost1=0.00748134, cost2=-0.00397609\n",
      "     -- mean(hitsP)=0.888935, mean(diffsP)=0.591409 mean(hitsA)=0.664358, mean(diffsA)=0.203808\n",
      "40: eta=0.0935109 cost=0.00350525 jtype=constrained costheta=-0.842 ps=[0.212465, -1.8764, -1.34409, -0.306743, 0.318357, 0.170416, 0.101553, 0.0339742, 0.0172467]\n",
      "     -- cost=0.00272289,   cost1=0.0068635, cost2=-0.00414061\n",
      "     -- mean(hitsP)=0.89199, mean(diffsP)=0.614343 mean(hitsA)=0.669137, mean(diffsA)=0.213779\n",
      "41: eta=0.102862 cost=0.00272289 jtype=constrained costheta=-0.869 ps=[0.212322, -1.89439, -1.27979, -0.331952, 0.319947, 0.173126, 0.101772, 0.035351, 0.0197915]\n",
      "     -- cost=0.00189824,   cost1=0.00626638, cost2=-0.00436815\n",
      "     -- mean(hitsP)=0.891559, mean(diffsP)=0.642286 mean(hitsA)=0.672311, mean(diffsA)=0.231344\n",
      "42: eta=0.113148 cost=0.00189824 jtype=constrained costheta=-0.975 ps=[0.211166, -1.9118, -1.2057, -0.353996, 0.3217, 0.176408, 0.102002, 0.0374236, 0.023787]\n",
      "     -- cost=0.00101058,   cost1=0.00569603, cost2=-0.00468545\n",
      "     -- mean(hitsP)=0.892317, mean(diffsP)=0.680147 mean(hitsA)=0.676034, mean(diffsA)=0.256943\n",
      "43: eta=0.124463 cost=0.00101058 jtype=constrained costheta=-0.929 ps=[0.208402, -1.92828, -1.11951, -0.368465, 0.324083, 0.180378, 0.102238, 0.0407761, 0.0286021]\n",
      "     -- cost=1.24071e-05,   cost1=0.00521061, cost2=-0.00519821\n",
      "     -- mean(hitsP)=0.892674, mean(diffsP)=0.734802 mean(hitsA)=0.679132, mean(diffsA)=0.304839\n",
      "44: eta=0.136909 cost=1.24071e-05 jtype=constrained costheta=-0.906 ps=[0.203393, -1.94318, -1.02101, -0.370337, 0.328488, 0.18503, 0.102459, 0.0466439, 0.0347482]\n",
      "     -- cost=-0.00122634,   cost1=0.00479189, cost2=-0.00601823\n",
      "     -- mean(hitsP)=0.892913, mean(diffsP)=0.809179 mean(hitsA)=0.68102, mean(diffsA)=0.394466\n",
      "45: eta=0.1506 cost=-0.00122634 jtype=constrained costheta=-0.868 ps=[0.195973, -1.95584, -0.910017, -0.358893, 0.338602, 0.190023, 0.102638, 0.056779, 0.041898]\n",
      "     -- cost=-0.00282954,   cost1=0.00422283, cost2=-0.00705237\n",
      "     -- mean(hitsP)=0.891953, mean(diffsP)=0.881258 mean(hitsA)=0.68209, mean(diffsA)=0.529215\n",
      "46: eta=0.16566 cost=-0.00282954 jtype=constrained costheta=-0.842 ps=[0.187107, -1.96548, -0.784654, -0.342626, 0.36128, 0.194803, 0.102763, 0.0720191, 0.0492702]\n",
      "     -- cost=-0.00465415,   cost1=0.00270321, cost2=-0.00735736\n",
      "     -- mean(hitsP)=0.909728, mean(diffsP)=0.896088 mean(hitsA)=0.692784, mean(diffsA)=0.575383\n",
      "47: eta=0.182226 cost=-0.00465415 jtype=constrained costheta=-0.469 ps=[0.177676, -1.97121, -0.6377, -0.330338, 0.374955, 0.198868, 0.102842, 0.0842039, 0.0539716]\n",
      "     -- cost=-0.0062798,   cost1=0.00132115, cost2=-0.00760095\n",
      "     -- mean(hitsP)=0.913363, mean(diffsP)=0.885292 mean(hitsA)=0.693227, mean(diffsA)=0.634899\n",
      "48: eta=0.200449 cost=-0.0062798 jtype=constrained costheta=-0.613 ps=[0.167742, -1.96909, -0.476605, -0.299224, 0.399276, 0.203589, 0.102865, 0.0995063, 0.0662952]\n",
      "     -- cost=-0.00458207,   cost1=0.00235174, cost2=-0.00693381\n",
      "     -- mean(hitsP)=0.854376, mean(diffsP)=0.780593 mean(hitsA)=0.672436, mean(diffsA)=0.606168\n",
      "eta going down: new_cost-cost=0.00169773 and jumptype='constrained'\n",
      "49: eta=0.100224 cost=-0.0062798 jtype=constrained costheta=NaN ps=[0.167742, -1.96909, -0.476605, -0.299224, 0.399276, 0.203589, 0.102865, 0.0995063, 0.0662952]\n",
      "     -- cost=-0.00678096,   cost1=0.00072868, cost2=-0.00750964\n",
      "     -- mean(hitsP)=0.898659, mean(diffsP)=0.853244 mean(hitsA)=0.680434, mean(diffsA)=0.648684\n",
      "50: eta=0.110247 cost=-0.00678096 jtype=constrained costheta=-0.268 ps=[0.162203, -1.96736, -0.392473, -0.267801, 0.409665, 0.207062, 0.102879, 0.107635, 0.0777004]\n",
      "     -- cost=-0.00703842,   cost1=0.000735069, cost2=-0.00777348\n",
      "     -- mean(hitsP)=0.905744, mean(diffsP)=0.866267 mean(hitsA)=0.68383, mean(diffsA)=0.68843\n",
      "51: eta=0.121272 cost=-0.00703842 jtype=constrained costheta=-0.171 ps=[0.159127, -1.94804, -0.391919, -0.229626, 0.41282, 0.222457, 0.102846, 0.104796, 0.0800895]\n",
      "     -- cost=-0.00722644,   cost1=0.000735483, cost2=-0.00796192\n",
      "     -- mean(hitsP)=0.906684, mean(diffsP)=0.875523 mean(hitsA)=0.68147, mean(diffsA)=0.716862\n",
      "52: eta=0.133399 cost=-0.00722644 jtype=constrained costheta=-0.267 ps=[0.157426, -1.92743, -0.385956, -0.225717, 0.413929, 0.242738, 0.102803, 0.105497, 0.0876644]\n",
      "     -- cost=-0.00741821,   cost1=0.000650271, cost2=-0.00806848\n",
      "     -- mean(hitsP)=0.906174, mean(diffsP)=0.885067 mean(hitsA)=0.685097, mean(diffsA)=0.72863\n",
      "53: eta=0.146739 cost=-0.00741821 jtype=constrained costheta=-0.186 ps=[0.155818, -1.90004, -0.385687, -0.223652, 0.41419, 0.267191, 0.102712, 0.102511, 0.0968885]\n",
      "     -- cost=-0.00759924,   cost1=0.000610359, cost2=-0.0082096\n",
      "     -- mean(hitsP)=0.906376, mean(diffsP)=0.896129 mean(hitsA)=0.686664, mean(diffsA)=0.745791\n",
      "54: eta=0.161412 cost=-0.00759924 jtype=constrained costheta=-0.735 ps=[0.153906, -1.86521, -0.384086, -0.226183, 0.417441, 0.296791, 0.10256, 0.101086, 0.108086]\n",
      "     -- cost=-0.00776689,   cost1=0.000588736, cost2=-0.00835563\n",
      "     -- mean(hitsP)=0.907054, mean(diffsP)=0.907463 mean(hitsA)=0.687738, mean(diffsA)=0.763663\n",
      "55: eta=0.177554 cost=-0.00776689 jtype=constrained costheta=-0.471 ps=[0.151252, -1.81992, -0.383192, -0.232102, 0.422745, 0.33236, 0.102309, 0.099812, 0.121305]\n",
      "     -- cost=-0.00792666,   cost1=0.000536447, cost2=-0.00846311\n",
      "     -- mean(hitsP)=0.907991, mean(diffsP)=0.916968 mean(hitsA)=0.688905, mean(diffsA)=0.775653\n",
      "56: eta=0.195309 cost=-0.00792666 jtype=constrained costheta=-0.294 ps=[0.14843, -1.76076, -0.38224, -0.246679, 0.427542, 0.374713, 0.102058, 0.0982568, 0.137025]\n",
      "     -- cost=-0.00807857,   cost1=0.000533301, cost2=-0.00861188\n",
      "     -- mean(hitsP)=0.908957, mean(diffsP)=0.925898 mean(hitsA)=0.688843, mean(diffsA)=0.796478\n",
      "57: eta=0.21484 cost=-0.00807857 jtype=constrained costheta=-0.125 ps=[0.145212, -1.68752, -0.380524, -0.250779, 0.433282, 0.426236, 0.101918, 0.0960028, 0.155355]\n",
      "     -- cost=-0.00822507,   cost1=0.000556379, cost2=-0.00878145\n",
      "     -- mean(hitsP)=0.909981, mean(diffsP)=0.935832 mean(hitsA)=0.690873, mean(diffsA)=0.820459\n",
      "58: eta=0.236324 cost=-0.00822507 jtype=constrained costheta=-0.194 ps=[0.140784, -1.59854, -0.379391, -0.249329, 0.446334, 0.488724, 0.101627, 0.0925578, 0.176563]\n",
      "     -- cost=-0.0083099,   cost1=0.000648096, cost2=-0.008958\n",
      "     -- mean(hitsP)=0.912895, mean(diffsP)=0.952116 mean(hitsA)=0.701349, mean(diffsA)=0.839484\n",
      "59: eta=0.259956 cost=-0.0083099 jtype=constrained costheta=-0.170 ps=[0.137727, -1.53931, -0.384667, -0.258686, 0.473376, 0.581187, 0.101239, 0.0897453, 0.20564]\n",
      "     -- cost=0.037072,   cost1=0.0445786, cost2=-0.00750659\n",
      "     -- mean(hitsP)=0.871653, mean(diffsP)=0.856854 mean(hitsA)=0.420193, mean(diffsA)=0.644463\n",
      "eta going down: new_cost-cost=0.0453819 and jumptype='constrained'\n",
      "60: eta=0.129978 cost=-0.0083099 jtype=constrained costheta=NaN ps=[0.137727, -1.53931, -0.384667, -0.258686, 0.473376, 0.581187, 0.101239, 0.0897453, 0.20564]\n",
      "     -- cost=-0.0017968,   cost1=0.00673489, cost2=-0.00853169\n",
      "     -- mean(hitsP)=0.900725, mean(diffsP)=0.919951 mean(hitsA)=0.599983, mean(diffsA)=0.786387\n",
      "eta going down: new_cost-cost=0.00651311 and jumptype='constrained'\n",
      "61: eta=0.0649891 cost=-0.0083099 jtype=constrained costheta=NaN ps=[0.137727, -1.53931, -0.384667, -0.258686, 0.473376, 0.581187, 0.101239, 0.0897453, 0.20564]\n",
      "     -- cost=-0.00725015,   cost1=0.00164606, cost2=-0.00889621\n",
      "     -- mean(hitsP)=0.913016, mean(diffsP)=0.935503 mean(hitsA)=0.658853, mean(diffsA)=0.843738\n",
      "eta going down: new_cost-cost=0.00105976 and jumptype='constrained'\n",
      "62: eta=0.0324945 cost=-0.0083099 jtype=constrained costheta=NaN ps=[0.137727, -1.53931, -0.384667, -0.258686, 0.473376, 0.581187, 0.101239, 0.0897453, 0.20564]\n",
      "     -- cost=-0.00815722,   cost1=0.000820241, cost2=-0.00897746\n",
      "     -- mean(hitsP)=0.91321, mean(diffsP)=0.941237 mean(hitsA)=0.676304, mean(diffsA)=0.854255\n",
      "eta going down: new_cost-cost=0.000152689 and jumptype='constrained'\n",
      "63: eta=0.0162473 cost=-0.0083099 jtype=constrained costheta=NaN ps=[0.137727, -1.53931, -0.384667, -0.258686, 0.473376, 0.581187, 0.101239, 0.0897453, 0.20564]\n",
      "     -- cost=-0.00834334,   cost1=0.000676006, cost2=-0.00901935\n",
      "     -- mean(hitsP)=0.912164, mean(diffsP)=0.946126 mean(hitsA)=0.685022, mean(diffsA)=0.857743\n",
      "64: eta=0.017872 cost=-0.00834334 jtype=constrained costheta=-0.428 ps=[0.13758, -1.5378, -0.381268, -0.265995, 0.462829, 0.58072, 0.101231, 0.0952856, 0.209165]\n",
      "     -- cost=-0.00833655,   cost1=0.00051743, cost2=-0.00885398\n",
      "     -- mean(hitsP)=0.904185, mean(diffsP)=0.946873 mean(hitsA)=0.699287, mean(diffsA)=0.823924\n",
      "eta going down: new_cost-cost=6.78691e-06 and jumptype='constrained'\n",
      "65: eta=0.008936 cost=-0.00834334 jtype=constrained costheta=NaN ps=[0.13758, -1.5378, -0.381268, -0.265995, 0.462829, 0.58072, 0.101231, 0.0952856, 0.209165]\n",
      "     -- cost=-0.00840791,   cost1=0.000555166, cost2=-0.00896307\n",
      "     -- mean(hitsP)=0.908173, mean(diffsP)=0.947319 mean(hitsA)=0.692009, mean(diffsA)=0.845296\n",
      "66: eta=0.0098296 cost=-0.00840791 jtype=constrained costheta=-0.638 ps=[0.137621, -1.53769, -0.383196, -0.263256, 0.468019, 0.581147, 0.101228, 0.0912778, 0.211702]\n",
      "     -- cost=-0.0084216,   cost1=0.000570751, cost2=-0.00899235\n",
      "     -- mean(hitsP)=0.906452, mean(diffsP)=0.945066 mean(hitsA)=0.68821, mean(diffsA)=0.853404\n",
      "67: eta=0.0108126 cost=-0.0084216 jtype=constrained costheta=-0.256 ps=[0.137459, -1.53616, -0.380921, -0.262218, 0.472444, 0.583536, 0.101217, 0.093085, 0.214706]\n",
      "     -- cost=-0.00842893,   cost1=0.000571424, cost2=-0.00900036\n",
      "     -- mean(hitsP)=0.906104, mean(diffsP)=0.945135 mean(hitsA)=0.687964, mean(diffsA)=0.854936\n",
      "68: eta=0.0118938 cost=-0.00842893 jtype=constrained costheta=-0.787 ps=[0.137218, -1.53319, -0.380787, -0.261372, 0.473004, 0.587846, 0.101199, 0.0928945, 0.216484]\n",
      "     -- cost=-0.00843671,   cost1=0.000570836, cost2=-0.00900755\n",
      "     -- mean(hitsP)=0.906094, mean(diffsP)=0.945327 mean(hitsA)=0.687947, mean(diffsA)=0.856182\n",
      "69: eta=0.0130832 cost=-0.00843671 jtype=constrained costheta=-0.988 ps=[0.136964, -1.52987, -0.380626, -0.260815, 0.473536, 0.5927, 0.101178, 0.0927039, 0.218158]\n",
      "     -- cost=-0.00844509,   cost1=0.000570752, cost2=-0.00901584\n",
      "     -- mean(hitsP)=0.906186, mean(diffsP)=0.945655 mean(hitsA)=0.68796, mean(diffsA)=0.857514\n",
      "70: eta=0.0143915 cost=-0.00844509 jtype=constrained costheta=-0.994 ps=[0.136696, -1.52614, -0.380552, -0.260404, 0.474181, 0.598067, 0.101155, 0.0925197, 0.219922]\n",
      "     -- cost=-0.0084541,   cost1=0.000571044, cost2=-0.00902515\n",
      "     -- mean(hitsP)=0.906326, mean(diffsP)=0.946095 mean(hitsA)=0.687988, mean(diffsA)=0.858934\n",
      "71: eta=0.0158307 cost=-0.0084541 jtype=constrained costheta=-0.992 ps=[0.136409, -1.52193, -0.380565, -0.260111, 0.474952, 0.60397, 0.101128, 0.0923272, 0.221829]\n",
      "     -- cost=-0.00846377,   cost1=0.000571638, cost2=-0.00903541\n",
      "     -- mean(hitsP)=0.906506, mean(diffsP)=0.946642 mean(hitsA)=0.688031, mean(diffsA)=0.86044\n",
      "72: eta=0.0174137 cost=-0.00846377 jtype=constrained costheta=-0.987 ps=[0.136102, -1.51715, -0.38066, -0.259923, 0.475859, 0.610442, 0.101098, 0.0921192, 0.223893]\n",
      "     -- cost=-0.0084741,   cost1=0.000572555, cost2=-0.00904666\n",
      "     -- mean(hitsP)=0.906729, mean(diffsP)=0.9473 mean(hitsA)=0.688088, mean(diffsA)=0.862032\n",
      "73: eta=0.0191551 cost=-0.0084741 jtype=constrained costheta=-0.980 ps=[0.135769, -1.5117, -0.380838, -0.259832, 0.476922, 0.617514, 0.101063, 0.0918912, 0.226116]\n",
      "     -- cost=-0.00848508,   cost1=0.000573895, cost2=-0.00905898\n",
      "     -- mean(hitsP)=0.907005, mean(diffsP)=0.948083 mean(hitsA)=0.688163, mean(diffsA)=0.863712\n",
      "74: eta=0.0210706 cost=-0.00848508 jtype=constrained costheta=-0.967 ps=[0.135409, -1.50544, -0.381109, -0.259846, 0.478175, 0.625206, 0.101023, 0.0916393, 0.228491]\n",
      "     -- cost=-0.00849668,   cost1=0.000575882, cost2=-0.00907256\n",
      "     -- mean(hitsP)=0.907347, mean(diffsP)=0.949028 mean(hitsA)=0.688254, mean(diffsA)=0.865484\n",
      "75: eta=0.0231777 cost=-0.00849668 jtype=constrained costheta=-0.945 ps=[0.135018, -1.49819, -0.38151, -0.259997, 0.479666, 0.63352, 0.100976, 0.0913614, 0.230995]\n",
      "     -- cost=-0.00850884,   cost1=0.000578989, cost2=-0.00908783\n",
      "     -- mean(hitsP)=0.907778, mean(diffsP)=0.950212 mean(hitsA)=0.688361, mean(diffsA)=0.867354\n",
      "76: eta=0.0254954 cost=-0.00850884 jtype=constrained costheta=-0.913 ps=[0.134598, -1.48971, -0.382126, -0.26038, 0.481477, 0.642425, 0.100922, 0.0910589, 0.233584]\n",
      "     -- cost=-0.00852148,   cost1=0.000584351, cost2=-0.00910583\n",
      "     -- mean(hitsP)=0.908331, mean(diffsP)=0.951826 mean(hitsA)=0.688476, mean(diffsA)=0.86934\n",
      "77: eta=0.028045 cost=-0.00852148 jtype=constrained costheta=-0.867 ps=[0.134163, -1.47972, -0.383187, -0.261255, 0.483753, 0.651858, 0.100858, 0.0907442, 0.236178]\n",
      "     -- cost=-0.00853454,   cost1=0.000595069, cost2=-0.00912961\n",
      "     -- mean(hitsP)=0.909053, mean(diffsP)=0.954406 mean(hitsA)=0.688584, mean(diffsA)=0.871515\n",
      "78: eta=0.0308495 cost=-0.00853454 jtype=constrained costheta=-0.815 ps=[0.133768, -1.46794, -0.385399, -0.263419, 0.486793, 0.661781, 0.100782, 0.0904704, 0.238635]\n",
      "     -- cost=-0.00854799,   cost1=0.000614514, cost2=-0.0091625\n",
      "     -- mean(hitsP)=0.90989, mean(diffsP)=0.958584 mean(hitsA)=0.688674, mean(diffsA)=0.873916\n",
      "79: eta=0.0339344 cost=-0.00854799 jtype=constrained costheta=-0.789 ps=[0.133547, -1.45445, -0.389992, -0.268465, 0.491087, 0.672265, 0.100694, 0.0903793, 0.24077]\n",
      "     -- cost=-0.0085614,   cost1=0.000621348, cost2=-0.00918275\n",
      "     -- mean(hitsP)=0.910673, mean(diffsP)=0.960883 mean(hitsA)=0.688849, mean(diffsA)=0.875667\n",
      "80: eta=0.0373279 cost=-0.0085614 jtype=constrained costheta=-0.864 ps=[0.133149, -1.4378, -0.392672, -0.27256, 0.495947, 0.682312, 0.100587, 0.0902208, 0.242903]\n",
      "     -- cost=-0.00857421,   cost1=0.000623863, cost2=-0.00919808\n",
      "     -- mean(hitsP)=0.911321, mean(diffsP)=0.962306 mean(hitsA)=0.688907, mean(diffsA)=0.877309\n",
      "81: eta=0.0410607 cost=-0.00857421 jtype=constrained costheta=-0.745 ps=[0.132538, -1.41728, -0.394342, -0.27538, 0.50142, 0.691174, 0.10045, 0.089831, 0.244777]\n",
      "     -- cost=-0.00858597,   cost1=0.00062488, cost2=-0.00921085\n",
      "     -- mean(hitsP)=0.911836, mean(diffsP)=0.963358 mean(hitsA)=0.688833, mean(diffsA)=0.878811\n",
      "82: eta=0.0451667 cost=-0.00858597 jtype=constrained costheta=-0.756 ps=[0.131709, -1.39298, -0.395714, -0.277848, 0.507843, 0.698642, 0.100269, 0.0892542, 0.24629]\n",
      "     -- cost=-0.00859599,   cost1=0.000625243, cost2=-0.00922124\n",
      "     -- mean(hitsP)=0.912242, mean(diffsP)=0.964161 mean(hitsA)=0.688654, mean(diffsA)=0.880086\n",
      "83: eta=0.0496834 cost=-0.00859599 jtype=constrained costheta=-0.876 ps=[0.13055, -1.36554, -0.397078, -0.280425, 0.515392, 0.704874, 0.100006, 0.0885224, 0.247479]\n",
      "     -- cost=-0.00860343,   cost1=0.000625451, cost2=-0.00922888\n",
      "     -- mean(hitsP)=0.912558, mean(diffsP)=0.964719 mean(hitsA)=0.688436, mean(diffsA)=0.881057\n",
      "84: eta=0.0546518 cost=-0.00860343 jtype=constrained costheta=-0.556 ps=[0.128659, -1.33726, -0.398791, -0.283385, 0.523958, 0.710146, 0.0995342, 0.0876686, 0.248421]\n",
      "     -- cost=-0.00860784,   cost1=0.000624246, cost2=-0.00923209\n",
      "     -- mean(hitsP)=0.912729, mean(diffsP)=0.964921 mean(hitsA)=0.688346, mean(diffsA)=0.881497\n",
      "85: eta=0.0601169 cost=-0.00860784 jtype=constrained costheta=-0.250 ps=[0.124972, -1.31749, -0.401806, -0.287354, 0.532178, 0.713865, 0.0984525, 0.0869283, 0.249117]\n",
      "     -- cost=-0.00861044,   cost1=0.000621129, cost2=-0.00923157\n",
      "     -- mean(hitsP)=0.912682, mean(diffsP)=0.964821 mean(hitsA)=0.688371, mean(diffsA)=0.881492\n",
      "86: eta=0.0661286 cost=-0.00861044 jtype=constrained costheta=-0.431 ps=[0.120027, -1.31228, -0.406033, -0.292155, 0.537979, 0.715102, 0.0967068, 0.0865972, 0.249497]\n",
      "     -- cost=-0.00861246,   cost1=0.00061813, cost2=-0.00923059\n",
      "     -- mean(hitsP)=0.912591, mean(diffsP)=0.964711 mean(hitsA)=0.688413, mean(diffsA)=0.881407\n",
      "87: eta=0.0727415 cost=-0.00861246 jtype=constrained costheta=-0.098 ps=[0.11497, -1.31268, -0.410601, -0.297189, 0.542477, 0.715373, 0.0945361, 0.0864726, 0.249695]\n",
      "     -- cost=-0.00861425,   cost1=0.000615305, cost2=-0.00922955\n",
      "     -- mean(hitsP)=0.912509, mean(diffsP)=0.964611 mean(hitsA)=0.688478, mean(diffsA)=0.8813\n",
      "88: eta=0.0800156 cost=-0.00861425 jtype=constrained costheta=-0.081 ps=[0.110042, -1.31444, -0.4151, -0.302091, 0.546529, 0.715358, 0.0920336, 0.0864004, 0.249805]\n",
      "     -- cost=-0.00861591,   cost1=0.000612605, cost2=-0.00922851\n",
      "     -- mean(hitsP)=0.912435, mean(diffsP)=0.964504 mean(hitsA)=0.688549, mean(diffsA)=0.881198\n",
      "89: eta=0.0880172 cost=-0.00861591 jtype=constrained costheta=-0.077 ps=[0.105384, -1.31635, -0.41932, -0.306648, 0.5503, 0.715219, 0.0892615, 0.0863359, 0.249869]\n",
      "     -- cost=-0.00861748,   cost1=0.000610059, cost2=-0.00922754\n",
      "     -- mean(hitsP)=0.912368, mean(diffsP)=0.964391 mean(hitsA)=0.68862, mean(diffsA)=0.881117\n",
      "90: eta=0.0968189 cost=-0.00861748 jtype=constrained costheta=-0.075 ps=[0.101101, -1.31818, -0.423155, -0.310749, 0.553752, 0.715017, 0.0862828, 0.0862725, 0.249909]\n",
      "     -- cost=-0.00861899,   cost1=0.000607703, cost2=-0.00922669\n",
      "     -- mean(hitsP)=0.912305, mean(diffsP)=0.964277 mean(hitsA)=0.688687, mean(diffsA)=0.881061\n",
      "91: eta=0.106501 cost=-0.00861899 jtype=constrained costheta=-0.074 ps=[0.09726, -1.31987, -0.426555, -0.314345, 0.556842, 0.714782, 0.0831607, 0.086211, 0.249935]\n",
      "     -- cost=-0.00862044,   cost1=0.000605554, cost2=-0.00922599\n",
      "     -- mean(hitsP)=0.912247, mean(diffsP)=0.964168 mean(hitsA)=0.688747, mean(diffsA)=0.881031\n",
      "92: eta=0.117151 cost=-0.00862044 jtype=constrained costheta=-0.076 ps=[0.0939029, -1.32139, -0.429493, -0.317415, 0.559542, 0.714531, 0.0799574, 0.0861531, 0.249952]\n",
      "     -- cost=-0.00862183,   cost1=0.00060362, cost2=-0.00922545\n",
      "     -- mean(hitsP)=0.912195, mean(diffsP)=0.964066 mean(hitsA)=0.688801, mean(diffsA)=0.881023\n",
      "93: eta=0.128866 cost=-0.00862183 jtype=constrained costheta=-0.079 ps=[0.091056, -1.32272, -0.431953, -0.319955, 0.561836, 0.714276, 0.0767336, 0.0861001, 0.249964]\n",
      "     -- cost=-0.00862315,   cost1=0.0006019, cost2=-0.00922505\n",
      "     -- mean(hitsP)=0.912148, mean(diffsP)=0.963976 mean(hitsA)=0.688848, mean(diffsA)=0.881034\n",
      "94: eta=0.141753 cost=-0.00862315 jtype=constrained costheta=-0.085 ps=[0.0887341, -1.32384, -0.433932, -0.321967, 0.563716, 0.714025, 0.0735477, 0.086053, 0.249973]\n",
      "     -- cost=-0.0086244,   cost1=0.000600392, cost2=-0.00922479\n",
      "     -- mean(hitsP)=0.912107, mean(diffsP)=0.963901 mean(hitsA)=0.688888, mean(diffsA)=0.881058\n",
      "95: eta=0.155928 cost=-0.0086244 jtype=constrained costheta=-0.095 ps=[0.0869378, -1.32474, -0.435436, -0.323469, 0.565186, 0.713782, 0.0704543, 0.0860126, 0.24998]\n",
      "     -- cost=-0.00862558,   cost1=0.000599088, cost2=-0.00922466\n",
      "     -- mean(hitsP)=0.912073, mean(diffsP)=0.963843 mean(hitsA)=0.688922, mean(diffsA)=0.88109\n",
      "96: eta=0.171521 cost=-0.00862558 jtype=constrained costheta=-0.106 ps=[0.0856464, -1.32541, -0.43649, -0.324493, 0.566262, 0.713551, 0.0675032, 0.0859791, 0.249984]\n",
      "     -- cost=-0.00862666,   cost1=0.000597976, cost2=-0.00922464\n",
      "     -- mean(hitsP)=0.912044, mean(diffsP)=0.9638 mean(hitsA)=0.688951, mean(diffsA)=0.881127\n",
      "97: eta=0.188673 cost=-0.00862666 jtype=constrained costheta=-0.118 ps=[0.0848084, -1.32588, -0.437145, -0.325098, 0.566985, 0.713337, 0.0647373, 0.0859525, 0.249988]\n",
      "     -- cost=-0.00862765,   cost1=0.000597036, cost2=-0.00922469\n",
      "     -- mean(hitsP)=0.912021, mean(diffsP)=0.963772 mean(hitsA)=0.688975, mean(diffsA)=0.881165\n",
      "98: eta=0.20754 cost=-0.00862765 jtype=constrained costheta=-0.122 ps=[0.0843363, -1.32619, -0.437482, -0.325373, 0.567423, 0.71314, 0.062192, 0.0859317, 0.249991]\n",
      "     -- cost=-0.00862854,   cost1=0.000596243, cost2=-0.00922478\n",
      "     -- mean(hitsP)=0.912001, mean(diffsP)=0.963754 mean(hitsA)=0.688996, mean(diffsA)=0.881202\n",
      "99: eta=0.228294 cost=-0.00862854 jtype=constrained costheta=-0.115 ps=[0.0841181, -1.32637, -0.437601, -0.325426, 0.567659, 0.712962, 0.059894, 0.0859155, 0.249993]\n",
      "     -- cost=-0.00862932,   cost1=0.000595572, cost2=-0.00922489\n",
      "     -- mean(hitsP)=0.911986, mean(diffsP)=0.963742 mean(hitsA)=0.689013, mean(diffsA)=0.881236\n",
      "100: eta=0.251123 cost=-0.00862932 jtype=constrained costheta=-0.099 ps=[0.0840434, -1.32648, -0.437601, -0.325361, 0.567778, 0.712805, 0.0578615, 0.0859027, 0.249995]\n",
      "     -- cost=-0.00862999,   cost1=0.000595005, cost2=-0.009225\n",
      "     -- mean(hitsP)=0.911973, mean(diffsP)=0.963734 mean(hitsA)=0.689027, mean(diffsA)=0.881265\n",
      "101: eta=0.276236 cost=-0.00862999 jtype=constrained costheta=-0.081 ps=[0.0840309, -1.32656, -0.437555, -0.325256, 0.567841, 0.71267, 0.0561033, 0.0858921, 0.249996]\n",
      "     -- cost=-0.00863056,   cost1=0.000594533, cost2=-0.00922509\n",
      "     -- mean(hitsP)=0.911962, mean(diffsP)=0.963727 mean(hitsA)=0.689039, mean(diffsA)=0.88129\n",
      "102: eta=0.303859 cost=-0.00863056 jtype=constrained costheta=-0.065 ps=[0.0840367, -1.32661, -0.437501, -0.325152, 0.567881, 0.712556, 0.054619, 0.0858834, 0.249997]\n",
      "     -- cost=-0.00863102,   cost1=0.000594146, cost2=-0.00922517\n",
      "     -- mean(hitsP)=0.911954, mean(diffsP)=0.963722 mean(hitsA)=0.689049, mean(diffsA)=0.881311\n",
      "103: eta=0.334245 cost=-0.00863102 jtype=constrained costheta=-0.052 ps=[0.0840443, -1.32665, -0.437454, -0.325065, 0.567911, 0.712463, 0.0533991, 0.0858763, 0.249998]\n",
      "     -- cost=-0.00863139,   cost1=0.00059384, cost2=-0.00922523\n",
      "     -- mean(hitsP)=0.911947, mean(diffsP)=0.963718 mean(hitsA)=0.689056, mean(diffsA)=0.881327\n",
      "104: eta=0.36767 cost=-0.00863139 jtype=constrained costheta=-0.042 ps=[0.08405, -1.32669, -0.437418, -0.324995, 0.567936, 0.71239, 0.0524255, 0.0858706, 0.249999]\n",
      "     -- cost=-0.00863167,   cost1=0.000593604, cost2=-0.00922528\n",
      "     -- mean(hitsP)=0.911942, mean(diffsP)=0.963716 mean(hitsA)=0.689062, mean(diffsA)=0.88134\n",
      "105: eta=0.404437 cost=-0.00863167 jtype=constrained costheta=-0.034 ps=[0.0840537, -1.32671, -0.43739, -0.324943, 0.567955, 0.712334, 0.0516731, 0.0858662, 0.249999]\n",
      "     -- cost=-0.00863188,   cost1=0.00059343, cost2=-0.00922531\n",
      "     -- mean(hitsP)=0.911938, mean(diffsP)=0.963714 mean(hitsA)=0.689067, mean(diffsA)=0.881349\n",
      "106: eta=0.44488 cost=-0.00863188 jtype=constrained costheta=-0.027 ps=[0.0840561, -1.32673, -0.43737, -0.324905, 0.567969, 0.712292, 0.0511119, 0.0858629, 0.249999]\n",
      "     -- cost=-0.00863204,   cost1=0.000593306, cost2=-0.00922534\n",
      "     -- mean(hitsP)=0.911936, mean(diffsP)=0.963712 mean(hitsA)=0.68907, mean(diffsA)=0.881356\n",
      "107: eta=0.489368 cost=-0.00863204 jtype=constrained costheta=-0.022 ps=[0.0840575, -1.32675, -0.437356, -0.324878, 0.56798, 0.712263, 0.0507093, 0.0858606, 0.25]\n",
      "     -- cost=-0.00863214,   cost1=0.000593222, cost2=-0.00922536\n",
      "     -- mean(hitsP)=0.911934, mean(diffsP)=0.963712 mean(hitsA)=0.689072, mean(diffsA)=0.881361\n",
      "108: eta=0.538305 cost=-0.00863214 jtype=constrained costheta=-0.017 ps=[0.0840585, -1.32676, -0.437347, -0.32486, 0.567987, 0.712243, 0.0504326, 0.0858591, 0.25]\n",
      "     -- cost=-0.00863221,   cost1=0.000593168, cost2=-0.00922538\n",
      "     -- mean(hitsP)=0.911933, mean(diffsP)=0.963711 mean(hitsA)=0.689073, mean(diffsA)=0.881364\n",
      "109: eta=0.592136 cost=-0.00863221 jtype=constrained costheta=-0.014 ps=[0.084059, -1.32676, -0.437341, -0.324848, 0.567991, 0.712231, 0.0502511, 0.0858581, 0.25]\n",
      "     -- cost=-0.00863225,   cost1=0.000593135, cost2=-0.00922539\n",
      "     -- mean(hitsP)=0.911933, mean(diffsP)=0.963711 mean(hitsA)=0.689074, mean(diffsA)=0.881366\n",
      "110: eta=0.651349 cost=-0.00863225 jtype=constrained costheta=-0.011 ps=[0.0840594, -1.32677, -0.437337, -0.324841, 0.567994, 0.712223, 0.050138, 0.0858575, 0.25]\n",
      "     -- cost=-0.00863228,   cost1=0.000593117, cost2=-0.00922539\n",
      "     -- mean(hitsP)=0.911933, mean(diffsP)=0.963711 mean(hitsA)=0.689075, mean(diffsA)=0.881367\n",
      "111: eta=0.716484 cost=-0.00863228 jtype=constrained costheta=-0.009 ps=[0.0840595, -1.32677, -0.437335, -0.324838, 0.567995, 0.712219, 0.0500714, 0.0858572, 0.25]\n",
      "     -- cost=-0.00863225,   cost1=0.000593141, cost2=-0.00922539\n",
      "     -- mean(hitsP)=0.911933, mean(diffsP)=0.963711 mean(hitsA)=0.689074, mean(diffsA)=0.881366\n",
      "eta going down: new_cost-cost=2.85544e-08 and jumptype='constrained'\n",
      "112: eta=0.358242 cost=-0.00863228 jtype=constrained costheta=NaN ps=[0.0840595, -1.32677, -0.437335, -0.324838, 0.567995, 0.712219, 0.0500714, 0.0858572, 0.25]\n",
      "     -- cost=-0.00863228,   cost1=0.000593114, cost2=-0.0092254\n",
      "     -- mean(hitsP)=0.911933, mean(diffsP)=0.963712 mean(hitsA)=0.689075, mean(diffsA)=0.881368\n",
      "113: eta=0.394066 cost=-0.00863228 jtype=constrained costheta=-0.007 ps=[0.0840595, -1.32677, -0.437335, -0.324838, 0.567995, 0.712219, 0.0500497, 0.0858572, 0.25]\n",
      "     -- cost=-0.00863227,   cost1=0.000593122, cost2=-0.0092254\n",
      "     -- mean(hitsP)=0.911933, mean(diffsP)=0.963712 mean(hitsA)=0.689075, mean(diffsA)=0.881367\n",
      "eta going down: new_cost-cost=9.14934e-09 and jumptype='constrained'\n",
      "114: eta=0.197033 cost=-0.00863228 jtype=constrained costheta=NaN ps=[0.0840595, -1.32677, -0.437335, -0.324838, 0.567995, 0.712219, 0.0500497, 0.0858572, 0.25]\n",
      "     -- cost=-0.00863229,   cost1=0.000593112, cost2=-0.0092254\n",
      "     -- mean(hitsP)=0.911933, mean(diffsP)=0.963712 mean(hitsA)=0.689075, mean(diffsA)=0.881368\n",
      "115: eta=0.216736 cost=-0.00863229 jtype=constrained costheta=-0.028 ps=[0.0840596, -1.32677, -0.437335, -0.324837, 0.567996, 0.712219, 0.0500408, 0.0858571, 0.25]\n",
      "     -- cost=-0.00863229,   cost1=0.000593109, cost2=-0.0092254\n",
      "     -- mean(hitsP)=0.911933, mean(diffsP)=0.963712 mean(hitsA)=0.689075, mean(diffsA)=0.881368\n",
      "116: eta=0.23841 cost=-0.00863229 jtype=constrained costheta=-0.104 ps=[0.0840596, -1.32677, -0.437335, -0.324837, 0.567996, 0.712218, 0.0500327, 0.0858571, 0.25]\n",
      "     -- cost=-0.00863229,   cost1=0.000593107, cost2=-0.0092254\n",
      "     -- mean(hitsP)=0.911933, mean(diffsP)=0.963712 mean(hitsA)=0.689075, mean(diffsA)=0.881368\n",
      "117: eta=0.262251 cost=-0.00863229 jtype=constrained costheta=-0.085 ps=[0.0840596, -1.32677, -0.437334, -0.324836, 0.567996, 0.712217, 0.0500257, 0.085857, 0.25]\n",
      "     -- cost=-0.00863229,   cost1=0.000593105, cost2=-0.0092254\n",
      "     -- mean(hitsP)=0.911933, mean(diffsP)=0.963711 mean(hitsA)=0.689075, mean(diffsA)=0.881368\n",
      "118: eta=0.288476 cost=-0.00863229 jtype=constrained costheta=-0.069 ps=[0.0840596, -1.32677, -0.437334, -0.324836, 0.567996, 0.712217, 0.0500197, 0.085857, 0.25]\n",
      "     -- cost=-0.0086323,   cost1=0.000593103, cost2=-0.0092254\n",
      "     -- mean(hitsP)=0.911933, mean(diffsP)=0.963711 mean(hitsA)=0.689075, mean(diffsA)=0.881369\n",
      "119: eta=0.317324 cost=-0.0086323 jtype=constrained costheta=-0.056 ps=[0.0840597, -1.32677, -0.437334, -0.324835, 0.567996, 0.712217, 0.0500147, 0.085857, 0.25]\n",
      "     -- cost=-0.0086323,   cost1=0.000593102, cost2=-0.0092254\n",
      "     -- mean(hitsP)=0.911933, mean(diffsP)=0.963711 mean(hitsA)=0.689075, mean(diffsA)=0.881369\n",
      "120: eta=0.349056 cost=-0.0086323 jtype=constrained costheta=-0.046 ps=[0.0840597, -1.32677, -0.437334, -0.324835, 0.567996, 0.712216, 0.0500107, 0.0858569, 0.25]\n",
      "     -- cost=-0.0086323,   cost1=0.000593101, cost2=-0.0092254\n",
      "     -- mean(hitsP)=0.911933, mean(diffsP)=0.963711 mean(hitsA)=0.689075, mean(diffsA)=0.881369\n",
      "121: eta=0.383962 cost=-0.0086323 jtype=constrained costheta=-0.037 ps=[0.0840597, -1.32677, -0.437334, -0.324835, 0.567997, 0.712216, 0.0500075, 0.0858569, 0.25]\n",
      "     -- cost=-0.0086323,   cost1=0.0005931, cost2=-0.0092254\n",
      "     -- mean(hitsP)=0.911933, mean(diffsP)=0.963711 mean(hitsA)=0.689075, mean(diffsA)=0.881369\n",
      "About to break -- tol=1e-09, new_cost-cost=-9.09012e-10, eta=0.383962\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1×9 Array{Float64,2}:\n",
       " 0.0840597  -1.32677  -0.437334  -0.324835  …  0.0500075  0.0858569  0.25"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = [\"sW\", \"vW\", \"hW\", \"constant_excitation\", \"right_light_excitation\", \"target_period_excitation\", \"sigma\"]\n",
    "seed = [0.2,   -1.7, -1.7,      0.19,                0.5,                       1,                       0.1]\n",
    "seed = [0.2,   0.17,  0.17,      0.19,                0.5,                       1,                       0.1]\n",
    "seed = [0.2,   -1.7, -1.7,      -0.19,                0.5,                       1,                       0.1]\n",
    "seed = [0.2,   -1.7, -1.7,      0.39,                0.15,                       0.1,                     0.1]\n",
    "bbox = Dict(:sW=>[0 3], :vW=>[-3 3], :hW=>[-3 3], :constant_excitation=>[-2 2],\n",
    ":right_light_excitation=>[0.05 4], :target_period_excitation=>[0.05 4], :sigma=>[0.05 1])\n",
    "model_params = merge(model_params, Dict(:post_target_period=>0.5))\n",
    "\n",
    "\n",
    "# Now with constant_pro_bias and a fixed sigma=0.1\n",
    "args = [\"sW\", \"vW\", \"hW\", \"dW\", \"constant_excitation\", \"right_light_excitation\", \"target_period_excitation\"]\n",
    "seed = [0.2,   -1.7, -1.7,  0,    0.39,                0.15,                       0.1]\n",
    "args = [args ; [\"const_pro_bias\", \"sigma\"]]\n",
    "seed = [seed ; [0.1,               0.1]]\n",
    "model_params = merge(model_params, Dict(:post_target_period=>0.5))\n",
    "bbox = Dict(:sW=>[0 3], :vW=>[-3 3], :hW=>[-3 3], :dW=>[-3 3], :constant_excitation=>[-2 2],\n",
    ":right_light_excitation=>[0.05 4], :target_period_excitation=>[0.05 4], :const_pro_bias=>[-2 2],\n",
    ":sigma=>[0.01 0.25])\n",
    "\n",
    "# seed = [0.0840597,  -1.32677,  -0.437334,  -0.324835,  0.567997, 0.712216,  0.0500075,  0.0858569,  0.25]\n",
    "\n",
    "# ==========\n",
    "\n",
    "nPro=100; nAnti=100\n",
    "\n",
    "rule_and_delay_periods = [0.4, 1.2]\n",
    "post_target_periods    = [0.5, 1.5]\n",
    "\n",
    "pars, traj, cost, cpm_traj = bbox_Hessian_keyword_minimization(seed, args, bbox, \n",
    "(;params...) -> JJ(nPro, nAnti; rule_and_delay_periods=rule_and_delay_periods,\n",
    "post_target_periods=post_target_periods,\n",
    "seedrand=31, cbeta=0.01, verbose=true, merge(model_params, Dict(params))...),\n",
    "start_eta = 0.01, tol=1e-9, verbose=true)\n",
    "\n",
    "pars'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9×2 Array{Any,2}:\n",
       " \"sW\"                         0.0840597\n",
       " \"vW\"                        -1.32677  \n",
       " \"hW\"                        -0.437334 \n",
       " \"dW\"                        -0.324835 \n",
       " \"constant_excitation\"        0.567997 \n",
       " \"right_light_excitation\"     0.712216 \n",
       " \"target_period_excitation\"   0.0500075\n",
       " \"const_pro_bias\"             0.0858569\n",
       " \"sigma\"                      0.25     "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[args pars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17.340804 seconds (33.19 M allocations: 1.907 GB, 2.10% gc time)\n",
      "Pro % correct = 87.1%\n",
      "Anti % correct = 67.2% \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.00115953051519695"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------\n",
    "# NOW EVALUATE RESULTS HERE\n",
    "# --------------------\n",
    "\n",
    "my_params = make_dict([args; \"plot_list\"; \"post_target_period\"; \"rule_and_delay_period\"; \"dt\"], \n",
    "[pars; [[1:10;]]; 1.5; 1; 0.01], model_params)\n",
    "\n",
    "run_factor = 10\n",
    "\n",
    "proVs, antiVs = @time(run_ntrials(nPro*run_factor, nAnti*run_factor; plot_list=[], my_params...))\n",
    "\n",
    "if nPro>0;  @printf(\"Pro %% correct = %g%%\\n\", 100*length(find(proVs[1,:].>proVs[4,:]))/(nPro*run_factor)); end;\n",
    "if nAnti>0; @printf(\"Anti %% correct = %g%% \\n\", 100*length(find(antiVs[1,:].<antiVs[4,:]))/(nAnti*run_factor)); end;\n",
    "\n",
    "\n",
    "figure(3); clf();\n",
    "ax1 = subplot(2,1,1)\n",
    "h = plt[:hist](proVs[1,:]-proVs[4,:],-1:0.02:1)\n",
    "title(\"PRO Vr - Vl\")\n",
    "remove_xtick_labels(ax1)\n",
    "vlines(0, ylim()[1], ylim()[2])\n",
    "\n",
    "ax2 = subplot(2,1,2)\n",
    "h = plt[:hist](antiVs[1,:]-antiVs[4,:],-1:0.02:1)\n",
    "title(\"ANTI Vr - Vl\")\n",
    "vlines(0, ylim()[1], ylim()[2])\n",
    "\n",
    "figure(1); clf(); figure(2); clf();\n",
    "\n",
    "JJ(nPro, nAnti; rule_and_delay_periods=my_params[:rule_and_delay_period], \n",
    "post_target_periods=my_params[:post_target_period], my_params...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of a ProAnti network optimization\n",
    "\n",
    "This one uses two rule periods and two post_target_periods to try to get some stability in the trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: eta=0.01 ps=[0.200, -1.700, -1.700, 0.390, 0.150, 0.100, 0.100]\n",
      "     -- cost=0.0609354,   cost1=0.0702293, cost2=-0.0092939\n",
      "     -- mean(hitsP)=0.574693, mean(diffsP)=0.952638 mean(hitsA)=0.522901, mean(diffsA)=0.906143\n",
      "     -- cost=0.0594435,   cost1=0.0686243, cost2=-0.00918085\n",
      "     -- mean(hitsP)=0.57821, mean(diffsP)=0.941545 mean(hitsA)=0.525837, mean(diffsA)=0.894625\n",
      "1: eta=0.011 cost=0.0594435 jtype=constrained costheta=-0.985 ps=[0.199944, -1.70014, -1.69931, 0.382009, 0.150148, 0.0999991, 0.0947458]\n",
      "     -- cost=0.0586714,   cost1=0.067794, cost2=-0.0091226\n",
      "     -- mean(hitsP)=0.583192, mean(diffsP)=0.934959 mean(hitsA)=0.522194, mean(diffsA)=0.889561\n",
      "2: eta=0.0121 cost=0.0586714 jtype=constrained costheta=-0.428 ps=[0.200019, -1.70054, -1.69829, 0.373021, 0.150443, 0.0999975, 0.0994822]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1×7 Array{Float64,2}:\n",
       " 0.200019  -1.70054  -1.69829  0.373021  0.150443  0.0999975  0.0994822"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = [\"sW\", \"vW\", \"hW\", \"constant_excitation\", \"right_light_excitation\", \"target_period_excitation\", \"sigma\"]\n",
    "seed = [0.2,   -1.7, -1.7,      0.19,                0.5,                       1,                       0.1]\n",
    "seed = [0.2,   0.17,  0.17,      0.19,                0.5,                       1,                       0.1]\n",
    "seed = [0.2,   -1.7, -1.7,      -0.19,                0.5,                       1,                       0.1]\n",
    "seed = [0.2,   -1.7, -1.7,      0.39,                0.15,                       0.1,                       0.1]\n",
    "bbox = Dict(:sW=>[0 3], :vW=>[-3 3], :hW=>[-3 3], :constant_excitation=>[-2 2],\n",
    ":right_light_excitation=>[0.05 4], :target_period_excitation=>[0.05 4], :sigma=>[0.05 1])\n",
    "model_params = merge(model_params, Dict(:post_target_period=>0.5))\n",
    "\n",
    "\n",
    "# Now with constant_pro_bias and a fixed sigma=0.1\n",
    "args = [\"sW\", \"vW\", \"hW\", \"constant_excitation\", \"right_light_excitation\", \"target_period_excitation\", \"const_pro_bias\"]\n",
    "seed = [0.2,   -1.7, -1.7,      0.39,                0.15,                       0.1,                       0.1]\n",
    "model_params = merge(model_params, Dict(:post_target_period=>0.5, :sigma=>0.1))\n",
    "bbox = Dict(:sW=>[0 3], :vW=>[-3 3], :hW=>[-3 3], :constant_excitation=>[-2 2],\n",
    ":right_light_excitation=>[0.05 4], :target_period_excitation=>[0.05 4], :const_pro_bias=>[-2 2])\n",
    "\n",
    "# ==========\n",
    "\n",
    "nPro=100; nAnti=100\n",
    "\n",
    "rule_and_delay_periods = [0.4, 0.8]\n",
    "post_target_periods    = [0.5, 1]\n",
    "\n",
    "pars, traj, cost, cpm_traj = bbox_Hessian_keyword_minimization(seed, args, bbox, \n",
    "(;params...) -> JJ(nPro, nAnti; rule_and_delay_periods=rule_and_delay_periods,\n",
    "post_target_periods=post_target_periods,\n",
    "seedrand=31, cbeta=0.01, verbose=true, merge(model_params, Dict(params))...),\n",
    "start_eta = 0.01, tol=1e-12, verbose=true, maxiter=2)\n",
    "\n",
    "pars'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05867137476232549"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8.740874 seconds (13.19 M allocations: 798.834 MB, 1.98% gc time)\n",
      "Pro % correct = 90.7%\n",
      "Anti % correct = 67.1% \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.0016377281161004287"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------\n",
    "# NOW EVALUATE RESULTS HERE\n",
    "# --------------------\n",
    "\n",
    "my_params = make_dict([args; \"plot_list\"; \"post_target_period\"; \"rule_and_delay_period\"; \"dt\"], \n",
    "[pars; [[1:10;]]; 1.5; 0.5; 0.02], model_params)\n",
    "\n",
    "run_factor = 10\n",
    "\n",
    "proVs, antiVs = @time(run_ntrials(nPro*run_factor, nAnti*run_factor; plot_list=[], my_params...))\n",
    "\n",
    "if nPro>0;  @printf(\"Pro %% correct = %g%%\\n\", 100*length(find(proVs[1,:].>proVs[4,:]))/(nPro*run_factor)); end;\n",
    "if nAnti>0; @printf(\"Anti %% correct = %g%% \\n\", 100*length(find(antiVs[1,:].<antiVs[4,:]))/(nAnti*run_factor)); end;\n",
    "\n",
    "\n",
    "figure(3); clf();\n",
    "ax1 = subplot(2,1,1)\n",
    "h = plt[:hist](proVs[1,:]-proVs[4,:],-1:0.02:1)\n",
    "title(\"PRO Vr - Vl\")\n",
    "remove_xtick_labels(ax1)\n",
    "vlines(0, ylim()[1], ylim()[2])\n",
    "\n",
    "ax2 = subplot(2,1,2)\n",
    "h = plt[:hist](antiVs[1,:]-antiVs[4,:],-1:0.02:1)\n",
    "title(\"ANTI Vr - Vl\")\n",
    "vlines(0, ylim()[1], ylim()[2])\n",
    "\n",
    "figure(1); clf(); figure(2); clf();\n",
    "\n",
    "JJ(nPro, nAnti; rule_and_delay_periods=my_params[:rule_and_delay_period], \n",
    "post_target_periods=my_params[:post_target_period], my_params...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition wallwrap(Any, Any) in module Main at In[134]:18 overwritten at In[138]:18.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'wallwrap :: Tuple{Any,Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition vector_wrap(Any, Any, Any) in module Main at In[134]:44 overwritten at In[138]:44.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'vector_wrap :: Tuple{Any,Any,Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition inverse_wall(Any, Any, Any) in module Main at In[134]:68 overwritten at In[138]:68.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'inverse_wall :: Tuple{Any,Any,Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition inverse_wall(Any, Any) in module Main at In[134]:91 overwritten at In[138]:91.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'inverse_wall :: Tuple{Any,Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition bbox_Hessian_keyword_minimization(Any, Any, Any, Any) in module Main at In[134]:193 overwritten at In[138]:193.\n",
      "WARNING: Method definition #bbox_Hessian_keyword_minimization(Array{Any, 1}, Main.#bbox_Hessian_keyword_minimization, Any, Any, Any, Any) in module Main overwritten.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'bbox_Hessian_keyword_minimization :: Tuple{Any,Any,Any,Any}' in module 'Main'.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bbox_Hessian_keyword_minimization"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################################\n",
    "#                                                    #\n",
    "#         BBOX_HESSIAN_KEYWORD_MINIMIZATION          #\n",
    "#                                                    #\n",
    "######################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "pdict = wallwrap(bdict, pdict)\n",
    "Given bdict, a dictionary of symbols to [minval, maxval] vectors, and pdict, a dictionary of symbols\n",
    "to values (or, alternatively, an Array of (Symbol, value) tuples], goes through each of the symbols in \n",
    "bdict and modifies the corresponding value in pdict putting it through a tanh so the final output lies \n",
    "within the limits in bdict.  Returns the new pdict.  Makes a copy of pdict so as not to modify the original.\n",
    "\"\"\"\n",
    "function wallwrap(bdict, epdict)\n",
    "    local pdict = two_level_copy(epdict)\n",
    "    if typeof(pdict)<:Array\n",
    "        pdict = Dict(pdict)\n",
    "    end\n",
    "\n",
    "    allkeys = keys(bdict)\n",
    "\n",
    "    for k in allkeys\n",
    "        local bbox = bdict[k]\n",
    "        d = 0.5*(bbox[2] - bbox[1])\n",
    "        m = 0.5*(bbox[2] + bbox[1])\n",
    "\n",
    "        pdict[k] = bbox[1] + d*(tanh((pdict[k]-m)/d)+1)\n",
    "    end\n",
    "    return pdict\n",
    "end\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "params = vector_wrap(bbox, args, eparams)\n",
    "Given bdict, a dictionary of symbols to [minval, maxval] vectors, args, an array of strings representing\n",
    "symbols, and params, an array of values corresponding to the args list, puts each param that has an entry \n",
    "in bdict through the tanh-walling mechanism, and returns the result. Does not modify the contents of the \n",
    "original params vector (or bdict or args).\n",
    "\"\"\"\n",
    "function vector_wrap(bbox, args, eparams)\n",
    "    local params = two_level_copy(eparams)\n",
    "    pdict = wallwrap(bbox, make_dict(args, params))\n",
    "    i=1; j=1\n",
    "    for i=1:length(args)\n",
    "        if typeof(args[i])<:Array\n",
    "            params[j:j+args[i][2]-1] = pdict[Symbol(args[i][1])]\n",
    "            j += args[i][2]-1\n",
    "        else\n",
    "            params[j] = pdict[Symbol(args[i])]\n",
    "        end\n",
    "    j = j+1\n",
    "    end\n",
    "    return params\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "params = inverse_wall(bdict, args, wparams)\n",
    "Given bdict, a dictionary of symbols to [minval, maxval] vectors, args, an array of strings representing\n",
    "symbols, and wparams, an array of values corresponding to the args list where each param that has an entry \n",
    "in bdict has alreadt been through the tanh-walling mechanism, UNwalls the ones that have a bdict entry and\n",
    "returns the result. Does not modify the contents of the original params vector (or bdict or args).\n",
    "\"\"\"\n",
    "function inverse_wall(bdict, args, wparams)\n",
    "    local params = two_level_copy(wparams)\n",
    "    pdict = inverse_wall(bdict, make_dict(args, params))\n",
    "    i=1; j=1\n",
    "    for i=1:length(args)\n",
    "        if typeof(args[i])<:Array\n",
    "            params[j:j+args[i][2]-1] = pdict[Symbol(args[i][1])]\n",
    "            j += args[i][2]-1\n",
    "        else\n",
    "            params[j] = pdict[Symbol(args[i])]\n",
    "        end\n",
    "        j = j+1\n",
    "    end\n",
    "    return params    \n",
    "end\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "pdict = inverse_wall(bdict, wdict)\n",
    "Given bdict, a dictionary of symbols to [minval, maxval] vectors, and wdict, a dictionary of symbols to values\n",
    "(or vectors of values)  UNwalls the ones that have a bdict entry and\n",
    "returns the result. Does not modify the contents of any dictionaries.\n",
    "\"\"\"\n",
    "function inverse_wall(bdict, wdict)\n",
    "    local pdict = two_level_copy(wdict)\n",
    "\n",
    "    allkeys = keys(bdict)\n",
    "    for k in allkeys\n",
    "        local bbox = bdict[k]\n",
    "        d = 0.5*(bbox[2] - bbox[1])\n",
    "        m = 0.5*(bbox[2] + bbox[1])\n",
    "\n",
    "        try\n",
    "            pdict[k] = m + d*0.5*log((pdict[k]-bbox[1])./(2*d - pdict[k] + bbox[1]))\n",
    "        catch\n",
    "            error(@sprintf(\"Had trouble with key %s\", string(k)))\n",
    "        end\n",
    "    end\n",
    "    return(pdict)\n",
    "end\n",
    "  \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "function bbox_Hessian_keyword_minimization(seed, args, bbox, func; wallwidth=NaN, start_eta=10, tol=1e-6, \n",
    "    maxiter=400, verbose=false)\n",
    "\n",
    "Like constrained_Hessian_minimization, but uses keyword_hessian!(). \n",
    "\n",
    "# PARAMETERS:\n",
    "\n",
    "- seed        column vector, representing the starting value of the parameters.\n",
    "\n",
    "- args        List of strings identifying parameters for differentiation, e.g., [\"const_E\", \"w_self]\n",
    "\n",
    "- bbox        If softbox=true (the default), should then be a Dict of Symbol=>[minval maxval] entries. An entry\n",
    "            in this Dict indicates that the corresponding parameter is to be bounded, as indicated by the associated \n",
    "            [minval maxval] vector. The bbox dictionary can have fewer entries than the number of parameters, and its\n",
    "            default value is Dict(), indicating an unbounded search.\n",
    "                If softbox=false, then bbox should be an nargs-by-2 matrix indicating the range for each argument,\n",
    "            with the minima (first column) and maxima (second column), and entries for ALL parameters.\n",
    "\n",
    "- func        func must take only optional keyword args, and must \n",
    "            take nderivs=0, difforder=0  and declare any new matrices using ForwardDiffZeros() instead of zeros()\n",
    "\n",
    "\n",
    "# OPTIONAL PARAMETERS:\n",
    "\n",
    "- start_eta    Starting value of the radius.  It's good to start with somethibg biggish, if it is\n",
    "             too much, it'll quickly get cut down.\n",
    "\n",
    "- tol=1e-6     Numerical tolerance. If a proposed jump produces a change in func that is less than\n",
    "             this, the minimization stops.\n",
    "\n",
    "- maxiter=400  Maximum number of iterations to do before stopping\n",
    "\n",
    "- verbose=false   If true, print out a report on each iteration of iteration number, radius size (eta),\n",
    "                what type jump was proposed (\"Newton\" means going straight to global min, \"constrained\" means jump has \n",
    "                norm eta, failed means that finding the minimum at a given radius somehow didn't work). Will also\n",
    "                print out the cosine of the angle between the proposed jump and the gradient.\n",
    "\n",
    "- verbose_level   If less than 2, regular verbose output, if 2 or greater, very verbose, for debugging.\n",
    "\n",
    "- softbox         If true, then bbox must be a Dict() and we use the tanh() mechanism for putting a fixed limit\n",
    "                on the parameters.\n",
    "\n",
    "- hardbox=false   If true, ignores wallwidth, and just rests parameter values to the bounding box if they go outside it.\n",
    "                If false, adds cost function \"walls\" to implement the bounding box.\n",
    "\n",
    "- walldith=NaN     Used for putting up cost function \"walls\" that implement the bounding box limits. Can be NaN.\n",
    "                If it is NaN, then the wallwidth is a constant factor of the range width for each argument. If not NaN, must\n",
    "                be an nargs-long vector that indicates the actual wall widths.\n",
    "\n",
    "- wallwidth_factor=0.18   Only relevant if wallwidth is NaN, otherwise ignored. For each arg, the wall width\n",
    "                is going to be wall_width_factor*(bbox[i,2] - bbox[i,1])\n",
    "\n",
    "\n",
    "# RETURNS:\n",
    "\n",
    "- params       A vector the size of seed that has the last values of the minimizing parameters for func\n",
    "- trajectory   A (2+length(params))-by-nsteps matrix. Each column corresponds to an iteration step, and contains\n",
    "                 the value of eta used, the cost, and the value of the parameters at that iteration\n",
    "- cost         Final value of objective function\n",
    "- cpm_traj     A 2-by-nsteps matrix, containing reports from the contrained parabolic minimization at each timestep.\n",
    "             The first row is niters (how many iterations cpm's 1-d minimization ran for) and the second row is\n",
    "             Dlambda, the last change in the parameter being minimized in cpm's internal search\n",
    "\n",
    "\n",
    "# EXAMPLE:\n",
    "\n",
    "```\n",
    "function tester(;x=5, y=10, z=20, nderivs=0, difforder=0)\n",
    "    return x^2*y + z/tanh(y)\n",
    "end\n",
    "\n",
    "params, trajectory = bbox_Hessian_keyword_minimization([0.5, 0.5], [\"x\", \"y\"], [1.1 2 ; 1.1 4], tester, \n",
    "    verbose=true, tol=1e-12, start_eta=1);\n",
    "```\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "function bbox_Hessian_keyword_minimization(seed, args, bbox, func; start_eta=0.1, tol=1e-6, maxiter=400,\n",
    "    verbose=false, verbose_level=1, verbose_every=1, \n",
    "    softbox=true, hardbox=false, wallwidth=NaN, wallwidth_factor=0.18)\n",
    "\n",
    "      \n",
    "    \"\"\"\n",
    "    Given args, a list of string representing the arguments of interest, a bounding box for each,\n",
    "    and a Symbol=>value dictionary with the corresponding parameters, computes and returns a high cost for \n",
    "    being outside the bounding box\n",
    "    \"\"\"\n",
    "    function wall_cost(args, bbox; wallwidth=NaN, nderivs=0, difforder=0, pars...) \n",
    "        myparams = ForwardDiffZeros(length(pars), 1, nderivs=nderivs, difforder=difforder)\n",
    "        pars2 = Dict()\n",
    "        for i in [1:length(pars);]\n",
    "            pars2[string(pars[i][1])] = pars[i][2]\n",
    "        end\n",
    "        for i in [1:length(args);]\n",
    "            myparams[i] = pars2[args[i]]\n",
    "        end\n",
    "        \n",
    "        if isnan(wallwidth)\n",
    "            # We know that we're going to be taking hessian for params, so declare zeros accordingly:\n",
    "            wallwidth = ForwardDiffZeros(length(myparams), 1, nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "            for i in [1:length(myparams);]\n",
    "                wallwidth[i] = wallwidth_factor*(bbox[i,2]-bbox[i,1])\n",
    "            end\n",
    "        end\n",
    "\n",
    "        retval = 0\n",
    "        for i in [1:length(myparams);]\n",
    "            if myparams[i]<bbox[i,1]\n",
    "                retval += cosh((bbox[i,1]-myparams[i])/wallwidth[i])-1.0\n",
    "            elseif bbox[i,2] < myparams[i]\n",
    "                retval += cosh((myparams[i]-bbox[i,2])/wallwidth[i])-1.0                \n",
    "            end\n",
    "        end\n",
    "\n",
    "        return 2*retval\n",
    "    end\n",
    "\n",
    "    traj_increment = 100\n",
    "    params = 0  # Make sure to have this here so that params stays defined beyond the try/catch\n",
    "    if ( !(typeof(bbox)<:Dict) ); error(\"Currently only supporting softbox=true, bbox must be a Dict\"); end;\n",
    "    try\n",
    "        params = copy(inverse_wall(bbox, args, seed))\n",
    "    catch y\n",
    "        @printf(\"inverse_wall failed with error %s\\n\", y)\n",
    "        error(\"Were all initial param values within the indicated walls?\")\n",
    "    end\n",
    "    eta = start_eta\n",
    "    trajectory = zeros(2+length(params), traj_increment); cpm_traj = zeros(2, traj_increment)\n",
    "\n",
    "    if verbose\n",
    "        @printf \"%d: eta=%g ps=\" 0 eta \n",
    "        print_vector(vector_wrap(bbox, args, params))\n",
    "        @printf \"\\n\"\n",
    "    end\n",
    "    \n",
    "    if softbox\n",
    "        if !(typeof(bbox)<:Dict); error(\"bhm: If softbox=true, then bbox must eb a Dict\"); end\n",
    "        cost, grad, hess = keyword_vgh((;pars...)->func(;wallwrap(bbox, pars)...), args, params)\n",
    "    elseif hardbox\n",
    "        cost, grad, hess = keyword_vgh((;pars...) -> func(;pars...), args, params)\n",
    "    else\n",
    "        cost, grad, hess = keyword_vgh((;pars...) -> func(;pars...) + wall_cost(args, bbox; wallwidth=wallwidth, pars...),\n",
    "            args, params)        \n",
    "    end\n",
    "        \n",
    "    chessdelta = zeros(size(params))\n",
    "    \n",
    "    i=0  # here so variable i is available outside the loop\n",
    "    for i in [1:maxiter;]\n",
    "        if i > size(trajectory, 2)\n",
    "            trajectory = [trajectory zeros(2+length(params), traj_increment)]\n",
    "            cpm_traj   = [cpm_traj   zeros(2, traj_increment)]\n",
    "        end\n",
    "        trajectory[1:2, i]   = [eta;cost]\n",
    "        trajectory[3:end, i] = vector_wrap(bbox, args, params)\n",
    "        \n",
    "        hessdelta  = - inv(hess)*grad\n",
    "        try\n",
    "            if verbose && verbose_level >= 2\n",
    "                @printf(\"bhm: about to try cpm with grad : \"); print_vector_g(grad); print(\"\\n\")\n",
    "                @printf(\"bhm:   hess :\"); print_vector_g(hess[:]); print(\"\\n\");\n",
    "            end\n",
    "            if verbose && verbose_level >= 2\n",
    "                cpm_out = constrained_parabolic_minimization(hess, grad'', eta, \n",
    "                    maxiter=500, tol=1e-20, do_plot=true, verbose=true)                \n",
    "            else\n",
    "                cpm_out = constrained_parabolic_minimization(hess, grad'', eta, maxiter=500, tol=1e-20)\n",
    "            end\n",
    "            chessdelta = cpm_out[1]; cpm_traj[1,i] = cpm_out[5]; cpm_traj[2,i] = cpm_out[6]\n",
    "            jumptype = \"not failed\"\n",
    "        catch y\n",
    "            jumptype = \"failed\"\n",
    "            if verbose\n",
    "                @printf \"Constrained parabolic minimization failed with error %s\\n\" y\n",
    "                @printf \"\\n\"\n",
    "                @printf \"eta was %g\\n\" eta\n",
    "                @printf \"grad was\\n\"\n",
    "                print_vector(grad)\n",
    "                @printf \"\\n\\nhess was\\n\"\n",
    "                for k in [1:length(grad);]\n",
    "                    print_vector(hess[k,:])\n",
    "                    @printf \"\\n\"\n",
    "                end\n",
    "                @printf \"\\n\"\n",
    "                matwrite(\"error_report.mat\", Dict(\"grad\"=>grad, \"hess\"=>hess, \"eta\"=>eta))\n",
    "            end\n",
    "            break\n",
    "        end\n",
    "\n",
    "        if norm(hessdelta) <= eta\n",
    "            new_params = params + hessdelta\n",
    "            jumptype = \"Newton\"\n",
    "        elseif jumptype != \"failed\" \n",
    "            new_params = params + chessdelta\n",
    "            jumptype  = \"constrained\"\n",
    "        end\n",
    "\n",
    "        if jumptype != \"failed\"\n",
    "            if softbox\n",
    "                new_cost, new_grad, new_hess = \n",
    "                    keyword_vgh((;pars...) -> func(;wallwrap(bbox, pars)...), args, new_params)\n",
    "                if verbose && verbose_level >=2\n",
    "                    @printf(\"bhm: had new_params = : \"); print_vector_g(vector_wrap(bbox, args, params)); print(\"\\n\");\n",
    "                    @printf(\"bhm: and my bbox was : \"); print(bbox); print(\"\\n\")\n",
    "                    @printf(\"bhm: and my wallwrap output was : \"); print(wallwrap(bbox, make_dict(args, new_params))); print(\"\\n\")\n",
    "                    @printf(\"bhm: and this produced new_grad : \"); print_vector_g(new_grad); print(\"\\n\")\n",
    "                    @printf(\"bhm:   new_hess :\"); print_vector_g(new_hess[:]); print(\"\\n\");                                        \n",
    "                end\n",
    "            elseif hardbox\n",
    "                for p in [1:length(new_params);]\n",
    "                    if new_params[p] < bbox[p,1]; new_params[p] = bbox[p,1]; end\n",
    "                    if bbox[p,2] < new_params[p]; new_params[p] = bbox[p,2]; end\n",
    "                 end        \n",
    "                \n",
    "                new_cost, new_grad, new_hess = keyword_vgh((;pars...) -> func(;pars...), args, new_params)\n",
    "            else\n",
    "                new_cost, new_grad, new_hess = keyword_vgh((;pars...) -> func(;pars...) + \n",
    "                        wall_cost(args, bbox; wallwidth=wallwidth, pars...),\n",
    "                    args, new_params)                \n",
    "            end\n",
    "            \n",
    "            if abs(new_cost - cost) < tol || eta < tol\n",
    "                if verbose\n",
    "                    @printf(\"About to break -- tol=%g, new_cost-cost=%g, eta=%g\\n\", tol, new_cost-cost, eta)\n",
    "                end\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "\n",
    "        if jumptype == \"failed\" || new_cost >= cost  \n",
    "            if verbose\n",
    "                @printf(\"eta going down: new_cost-cost=%g and jumptype='%s'\\n\", new_cost-cost, jumptype)\n",
    "                if verbose_level >= 2\n",
    "                    nwp = vector_wrap(bbox, args, new_params); wp = vector_wrap(bbox, args, params)\n",
    "                    @printf(\"   vvv: proposed new params were : \"); print_vector_g(nwp); print(\"\\n\")\n",
    "                    @printf(\"   vvv: proposed delta params was : \"); print_vector_g(nwp-wp); print(\"\\n\")\n",
    "                    @printf(\"   vvv: grad was : \"); print_vector_g(grad); print(\"\\n\")\n",
    "                    costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "                    @printf(\"   vvv: costheta of proposed jump was %g\\n\", costheta)\n",
    "                end\n",
    "            end\n",
    "            eta = eta/2\n",
    "            costheta = NaN\n",
    "            if eta < tol\n",
    "                if verbose\n",
    "                    @printf(\"About to break -- tol=%g, new_cost-cost=%g, eta=%g\\n\", tol, new_cost-cost, eta)\n",
    "                end\n",
    "                break\n",
    "            end\n",
    "        else\n",
    "            eta = eta*1.1\n",
    "            costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "\n",
    "            params = new_params\n",
    "            cost = new_cost\n",
    "            grad = new_grad\n",
    "            hess = new_hess\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            if rem(i, verbose_every)==0\n",
    "                @printf \"%d: eta=%g cost=%g jtype=%s costheta=%.3f ps=\" i eta cost jumptype costheta\n",
    "                print_vector_g(vector_wrap(bbox, args, params))\n",
    "                @printf \"\\n\"\n",
    "                if verbose_level >= 3\n",
    "                    @printf \"    At this point, grad is =\"\n",
    "                    print_vector_g(grad)\n",
    "                    @printf \"\\n\"                \n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    trajectory = trajectory[:,1:i]; cpm_traj = cpm_traj[:,1:i]\n",
    "    return vector_wrap(bbox, args, params), trajectory, cost, cpm_traj\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bbox_Hessian_keyword_minimization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the distribution of VR - VL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ntrials = 500\n",
    "proVs, antiVs = @time(run_ntrials(ntrials; plot_list=[], model_params...))\n",
    "\n",
    "@printf(\"Pro %% correct = %g%%\\n\", 100*length(find(proVs[1,:].>proVs[4,:]))/ntrials)\n",
    "@printf(\"Anti %% correct = %g%% \\n\", 100*length(find(antiVs[1,:].<antiVs[4,:]))/ntrials)\n",
    "\n",
    "figure(1); clf();\n",
    "ax1 = subplot(2,1,1)\n",
    "h = plt[:hist](proVs[1,:]-proVs[4,:],-0.1:0.002:0.1)\n",
    "title(\"PRO Vr - Vl\")\n",
    "remove_xtick_labels(ax1)\n",
    "vlines(0, ylim()[1], ylim()[2])\n",
    "\n",
    "ax2 = subplot(2,1,2)\n",
    "h = plt[:hist](antiVs[1,:]-antiVs[4,:],-0.1:0.002:0.1)\n",
    "title(\"ANTI Vr - Vl\")\n",
    "vlines(0, ylim()[1], ylim()[2])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.2",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
