{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION\n",
    "\n",
    "This notebook tries to take what was learned from the MGO and apply it to the ProAnti model.  First step is trying to find the Marino parameters with big tau and dt.  \n",
    "\n",
    "Currently (20-July-2017) instead trying to start off from Marino's model_gradient_kwargs_fast.jl.  See \"Marino Fast Model.ipynb\"\n",
    "\n",
    "# ==========\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using PyCall\n",
    "using PyPlot\n",
    "using ForwardDiff\n",
    "using DiffBase\n",
    "\n",
    "pygui(true)\n",
    "\n",
    "import Base.convert\n",
    "convert(::Type{Float64}, x::ForwardDiff.Dual) = Float64(x.value)\n",
    "function convert(::Array{Float64}, x::Array{ForwardDiff.Dual}) \n",
    "    y = zeros(size(x)); \n",
    "    for i in 1:prod(size(x)) \n",
    "        y[i] = convert(Float64, x[i]) \n",
    "    end\n",
    "    return y\n",
    "end\n",
    "\n",
    "include(\"hessian_utils.jl\")\n",
    "\n",
    "\"\"\"\n",
    "We define functions to convert Duals, the variable types used by ForwardDiff, \n",
    "to Floats. This is useful if we want to print out the value of a variable \n",
    "(since print doesn't know how to Duals). Note that after being converted to a Float, no\n",
    "differentiation by ForwardDiff can happen!  e.g. after\n",
    "    x = convert(Float64, y)\n",
    "ForwardDiff can still differentiate y, but it can't differentiate x\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The main model dynamics function\n",
    "\n",
    "Note that the documenation indicates some default values for the optional parameters; these values need to be updated to what the code actually says below. The actual defaults are much closer to what Marino's May 2017 model has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "function t, U, V, W = run_dynamics(trial_type; kw_params)\n",
    "\n",
    "    Runs the 4-way mutual inhibition model\n",
    "    \n",
    "    OBLIGATORY PARAMS:\n",
    "    ------------------\n",
    "\n",
    "        trial_type    Must be either \"pro\" or \"anti\"\n",
    "\n",
    "\n",
    "    OPTIONAL PARAMS:\n",
    "    ----------------\n",
    "    \n",
    "        vwi = 1.5       Weight between ProContra and AntiIpsi units (on each side of the brain)\n",
    "        hwi = 1.5       Weight between ProContra units across the brain; also between AntiContra\n",
    "        const_pro_bias = 0.03     Extra positive input to the two Pro units\n",
    "        const_E = 0.19                Constant positive input to all four units\n",
    "        U_rest = -1     Resting point for U in the absance of other inputs\n",
    "        g_leak = 0.5    Mutliplies (U_rest - U) for the dynamics\n",
    "        theta = 1       Threshold on U for sigmoidal transform from U to V \n",
    "        beta  = 1       Scaling on sigmoid going from U to V:   V = 0.5*tanh((U-theta)/beta) + 0.5\n",
    "        dt=0.02         Timestep\n",
    "        sigma=0.1,      added standard deviation on U per unit time\n",
    "        rule_period = 0.5       in seconds\n",
    "        delay_period = 0.5      in seconds. Opto will happen during this period\n",
    "        target_period = 0.1     right_light_input and target_extra_E will happen during this period; \n",
    "                                Pro v Anti input will be turned off; \n",
    "        post_target_period = 0.5  target_extra_E will still be on, but right_side_input won't\n",
    "        tau=4.4         Time constant of dynamics, in millisecs\n",
    "        marino_tau = true   If True, tau applies only to leak term; if False, applies to entire dUdt\n",
    "        start_U = [-7, -7, -7, -7]\n",
    "        const_E = 0.15  Constant excitation added to all units\n",
    "        right_light_input=1     Extra excitation to right side of the brain units during the target period\n",
    "        right_light_pro_extra   Even further excitation added to pro Right side units during target\n",
    "        pro_self_ex = 0   Self excitation weight of Pro units\n",
    "        anti_self_ex = 0  Self excitation weight of Anti units\n",
    "        pfc_anti_input = 0.05    Input to Anti units during rule and delay periods\n",
    "        pfc_pro_input = np.nan   Input to Pro units during rule and delay periods (default means same as Anti)\n",
    "        const_pro_bias = 0       A constant extra input to the Pro units\n",
    "        target_extra_E = 0.25    Extra excitation added to all units during target and post_target periods\n",
    "        opto='off'    Whether there is optogenetic-induced scaling of outputs.\n",
    "            ='on'     Opto will be done during the delay period only\n",
    "            ='dt'     Opto will be done during the delay plus the target period\n",
    "           opto_scaling=0.5      Factor by which to scale the weight matrix during opto \n",
    "           opto_scale_on_E=1     Factor by which to scale the constant excitation during opto\n",
    "           opto_conductance=0    How much conductance to add to gleak during opto\n",
    "           opto_current=0        Added to dUdt at each time step during opto\n",
    "        unilateral_opto = False If True, then opto_scale_on_E will be forced to 0, and opto_scaling will apply\n",
    "                                to only one side\n",
    "        do_plot = True  whether or not to plot the results\n",
    "        fignum=1        figure on which to plot\n",
    "        decision_threshold   If |V(Pro_R) - V(Pro_L)| >= this number, a proper answer is produced. \n",
    "                The target light is presented to the right, so Right means \"pro\"\n",
    "\n",
    "\n",
    "    RETURNS:\n",
    "    --------\n",
    "\n",
    "        response     +1 for Pro, -1 for Anti, 0 for undefined \n",
    "                        if |V(Pro_R) - V(Pro_L)| < decision_threshold\n",
    "        t    Time vector\n",
    "        U   U matrix, size 4-by-len(t). Order is ProContra on right side, AntiIpsi on right\n",
    "                    side, ProContra on left side, AntiIpsi on left.\n",
    "        V   V matrix = 0.5*np.tanh((U-theta)/beta) + 0.5\n",
    "        W   Weight matrix between units\n",
    "        \n",
    "\"\"\"\n",
    "function run_dynamics(trial_type; opto=\"off\", opto_scaling=0.8, opto_scale_on_E=1,\n",
    "    vwi = 1.5, hwi = 1.5, const_pro_bias=0.03, const_E=0.19,\n",
    "    opto_conductance = 0, opto_current=0, \n",
    "    right_light_pro_extra = 0, right_light_input=1, \n",
    "    pro_self_ex = 0.7, anti_self_ex = 0.7, \n",
    "    tau=0.1, marino_tau = false, dt=0.02, target_extra_E = 0.25,\n",
    "    pfc_anti_input = 0.15, pfc_pro_input = 0.15,\n",
    "    sigma=0.1, start_U = [-1, -1, -1, -1], do_plot = false, fignum=1, plot_V_only=false,\n",
    "    g_leak = 0.25, U_rest = -1, theta = 1, beta = 1,\n",
    "    rule_period = 0.1, delay_period = 1, target_period = 0.1,\n",
    "    post_target_period = 1, decision_threshold = 0.3, nderivs=0, difforder=0)\n",
    "    \n",
    "\n",
    "    t = [0 : dt : rule_period + delay_period + target_period + post_target_period;] \n",
    "\n",
    "    V = ForwardDiffZeros(4, length(t), nderivs=nderivs, difforder=difforder)\n",
    "    U = ForwardDiffZeros(4, length(t), nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    U[:,1] = start_U\n",
    "    V[:,1] = 0.5*tanh((U[:,1]-theta)/beta) + 0.5\n",
    "\n",
    "    W = [pro_self_ex -vwi -hwi 0; -vwi anti_self_ex 0 -hwi; \n",
    "        -hwi 0 pro_self_ex -vwi; 0 -hwi -vwi anti_self_ex]\n",
    "\n",
    "    E = const_E\n",
    "    \n",
    "    for i in [2:length(t);]  # the funny semicolon appears to be necessary in Julia\n",
    "        if marino_tau\n",
    "            dUdt = E + W * V[:,i-1] + g_leak*(U_rest - U[:,i-1])/tau\n",
    "        else\n",
    "            dUdt = E + W * V[:,i-1] + g_leak*(U_rest - U[:,i-1])\n",
    "        end\n",
    "    \n",
    "        if t[i] < rule_period + delay_period\n",
    "            if trial_type==\"anti\"\n",
    "                dUdt[[2,4]] += pfc_anti_input\n",
    "            elseif trial_type == \"pro\"\n",
    "                dUdt[[1,3]] += pfc_pro_input\n",
    "            else\n",
    "            end\n",
    "            \n",
    "        elseif t[i] < rule_period + delay_period + target_period\n",
    "            dUdt[[1,2]] += right_light_input\n",
    "            dUdt[1]     += right_light_pro_extra\n",
    "            dUdt        += target_extra_E\n",
    "        else\n",
    "            dUdt        += target_extra_E\n",
    "        end\n",
    "    \n",
    "        dUdt[[1,3]] += const_pro_bias\n",
    "        \n",
    "        if marino_tau\n",
    "            try\n",
    "                U[:,i] = U[:,i-1] +       dt*dUdt + sigma*randn(4)*sqrt(dt)\n",
    "            catch\n",
    "                @printf \"yep\\n\"\n",
    "            end\n",
    "        else\n",
    "            U[:,i] = U[:,i-1] + (dt/tau)*dUdt + sigma*randn(4)*sqrt(dt)\n",
    "        end\n",
    "    \n",
    "        V[:,i] = 0.5*tanh((U[:,i]-theta)/beta) + 0.5\n",
    "    end    \n",
    "\n",
    "    if do_plot\n",
    "        figure(fignum); \n",
    "        if !plot_V_only\n",
    "            subplot(3,1,1)\n",
    "        end\n",
    "        h = plot(t, V'); \n",
    "        setp(h[1], color=[0, 0, 1])\n",
    "        setp(h[2], color=[1, 0, 0])\n",
    "        setp(h[3], color=[1, 0.5, 0.5])\n",
    "        setp(h[4], color=[0, 1, 1])\n",
    "        ylabel(\"V\")\n",
    "\n",
    "        ax = gca()\n",
    "        yl = [ylim()[1], ylim()[2]]\n",
    "        vlines([rule_period, rule_period+delay_period, \n",
    "            rule_period+delay_period+target_period], \n",
    "            -0.05, 1.05, linewidth=2)\n",
    "        if yl[1]<0.02\n",
    "            yl[1] = -0.02\n",
    "        end\n",
    "        if yl[2]>0.98\n",
    "            yl[2] = 1.02\n",
    "        end\n",
    "        ylim(yl)\n",
    "        grid(true)\n",
    "        \n",
    "        if !plot_V_only\n",
    "            subplot(3,1,2)\n",
    "            hu = plot(t, U')\n",
    "            setp(hu[1], color=[0, 0, 1])\n",
    "            setp(hu[2], color=[1, 0, 0])\n",
    "            setp(hu[3], color=[1, 0.5, 0.5])\n",
    "            setp(hu[4], color=[0, 1, 1])\n",
    "            ylabel(\"U\"); ylim(-100, 100)\n",
    "            vlines([rule_period, rule_period+delay_period, \n",
    "                rule_period+delay_period+target_period], \n",
    "                ylim()[1], ylim()[2], linewidth=2)\n",
    "\n",
    "            grid(true)\n",
    "\n",
    "            subplot(3,1,3)\n",
    "            hr = plot(t, V[1,:] - V[3,:])\n",
    "            ylim([-1.1, 1.1])\n",
    "            vlines([rule_period, rule_period+delay_period, \n",
    "                rule_period+delay_period+target_period], \n",
    "                ylim()[1], ylim()[2], linewidth=2)\n",
    "            ylabel(\"Pro R - Pro L\")\n",
    "            grid(true)\n",
    "        end\n",
    "\n",
    "        xlabel(\"t\"); \n",
    "    end\n",
    "    \n",
    "#    if V[1,end] - V[3,end] > decision_threshold\n",
    "#        answer = 1\n",
    "#    elseif V[1,end] - V[3,end] < -decision_threshold\n",
    "#        answer = -1\n",
    "#    else\n",
    "#        answer = 0\n",
    "#    end\n",
    "\n",
    "    answer1 = 0.5 + 0.5*tanh(((V[1,end] - V[3,end]) - decision_threshold)/0.1)\n",
    "    answer2 = 0.5 + 0.5*tanh(((V[3,end] - V[1,end]) - decision_threshold)/0.1)\n",
    "    answer  = answer1 - answer2\n",
    "    \n",
    "    return answer, t, U, V, W \n",
    "end\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to find equivalent parameters with reasonable tau and dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ntrys = 10\n",
    "\n",
    "\n",
    "\n",
    "mypars = Dict(:do_plot=>true, :sigma=>0.1, :pfc_anti_input=>0.15, :pfc_pro_input=>0.15,\n",
    ":tau=>0.1, :dt=>0.02, :const_E=>0.19, :const_pro_bias=>0.03, :vwi=>1.5, :hwi=>1.5, :marino_tau=>false,\n",
    ":right_light_input=>1, :right_light_pro_extra=>0.1, :target_extra_E=>0.25, \n",
    ":start_U=>[-1,-1,-1,-1], :target_period=>0.1, :delay_period=>1, :rule_period=>0.1, :post_target_period=>1,\n",
    ":g_leak=>0.25, :theta=>1, :beta=>1, :U_rest=>-1, :pro_self_ex=>0.7, :anti_self_ex=>0.7)\n",
    "\n",
    "# FROM PYTHON:\n",
    "# \"Correct target selection\"\n",
    "#                       right_light_pro_extra = 0, right_light_input=1,\n",
    "#                       sigma=0.1,\n",
    "#                       vwi = 1.5, hwi = 1.5, const_E = 0.19, target_extra_E=1,\n",
    "#                       pfc_anti_input = 0.1,\n",
    "#                       pro_self_ex=0.7, anti_self_ex=0.7,\n",
    "#                       start_U = [-1, -1, -1, -1], g_leak = 0.25, theta=1, fignum=fig1)\n",
    "\n",
    "# Function defaults:\n",
    "#               opto_conductance = 0, opto_current=0, \n",
    "#                right_light_pro_extra = 0.25, right_light_input=1, \n",
    "#                vwi = 1.5, hwi = 1.5, pro_self_ex = 0, anti_self_ex = 0, tau=0.1, marino_tau = False,\n",
    "#                dt=0.02, const_E = 0.15, target_extra_E = 0.25,\n",
    "#                pfc_anti_input = 0.05, pfc_pro_input = np.nan, const_pro_bias = 0,\n",
    "#                sigma=0.1,\n",
    "#                start_U = [-7, -7, -7, -7], do_plot = True,\n",
    "#                g_leak = 0.5, U_rest = -1,\n",
    "#                fignum=1, theta = 1, beta = 1,\n",
    "#                rule_period = 0.5, delay_period = 0.5, target_period = 0.1,\n",
    "#                post_target_period = 0.5):\n",
    "\n",
    "\n",
    "figure(3); clf();\n",
    "for i in [1:ntrys;]\n",
    "    answer, t, U, V, W = run_dynamics(\"pro\"; fignum=3, do_plot=true)\n",
    "    # println(answer)\n",
    "end\n",
    "\n",
    "figure(4); clf();\n",
    "for i in [1:ntrys;]\n",
    "    answer, t, U, V, W = run_dynamics(\"anti\"; fignum=4, do_plot=true)\n",
    "    # println(answer)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ntrys = 1000\n",
    "\n",
    "function perf(ntrys)\n",
    "    mypars = Dict(:do_plot=>false, :sigma=>0.1, :pfc_anti_input=>0.15, :pfc_pro_input=>0.15,\n",
    "    :tau=>0.1, :dt=>0.02, :const_E=>0.19, :const_pro_bias=>0.03, :vwi=>1.5, :hwi=>1.5, :marino_tau=>false,\n",
    "    :right_light_input=>1, :right_light_pro_extra=>0.0, :target_extra_E=>0.25, \n",
    "    :start_U=>[-1,-1,-1,-1], :target_period=>0.1, :delay_period=>1, :rule_period=>0.1, :post_target_period=>1,\n",
    "    :g_leak=>0.25, :theta=>1, :beta=>1, :U_rest=>-1, :pro_self_ex=>0.7, :anti_self_ex=>0.7)\n",
    "\n",
    "    results = zeros(2,ntrys)\n",
    "    for i in [1:ntrys;]\n",
    "        answer, t, U, V, W = run_dynamics(\"pro\"; fignum=3, mypars...)\n",
    "        results[1,i] = sign(V[1,end]-V[3,end])\n",
    "\n",
    "        answer, t, U, V, W = run_dynamics(\"anti\"; fignum=3, mypars...)\n",
    "        results[2,i] = sign(V[3,end]-V[1,end])\n",
    "    end\n",
    "    return results\n",
    "end\n",
    "\n",
    "results = @time(perf(ntrys))\n",
    "\n",
    "@printf \"Pro perf = %g , Anti perf = %g\\n\"  mean(results[1,:]) mean(results[2,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the cost function Jcost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function Jcost(;nderivs=0, difforder=0, ntrials=100, seedrand=NaN, targets=[0.8, 0.7],\n",
    "    theta1=0.15, theta2=0.2, beta=0.05, verbose=false, do_plot=false, plot_trials=[1], params...)\n",
    "\n",
    "    if !isnan(seedrand)\n",
    "        srand(seedrand)\n",
    "    else  # if the random seed is passed as NaN, use the system time in milliseconds\n",
    "        srand(convert(Int64, round(1000*time())))\n",
    "    end\n",
    "    \n",
    "    Vpro = ForwardDiffZeros(4, ntrials, nderivs=nderivs, difforder=difforder)\n",
    "    Vant = ForwardDiffZeros(4, ntrials, nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    \n",
    "    if do_plot; figure(1); clf(); subplot(2,1,1); end\n",
    "    for i in [1:ntrials;]\n",
    "        if do_plot && ~isempty(find(plot_trials.==i))\n",
    "            V = run_dynamics(\"pro\"; do_plot=true, plot_V_only=true, fignum=1,\n",
    "                nderivs=nderivs, difforder=difforder, params...)[3][:,end]\n",
    "        else\n",
    "            V = run_dynamics(\"pro\"; do_plot=false, plot_V_only=true, fignum=1,\n",
    "                nderivs=nderivs, difforder=difforder, params...)[3][:,end]\n",
    "        end\n",
    "        Vpro[:,i] = V\n",
    "    end\n",
    "    if do_plot; subplot(2,1,2); end\n",
    "    for i in [1:ntrials;]\n",
    "        if do_plot && ~isempty(find(plot_trials.==i))\n",
    "            V = run_dynamics(\"anti\"; do_plot=true, plot_V_only=true, fignum=1, \n",
    "                nderivs=nderivs, difforder=difforder, params...)[3][:,end]\n",
    "        else\n",
    "            V = run_dynamics(\"anti\"; do_plot=false, plot_V_only=true, fignum=1,\n",
    "                nderivs=nderivs, difforder=difforder, params...)[3][:,end]\n",
    "        end\n",
    "        Vant[:,i] = V\n",
    "    end\n",
    "    \n",
    "    hpro = 0.5*(1 + tanh.((Vpro[1,:]  - Vpro[3,:])/theta1))\n",
    "    hant = 0.5*(1 + tanh.((Vant[3,:]  - Vant[1,:])/theta1))\n",
    "    hits = [hpro ; hant]\n",
    "    dpro   = tanh((Vpro[1,:]  - Vpro[3,:]) /theta2).^2\n",
    "    dant   = tanh((Vant[1,:]  - Vant[3,:]) /theta2).^2\n",
    "    diffs = [dpro ; dant]\n",
    "        \n",
    "    cost1 = 0.5*(mean(hpro) - targets[1])^2 + 0.5*(mean(hant) - targets[2])^2\n",
    "    cost2 = -mean(diffs) \n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"        cost1=%g, cost2=%g, mean(hpro)=%.3f, mean(hant)=%.3f, mean(diffs)=%.3f\\n\", \n",
    "            convert(Float64, cost1), beta*convert(Float64, cost2), \n",
    "            convert(Float64, mean(hpro)), convert(Float64, mean(hant)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    if do_plot\n",
    "        figure(2); clf();\n",
    "        subplot(2,1,1)\n",
    "        plot(1:2*ntrials, hits, \".\"); vlines(ntrials+0.5, ylim()[1], ylim()[2]) \n",
    "        xlim(-1, ntrials*2+1)\n",
    "        title(\"hits\")\n",
    "\n",
    "        subplot(2,1,2)\n",
    "        plot(1:2*ntrials, diffs, \".\"); vlines(ntrials+0.5, ylim()[1], ylim()[2]) \n",
    "        xlim(-1, ntrials*2+1)\n",
    "        title(\"diffs\")\n",
    "    end\n",
    "    \n",
    "    return cost1 + beta*cost2, hits, diffs    \n",
    "end\n",
    "\n",
    "\n",
    "val, grad, hess = keyword_vgh((;pars...)->Jcost(;do_plot=true, ntrials=100, seedrand=10, pars...)[1], \n",
    "    [\"const_E\", \"sigma\", \"theta1\"], [0.19, 0.1, 0.15])\n",
    "\n",
    "hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "args = [\"const_E\", \"vwi\", \"hwi\", \"const_pro_bias\", \"right_light_input\", \"sigma\"]\n",
    "goods = [0.19, 1.5, 1.5, 0.03, 1, 0.05]\n",
    "bbox = [0 3 ;\n",
    "        0 6 ; \n",
    "        0 6 ; \n",
    "        0.01 1 ; \n",
    "        0.01 4 ; \n",
    "        0.005 0.4]\n",
    "\n",
    "# run_dynamics()\n",
    "func = (;pars...) -> Jcost(; do_plot=true, theta1=1.5, theta2=2,\n",
    "ntrials=200, verbose=true, seedrand=3, post_target_period=0.4, pars...)[1]\n",
    "\n",
    "params, trajectory = bbox_Hessian_keyword_minimization(goods, args, bbox, func, verbose=true, \n",
    "start_eta=0.01, tol=1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure(1); clf();\n",
    "func(; do_plot=true, ntrials=200, plot_trials=1:10, verbose=true, make_dict(args, trajectory[3:end,end])...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trajectory[:,end]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function make_dict(args, x::Vector)\n",
    "    kwargs = Dict();    \n",
    "    for i in [1:length(args);]    \n",
    "        kwargs = merge(kwargs, Dict(Symbol(args[i]) => x[i]))        \n",
    "    end    \n",
    "    return kwargs\n",
    "end \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "println(args)\n",
    "trajectory[:, [1 ;end]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ps = [0.222, 1.543, 1.458, 0.065, 1.043, 0.091]  # these are from training with ntrials=4000 and interrupting kernel. Looked good.\n",
    "ntrials = 4000\n",
    "\n",
    "# ps = [0.222, 0.1543, 0.1458, 0.065, 1.043, 0.091]  # Exptl.\n",
    "# ntrials = 2000\n",
    "\n",
    "# ps = trajectory[3:end,end]  # these are the old ones, from training with ntrials=100 and getting stuck at sigma=0.133\n",
    "\n",
    "value, grad = keyword_vgh((;pars...) -> Jcost(; do_plot=true, theta1=0.15, theta2=0.2, \n",
    "    ntrials=100, verbose=true, seedrand=3, pars...)[1], \n",
    "    [\"const_E\", \"vwi\", \"hwi\", \"const_pro_bias\", \"right_light_input\", \"sigma\"], [ps[1:end-1]; [0.133]])\n",
    "# [0.19, 1.5, 1.5, 0.03, 1.0, 0.05])\n",
    "\n",
    "#\n",
    "#   RIGHT HERE: with theta=0.15 and theat2=0.2, and ntrials=100,\n",
    "#          SETTING SIGMA=0.133 above makes gradient minute; 0.15 makes it non-negligible. \n",
    "#\n",
    "#       If  we set theta1=1.5 and theta2=2  training stops at 0.133 because we're doing Newton jumps. A genuine local minimum?\n",
    "#\n",
    "#  Adding trials increases odds of some trial not hitting total ceiling, so gradient is non-negligible again\n",
    "#  at ntrials=4000.  Training with ntrials=4000 was slow, but worked on getting a small sigma to grow to its\n",
    "#  optimal point. Training was done with theta1=1.5 and theta2=2\n",
    "\n",
    "@printf \"cost=%g\\n\" value\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trajectory[2:end,[1 end]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "args = [\"const_E\", \"vwi\", \"hwi\", \"const_pro_bias\"]\n",
    "goods = [0.19, 1.5, 1.5, 0.03]\n",
    "bbox = [0 3 ;\n",
    "        0 6 ; \n",
    "        0 6 ; \n",
    "        0.01 1]\n",
    "\n",
    "\n",
    "ntrials =  200\n",
    "max_attempts = 50\n",
    "figure(4); clf();\n",
    "nruns = 20\n",
    "attempts_needed = zeros(1, nruns)\n",
    "final_costs     = zeros(1, nruns)\n",
    "randseeds       = zeros(Int64, 1, nruns)\n",
    "final_params    = zeros(length(args), nruns)\n",
    "\n",
    "for k in [1:nruns;]            \n",
    "    local i\n",
    "    local params\n",
    "    local trajectory\n",
    "    \n",
    "    for i in [1:max_attempts;]                        \n",
    "        randseeds[k] = convert(Int64, rem(round(time()*1e12), 1e6))\n",
    "        \n",
    "        \n",
    "        func = (;pars...) -> Jcost(;beta=0.01, min_sigma=0.01, max_sigma=20, \n",
    "        rulestrength=2, ntrials=ntrials, seedrand=randseeds[k],    \n",
    "            do_plot=false, plot_trials=[5,6,7,8,9], verbose=false, dt=25, \n",
    "            cue_period=200, delay_period=200, response_period=300,  \n",
    "            pars...)[1]\n",
    "\n",
    "        seed = rand(length(args),1)[:].*(bbox[:,2]-bbox[:,1]) + bbox[:,1]\n",
    "\n",
    "        params, trajectory = bbox_Hessian_keyword_minimization(seed, args, bbox, func, hardbox=false,\n",
    "            verbose=false, tol=1e-12, start_eta=1, wallwidth_factor=0.18);\n",
    "\n",
    "        @printf \"Attempt %d with seed %d ended with cost %g\\n\" i randseeds[k] trajectory[2,end]\n",
    "\n",
    "        if trajectory[2,end] <= -0.005\n",
    "            @printf \"        ---success!\\n\"        \n",
    "            break    \n",
    "        end\n",
    "    end\n",
    "\n",
    "    \n",
    "    final_params[:,k] = params\n",
    "    final_costs[k] = trajectory[2,end]\n",
    "    attempts_needed[k] = i\n",
    "    @printf \"\\n---------- Finished run %d --------\\n\" k\n",
    "end\n",
    "\n",
    "matwrite(\"attempt_distribution.mat\", Dict(\"attempts_needed\" => attempts_needed, \"ntrials\"=>ntrials, \n",
    "\"max_attempts\" => max_attempts, \"randseeds\"=>randseeds, \"final_costs\"=>final_costs, \"final_params\"=>final_params,\n",
    "\"args\"=>args, \"bbox\"=>bbox, \"ntrials\"=>ntrials))\n",
    "\n",
    "figure(4); clf();\n",
    "ax = gca();\n",
    "ax[:hist](attempts_needed');\n",
    "xlabel(\"no. of atempts needed\")\n",
    "ylabel(\"no. of runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old-- run the dynamics with Marino's parameters just to test it\n",
    "\n",
    "Param values may be off since we changed the defaults in run_dynamics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ntrys = 4\n",
    "\n",
    "figure(1); clf();\n",
    "for i in [1:ntrys;]\n",
    "    answer, t, U, V, W = run_dynamics(\"pro\"; do_plot=true, sigma=0.4, \n",
    "    pfc_anti_input = 1.6, pfc_pro_input = 0.05, fignum=1)\n",
    "    # println(answer)\n",
    "end\n",
    "\n",
    "figure(2); clf();\n",
    "for i in [1:ntrys;]\n",
    "    answer, t, U, V, W = run_dynamics(\"anti\"; do_plot=true, sigma=0.4,\n",
    "    pfc_anti_input = 1.6, pfc_pro_input = 0.05, fignum=2)\n",
    "    # println(answer)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================\n",
    "# .\n",
    "#              Thinking about the cost function  \n",
    "# .\n",
    "# ========================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "npoints = 100\n",
    "data_sigma = 10\n",
    "\n",
    "\n",
    "function make_data(;npoints=100, data_sigma=10, seedrand=NaN)\n",
    "    if ~isnan(seedrand)\n",
    "        srand(seedrand)\n",
    "    end\n",
    "    return data_sigma*randn(npoints,1)\n",
    "end\n",
    "\n",
    "data1 = make_data(npoints=100, data_sigma=10, seedrand=10);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function J(data1; threshold=0.5, inv_slope=4, theta1 = 0.15, theta2=0.2, beta=0.05,\n",
    "    do_plot=true, nderivs=0, difforder=0, verbose=true)\n",
    "    npoints = length(data1)\n",
    "\n",
    "    d1 = tanh((data1 - threshold)/inv_slope)/2+0.5\n",
    "\n",
    "    hits = 0.5*(1 + tanh.((d1-0.5)/theta1))\n",
    "    difs = tanh((d1 - 0.5)/theta2).^2\n",
    "    \n",
    "    if do_plot\n",
    "        figure(1); clf();\n",
    "        subplot(3,1,1)\n",
    "        plot(1:npoints, d1, \"b.\")\n",
    "        ylabel(\"d1\"); \n",
    "        title(@sprintf(\"threshold=%.3f inv_slope=%.3f\", convert(Float64, threshold), convert(Float64, inv_slope)))\n",
    "        subplot(3,1,2)\n",
    "        plot(1:npoints, hits, \".\")\n",
    "        ylabel(\"hits\")\n",
    "        subplot(3,1,3)\n",
    "        plot(1:npoints, difs, \".\")\n",
    "        ylabel(\"difs\")        \n",
    "        title(@sprintf(\"<hits>=%.3f <difs>=%.3f\", convert(Float64, mean(hits)), convert(Float64, mean(difs))))\n",
    "    end\n",
    "    cost1 = (mean(hits) - 0.75)^2\n",
    "    cost2 = -mean(difs) \n",
    "\n",
    "    cost = cost1 + beta*cost2\n",
    "\n",
    "    if verbose\n",
    "        @printf(\"        cost1=%g, cost2=%g, mean(hits)=%.4f, mean(difs)=%.4f\\n\", \n",
    "            convert(Float64, cost1), beta*convert(Float64, cost2), \n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(difs)))\n",
    "    end\n",
    "\n",
    "    return cost\n",
    "end\n",
    "\n",
    "J(data1, do_plot=true)\n",
    "\n",
    "\n",
    "func = (;pars...) -> J(data1; do_plot=true, theta1=0.15, theta2=0.2, pars...)\n",
    "\n",
    "val, grad, hess = keyword_vgh(func, [\"threshold\", \"inv_slope\"], [0.5, 4])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring whether beta and number of points matters\n",
    "\n",
    "It does. Very small beta at first, and lots of points, are good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This gets us totally stuck: data1 = make_data(npoints=100, data_sigma=10, seedrand=10); \n",
    "# seed = [0.959544, 0.01]  \n",
    "# args = [\"threshold\", \"inv_slope\"]\n",
    "# beta=0, theta1=0.15, theta2=0.2, bbox = [-20.1 20.1 ; 0.01 20]\n",
    "#\n",
    "# Even npoints=1000 got stuck\n",
    "# Even npoints=10000 got stuck!   At [-3.64425,0.00998397]\n",
    "#     And making beta=0 removes the stickpoint\n",
    "#\n",
    "# Lesson seems to be start with a really small beta -- otherwise the inv_slope parameter gets pushed towards being \n",
    "# small way too fast.\n",
    "#\n",
    "# Starting with big inv_slope (0.5) but a beta=0.005 worked out with 10,000 points. \n",
    "# Doesn't work with 1,000, *does* work with 2500 and a couple of random seeds. \n",
    "# Starting with inv_slope of 0.5 or of 4 were both fine.\n",
    "#\n",
    "# Now what if we start with a small inv_slope (0.01)? Would a beta of 0.005 still work? Nope, got stuck.\n",
    "# what about theta1 -- does it matter?\n",
    "\n",
    "data1 = make_data(npoints=2500, data_sigma=10, seedrand=20);\n",
    "\n",
    "seed = [-10, 0.2]\n",
    "args = [\"threshold\", \"inv_slope\"]\n",
    "bbox = [-20.1 20.1 ; 0.01 20]\n",
    "\n",
    "seed = [-6.779, 3]\n",
    "seed = [-4, 0.01]\n",
    "\n",
    "params, traj, cost = bbox_Hessian_keyword_minimization(seed, args, bbox, \n",
    "(;pars...)->J(data1;do_plot=true, beta=0.005, theta1=0.15, theta2=0.2, pars...), \n",
    "verbose=true, verbose_every=10, start_eta=0.001, tol=1e-15, wallwidth_factor=0.01, maxiter=4000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring whether theta matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "data1 = make_data(npoints=100, data_sigma=10, seedrand=10);\n",
    "\n",
    "args = [\"threshold\", \"slope\"]\n",
    "bbox = [-20.1 20.1 ; 0.01 200]\n",
    "\n",
    "# This gets stuck at [-4.404, 12.263]\n",
    "seed = [0.5, 10.1]\n",
    "\n",
    "params, traj, cost = bbox_Hessian_keyword_minimization(seed, args, bbox, \n",
    "(;pars...)->J2(data1;do_plot=true, beta=0.005, theta1=0.15, theta2=0.2, pars...), \n",
    "verbose=true, verbose_every=10, start_eta=0.1, tol=1e-9, wallwidth_factor=0.01, maxiter=400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trust region method for Hessian minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "function trust_region_Hessian_minimization(seed, func; start_eta=10, tol=1e-6, maxiter=400,\n",
    "    verbose=false)\n",
    "\n",
    "(below, x stands for delta_x, the step from the current x=x0 position at which the cost = const)\n",
    "\n",
    "cost = 0.5*x'*H*x + grad*x + const\n",
    "\n",
    "dcost/dx = H*x + grad  ;   dcost/dx = 0  ==> x =  - inv(H)*grad\n",
    "\n",
    "Trust-region says have a parameter lambda, and replace H with hat{H} = H +  I/eta.  \n",
    "When eta is very large, this is equivalent to a straight Newton method jump, \n",
    "because hat{H} ~= H.  But when eta is small, this is more like a small gradient\n",
    "descent step, because for small eta inv(hat{H}) ~= eta and therefore the delta x is like \n",
    "-eta*grad.  So, if the cost function is going down, make eta larger, and if it is going\n",
    "up, make eta a lot smaller. Just like we do in other adaptive methods\n",
    "\n",
    "PARAMETERS:\n",
    "===========\n",
    "\n",
    "seed        column vector, representing the starting value of the parameters.\n",
    "\n",
    "func        Function that takes a vector and returns a scalar.  If you want to\n",
    "            work with a function that tales more parameterrs and returns more than one \n",
    "            output, you can use something like\n",
    "\n",
    "                    x -> orig_func(x, other_params)[1]\n",
    "\n",
    "            You only need the \"[1]\" part if the orig_func returns more outputs than a scalar. \n",
    "\n",
    "OPTIONAL PARAMETERS:\n",
    "====================\n",
    "\n",
    "start_eta=10   Starting value of eta.  It's good to start with somethibg biggish, if it is\n",
    "               too much, it'll quickly get cut down.\n",
    "\n",
    "tol=1e-6       Numerical tolerance. If a proposed jump produces a change in func that is less than\n",
    "               this, the minimization stops.\n",
    "\n",
    "maxiter=400    Maximum number of iterations to do before stopping\n",
    "\n",
    "verbose=false   If true, print out a report on each iteration of iteration number, radius size (eta),\n",
    "                what type jump was proposed (\"Newton\" means going straight to global min, \"constrained\" means jump has \n",
    "                norm eta, failed means that finding the minimum at a given radius somehow didn't work). Will also\n",
    "                print out the cosine of the angle between the proposed jump and the gradient.\n",
    "\n",
    "RETURNS:\n",
    "========\n",
    "\n",
    "params       A vector the size of seed that has the last values of the minimizing parameters for func\n",
    "\n",
    "\"\"\"\n",
    "function trust_region_Hessian_minimization(seed, func; start_eta=10, tol=1e-6, maxiter=400,\n",
    "    verbose=false)\n",
    "\n",
    "    params = seed\n",
    "    eta = start_eta\n",
    "\n",
    "    cost, grad, hess = vgh(func, params)\n",
    "\n",
    "\n",
    "    for i in [1:maxiter;]\n",
    "        hathess    = hess + eye(length(grad), length(grad))/eta        \n",
    "        new_params = params - inv(hathess)*grad\n",
    "        new_cost, new_grad, new_hess = vgh(func, new_params)\n",
    "            \n",
    "        if abs(new_cost - cost) < tol\n",
    "            break\n",
    "        end\n",
    "\n",
    "        if new_cost >= cost\n",
    "            eta = eta/2\n",
    "            costheta = NaN\n",
    "        else\n",
    "            eta = eta*1.1\n",
    "            costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "\n",
    "            params = new_params\n",
    "            cost = new_cost\n",
    "            grad = new_grad\n",
    "            hess = new_hess\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            @printf \"%d: eta=%.3f cost=%.4f costheta=%.3f ps=\" i eta cost  costheta\n",
    "            print_vector(params)\n",
    "            @printf \"\\n\"\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return params\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "npoints = 1000\n",
    "args = [\"baseline\", \"amplitude\", \"threshold\", \"slope\"]\n",
    "\n",
    "params = [1 5 0.5 0.8]\n",
    "x = rand(npoints, 1)*6-3\n",
    "y = params[1] + params[2]*0.5*(tanh((x-params[3])/params[4])+1) + randn(npoints,1)*2\n",
    "\n",
    "\n",
    "figure(1); clf();\n",
    "subplot(3,1,1);\n",
    "plot(x, y, \".\")\n",
    "\n",
    "seed = [8, 3.1, 0, -4]\n",
    "xx = -3:0.01:3\n",
    "\n",
    "plot(xx, seed[1] + seed[2]*0.5*(tanh((xx-seed[3])/seed[4])+1), \"g-\")\n",
    "\n",
    "function JJ(x, y; baseline=0, amplitude=1, threshold=0, slope=1)\n",
    "    yhat =  baseline + amplitude*0.5*(tanh((x-threshold)/slope)+1) \n",
    "    err = yhat - y\n",
    "    return sum(err.*err)\n",
    "end\n",
    "\n",
    "opars = trust_region_Hessian_minimization(seed, (w) -> JJ(x, y; make_dict(args, w)...), \n",
    "verbose=false, start_eta=0.001)\n",
    "\n",
    "plot(xx, opars[1] + opars[2]*0.5*(tanh((xx-opars[3])/opars[4])+1), \"r-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "function value, gradient, hessian = vgh(func, x0)\n",
    "\n",
    "Wrapper for ForwardDiff.hessian!() that computes and returns all three of a function's value, gradient, and hessian.\n",
    "\n",
    "EXAMPLE:\n",
    "========\n",
    "\n",
    "function tester(x::Vector)\n",
    "\n",
    "    return sum(x.*x)\n",
    "end\n",
    "\n",
    "value, grad, hess = vgh(tester, [10, 3.1])\n",
    "\"\"\"\n",
    "function vgh(func, x0)\n",
    "    out = DiffBase.HessianResult(x0)             \n",
    "    ForwardDiff.hessian!(out, func, x0)\n",
    "    value    = DiffBase.value(out)\n",
    "    gradient = DiffBase.gradient(out)\n",
    "    hessian  = DiffBase.hessian(out)\n",
    "    \n",
    "    return value, gradient, hessian    \n",
    "end\n",
    "\n",
    "\n",
    "function tester(x)\n",
    "    return sum(x.*x)\n",
    "end\n",
    "\n",
    "value, grad, hess = vgh(tester, [1.1, 2.2, 3.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data1 = make_data(npoints=100, data_sigma=10, seedrand=11);\n",
    "\n",
    "args = [\"threshold\", \"slope\"]\n",
    "seed = [0.5, 10.1]\n",
    "\n",
    "params = seed; \n",
    "\n",
    "n_noise  = 5\n",
    "n_params = length(seed)\n",
    "\n",
    "\n",
    "noiseval, gradmag, J2noisegrad, J2parmsgrad, init_J2noisegrad= \n",
    "    bring_the_noise((;pars...) -> J2(data1;do_plot=false, verbose=false, pars...), \n",
    "    verbose=true, args, params, n_noise)        \n",
    "\n",
    "@printf(\"|J2_noisegrad|^2 = %g, |J2_parmsgrad|2 = %g, |grad|^2 = %g, |init_J2_noisegrad|^2 - %g\\n\", \n",
    "sum(J2noisegrad.*J2noisegrad), sum(J2parmsgrad.*J2parmsgrad), gradmag, sum(init_J2noisegrad.*init_J2noisegrad))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimizing with noise to maximize parameter gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data1 = make_data(npoints=100, data_sigma=10, seedrand=15);\n",
    "\n",
    "args = [\"threshold\", \"slope\"]\n",
    "bbox = [-20.1 20.1 ; 0.01 200]\n",
    "\n",
    "seed = [0.5, 10.1]\n",
    "seed = [-4.758, 1.1]\n",
    "\n",
    "params = seed; new_params = 0; new_cost = 0;\n",
    "eta = 0.1\n",
    "beta = 0.0005\n",
    "\n",
    "n_noise  = 5\n",
    "n_params = length(seed)\n",
    "maxiter  = 200\n",
    "tol      = 1e-9\n",
    "verbose  = true\n",
    "verbose_level = 2\n",
    "\n",
    "func       = (;pars...)     -> J2(data1; beta=beta, do_plot=false, verbose=false, pars...)\n",
    "func_noisy = (nval;pars...) -> func(;nnoise=n_noise, noisy=nval, pars...)\n",
    "\n",
    "trajectory = zeros(2 + n_params + n_noise, 0)\n",
    "\n",
    "noiseval, gradmag, J2noisegrad, J2parmsgrad, initJ2noisegrad =  bring_the_noise(func, args, seed, n_noise)\n",
    "\n",
    "cost, grad, hess = vgh( (w) -> func_noisy(noiseval; do_plot=true, verbose=true, make_dict(args, w)...), params)\n",
    "\n",
    "#bring_the_noise((;pars...) -> J2(data1;do_plot=false, verbose=false, beta=beta, pars...), \n",
    "#        verbose=false, args, params, n_noise)        \n",
    "# cost, grad, hess = vgh( (w) -> J2([data1;noiseval]; do_plot=true, verbose=true, beta=beta, make_dict(args, w)...), params)\n",
    "\n",
    "@printf(\"|J2_noisegrad|^2 = %g, |J2_parmsgrad|2 = %g, |grad|^2 = %g, |init_J2noisegrad|^2 = %g\\n\", \n",
    "    sum(J2noisegrad.*J2noisegrad), sum(J2parmsgrad.*J2parmsgrad), \n",
    "    sum(grad.*grad), sum(init_J2noisegrad.*init_J2noisegrad))\n",
    "\n",
    "for i in 1:maxiter;\n",
    "    hathess    = hess + eye(length(grad), length(grad))/eta        \n",
    "    new_params = params - inv(hathess)*grad\n",
    "    new_cost, new_grad, new_hess = vgh((w)->func_noisy(noiseval;do_plot=true, verbose=true, make_dict(args, w)...), new_params)\n",
    "    # vgh((w) -> J2([data1;noiseval]; do_plot=true, verbose=true, beta=beta, make_dict(args, w)...), new_params)\n",
    "            \n",
    "    delta_cost = new_cost - cost\n",
    "    if abs(delta_cost) < tol\n",
    "        break\n",
    "    end\n",
    "\n",
    "    new_noiseval, new_gradmag, new_J2mag_noisegrad, new_J2mag_parmsgrad, new_init_J2mag_noisegrad = \n",
    "    bring_the_noise(func, args, new_params, n_noise; init_noise=noiseval)\n",
    "#    bring_the_noise((;pars...) -> J2(data1;do_plot=false, verbose=false, beta=beta, pars...), \n",
    "#        args, new_params, n_noise, init_noise=noiseval, ncycles=150, growth_factor=1.5)\n",
    "    \n",
    "    iJ2m_n = sum(new_init_J2mag_noisegrad.*new_init_J2mag_noisegrad)\n",
    "\n",
    "    if new_cost >= cost || new_gradmag < 1e-8 || iJ2m_n < 1e-15\n",
    "        if verbose\n",
    "            if new_cost >= cost\n",
    "                @printf(\"--- cost went up\\n\")\n",
    "            elseif new_gradmag < 1e-8\n",
    "                @printf(\"--- new_gradmag was too small, it was %g\\n\", new_gradmag)\n",
    "            else\n",
    "                @printf(\"--- initial grad of J2 = |dJ/dw| w.r.t. noise was too small, it was %g\\n\", iJ2m_n)\n",
    "            end\n",
    "        end\n",
    "        eta = eta/2\n",
    "        costheta = NaN\n",
    "    else\n",
    "        eta = eta*1.1\n",
    "        costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "        \n",
    "        params = new_params\n",
    "        noiseval = new_noiseval\n",
    "        gradmag  = new_gradmag\n",
    "\n",
    "        cost, grad, hess = vgh( (w) -> func_noisy(noiseval; do_plot=true, verbose=true, make_dict(args, w)...), params)\n",
    "\n",
    "#        cost, grad, hess = vgh( (w) -> J2([data1;noiseval]; do_plot=true, verbose=true, beta=beta, make_dict(args, w)...), params)\n",
    "    end\n",
    "\n",
    "    if verbose\n",
    "        @printf(\"%d: eta=%g cost=%.4f Dcost=%g costheta=%.3f gradmag=%g, ps=\", \n",
    "            i, eta, cost, delta_cost, costheta, gradmag)\n",
    "        print_vector(params)\n",
    "        @printf \"\\n\"\n",
    "        if verbose_level >= 2\n",
    "            @printf(\"    noiseval=\"); print_vector(noiseval); @printf(\"\\n\")\n",
    "            J2m_n  = sum(new_J2mag_noisegrad.*new_J2mag_noisegrad)\n",
    "            J2m_p  = sum(new_J2mag_parmsgrad.*new_J2mag_parmsgrad)\n",
    "            iJ2m_n = sum(new_init_J2mag_noisegrad.*new_init_J2mag_noisegrad)\n",
    "            \n",
    "            @printf(\"    init_J2mag_noisegrad= = %g, J2mag_noisegrad = %g,  J2mag_parmsgrad = %g\\n\",\n",
    "                iJ2m_n, J2m_n, J2m_p)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    trajectory = [trajectory [i; eta; params; noiseval]]\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data1 = make_data(npoints=100, data_sigma=10, seedrand=15);\n",
    "\n",
    "args = [\"threshold\", \"slope\"]\n",
    "bbox = [-20.1 20.1 ; 0.01 200]\n",
    "\n",
    "seed = [0.5, 10.1]\n",
    "# seed = [-7.758, 1.5]\n",
    "\n",
    "params = seed; new_params = 0; new_cost = 0;\n",
    "eta = 0.1\n",
    "beta = 0.005\n",
    "\n",
    "n_noise  = 1\n",
    "n_params = length(seed)\n",
    "maxiter  = 200\n",
    "tol      = 1e-9\n",
    "verbose  = true\n",
    "verbose_level = 2\n",
    "\n",
    "func       = (;pars...)     -> J2(data1; beta=beta, do_plot=false, verbose=false, pars...)\n",
    "func_noisy = (nval;pars...) -> func(;nnoise=n_noise, noisy=nval, pars...)\n",
    "\n",
    "trajectory = zeros(2 + n_params + n_noise, 0)\n",
    "\n",
    "noiseval, gradmag, J2noisegrad, J2parmsgrad, initJ2noisegrad =  bring_the_noise(func, args, seed, n_noise)\n",
    "\n",
    "cost, grad, hess = vgh( (w) -> func_noisy(noiseval; do_plot=true, verbose=true, make_dict(args, w)...), params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "size(trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "J2(data1; do_plot=true, verbose=true, beta=beta, make_dict(args, params)...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 71; eta = trajectory[2,i]; params = trajectory[3:4,i]; noiseval = trajectory[5:end,i]\n",
    "trajectory[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[params  new_params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dd = data1[1:100]\n",
    "new_params = [-8.033, 1.106]\n",
    "\n",
    "#dd = []\n",
    "cost, grad, hess = vgh( (w) -> J2([dd ; noiseval]; do_plot=true, verbose=true, beta=beta, make_dict(args, w)...), new_params)\n",
    "    hathess    = hess + eye(length(grad), length(grad))/eta        \n",
    "    new_params = params - inv(hathess)*grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-life testing of bring_the_noise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dd = data1[1:99]\n",
    "dd = [data1[1:10] ; data1[91:100]]\n",
    "dd = -8.01*ones(30,1)\n",
    "#dd = []\n",
    "cost, grad, hess = vgh( (w) -> J2([dd ; noiseval]; do_plot=true, verbose=true, beta=beta, make_dict(args, w)...), params)\n",
    "\n",
    "    hathess    = hess + eye(length(grad), length(grad))/eta        \n",
    "    new_params = params - inv(hathess)*grad\n",
    "    new_params = [-8.033, 1.106]\n",
    "\n",
    "    new_cost, new_grad, new_hess = \n",
    "vgh((w) -> J2([dd ; noiseval]; do_plot=true, verbose=true, beta=beta, make_dict(args, w)...), new_params)\n",
    "            \n",
    "    new_noiseval, new_gradmag, new_J2mag_noisegrad, new_J2mag_parmsgrad, new_init_J2mag_noisegrad = \n",
    "bring_the_noise((;pars...) -> J2(dd;do_plot=true, verbose=false, beta=beta, pars...), \n",
    "args, new_params, n_noise, init_noise=noiseval, verbose=true, ncycles=100, growth_factor=1.5)\n",
    "    \n",
    "    iJ2m_n = sum(new_init_J2mag_noisegrad.*new_init_J2mag_noisegrad)\n",
    "    J2m_n = sum(new_J2mag_noisegrad.*new_J2mag_noisegrad)\n",
    "    @printf(\"iJ2m_n = %g, J2m_n = %g\\n\", iJ2m_n, J2m_n)\n",
    "\n",
    "new_noiseval\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bring_the_noise((;pars...) -> J2(data1;do_plot=true, verbose=true, pars...), \n",
    "args, new_params, n_noise, init_noise=[-7.98], verbose=true, ncycles=1, start_eta=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function compute_noise_grad(func, args, seed, n_noise_params, init_noise) \n",
    "\n",
    "    nparams = length(seed)\n",
    "    n_noise_params = length(init_noise)\n",
    "    \n",
    "    myargs    = Array{Any, 1}(nparams+1)\n",
    "    for i=1:nparams\n",
    "        myargs[i] = args[i]\n",
    "    end\n",
    "    myargs[nparams+1] = [\"noisy\", n_noise_params]\n",
    "\n",
    "    value, grad, hess = keyword_vgh((;pars...) -> func(; nnoise=n_noise_params, pars...), \n",
    "        myargs, [seed ; init_noise])\n",
    "    pgrad = grad[1:nparams]\n",
    "\n",
    "    noise_grad = hess[nparams+1:end,1:nparams]*grad[1:nparams]\n",
    "    current_grad_mag = sum(pgrad.*pgrad)\n",
    "\n",
    "    return noise_grad, pgrad, current_grad_mag\n",
    "end\n",
    "\n",
    "noise_grad, pgrad, current_grad_mag = compute_noise_grad((;pars...) -> J2(data1;do_plot=true, verbose=false, beta=0, pars...), \n",
    "    args, new_params, 1, -8.0);\n",
    "noise_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epsilon = 0.0000001;\n",
    "funny = (epsilon) -> 0.5*compute_noise_grad((;pars...) -> J2(data1;do_plot=true, verbose=false, pars...), \n",
    "args, new_params, 1, -7.98+epsilon)[3]\n",
    "[funny(epsilon) - funny(0)]/epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pgrad = compute_noise_grad((;pars...) -> J2(data1;do_plot=true, verbose=false, pars...), args, new_params, 1, -8.00)[2];\n",
    "sum(pgrad.*pgrad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function BRING_THE_NOISE()\n",
    "\n",
    "**Lessons I may have learnt:**\n",
    "\n",
    "1. The zone of succes for bring_the_noise() may be narrow; therefore, for the adaptive eta, it pays to make the growth factor small. If a big jump takes us too far, we can end up in a zero gradient region.\n",
    "2. It also pays to have enough ncycles that we really finish\n",
    "3. Finally, in J(), the diffs factor (which applies through theta2, when beta > 0), can actually make for local maxima. There can be a zone where |dJ/dw|^2 goes through a narrow maximum that is actually the one we want; further off, there might be a local minimum and then further positive gradient zones. If we have large jumps and skip over the narrow maximum and the local minimum, then we go all the way off into badland, without a hope of returning.  Reinforcing the importance of a small growth factor.\n",
    "\n",
    "**New issue revealed:**  FIXED:   ~~If we suddenly move into a region where the gradient is much larger than in the previous step size, we may take an unfortunate big jump. It's nto all about |eta|.  We have to fix that.~~\n",
    "\n",
    "**Another thing:** In our current cost function (I'll call it J() here although above it is defined as J2, because I'm resrving J2 for J2 = |dJ/dw|^2 ), dJ/dw and therefore J2 *does* depend on non-noise data points, and the gradient can therefore interfere there. This may be a particularly strong problem when we get close to the optimum (wishful thinking, this, trying to just wish it away?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "noise_value, grad_magnitude = bring_the_noise(func, args, seed, n_noise_params; \n",
    "            init_noise=NaN, verbose=false, ncycles=100)\n",
    "\n",
    "Given a scalar function that takes some keyword-value arguments, as well as a \"noise\" vector \n",
    "(it doesn't really have to represent noise, it could be anything), finds the value of the \"noise\" vector \n",
    "that would maximize the magnitude of the gradient of func w.r.t. the keyword-value parameters.\n",
    "\n",
    "PARAMETERS:\n",
    "===========\n",
    "\n",
    "func       A scalar function, with keyword-value parameters.  These MUST include nnoise=0, which \n",
    "           will be used to indicate the length of the noise vector, and noisy=[], which will be used to indicate\n",
    "           the value of the noise vector itself.  They MUST also include nderivs=0 and difforder=0, used internally\n",
    "           together with ForwardDiffZeros in order to make sure new arrays and vectors are differentiable.\n",
    "\n",
    "args       A list of strings, indicating the keyword parameters for which differentiation is desired.\n",
    "\n",
    "seed       A list of the initial values (all scalars) of those keyword parameters\n",
    "\n",
    "n_noise_params    The desired length of the \"noise\" vector\n",
    "\n",
    "\n",
    "OPTIONAL PARAMETERS:\n",
    "====================\n",
    "\n",
    "init_noise  Default NaN, in which case it is ignored and noise is initialized randomly. If not NaN, it \n",
    "            should be a column vector, length n_noise_p repraesenting the initil value of the \"noise\"\n",
    "\n",
    "verbose     Default false. If true, prints out debugging information at each cycle of the iterative search for the\n",
    "            best noise vector value\n",
    "      \n",
    "ncycles     Default 100. Number of iterations of the adaptive gradient descent that will be used to find the \n",
    "            best noise vector value.\n",
    "\n",
    "start_eta   Default 1. starting value of learning rate.\n",
    "\n",
    "growth_factor    Default 1.2.  Factor by which eta gets multiplies every time a step successfully leads to an\n",
    "            increase in J2 = |d(func)/d(params)|^2\n",
    "\n",
    "RETURNS:\n",
    "========\n",
    "\n",
    "noise_val   The value of the noise at the end of the iterations seeking to maximize |d(func)/d(params)|^2\n",
    "\n",
    "paramgrad_mag  J2 = |d(func)/d(params)|^2\n",
    "\n",
    "noise_grad    d(J2)/dnoise    since we're trying to find the noise that maximizes J2, \n",
    "                            if we were successful this will be very small at the end of the iterations\n",
    "\n",
    "param_grad    d(J2)/dparams\n",
    "\n",
    "init_noise_grad   d(J2)/dnoise at the beginning (not end) of the iterations\n",
    "\n",
    "\"\"\"\n",
    "function bring_the_noise(func, args, seed, n_noise_params; init_noise = NaN, verbose=false, \n",
    "    ncycles=100, start_eta=1, growth_factor = 1.2)\n",
    "\n",
    "    function unit_vector(vec)\n",
    "        return vec/sqrt(sum(vec.*vec))\n",
    "    end\n",
    "    \n",
    "    if length(init_noise)==1 && isnan(init_noise[1])\n",
    "        noise_val = randn(n_noise_params, 1)\n",
    "    else\n",
    "        noise_val = init_noise\n",
    "    end\n",
    "    nparams   = length(seed)\n",
    "    myargs    = Array{Any, 1}(nparams+1)\n",
    "    \n",
    "    for i=1:nparams\n",
    "        myargs[i] = args[i]\n",
    "    end\n",
    "    myargs[nparams+1] = [\"noisy\", n_noise_params]\n",
    "    \n",
    "    eta = start_eta\n",
    "    \n",
    "    value, grad, hess = keyword_vgh((;pars...) -> func(; nnoise=n_noise_params, pars...), \n",
    "        myargs, [seed ; noise_val])\n",
    "    pgrad = grad[1:nparams]\n",
    "\n",
    "    noise_grad = hess[nparams+1:end,1:nparams]*grad[1:nparams]\n",
    "    current_grad_mag = sum(pgrad.*pgrad)\n",
    "    \n",
    "    starting_step_size = start_eta*sqrt(sum(noise_grad.*noise_grad))\n",
    "    step_size = starting_step_size\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"0: eta is %g, noise_grad is \", eta); print_vector_g(noise_grad);\n",
    "        @printf(\"  |pgrad|^2 is %g\\n\", current_grad_mag)\n",
    "    end\n",
    "    \n",
    "    init_noise_grad = noise_grad\n",
    "\n",
    "    for i=1:ncycles\n",
    "        new_noise_val = noise_val + step_size*unit_vector(noise_grad)\n",
    "\n",
    "        value, grad, hess = keyword_vgh((;pars...) -> func(; nnoise=n_noise_params, pars...), \n",
    "            myargs, [seed ; new_noise_val])\n",
    "        new_noise_grad = hess[nparams+1:end,1:nparams]*grad[1:nparams]\n",
    "        pgrad = grad[1:nparams]\n",
    "        \n",
    "        if verbose\n",
    "            @printf(\"%d: step_size is %g, |pgrad|^2 is %g, delta in |pgrad|^2 is %g\\n\", i, step_size, \n",
    "            sum(pgrad.*pgrad), sum(pgrad.*pgrad)-current_grad_mag)\n",
    "            @printf(\"new_noise_val: \");  print_vector(new_noise_val);        @printf(\"\\n\")\n",
    "            @printf(\"new_noise_grad: \"); print_vector_g(new_noise_grad); @printf(\"\\n\")\n",
    "        end\n",
    "        \n",
    "        if sum(pgrad.*pgrad)-current_grad_mag > 0\n",
    "            step_size *= growth_factor\n",
    "            noise_val  = new_noise_val\n",
    "            noise_grad = new_noise_grad \n",
    "            current_grad_mag = sum(pgrad.*pgrad)\n",
    "        elseif sum(pgrad.*pgrad)-current_grad_mag == 0\n",
    "            break\n",
    "        else\n",
    "            step_size /=5\n",
    "            if verbose\n",
    "                @printf(\"   Going back to noise_val: \"); print_vector(noise_val); @printf(\"\\n\")\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return noise_val, sum(pgrad.*pgrad), noise_grad, hess[1:nparams,1:nparams]*grad[1:nparams], init_noise_grad\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function J2(data1; nnoise=0, noisy=[], threshold=0.5, slope=0.25, theta1 = 0.15, theta2=0.2, beta=0.005,\n",
    "    do_plot=true, nderivs=0, difforder=0, verbose=true)\n",
    "\n",
    "    if nnoise > 0\n",
    "        data1 = [data1 ; noisy]\n",
    "    end\n",
    "    npoints = length(data1)\n",
    "\n",
    "    d1 = tanh((data1 - threshold)*slope)/2+0.5\n",
    "\n",
    "    hits = 0.5*(1 + tanh.((d1-0.5)/theta1))\n",
    "    difs = tanh((d1 - 0.5)/theta2).^2\n",
    "    \n",
    "    if do_plot\n",
    "        figure(1); clf();\n",
    "        subplot(3,1,1)\n",
    "        plot(1:npoints, d1, \"b.\"); # @printf(\"Plotted %d points\\n\", npoints)\n",
    "        ylabel(\"d1\"); \n",
    "        title(@sprintf(\"threshold=%.3f slope=%.3f\", convert(Float64, threshold), convert(Float64, slope)))\n",
    "        subplot(3,1,2)\n",
    "        plot(1:npoints, hits, \".\")\n",
    "        ylabel(\"hits\")\n",
    "        subplot(3,1,3)\n",
    "        plot(1:npoints, difs, \".\")\n",
    "        ylabel(\"difs\")        \n",
    "        title(@sprintf(\"<hits>=%.3f <difs>=%.3f\", convert(Float64, mean(hits)), convert(Float64, mean(difs))))\n",
    "    end\n",
    "    cost1 = (mean(hits) - 0.75)^2\n",
    "    cost2 = -mean(difs) \n",
    "\n",
    "    cost = cost1 + beta*cost2\n",
    "\n",
    "    if verbose\n",
    "        @printf(\"        cost1=%g, cost2=%g, mean(hits)=%.4f, mean(difs)=%.4f\\n\", \n",
    "            convert(Float64, cost1), beta*convert(Float64, cost2), \n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(difs)))\n",
    "    end\n",
    "\n",
    "    return cost\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = subplot(3,1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_noiseval, new_gradmag = \n",
    "    bring_the_noise((;pars...) -> J2(data1;do_plot=false, verbose=false, pars...), \n",
    "    args, new_params, n_noise, init_noise=noiseval, verbose=true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "func = (w;pars...) -> J2(data1)\n",
    "\n",
    "cost, grad, hess = vgh(func, params)\n",
    "\n",
    "\n",
    "    for i in [1:maxiter;]\n",
    "        hathess    = hess + eye(length(grad), length(grad))/eta        \n",
    "        new_params = params - inv(hathess)*grad\n",
    "        new_cost, new_grad, new_hess = vgh(func, new_params)\n",
    "            \n",
    "        if abs(new_cost - cost) < tol\n",
    "            break\n",
    "        end\n",
    "\n",
    "        if new_cost >= cost\n",
    "            eta = eta/2\n",
    "            costheta = NaN\n",
    "        else\n",
    "            eta = eta*1.1\n",
    "            costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "\n",
    "            params = new_params\n",
    "            cost = new_cost\n",
    "            grad = new_grad\n",
    "            hess = new_hess\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            @printf \"%d: eta=%.3f cost=%.4f costheta=%.3f ps=\" i eta cost  costheta\n",
    "            print_vector(params)\n",
    "            @printf \"\\n\"\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return params\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "J2(data1, do_plot=true, beta=0.005, theta1=0.6, theta2=0.2, threshold=-4, slope=10.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rem(12,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val, grad, hess = keyword_vgh((;pars...)->J(data1;do_plot=true, beta=0.05, theta1=0.15, theta2=0.2, pars...), \n",
    "    [\"threshold\", \"inv_slope\"], [-3.64425,0.00998397])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weird gradient idea\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function vgh(func, pars)\n",
    "    \n",
    "    out = DiffBase.HessianResult(pars)\n",
    "    ForwardDiff.hessian!(out, func, pars)\n",
    "    value = DiffBase.value(out)\n",
    "    grad  = DiffBase.gradient(out)\n",
    "    hess  = DiffBase.hessian(out)\n",
    "\n",
    "    return value, grad, hess\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function googaa(x::Vector)\n",
    "    return sum(x.*x)\n",
    "end\n",
    "\n",
    "googaa(randn(1,100)[:])\n",
    "\n",
    "vgh(googaa, randn(1,10)[:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "npoints = 100\n",
    "nnoise  = 2\n",
    "data_sigma = 10\n",
    "\n",
    "\n",
    "function make_data(;npoints=100, data_sigma=10, seedrand=NaN)\n",
    "    if ~isnan(seedrand)\n",
    "        srand(seedrand)\n",
    "    end\n",
    "    return data_sigma*randn(npoints,1)\n",
    "end\n",
    "\n",
    "data1 = make_data(npoints=100, data_sigma=10, seedrand=10);\n",
    "\n",
    "\n",
    "function J2(data1; nnoise=0, noisy=[], threshold=0.5, slope=0.25, theta1 = 0.15, theta2=0.2, beta=0.05,\n",
    "    do_plot=true, nderivs=0, difforder=0, verbose=true)\n",
    "\n",
    "    if nnoise > 0\n",
    "        data1 = [data1 ; noisy]\n",
    "    end\n",
    "    npoints = length(data1)\n",
    "\n",
    "    d1 = tanh((data1 - threshold)*slope)/2+0.5\n",
    "\n",
    "    hits = 0.5*(1 + tanh.((d1-0.5)/theta1))\n",
    "    difs = tanh((d1 - 0.5)/theta2).^2\n",
    "    \n",
    "    if do_plot\n",
    "        figure(1); clf();\n",
    "        subplot(3,1,1)\n",
    "        plot(1:npoints, d1, \"b.\")\n",
    "        ylabel(\"d1\"); \n",
    "        title(@sprintf(\"threshold=%.3f slope=%.3f\", convert(Float64, threshold), convert(Float64, slope)))\n",
    "        subplot(3,1,2)\n",
    "        plot(1:npoints, hits, \".\")\n",
    "        ylabel(\"hits\")\n",
    "        subplot(3,1,3)\n",
    "        plot(1:npoints, difs, \".\")\n",
    "        ylabel(\"difs\")        \n",
    "        title(@sprintf(\"<hits>=%.3f <difs>=%.3f\", convert(Float64, mean(hits)), convert(Float64, mean(difs))))\n",
    "    end\n",
    "    cost1 = (mean(hits) - 0.75)^2\n",
    "    cost2 = -mean(difs) \n",
    "\n",
    "    cost = cost1 + beta*cost2\n",
    "\n",
    "    if verbose\n",
    "        @printf(\"        cost1=%g, cost2=%g, mean(hits)=%.4f, mean(difs)=%.4f\\n\", \n",
    "            convert(Float64, cost1), beta*convert(Float64, cost2), \n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(difs)))\n",
    "    end\n",
    "\n",
    "    return cost\n",
    "end\n",
    "\n",
    "value, grad, hess = keyword_vgh((;pars...)->J2(data1, do_plot=true, nnoise=2;pars...), \n",
    "[\"threshold\", \"slope\", [\"noisy\", 2]], [0.5, 0.25, 0.1, 0.2])\n",
    "\n",
    "hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "func = (;pars...)->J2(data1; do_plot=false, verbose=false, pars...)\n",
    "args = [\"threshold\", \"slope\"]\n",
    "seed = [0.05, 10]\n",
    "\n",
    "n_noise_params = 5\n",
    "\n",
    "    noise_val = randn(n_noise_params, 1)\n",
    "    nparams   = length(seed)\n",
    "    myargs    = Array{Any, 1}(nparams+1)\n",
    "\n",
    "    for i=1:nparams\n",
    "        myargs[i] = args[i]\n",
    "    end\n",
    "    myargs[nparams+1] = [\"noisy\", n_noise_params]\n",
    "    \n",
    "    eta = 1\n",
    "\n",
    "myargs\n",
    "keyword_vgh((;pars...) -> func(; nnoise=n_noise_params, pars...), \n",
    "        myargs, [seed ; noise_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "noiseval, grad_mag = bring_the_noise( (;pars...)->J2(data1; do_plot=false, verbose=false, pars...), \n",
    "[\"threshold\", \"slope\"], [0.5, 10], 5, verbose=false)\n",
    "\n",
    "J2([data1;noiseval]; do_plot=true, threshold=0.5, slope=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = Array{Any, 1}(3)\n",
    "a[1] = \"a\"\n",
    "a[2] = \"bee\"\n",
    "a[3] = [\"hmm\" 3]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nnoise = 1\n",
    "noise_val = data_sigma*randn(nnoise,1); orig_noise_val = noise_val\n",
    "\n",
    "args = [\"threshold\", \"slope\", [\"noisy\", nnoise]]\n",
    "seed = [0.5, 20]\n",
    "nparams = length(seed)\n",
    "\n",
    "eta = 1\n",
    "\n",
    "value, grad, hess = keyword_vgh((;pars...) -> J2(data1, do_plot=true, nnoise=nnoise;pars...), args, [seed;noise_val])\n",
    "noise_grad = hess[nparams+1:end,1:nparams]*grad[1:nparams]\n",
    "current_grad_mag = sum(grad.*grad)\n",
    "@printf(\"|grad|^2 is %g\\n\\n\", current_grad_mag)\n",
    "\n",
    "new_noise_val=0; new_noise_grad=0\n",
    "for i=1:100\n",
    "    new_noise_val = noise_val + eta*noise_grad\n",
    "\n",
    "    value, grad, hess = keyword_vgh((;pars...) -> J2(data1, do_plot=false, verbose=false, nnoise=nnoise;pars...), \n",
    "        args, [seed;new_noise_val])\n",
    "    new_noise_grad = hess[nparams+1:end,1:nparams]*grad[1:nparams]\n",
    "    if i<0\n",
    "        @printf(\"%d: eta is %g, |grad|^2 is %g, delta in |grad|^2 is %g\\n\", i, eta, \n",
    "        sum(grad.*grad), sum(grad.*grad)-current_grad_mag)\n",
    "        @printf(\"noise_val: \");      print_vector(noise_val);        @printf(\"\\n\")\n",
    "        @printf(\"new_noise_grad: \"); print_vector_g(new_noise_grad); @printf(\"\\n\")\n",
    "    end\n",
    "    if sum(grad.*grad)-current_grad_mag >= 0\n",
    "        eta *= 2\n",
    "        noise_val  = new_noise_val\n",
    "        noise_grad = new_noise_grad \n",
    "        current_grad_mag = sum(grad.*grad)\n",
    "    else\n",
    "        eta /=10\n",
    "    end\n",
    "end\n",
    "\n",
    "@printf(\"|grad|^2 is %g\\n\\n\", sum(grad.*grad))\n",
    "@printf(\"%d: eta is %g, |grad|^2 is %g, delta in |grad|^2 is %g\\n\", i, eta, \n",
    "sum(grad.*grad), sum(grad.*grad)-current_grad_mag)\n",
    "@printf(\"noise_val: \");      print_vector(noise_val);        @printf(\"\\n\")\n",
    "@printf(\"new_noise_grad: \"); print_vector_g(new_noise_grad); @printf(\"\\n\")\n",
    "\n",
    "# J2(data1, do_plot=true, verbose=true, nnoise=nnoise; make_dict(args, [seed;orig_noise_val])...)\n",
    "\n",
    "J2(data1, do_plot=true, verbose=true, nnoise=nnoise; make_dict(args, [seed;new_noise_val])...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_noise_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "noise_val = [0.2, 0.1]\n",
    "args = [\"threshold\", \"slope\", [\"noisy\", 2]]\n",
    "seed = [0.5, 10]\n",
    "nnoise = length(noise_val)\n",
    "\n",
    "eta = 1\n",
    "nparams = length(grad)-nnoise\n",
    "\n",
    "value, grad, hess = keyword_vgh((;pars...) -> J2(data1, do_plot=true, nnoise=nnoise;pars...), args, [seed;noise_val])\n",
    "noise_grad = hess[nparams+1:end,1:nparams]*grad[1:nparams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hess[nparams+1:end, 1:nparams]*grad[1:nparams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "J2(data1, do_plot=true, verbose=true, nnoise=nnoise; make_dict(args, [seed;new_noise_val])...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "value, grad, hess = keyword_vgh((;pars...)->J2(data1, do_plot=true;pars...), [\"threshold\", \"slope\", \"noisy\"], \n",
    "[0.5, 100, 0.5])\n",
    "\n",
    "@printf(\"1000*J2 = %g\\n\", 1000*0.5*sum(grad[1:2].*grad[1:2]))\n",
    "\n",
    "grad[1]*hess[3,1] + grad[2]*hess[3,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "a=[1,2]\n",
    "b = [3,4]\n",
    "[a;b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glu = \"%.4f\\n\"\n",
    "\n",
    "# @eval @printf($fmt,1,2,3)\n",
    "\n",
    "@eval @printf($glu, pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "include(\"hessian_utils.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next cell tries out Marino's idea of doing a new random seed if the minimization gets stuck at a value of the cost function that we don't like.\n",
    "\n",
    "**2017-08-16 11:11am** : Works, but it's not clear to me that it is better than just restarting the search entirely. Too many searches get stuck at a too small value of the inv_slope.\n",
    "\n",
    "**Still to do:** work on scaling theta1 and theta2.  Unclear whether that will help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed = [0.5, 0.1]\n",
    "args = [\"threshold\", \"inv_slope\"]\n",
    "bbox = [-20.1 20.1 ; 0.001 20]\n",
    "\n",
    "func2 = (;pars...) -> func(;beta=0*0.05, pars...)\n",
    "\n",
    "niters = 0\n",
    "cost = 10\n",
    "while cost > 0\n",
    "    params, traj, cost = bbox_Hessian_keyword_minimization(seed, args, bbox, (;pars...)->func(;beta=0.001, pars...), verbose=true)\n",
    "    data1 = make_data(npoints=100, data_sigma=10)\n",
    "    func = (;pars...) -> J(data1; do_plot=true, theta1=0.15, theta2=0.2, pars...)\n",
    "    seed = params\n",
    "    niters = niters + 1\n",
    "    @printf(\"\\n\\n\\n========== WILL CONSIDER NEXT ITER AT COST = %g ========\\n\\n\\n\", cost)\n",
    "end\n",
    "\n",
    "@printf(\"\\n\\n   niter = %g\\n\\n\", niters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "func2(;make_dict([\"threshold\", \"inv_slope\"], [-9, 1])...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sandlot -- trash from here on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta2 = 0.2\n",
    "mudata = 0.9\n",
    "sigmadata = 0.2\n",
    "\n",
    "ntrials = 100\n",
    "\n",
    "u1 = randn(ntrials, 1)*sigmadata + mudata\n",
    "u2 = randn(ntrials, 1)*sigmadata - mudata\n",
    "\n",
    "v1 = tanh(u1)\n",
    "v2 = tanh(u2)\n",
    "\n",
    "diffs = (v1 - v2)/theta2\n",
    "sampsigma = sqrt(mean((v1-v2).^2))\n",
    "theta2 = sampsigma\n",
    "@printf \"sampsigma=%g, theta2=%g\\n\" sampsigma theta2\n",
    "\n",
    "tdiffs = tanh((v1-v2)/theta2).^2\n",
    "\n",
    "figure(1); clf(); \n",
    "subplot(4,1,1)\n",
    "plot(u1, \"b.\", u2, \"r.\"); ylabel(\"U\"); \n",
    "subplot(4,1,2)\n",
    "plot(v1, \"b.\", v2, \"r.\"); ylabel(\"V\")\n",
    "subplot(4,1,3)\n",
    "plot((v1-v2), \".\"); ylabel(\"V1-V2\")\n",
    "subplot(4,1,4)\n",
    "plot(tdiffs, \".\"); ylabel(\"TANH OF DIFFS\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.5.2",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
