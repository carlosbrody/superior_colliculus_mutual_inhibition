{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constrained Parabolic Minimization\n",
    "\n",
    "This notebook describes a function that, given a vector gradient and a matrix hessian that represent a parabolic approximation to a surface, finds the point that minimizes that surface, subject to the constraint that\n",
    "its radius from the origin is $r <= r_0$\n",
    "\n",
    "We have a surface with a quadratic cost function\n",
    "\n",
    "$$\n",
    "    J_0({\\bf x}) = {\\bf g} \\cdot {\\bf x} + \\frac{1}{2} {\\bf x}^T \\cdot H \\cdot {\\bf x}\n",
    "$$\n",
    "\n",
    "(note that we deal with Hessians so H should be symmetric) and we want to find the ${\\bf x}$ that minizimes $J$, subject to \n",
    "\n",
    "$$\n",
    "    {\\bf x}^T \\cdot {\\bf x} = r_0.\n",
    "$$ \n",
    "\n",
    "where $r_0$ is a constant. We use the lagrange multiplier $\\lambda$ to minimize\n",
    "\n",
    "$$\n",
    "    J({\\bf x}) = \\lambda {\\bf x}^T \\cdot {\\bf x} + {\\bf g} \\cdot {\\bf x} + \\frac{1}{2} {\\bf x}^T \\cdot H \\cdot {\\bf x} \\\\\n",
    "     = {\\bf g} \\cdot {\\bf x} + \\frac{1}{2} {\\bf x}^T \\cdot (H + \\lambda I) \\cdot {\\bf x} \n",
    "$$\n",
    "\n",
    "\n",
    "Given a value of $\\lambda$, this is minimized at\n",
    "\n",
    "$$\n",
    "    {\\bf x}_{min}(\\lambda) = (H + \\lambda I)^{-1} \\cdot {\\bf g}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Thus we want to find the value of $\\lambda$ that satisfies $ {\\bf x}_{min}(\\lambda)^T \\cdot {\\bf x}_{min}(\\lambda) = r_0.$\n",
    "\n",
    "The algorithm below first defines an internal cost function in a 1-dimensional space:\n",
    "\n",
    "$$\n",
    "Q(\\lambda; r_0) = (|{\\bf x}_{min}(\\lambda)| - r_0)^2\n",
    "$$\n",
    "\n",
    "When $Q(\\lambda) = 0$, we have found the $\\lambda$ that satisfies the constraint. Thus we do a one-dimensional search over $\\lambda$, which can be efficiently performed using Netwon's method. The algorithm below first scans over a grid of possible values of $\\lambda$, to find initial values that get us close to satisfying the constraint. At each of these candidate values, the algorithm then runs a Newton's method search to try to get a precise value of $\\lambda$ satisfying the constraint, and therefore minimizing the desired cost function $J$ subject to our constraint.\n",
    "\n",
    "Note: if the ${\\bf x}$ that would minimize $J$ subject to no constraints has a radius less than $r_0$, then we return that, effectively making the constraint $r <= r_0$, not just $r=r_0$.\n",
    "\n",
    "<br>\n",
    "\n",
    "Examples are below the function definition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using ForwardDiff   # for the minimizations\n",
    "using PyPlot        # for plotting examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First a helper function, one_d_minimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#@include_me  constrained_parabolic_minimization.jl\n",
    "\n",
    "\"\"\"\n",
    "function x, cost, iters_used, last_Delta_x = one_d_minimizer(seed, func; tol=1e-5, maxiter=100, start_eta=0.1)\n",
    "\n",
    "Minimizes a 1-d function using constrained Hessian minimization. \n",
    "We don't trust the long-range info from the Hessian too much, meaning that there's\n",
    "a given (adaptive) step size. If Newton's method suggests a step smaller than that step\n",
    "size, we take it. Otherwise, we only move in the direction of the gradient by the stepsize.\n",
    "\n",
    "Adaptive step size: Every step that the cost function gets smaller, the step grows by a factor \n",
    "of 1.1. If a step would have led to a larger cost function, the step is not taken, and\n",
    "the size falls by a factor of 2.\n",
    "\n",
    "PARAMETERS:\n",
    "===========\n",
    "\n",
    "seed       A float, the starting point for the minimization\n",
    "\n",
    "func       A one dimensional function, takes a float and returns a float that represents the current cost.\n",
    "\n",
    "\n",
    "OPTIONAL PARAMETERS:\n",
    "====================\n",
    "\n",
    "tol=1e-5   If a step would lead to a change in cost that is smaller in magnitude than tol, stop the minimization.\n",
    "\n",
    "maxiter=100   Maximum number of iterations for the minimization\n",
    "\n",
    "start_eta=0.1  The starting value of the step size\n",
    "\n",
    "\n",
    "RETURNS:\n",
    "========\n",
    "\n",
    "x0     the value of x that minimizes f(x)\n",
    "\n",
    "cost   The value of f(x0) \n",
    "\n",
    "niters  The number of iterations done\n",
    "\n",
    "Dparam  The last change to the parameter value\n",
    "\n",
    "\"\"\"\n",
    "function one_d_minimizer(seed, func; tol=1e-5, maxiter=100, start_eta=0.1)\n",
    "    eta = start_eta;\n",
    "    lambdavec = [seed]\n",
    "\n",
    "\n",
    "    out = DiffBase.HessianResult(lambdavec)\n",
    "    ForwardDiff.hessian!(out, func, lambdavec)\n",
    "    cost = DiffBase.value(out)\n",
    "    grad = DiffBase.gradient(out)\n",
    "    hess = DiffBase.hessian(out)\n",
    "\n",
    "    i = delta_lambda = 0;  # declare it so we can use it after the if\n",
    "    for i in [1:maxiter;]\n",
    "        h_delta = - grad./hess;\n",
    "        if abs(h_delta[1]) < eta\n",
    "            new_lambdavec = lambdavec + h_delta\n",
    "        else\n",
    "            new_lambdavec = lambdavec - eta*sign.(grad)\n",
    "        end\n",
    "\n",
    "        delta_lambda = new_lambdavec[1] - lambdavec[1]\n",
    "        if abs(delta_lambda) < tol\n",
    "            break\n",
    "        end\n",
    "        \n",
    "        ForwardDiff.hessian!(out, func, new_lambdavec)\n",
    "        new_cost = DiffBase.value(out)\n",
    "        new_grad = DiffBase.gradient(out)\n",
    "        new_hess = DiffBase.hessian(out)\n",
    "\n",
    "        if new_cost .< cost\n",
    "            lambdavec[1] = new_lambdavec[1]\n",
    "            cost = new_cost;\n",
    "            grad = new_grad;\n",
    "            hess = new_hess;\n",
    "\n",
    "            eta = eta*1.1\n",
    "        else\n",
    "            eta = eta/2\n",
    "        end\n",
    "\n",
    "        # @printf \"%d: cost=%.3f lambda=%.3f\\n\" i cost lambdavec[1]\n",
    "    end\n",
    "    \n",
    "    return lambdavec[1], cost, i, delta_lambda\n",
    "end\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The main constrained_parabolic_minimization() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#@include_me  constrained_parabolic_minimization.jl\n",
    "\n",
    "\"\"\"\n",
    "function constrained_parabolic_minimization(H, G, r; tol=1e-6, min_only=true, do_plot=false, \n",
    "    verbose=false, efactor=3.0, max_efactor_tries=10, \n",
    "    lambdastepsize=0.003, minimum_tol=1e-24, tol_delta=1e-3, maxiter=200)\n",
    "\n",
    "Given a Hessian matrix, a gradient vector, and a desired radius from the origin, finds the vector \n",
    "that minimizes the parabola defined by the Hessian and the gradient, subject to the constraint that the\n",
    "vector's length equals the desired radius.\n",
    "\n",
    "# PARAMETERS:\n",
    "\n",
    "- H      A square symmetric matrix. \n",
    "\n",
    "- G      A vector, length equal to the size(H,2)\n",
    "\n",
    "- r      desired radius\n",
    "\n",
    "# OPTIONAL PARAMETERS:\n",
    "\n",
    "- tol=1e-6        Numerical tolerance on the computations\n",
    "\n",
    "- min_only=true   Return only the minimum, or, if false, all xs for all lambdas that match x'*x = r^2 \n",
    "                    (some of thos might be maxima, not minima)\n",
    "\n",
    "- efactor=3       The initial exploration of lambdas will go from -efactor(max(absolute(eig(H)))) to +efactor(max(absolute(eig(H))))\n",
    "                If that does not produce a solution within the indicates tolerance, then efactor is multiplied by efactor_growth\n",
    "                and we try again, up to max_efactor_tries.\n",
    "\n",
    "- efactor_growth  see efactor\n",
    "\n",
    "- max_efactor_tries   see efactor\n",
    "\n",
    "- minimum_tol     Each candidate value of lambda, identified as a minimum in the grid scan and \n",
    "                proposed as satisfying |x|^2=r^2, is then used to seed a Newton's method one-d minimization with a certain tolerance.\n",
    "                If the results of that search do not satisfy |x|^2=r^2 within the desired tolerance, then the tolerance\n",
    "                for the 1-d search is reduced (to make the one-d search more exact), by a factor of tol_delta, \n",
    "                up to minimu_tol\n",
    "\n",
    "- tol_delta       see minimum_tol\n",
    "\n",
    "- max_iter        Maximum number of iterations in the 1-d search (see minimum_tol).\n",
    "\n",
    "\n",
    "- lambdastepsize=0.003    The step size for initial exploration of lambdas, un units of efactor. It sshould\n",
    "                probably scale with the smallest difference in the eigenvalues of H; that has not been implemented yet.\n",
    "\n",
    "- do_plot         If true, produces plot of the initial gridscans of lambda versus (x'*x - r^2)^2\n",
    "\n",
    "- verbose         If true, produces diagnostic info as it goes.\n",
    "\n",
    "\n",
    "RETURNS:\n",
    "========\n",
    "\n",
    "- x        The vector that minimizes 0.5*x'*H*x + x'*G subject to x'*x = r\n",
    " \n",
    "- J        0.5*x'*H*x + x'*G at the returned x\n",
    "\n",
    "- lambda   value of the Lagrange multiplier at which the radius constraint is satisfied\n",
    "\n",
    "- c        The squared difference between the length of x and r. Should be small, otherwise somthing went wrong!\n",
    "\n",
    "- niters   The number of iterations used in the one-d minimiation that identified the output lambdas.\n",
    "\n",
    "- Dlambda  The last change in lambda during the one-d minimization. If the one-d minimization did not reach\n",
    "         its maxiters, then this will be smaller than the one-d minimization's tolerance.\n",
    "\n",
    "\"\"\"\n",
    "function constrained_parabolic_minimization(H, G, r; tol=1e-6, min_only=true, \n",
    "    do_plot=false, verbose=false, efactor=3.0, efactor_growth=4, max_efactor_tries=10, \n",
    "    lambdastepsize=0.003, minimum_tol=1e-24, tol_delta=1e-3, maxiter=200)\n",
    "\n",
    "    #  --- First a couple of helper functions ----\n",
    "    \n",
    "    \"\"\"\n",
    "    function x_of_lambda(lambda)\n",
    "\n",
    "    Given square matrix H, vector G, and passed scalar lambda, returns the vector x that minimizes\n",
    "    \n",
    "    0.5 x'*H*x + x'*G - lambda *x'*x\n",
    "\n",
    "    \"\"\"\n",
    "    function x_of_lambda(lambda)\n",
    "        return inv(H - lambda*eye(size(H,1)))*(-G)\n",
    "    end\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    function q(lambda, r)\n",
    "\n",
    "    Returns the squared difference between r and the norm of x_of_lambda(lambda).\n",
    "    \"\"\"\n",
    "    function q(lambda, r)\n",
    "        return (r - norm(x_of_lambda(lambda)))^2\n",
    "    end\n",
    "\n",
    "\n",
    "    # efactor is the factor that multiplies the biggest eigenvalue of H, to determine the range over which we'll\n",
    "    # look for a lambda that satisfies the norm(x)==r requirement. If we don't find a solution, we iteratively \n",
    "    # increase efactor to try to get there, for a maximum of max_efactor_tries\n",
    "    for m=1:max_efactor_tries\n",
    "        # First scan lambda to find good candidates for minimizing the parabolic \n",
    "        # surface under the x'*x = r^2 constraint\n",
    "        L = eig(H)[1]\n",
    "        L0 = maximum(abs.(L))\n",
    "        lambdas = L0*efactor*[-1.0:lambdastepsize:1.0;]\n",
    "        costs = zeros(size(lambdas))\n",
    "        for i in [1:length(lambdas);]\n",
    "            try \n",
    "                costs[i] = q(lambdas[i], r)\n",
    "            catch\n",
    "                costs[i] = Inf\n",
    "            end\n",
    "        end\n",
    "\n",
    "        if do_plot\n",
    "            figure(2); clf();\n",
    "            plot(lambdas, costs, \"b.-\")\n",
    "            xlabel(\"lambda\")\n",
    "            ylabel(\"cost\")\n",
    "        end\n",
    "\n",
    "        # Take all candidates where the derivative of costs changes sign \n",
    "        # from negative to positive (those would be minima),\n",
    "        # plus the smallest and the largest lambdas tested, as candidates\n",
    "        g = append!(prepend!(find(diff(sign.(diff(costs))) .> 0.99), [1]), [length(lambdas)])\n",
    "        if verbose\n",
    "            @printf(\"cpm: g (candidate indices) are : \");           print_vector_g(g);        print(\"\\n\")\n",
    "            @printf(\"cpm: and their corresponding costs are : \");   print_vector(costs[g]);   print(\"\\n\");\n",
    "            @printf(\"cpm: and their corresponding lambdas are : \"); print_vector(lambdas[g]); print(\"\\n\");\n",
    "            @printf(\"cpm: the minimum cost was : %g\\n\", minimum(costs[g]))\n",
    "        end\n",
    "        # found_it_flag = 0  # A flag for when we've found at least one lambda that satisfies the r constraint\n",
    "        mytol = tol\n",
    "\n",
    "        while mytol > minimum_tol\n",
    "            lambdas_out = zeros(size(g))\n",
    "            costs_out   = zeros(size(g))\n",
    "            niters_out  = zeros(size(g))\n",
    "            Dlambda_out = zeros(size(g))\n",
    "            for i in [1:length(g);]\n",
    "                lambdas_out[i], costs_out[i], niters_out[i], Dlambda_out[i] = \n",
    "                one_d_minimizer(lambdas[g[i]], x -> q(x[1], r), start_eta=1, tol=mytol, maxiter=maxiter)\n",
    "            end\n",
    "\n",
    "            # Eliminate any lambdas where x'*x doesn't match our desired value r\n",
    "            I = find(costs_out .< tol)\n",
    "            lambdas_out = lambdas_out[I]; costs_out = costs_out[I];\n",
    "            niters_out  = niters_out[I];  Dlambda_out = Dlambda_out[I]\n",
    "\n",
    "            if length(I) > 0; break; end\n",
    "\n",
    "            mytol *= tol_delta\n",
    "        end\n",
    "        if verbose\n",
    "            @printf(\"%d : After searching for lambdas with efactor=%g, we found these : \", m, efactor)\n",
    "            print_vector_g(lambdas_out); print(\"\\n\")\n",
    "        end\n",
    "        if length(lambdas_out) > 0; break; end;\n",
    "        efactor = efactor*efactor_growth\n",
    "    end\n",
    "    \n",
    "    # Eliminate any repeated lambdas, to within the specified numerical tolerance.\n",
    "    I = setdiff(1:length(lambdas_out), find(diff(lambdas_out) .< tol))\n",
    "    lambdas_out = lambdas_out[I]; costs_out = costs_out[I];\n",
    "    niters_out  = niters_out[I];  Dlambda_out = Dlambda_out[I]\n",
    "    \n",
    "    # Find the parabolic estimate of the cost function at these points\n",
    "    J  = zeros(size(lambdas_out))\n",
    "    xs = zeros(length(G), length(lambdas_out))\n",
    "    for i in [1:length(J);]\n",
    "        xs[:,i] = x_of_lambda(lambdas_out[i])\n",
    "        J[i] = (0.5*xs[:,i]'*H*xs[:,i] + xs[:,i]'*G)[1]\n",
    "    end\n",
    "\n",
    "    # Find and return only the x that has the smallest J\n",
    "    if min_only\n",
    "        I = indmin(J)    \n",
    "    else\n",
    "        I = 1:length(J)\n",
    "    end\n",
    "    return xs[:,I], J[I], lambdas_out[I], costs_out[I], niters_out[I], Dlambda_out[I]\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "H = [2 1 ; 1 0.5]\n",
    "g = [0.3, 2]\n",
    "r0 = 7\n",
    "\n",
    "p = constrained_parabolic_minimization(H, g, r0)[1]\n",
    "\n",
    "\n",
    "xax = -11:0.1:11\n",
    "yax = -10:0.1:10\n",
    "X = repmat(xax', length(yax), 1)\n",
    "Y = repmat(yax, 1, length(xax))\n",
    "\n",
    "J = zeros(length(xax), length(yax))\n",
    "\n",
    "func = (x) -> (g'*x + 0.5*x'*H*x)[1]\n",
    "\n",
    "for xi=1:length(xax)\n",
    "    for yi=1:length(yax)\n",
    "        J[xi, yi] = func([xax[xi], yax[yi]])\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "figure(1); \n",
    "contour(X, Y, J', 80)\n",
    "vlines(0, ylim()[1], ylim()[2], \"r\")\n",
    "hlines(0, xlim()[1], xlim()[2], \"r\")\n",
    "\n",
    "theta = 0:0.01:(2*pi)\n",
    "plot(r0*cos.(theta), r0*sin.(theta), \"m-\")\n",
    "h = plot(p[1], p[2], \"ro\")\n",
    "axis(\"scaled\")\n",
    "xlabel(\"x\"); ylabel(\"y\")\n",
    "title(\"magenta line is constant radius, red dot is the minimum constrained to be on or less than that radius\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To double check that everything is working, we can look at the cost function on the radius at places close to the found minimum. In 2-d, it's easy, we know we're on a circle parametrized by an angle theta. Run the code below to see that adding or subtracting small amounts to theta leads to increases in the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta = atan(p[2]/p[1])\n",
    "func([r0*cos(theta), r0*sin(theta)])\n",
    "\n",
    "e = -0.001\n",
    "(func([r0*cos(theta+e), r0*sin(theta+e)]) - func([r0*cos(theta), r0*sin(theta)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.2",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
