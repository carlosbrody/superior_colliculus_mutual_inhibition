{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONTENTS**\n",
    "\n",
    "This notebook contains two sections. The first is straightforward optimization of a two-node memory-guided orienting network. It uses frozen noise and bbox_Hessian_keyword_minimization. It was mainly used to develop the methods for minimization.\n",
    "\n",
    "The second contained the development of fluxSense(), which was the idea that seeding with single trials that ended on the decision boundary would help avoid the vanishing gradients problem. However, closer inspection showed that it was not so much vanishing gradients as the term that encourages separation between the two final outputs. Moreover, the soft tanh() walls approach to bounding parameters seemed to solve even that. In the end, we didn't neet fluxSense() and its development was, for now, abandoned.\n",
    "\n",
    "For ProAnti, we moved to a new, fresh notebook.\n",
    "\n",
    "**=========**\n",
    "\n",
    "\n",
    "**Wed 2017-09-06 1pm:** Time to go to ProAnti. I don't think it's worth stopping to examine further whether fluxSense() is worth it, just doesn't seem the issue right now. Can revisit if necessary.\n",
    "\n",
    "**Wed 2017-09-06 11am:** Current status: everythign working, bbox_hessian_minimization fully debugged and report diagnostic info. Basic MGO minimizations are doing their thing.  Have now added self-connection weights, and can very successfully train the MGO network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">TABLE OF CONTENTS</h1>\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "**Updates to the table of contents are periodic, but run the cell below to first start or force an update.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://sites.google.com/site/brodylabhome/files/make_table_of_contents.js')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition @javascript_str(ANY<:Any) in module Main at In[49]:1 overwritten at In[313]:1.\n"
     ]
    }
   ],
   "source": [
    "macro javascript_str(s) display(\"text/javascript\", s); end\n",
    "\n",
    "javascript\"\"\"\n",
    "$.getScript('https://sites.google.com/site/brodylabhome/files/make_table_of_contents.js')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We define functions to convert Duals, the variable types used by ForwardDiff, \\nto Floats. This is useful if we want to print out the value of a variable \\n(since print doesn't know how to Duals). Note that after being converted to a Float, no\\ndifferentiation by ForwardDiff can happen!  e.g. after\\n    x = convert(Float64, y)\\nForwardDiff can still differentiate y, but it can't differentiate x\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "using PyCall\n",
    "using PyPlot\n",
    "using ForwardDiff\n",
    "using DiffBase\n",
    "using MAT\n",
    "\n",
    "pygui(true)\n",
    "\n",
    "import Base.convert\n",
    "convert(::Type{Float64}, x::ForwardDiff.Dual) = Float64(x.value)\n",
    "function convert(::Array{Float64}, x::Array{ForwardDiff.Dual}) \n",
    "    y = zeros(size(x)); \n",
    "    for i in 1:prod(size(x)) \n",
    "        y[i] = convert(Float64, x[i]) \n",
    "    end\n",
    "    return y\n",
    "end\n",
    "\n",
    "include(\"general_utils.jl\")\n",
    "include(\"hessian_utils.jl\")\n",
    "\n",
    "\"\"\"\n",
    "We define functions to convert Duals, the variable types used by ForwardDiff, \n",
    "to Floats. This is useful if we want to print out the value of a variable \n",
    "(since print doesn't know how to Duals). Note that after being converted to a Float, no\n",
    "differentiation by ForwardDiff can happen!  e.g. after\n",
    "    x = convert(Float64, y)\n",
    "ForwardDiff can still differentiate y, but it can't differentiate x\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup -- definitions of forwardModel() and backwardsModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition g(Any) in module Main at In[142]:5 overwritten at In[144]:5.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'g :: Tuple{Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition ginverse(Any) in module Main at In[142]:12 overwritten at In[144]:12.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'ginverse :: Tuple{Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition forwardModel(Any) in module Main at In[142]:78 overwritten at In[144]:78.\n",
      "WARNING: Method definition #forwardModel(Array{Any, 1}, Main.#forwardModel, Any) in module Main overwritten.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'forwardModel :: Tuple{Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition backwardsModel(Any) in module Main at In[142]:224 overwritten at In[144]:224.\n",
      "WARNING: Method definition #backwardsModel(Array{Any, 1}, Main.#backwardsModel, Any) in module Main overwritten.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'backwardsModel :: Tuple{Any}' in module 'Main'.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "backwardsModel"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "o = g(z)    squashing tanh function, running from 0 to 1, is equal to 0.5 when input is 0.\n",
    "\"\"\"\n",
    "function g(z)\n",
    "    return 0.5*tanh.(z)+0.5\n",
    "end\n",
    "    \n",
    "\"\"\"\n",
    "z = g^-1(o)    inverse of squashing tanh function, input must be in (0, 1), output is zero when passed 0.5.\n",
    "\"\"\"\n",
    "function ginverse(z)\n",
    "    return 0.5*log.(z./(1-z))\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "forwardModel(startU; dt=0.01, tau=0.1, nsteps=100, input=[0.1, 0], noise=[], W=[0 -5;-5 0], \n",
    "init_add=0, start_add=0, const_add=0, sigma=0, gleak=1, U_rest=0, \n",
    "    do_plot=false, nderivs=0, difforder=0, clearfig=true, fignum=1, dUdt_mag_only=false)\n",
    "\n",
    "Runs a tanh() style-network forwards in time, given its starting point, using simple Euler integration\n",
    "    tau dU/dt = -U + W*V + I\n",
    "    V = 0.5*tanh(U)+ 0.5\n",
    "\n",
    "**PARAMETERS:**\n",
    "\n",
    "startU     A column vector, nunits-by-1, indicating the values of U at time zero\n",
    "\n",
    "\n",
    "**OPTIONAL PARAMETERS**\n",
    "\n",
    "dt      Scalar, timestep size\n",
    "\n",
    "tau     Scalar, in seconds\n",
    "\n",
    "gleak   \n",
    "        dUdt will have a term equal to gleak*(U_rest - U)\n",
    "U_rest\n",
    "\n",
    "nsteps  Number of timesteps to run, including time=0.\n",
    "\n",
    "input   Either an nunits-by-1 vector, in which case inputs to each unit are constant\n",
    "        across time, or a matrix, nunits-by-nsteps, indicating input for each unit at each timepoint.\n",
    "\n",
    "W       Weight matrix, nunits-by-nunits\n",
    "\n",
    "init_add    DEPRECATED: Vector or scalar that gets added to the input current at very first timestep.\n",
    "            Deprecated because this made it dt-dependent. Replaced by start_add.\n",
    "\n",
    "start_add   Vector or scalar that gets added, once, to the initial U[:,1], before the integration process begins.\n",
    "\n",
    "const_add   Scalar that gets added to U after every timestep\n",
    "\n",
    "sigma       After each timestep, add sigma*sqrt(dt)*randn() to each element of U\n",
    "\n",
    "do_plot   Default false, if true, plots V of up to the first two dimensions\n",
    "\n",
    "fignum     Figure number on which to plot\n",
    "\n",
    "clrearfig  If true, the figure is first cleared, otherwise any plot ois overlaid\n",
    "\n",
    "nderivs, difforder     Required for making sure function can create its own arrays and \n",
    "                       still be differentiated\n",
    "\n",
    "dUdt_mag_only  If true, returns |dUdt|^2 from the first timestep only, then stops.\n",
    "\n",
    "** RETURNS:**\n",
    "\n",
    "Uend Vend       nunits-by-1 vectors representing the final values of U and V that were found.\n",
    "U, V            nunits-by-nsteps matrices containing the full trajectories\n",
    "\n",
    "\"\"\"\n",
    "function forwardModel(startU; dt=0.01, tau=0.1, nsteps=100, input=[], noise=[], W=[0 -5;-5 0], \n",
    "    init_add=0, start_add=0, const_add=0, do_plot=false, nderivs=0, difforder=0, clearfig=true, fignum=1,\n",
    "    dUdt_mag_only=false, sigma=0, g_leak=1, U_rest=0, theta=0, beta=1, \n",
    "    warn_if_unused_params=false, other_unused_params...)\n",
    "\n",
    "    if warn_if_unused_params && length(other_unused_params)>0\n",
    "        @printf(\"\\n\\n=== forwardModel warning, had unused params \")\n",
    "        for k in keys(Dict(other_unused_params))\n",
    "            @printf(\"%s, \", k)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    my_input = ForwardDiffZeros(size(input,1), size(input,2), nderivs=nderivs, difforder=difforder)\n",
    "    for i=1:prod(size(input)); my_input[i] = input[i]; end\n",
    "    input = my_input;\n",
    "    \n",
    "    nunits = length(startU)\n",
    "    if size(startU,2) > size(startU,1)\n",
    "        error(\"startU must be a column vector\")\n",
    "    end\n",
    "    \n",
    "    # --- formatting input ---\n",
    "    if ~(typeof(input)<:Array) || prod(size(input))==1  # was a scalar\n",
    "        input = input[1]*(1+ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    elseif length(input)==0 # was the empty matrix\n",
    "        input = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    elseif size(input,2)==1     # was a column vector\n",
    "        input = input*(1+ForwardDiffZeros(1, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    end    \n",
    "    # --- formatting noise ---\n",
    "    if ~(typeof(noise)<:Array) || prod(size(noise))==1  # was a scalar\n",
    "        noise = noise*(1+ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    elseif length(noise)==0 # was the empty matrix\n",
    "        noise = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    elseif size(noise,2)==1     # was a column vector\n",
    "        noise = noise*(1+ForwardDiffZeros(1, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    end    \n",
    "    \n",
    "    U = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    V = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    \n",
    "    if ~(typeof(W)<:Array); W = [W]; end\n",
    "\n",
    "    W     = reshape(W, nunits, nunits)\n",
    "    U     = reshape(U, nunits, nsteps)\n",
    "    V     = reshape(V, nunits, nsteps)\n",
    "    input = reshape(input, nunits, nsteps)\n",
    "    noise = reshape(noise, nunits, nsteps)\n",
    "\n",
    "    input[:,1] += init_add\n",
    "    input      += const_add\n",
    "\n",
    "    #@printf(\"size(U) is (%d,%d), and size(startU) is (%d,%d) and size(noise) is (%d,%d)\", \n",
    "    #    size(U,1), size(U,2), size(startU,1), size(startU,2), size(noise,1), size(noise,2))\n",
    "    # @printf(\"U[1]=%g, noise[1]=%g\\n\", startU, noise[1])\n",
    "    U[:,1] = startU + noise[:,1] + start_add; # @printf(\"Resulting U=%g\\n\", U[1])\n",
    "    V[:,1] = g((U[:,1]-theta)/beta); # @printf(\"Resulting V=%g\\n\", V[1])\n",
    "    \n",
    "    for i=2:nsteps\n",
    "        dUdt = g_leak*(U_rest -U[:,i-1]) + W*V[:,i-1] + input[:,i-1]\n",
    "        if dUdt_mag_only; return sum(dUdt.*dUdt); end;\n",
    "        # @printf(\"dUdt=%g\\n\", dUdt[1])\n",
    "        # @printf(\"i=%g\\n\", i)\n",
    "        # @printf(\"noise[2]=%g\\n\", noise[2])\n",
    "        U[:,i] = U[:,i-1] + (dt/tau)*dUdt + noise[:,i] + sigma*sqrt(dt)*randn(size(U,1),1)\n",
    "        # @printf(\"Resulting U[2]=%g\\n\", U[2])\n",
    "        V[:,i] = g((U[:,i]-theta)/beta)\n",
    "        # @printf(\"Resulting V[2]=%g\\n\", V[2])\n",
    "    end\n",
    "\n",
    "    if do_plot\n",
    "        figure(fignum)\n",
    "        if length(startU)==1\n",
    "            if clearfig; clf(); end;\n",
    "            t = (0:nsteps-1)*dt\n",
    "            plot(t, V[1,:], \"b-\")\n",
    "            plot(t[1], V[1,1], \"g.\")\n",
    "            plot(t[end], V[1,end], \"r.\")\n",
    "            xlabel(\"t\"); ylabel(\"V1\"); ylim([-0.01, 1.01])\n",
    "        elseif length(startU)>=2\n",
    "            if clearfig; clf(); end;\n",
    "            plot(V[1,:], V[2,:], \"b-\")\n",
    "            plot(V[1,1], V[2,1], \"g.\")\n",
    "            plot(V[1,end], V[2,end], \"r.\")\n",
    "            xlabel(\"V1\"); ylabel(\"V2\"); \n",
    "            xlim([-0.01, 1.01]); ylim([-0.01, 1.01])\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return U[:,end], V[:,end], U, V\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "backwardsModel(endU; dt=0.01, tau=0.1, nsteps=100, input=[0],noise=[],  W=[0 -5;-5 0], \n",
    "    do_plot=false, nderivs=0, difforder=0, clearfig=true, fignum=1, tol=1e-15, start_eta=10)\n",
    "\n",
    "Runs a tanh() style-network BACKWARDS in time, given its ending point, by making a backwards\n",
    "guess at each timepoint and then using Hessian minimization to find the backwards vector that correctly\n",
    "leads to the current timestep value.  Uses forwardModel() . The forwards equations are:\n",
    "\n",
    "    tau dU/dt = -U + W*V + I\n",
    "    V = 0.5*tanh(U)+ 0.5\n",
    "\n",
    "**PARAMETERS:**\n",
    "\n",
    "endU     A column vector, nunits-by-1, indicating the values of U at time=end\n",
    "\n",
    "\n",
    "**OPTIONAL PARAMETERS:**\n",
    "\n",
    "dt      Scalar, timestep size\n",
    "\n",
    "tau     Scalar, in seconds\n",
    "\n",
    "nsteps  Number of timesteps to run, including time=0.\n",
    "\n",
    "input   Either an nunits-by-1 vector, in which case inputs to each unit are constant\n",
    "        across time, or a matrix, nunits-by-nsteps, indicating input for each unit at each timepoint.\n",
    "\n",
    "W       Weight matrix, nunits-by-nunits\n",
    "\n",
    "do_plot   Default false, if true, plots V of up to the first two dimensions\n",
    "\n",
    "tol       Tolerance in the minimization procedure for finding each backwards timestep. Passed on\n",
    "          to trust_region_Hessian_minimization()\n",
    "\n",
    "start_eta   Passed on to trust_region_Hessian_minimization()\n",
    "\n",
    "fignum     Figure number on which to plot\n",
    "\n",
    "clrearfig  If true, the figure is first cleared, otherwise any plot ois overlaid\n",
    "\n",
    "nderivs, difforder     Required for making sure function can create its own arrays and \n",
    "                       still be differentiated\n",
    "\n",
    "\n",
    "\n",
    "** RETURNS:**\n",
    "\n",
    "Ustart Vstart   nunits-by-1 vectors representing the starting values of U and V that were found.\n",
    "U, V            nunits-by-nsteps matrices containing the full trajectories\n",
    "costs           1-by-nsteps vector with the final cost from the minimization procedure for each\n",
    "                timestep. This is the squared difference between the U[t+1] produced by the U[t] \n",
    "                guess and the actual U[t+1]\n",
    "\n",
    "\"\"\"\n",
    "function backwardsModel(endU; nsteps=100, start_eta=10, tol=1e-15, maxiter=400, \n",
    "    do_plot=false, init_add=0, start_add=0, dt=0.01, \n",
    "    input=[], noise=[], nderivs=0, difforder=0, clearfig=false, fignum=1, params...)    \n",
    "\n",
    "    nunits = length(endU)\n",
    "\n",
    "    # --- formatting input ---\n",
    "    if ~(typeof(input)<:Array) || prod(size(input))==1  # was a scalar\n",
    "        input = input[1]*(1+ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    elseif length(input)==0 # was the empty matrix\n",
    "        input = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    elseif size(input,2)==1     # was a column vector\n",
    "        input = input*(1+ForwardDiffZeros(1, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    end    \n",
    "    # --- formatting noise ---\n",
    "    if ~(typeof(noise)<:Array)  # was a scalar\n",
    "        noise = noise*(1+ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    elseif length(noise)==0 # was the empty matrix\n",
    "        noise = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    elseif size(noise,2)==1     # was a column vector\n",
    "        noise = noise*(1+ForwardDiffZeros(1, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    end    \n",
    "    \n",
    "    function J(U1, U2; nderivs=0, difforder=0, noise=[], inputs=[], pars...)\n",
    "        U2hat = forwardModel(U1; nsteps=2, noise=noise, input=input, nderivs=nderivs, difforder=difforder, pars...)[1]\n",
    "        U2hat = U2hat\n",
    "        DU = U2hat - U2\n",
    "    \n",
    "        return sum(DU.*DU)\n",
    "    end\n",
    "    \n",
    "    if length(noise)==0\n",
    "        noise = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    end\n",
    "\n",
    "    U = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    U = reshape(U, nunits, nsteps)\n",
    "    costs = ForwardDiffZeros(nsteps, 1, nderivs=nderivs, difforder=difforder)    \n",
    "    \n",
    "    U[:,end] = endU\n",
    "    for i=(nsteps-1):-1:1\n",
    "        if i==1\n",
    "            my_init_add = init_add\n",
    "            my_start_add = start_add\n",
    "        else\n",
    "            my_init_add = 0\n",
    "            my_start_add = 0\n",
    "        end\n",
    "                \n",
    "        U[:,i], costs[i] = trust_region_Hessian_minimization(U[:,i+1], \n",
    "            (x) -> J(x, U[:,i+1]; nderivs=length(endU), difforder=2, \n",
    "            input=input[:,i:i+1], noise = noise[:,i:i+1], \n",
    "            init_add=my_init_add, start_add=my_start_add, params...); \n",
    "            verbose=false, start_eta=start_eta, tol=tol, maxiter=maxiter)\n",
    "        if i>1; U[:,i] += noise[:,i]; end\n",
    "    end\n",
    "    \n",
    "    \n",
    "    V = g(U)\n",
    "    \n",
    "    if do_plot\n",
    "        figure(fignum)   \n",
    "        if typeof(params)<:Array; params = Dict(params); end;\n",
    "        if haskey(params, :dt);     dt     = params[:dt];     end\n",
    "        if haskey(params, :nsteps); nsteps = params[:nsteps]; end\n",
    "        if length(endU)==1\n",
    "            if clearfig; clf(); end;\n",
    "            t = (0:nsteps-1)*dt\n",
    "            plot(t, V[1,:], \"m-\")\n",
    "            plot(t[1], V[1,1], \"go\")\n",
    "            plot(t[end], V[1,end], \"ro\")            \n",
    "            ylim([-0.01, 1.01])\n",
    "        elseif length(endU)>=2\n",
    "            if clearfig; clf(); end;            \n",
    "            plot(V[1,:], V[2,:], \"m-\")\n",
    "            plot(V[1,1], V[2,1], \"go\")\n",
    "            plot(V[1,end], V[2,end], \"ro\")\n",
    "            xlim([-0.01, 1.01]); ylim([-0.01, 1.01])\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return U[:,1], V[:,1], U, V, costs\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing forward and backwards models with only 1 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ustart came back as 1.1\n"
     ]
    }
   ],
   "source": [
    "figure(1); clf();\n",
    "params = Dict(:noise => [0.1], :W => [-2], :nsteps=>10, :start_add=>-1.9)\n",
    "Uend = forwardModel([1.1]; do_plot=true, params...)[1]\n",
    "Ustart = backwardsModel(Uend; do_plot=true, tol=1e-30, params...)[1]\n",
    "@printf(\"Ustart came back as %g\\n\", Ustart[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing forward and backwards models now with 2 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ustart came back as : [0.1, 0.1]\n"
     ]
    }
   ],
   "source": [
    "nsteps=50\n",
    "params = Dict(:noise =>0.03*randn(2,nsteps) + [0.1,0]*ones(1,nsteps), :W => [0 -5; -5 0], :nsteps=>nsteps)\n",
    "\n",
    "Uend, Vend, U, V              = forwardModel([0.1,0.1]; do_plot=true, params...);\n",
    "Ustart, Vstart, bU, bV, costs = backwardsModel(Uend; do_plot=true, tol=1e-30, params...)\n",
    "\n",
    "@printf(\"Ustart came back as : \"); print_vector_g(Ustart); print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring dt-dependence of gradients and hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×5 Array{Float64,2}:\n",
       " 0.92859   -1.0449   1.11301  -1.12663  0.164904\n",
       " 0.918901  -1.01352  1.03901  -1.06345  0.512789"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# srand(111)\n",
    "startU=randn(100,2)-3\n",
    "startU=randn(100,2)-3\n",
    "sigma = 0\n",
    "\n",
    "# startU=0.1*randn(100,2)-3\n",
    "# startU=zeros(100,2)-3\n",
    "\n",
    "\n",
    "dt = 0.02\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "W = -4\n",
    "noise = 0\n",
    "input = 0\n",
    "sigma = 0\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    ":noise=>noise, :input=>input, :sigma=>sigma, :const_add=>0, :init_add=>0)\n",
    "\n",
    "forward = (startU; pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;     \n",
    "    forwardModel(startU; pars...)\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "# clf();\n",
    "# func = (;pars...) -> forward([-0.2, 0.3]; do_plot=true, merge(model_params, Dict(pars))...)\n",
    "# func(;W=-4)\n",
    "\n",
    "args = [\"W\", \"const_add\", [\"start_add\" 2], \"sigma\"]\n",
    "params = [-4.01, 0.5, 0.2, -0.2, 0.01]\n",
    "\n",
    "figure(1); clf();\n",
    "value1, grad1, hess1 = keyword_vgh((;pars...)->forward([-0.2, 0.3]; do_plot=true, merge(model_params, Dict(pars))...)[1][1], args, params)\n",
    "title(@sprintf(\"Running with dt=%g\", dt))\n",
    "\n",
    "dt = 0.005\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    ":noise=>noise, :input=>input, :sigma=>sigma, :const_add=>0, :init_add=>0)\n",
    "\n",
    "figure(2); clf();\n",
    "value2, grad2, hess2 = keyword_vgh((;pars...)->forward([-0.2, 0.3]; do_plot=true, fignum=2, merge(model_params, Dict(pars))...)[1][1], args, params)\n",
    "title(@sprintf(\"Running with dt=%g\", dt))\n",
    "\n",
    "[grad1[:]' ; grad2[:]']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO-DOs\n",
    "\n",
    "1. ~~Be able to use W as an optimizable parameter (including configs like \"all horizontal weights are the same\")~~ DONE!\n",
    "2. ~~Check out what is going on with the weird trajectories in the function-based MGO example~~  DONE: it's just the strong, single-timestep initial_add\n",
    "3. ~~Check out whether reducing beta solves the sticking issue even without extra finalFluxPoint locations~~. It does. Reducing beta from 0.01 to 0.003 was enough.  (We also needed dto change the cost_limit to -0.00288, since the range of costs changes when beta changes.)\n",
    "3. Find the saddle points and use those as the finalFluxPoint locations\n",
    "4. ===\n",
    "5. ~~Run a ProAnti model with noise only in initial conditions, and thus with the framework as we have it~~ (skipped, went straight to next step)\n",
    "6. ~~Make a cost function with frozen noise, and figure out how frozen noise will interact with the backwards trajectory in the minimizations~~\n",
    "7. ~~Make a forwards and backwards model with Urest, etc., just like in ProAnti()~~\n",
    "6. ===\n",
    "7. ~~Make sure that minimization procedures that use tanh() walls report the model parameter, not the control parameter~~ DONE\n",
    "7. ~~Figure out what is going on with the change in gradient and Hessian upon change of dt~~ DONE: it was just the init step\n",
    "7. ~~To really follow bbox_Hessian, printouts needs to be for the walled params, the trajectory should be for the walled params, and the seed should also take the walls into account.~~ DONE\n",
    "7. ~~Have one_d_minimization return the number of iterations it did and why it stopped; then have constrained_parabolic_minimization return the cost, maxiters, and stopping reason; then have bbox_Hessian_minimization return the trajectory of those, as a trace of what was going on.~~ DONE\n",
    "7. POSTPONED: Clean up examples of forward and backwards models and of 1-d use of fluxSense() function\n",
    "8. POSTPONED: Find a 2-d example where flux points are actually needed -- when beta=0, it is not so clear.\n",
    "8. POSTPONED: Measure gradient sensitivity to each of the endpoints in a set of trajectories, as a measure of whether fluxSense is needed or not.\n",
    "12. POSTPONED: Try to combinee fluxSense with bbox_Hessian_minimization9. ~~Fix the walls issue in bbox_Hessian_minimization using tanh encoding.~~\n",
    "\n",
    "\n",
    "1. ===\n",
    "2. ~~Optimize either an MGO or a ProAnti~~ DONE with MGO. Now on to ProAnti\n",
    "3. ~~Set up so we can easily change task period durations in JJ as we run the model to evaluate the results of model-fitting~~ DONE\n",
    "4. Have different task period durations while model-fitting\n",
    "5. Set up to do searches over parameter space\n",
    "6. Incorporate RT into fits?\n",
    "10. If fluxSense is needed in ProAnti, could try choosing the Anti unit endpoint values by maximizing the |dJ/dw|^2 over those values.\n",
    "11. Clean up the notebooks and write up what we've been doing!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing around with minimizing 2-d model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed = [-0.2, 0.2, 0.2, -1.5, 0.1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition JJ(Any) in module Main at In[47]:62 overwritten at In[48]:62.\n",
      "WARNING: Method definition #JJ(Array{Any, 1}, Main.#JJ, Any) in module Main overwritten.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- cost=0.0686324,   cost1=0.0687511, cost2=-0.00011865 :  mean(hits)=0.487796, mean(diffs)=0.0395499\n",
      "0: eta=1 ps=[-0.200, 0.200, 0.200, -1.500, 0.100]\n",
      "-- cost=0.0686324,   cost1=0.0687511, cost2=-0.00011865 :  mean(hits)=0.487796, mean(diffs)=0.0395499\n",
      "-- cost=0.0662608,   cost1=0.0663306, cost2=-6.98297e-05 :  mean(hits)=0.492453, mean(diffs)=0.0232766\n",
      "1: eta=1.1 cost=0.0662608 jtype=Newton costheta=-0.343 ps=[-0.112938, 0.349709, 0.101748, -1.26536, 0.103245]\n",
      "-- cost=0.0656768,   cost1=0.0657292, cost2=-5.24826e-05 :  mean(hits)=0.493623, mean(diffs)=0.0174942\n",
      "2: eta=1.21 cost=0.0656768 jtype=Newton costheta=-0.188 ps=[0.0772162, 0.616916, 0.0618108, -0.989811, 0.103702]\n",
      "-- cost=0.0658205,   cost1=0.0658659, cost2=-4.54283e-05 :  mean(hits)=0.493357, mean(diffs)=0.0151428\n",
      "eta going down: new_cost-cost=0.000143696 and jumptype='Newton'\n",
      "3: eta=0.605 cost=0.0656768 jtype=Newton costheta=NaN ps=[0.0772162, 0.616916, 0.0618108, -0.989811, 0.103702]\n",
      "-- cost=0.0658205,   cost1=0.0658659, cost2=-4.54283e-05 :  mean(hits)=0.493357, mean(diffs)=0.0151428\n",
      "eta going down: new_cost-cost=0.000143696 and jumptype='Newton'\n",
      "4: eta=0.3025 cost=0.0656768 jtype=Newton costheta=NaN ps=[0.0772162, 0.616916, 0.0618108, -0.989811, 0.103702]\n",
      "-- cost=0.0645488,   cost1=0.0646014, cost2=-5.25525e-05 :  mean(hits)=0.495832, mean(diffs)=0.0175175\n",
      "5: eta=0.33275 cost=0.0645488 jtype=constrained costheta=-0.719 ps=[0.274727, 0.41869, 0.0155098, -1.08603, 0.104242]\n",
      "-- cost=0.0614765,   cost1=0.0615494, cost2=-7.28586e-05 :  mean(hits)=0.501909, mean(diffs)=0.0242862\n",
      "6: eta=0.366025 cost=0.0614765 jtype=constrained costheta=-0.946 ps=[0.455695, 0.244242, 0.132879, -1.25726, 0.105988]\n",
      "-- cost=0.0486282,   cost1=0.0487824, cost2=-0.000154226 :  mean(hits)=0.529133, mean(diffs)=0.0514088\n",
      "7: eta=0.402628 cost=0.0486282 jtype=constrained costheta=-0.997 ps=[0.606886, 0.105449, 0.312739, -1.48064, 0.108562]\n",
      "-- cost=0.0133079,   cost1=0.0139258, cost2=-0.00061789 :  mean(hits)=0.631993, mean(diffs)=0.205963\n",
      "8: eta=0.44289 cost=0.0133079 jtype=constrained costheta=-1.000 ps=[0.755649, -0.026157, 0.504166, -1.74103, 0.111807]\n",
      "-- cost=0.000257413,   cost1=0.00210511, cost2=-0.00184769 :  mean(hits)=0.795881, mean(diffs)=0.615897\n",
      "9: eta=0.487179 cost=0.000257413 jtype=constrained costheta=-0.978 ps=[0.975464, -0.236981, 0.654068, -1.97877, 0.116327]\n",
      "-- cost=-0.00157669,   cost1=6.96102e-07, cost2=-0.00157738 :  mean(hits)=0.750834, mean(diffs)=0.525794\n",
      "10: eta=0.535897 cost=-0.00157669 jtype=Newton costheta=-0.587 ps=[1.02164, -0.0108635, 0.62631, -1.9228, 0.117996]\n",
      "-- cost=-0.00192434,   cost1=0.000676274, cost2=-0.00260062 :  mean(hits)=0.723995, mean(diffs)=0.866872\n",
      "11: eta=0.589487 cost=-0.00192434 jtype=constrained costheta=-0.796 ps=[0.821785, 0.247223, 0.818342, -2.23205, 0.123257]\n",
      "-- cost=-0.00271411,   cost1=6.39525e-05, cost2=-0.00277807 :  mean(hits)=0.742003, mean(diffs)=0.926022\n",
      "12: eta=0.648436 cost=-0.00271411 jtype=Newton costheta=-0.556 ps=[0.821891, 0.193543, 0.875512, -2.32724, 0.121511]\n",
      "-- cost=-0.00284202,   cost1=2.05373e-05, cost2=-0.00286256 :  mean(hits)=0.745468, mean(diffs)=0.954186\n",
      "13: eta=0.713279 cost=-0.00284202 jtype=Newton costheta=-0.248 ps=[0.72397, 0.0815275, 0.92699, -2.41553, 0.119098]\n",
      "-- cost=-0.00288506,   cost1=8.78345e-06, cost2=-0.00289385 :  mean(hits)=0.747036, mean(diffs)=0.964616\n",
      "14: eta=0.784607 cost=-0.00288506 jtype=Newton costheta=-0.210 ps=[0.603214, -0.0548918, 0.975406, -2.49512, 0.116614]\n",
      "-- cost=-0.00290096,   cost1=6.46895e-06, cost2=-0.00290743 :  mean(hits)=0.747457, mean(diffs)=0.969143\n",
      "15: eta=0.863068 cost=-0.00290096 jtype=Newton costheta=-0.147 ps=[0.375301, -0.293363, 1.03034, -2.58254, 0.1136]\n",
      "-- cost=-0.00291064,   cost1=1.10793e-05, cost2=-0.00292172 :  mean(hits)=0.746671, mean(diffs)=0.973907\n",
      "16: eta=0.949375 cost=-0.00291064 jtype=Newton costheta=-0.125 ps=[-0.101739, -0.779492, 1.10727, -2.71391, 0.109042]\n",
      "-- cost=-0.00209882,   cost1=0.000899802, cost2=-0.00299862 :  mean(hits)=0.779997, mean(diffs)=0.999541\n",
      "eta going down: new_cost-cost=0.000811819 and jumptype='constrained'\n",
      "17: eta=0.474687 cost=-0.00291064 jtype=constrained costheta=NaN ps=[-0.101739, -0.779492, 1.10727, -2.71391, 0.109042]\n",
      "-- cost=-0.00229665,   cost1=0.000648625, cost2=-0.00294527 :  mean(hits)=0.775468, mean(diffs)=0.981758\n",
      "eta going down: new_cost-cost=0.000613992 and jumptype='constrained'\n",
      "18: eta=0.237344 cost=-0.00291064 jtype=constrained costheta=NaN ps=[-0.101739, -0.779492, 1.10727, -2.71391, 0.109042]\n",
      "-- cost=-0.00286845,   cost1=4.55062e-05, cost2=-0.00291396 :  mean(hits)=0.756746, mean(diffs)=0.971319\n",
      "eta going down: new_cost-cost=4.21903e-05 and jumptype='constrained'\n",
      "19: eta=0.118672 cost=-0.00291064 jtype=constrained costheta=NaN ps=[-0.101739, -0.779492, 1.10727, -2.71391, 0.109042]\n",
      "-- cost=-0.00291069,   cost1=1.37804e-06, cost2=-0.00291207 :  mean(hits)=0.751174, mean(diffs)=0.97069\n",
      "20: eta=0.130539 cost=-0.00291069 jtype=constrained costheta=-0.090 ps=[-0.0962228, -0.781568, 1.15767, -2.78912, 0.110374]\n",
      "-- cost=-0.00292144,   cost1=7.15913e-06, cost2=-0.0029286 :  mean(hits)=0.747324, mean(diffs)=0.976199\n",
      "21: eta=0.143593 cost=-0.00292144 jtype=constrained costheta=-0.039 ps=[-0.0946473, -0.771467, 1.21975, -2.86746, 0.111792]\n",
      "-- cost=-0.00292227,   cost1=3.77572e-06, cost2=-0.00292604 :  mean(hits)=0.751943, mean(diffs)=0.975347\n",
      "22: eta=0.157952 cost=-0.00292227 jtype=constrained costheta=-0.035 ps=[-0.0923132, -0.770222, 1.27629, -2.95537, 0.11353]\n",
      "-- cost=-0.00292847,   cost1=9.24347e-06, cost2=-0.00293772 :  mean(hits)=0.74696, mean(diffs)=0.979239\n",
      "23: eta=0.173747 cost=-0.00292847 jtype=constrained costheta=-0.018 ps=[-0.0800473, -0.749842, 1.36128, -3.04012, 0.115212]\n",
      "-- cost=-0.00291325,   cost1=3.26152e-05, cost2=-0.00294587 :  mean(hits)=0.755711, mean(diffs)=0.981955\n",
      "eta going down: new_cost-cost=1.52223e-05 and jumptype='constrained'\n",
      "24: eta=0.0868737 cost=-0.00292847 jtype=constrained costheta=NaN ps=[-0.0800473, -0.749842, 1.36128, -3.04012, 0.115212]\n",
      "-- cost=-0.00292713,   cost1=9.25892e-06, cost2=-0.00293639 :  mean(hits)=0.753043, mean(diffs)=0.978795\n",
      "eta going down: new_cost-cost=1.34679e-06 and jumptype='constrained'\n",
      "25: eta=0.0434369 cost=-0.00292847 jtype=constrained costheta=NaN ps=[-0.0800473, -0.749842, 1.36128, -3.04012, 0.115212]\n",
      "-- cost=-0.00293064,   cost1=3.26848e-06, cost2=-0.00293391 :  mean(hits)=0.751808, mean(diffs)=0.97797\n",
      "26: eta=0.0477805 cost=-0.00293064 jtype=constrained costheta=-0.042 ps=[-0.0780774, -0.74942, 1.37648, -3.06588, 0.115771]\n",
      "-- cost=-0.00293452,   cost1=1.12552e-06, cost2=-0.00293565 :  mean(hits)=0.748939, mean(diffs)=0.97855\n",
      "27: eta=0.0525586 cost=-0.00293452 jtype=constrained costheta=-0.022 ps=[-0.0767747, -0.745522, 1.39617, -3.0931, 0.116353]\n",
      "-- cost=-0.00293575,   cost1=2.00775e-09, cost2=-0.00293575 :  mean(hits)=0.750045, mean(diffs)=0.978583\n",
      "28: eta=0.0578145 cost=-0.00293575 jtype=constrained costheta=-0.011 ps=[-0.0774677, -0.745455, 1.41562, -3.12338, 0.117064]\n",
      "-- cost=-0.00293654,   cost1=6.97725e-08, cost2=-0.00293661 :  mean(hits)=0.749736, mean(diffs)=0.978869\n",
      "29: eta=0.0635959 cost=-0.00293654 jtype=constrained costheta=-0.013 ps=[-0.0783256, -0.744805, 1.44056, -3.15536, 0.117805]\n",
      "-- cost=-0.00293718,   cost1=1.22753e-08, cost2=-0.00293719 :  mean(hits)=0.749889, mean(diffs)=0.979063\n",
      "30: eta=0.0699555 cost=-0.00293718 jtype=constrained costheta=-0.018 ps=[-0.0801053, -0.745193, 1.46612, -3.19054, 0.118677]\n",
      "-- cost=-0.00293768,   cost1=2.15906e-08, cost2=-0.0029377 :  mean(hits)=0.749853, mean(diffs)=0.979234\n",
      "31: eta=0.076951 cost=-0.00293768 jtype=constrained costheta=-0.096 ps=[-0.0821453, -0.745674, 1.49504, -3.22846, 0.119642]\n",
      "-- cost=-0.00293807,   cost1=1.7614e-08, cost2=-0.00293809 :  mean(hits)=0.749867, mean(diffs)=0.979363\n",
      "32: eta=0.0846461 cost=-0.00293807 jtype=constrained costheta=-0.009 ps=[-0.0852402, -0.747187, 1.52449, -3.26998, 0.120797]\n",
      "-- cost=-0.00293835,   cost1=2.79069e-08, cost2=-0.00293838 :  mean(hits)=0.749833, mean(diffs)=0.979461\n",
      "33: eta=0.0931108 cost=-0.00293835 jtype=constrained costheta=-0.005 ps=[-0.0894229, -0.749758, 1.55398, -3.31527, 0.122236]\n",
      "-- cost=-0.00293856,   cost1=3.56531e-08, cost2=-0.00293859 :  mean(hits)=0.749811, mean(diffs)=0.979531\n",
      "34: eta=0.102422 cost=-0.00293856 jtype=constrained costheta=-0.002 ps=[-0.0946094, -0.75342, 1.57897, -3.36357, 0.125003]\n",
      "-- cost=-0.00293869,   cost1=4.14224e-08, cost2=-0.00293873 :  mean(hits)=0.749796, mean(diffs)=0.979576\n",
      "35: eta=0.112664 cost=-0.00293869 jtype=Newton costheta=-0.001 ps=[-0.0998685, -0.757333, 1.60175, -3.41426, 0.121983]\n",
      "-- cost=-0.00293878,   cost1=8.36299e-09, cost2=-0.00293879 :  mean(hits)=0.749909, mean(diffs)=0.979596\n",
      "36: eta=0.12393 cost=-0.00293878 jtype=Newton costheta=-0.001 ps=[-0.103349, -0.759929, 1.61709, -3.44911, 0.119844]\n",
      "-- cost=-0.00293881,   cost1=8.37884e-08, cost2=-0.00293889 :  mean(hits)=0.749711, mean(diffs)=0.97963\n",
      "37: eta=0.136323 cost=-0.00293881 jtype=Newton costheta=-0.001 ps=[-0.108362, -0.763726, 1.63877, -3.49622, 0.116854]\n",
      "-- cost=-0.00293888,   cost1=4.20086e-12, cost2=-0.00293888 :  mean(hits)=0.749998, mean(diffs)=0.979628\n",
      "38: eta=0.149956 cost=-0.00293888 jtype=Newton costheta=-0.001 ps=[-0.109818, -0.764804, 1.64566, -3.51266, 0.115794]\n",
      "-- cost=-0.0029377,   cost1=1.92904e-06, cost2=-0.00293963 :  mean(hits)=0.748611, mean(diffs)=0.979877\n",
      "eta going down: new_cost-cost=1.18179e-06 and jumptype='constrained'\n",
      "39: eta=0.0749779 cost=-0.00293888 jtype=constrained costheta=NaN ps=[-0.109818, -0.764804, 1.64566, -3.51266, 0.115794]\n",
      "-- cost=-0.00293887,   cost1=1.04061e-07, cost2=-0.00293897 :  mean(hits)=0.749677, mean(diffs)=0.979657\n",
      "eta going down: new_cost-cost=1.78223e-08 and jumptype='constrained'\n",
      "40: eta=0.037489 cost=-0.00293888 jtype=constrained costheta=NaN ps=[-0.109818, -0.764804, 1.64566, -3.51266, 0.115794]\n",
      "-- cost=-0.00293891,   cost1=6.13699e-09, cost2=-0.00293891 :  mean(hits)=0.749922, mean(diffs)=0.979638\n",
      "41: eta=0.0412378 cost=-0.00293891 jtype=constrained costheta=-0.029 ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00209909,   cost1=0.000899869, cost2=-0.00299895 :  mean(hits)=0.779998, mean(diffs)=0.999652\n",
      "eta going down: new_cost-cost=0.000839821 and jumptype='constrained'\n",
      "42: eta=0.0206189 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00209908,   cost1=0.000899865, cost2=-0.00299894 :  mean(hits)=0.779998, mean(diffs)=0.999648\n",
      "eta going down: new_cost-cost=0.000839828 and jumptype='constrained'\n",
      "43: eta=0.0103095 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00209904,   cost1=0.000899838, cost2=-0.00299888 :  mean(hits)=0.779997, mean(diffs)=0.999625\n",
      "eta going down: new_cost-cost=0.000839869 and jumptype='constrained'\n",
      "44: eta=0.00515473 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00228001,   cost1=0.000672223, cost2=-0.00295223 :  mean(hits)=0.775927, mean(diffs)=0.984078\n",
      "eta going down: new_cost-cost=0.000658897 and jumptype='constrained'\n",
      "45: eta=0.00257737 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.0028976,   cost1=0.000100016, cost2=-0.00299761 :  mean(hits)=0.760001, mean(diffs)=0.999204\n",
      "eta going down: new_cost-cost=4.13094e-05 and jumptype='constrained'\n",
      "46: eta=0.00128868 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00289594,   cost1=9.69889e-05, cost2=-0.00299293 :  mean(hits)=0.759848, mean(diffs)=0.997645\n",
      "eta going down: new_cost-cost=4.29624e-05 and jumptype='constrained'\n",
      "47: eta=0.000644341 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00289846,   cost1=7.42642e-05, cost2=-0.00297273 :  mean(hits)=0.758618, mean(diffs)=0.990909\n",
      "eta going down: new_cost-cost=4.0444e-05 and jumptype='constrained'\n",
      "48: eta=0.000322171 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00291812,   cost1=3.3319e-05, cost2=-0.00295144 :  mean(hits)=0.755772, mean(diffs)=0.983815\n",
      "eta going down: new_cost-cost=2.07823e-05 and jumptype='constrained'\n",
      "49: eta=0.000161085 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00293241,   cost1=9.96215e-06, cost2=-0.00294237 :  mean(hits)=0.753156, mean(diffs)=0.980791\n",
      "eta going down: new_cost-cost=6.49675e-06 and jumptype='constrained'\n",
      "50: eta=8.05427e-05 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00293726,   cost1=2.5067e-06, cost2=-0.00293976 :  mean(hits)=0.751583, mean(diffs)=0.979921\n",
      "eta going down: new_cost-cost=1.65185e-06 and jumptype='constrained'\n",
      "51: eta=4.02713e-05 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00293853,   cost1=5.74493e-07, cost2=-0.0029391 :  mean(hits)=0.750758, mean(diffs)=0.979702\n",
      "eta going down: new_cost-cost=3.76922e-07 and jumptype='constrained'\n",
      "52: eta=2.01357e-05 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00293883,   cost1=1.15893e-07, cost2=-0.00293895 :  mean(hits)=0.75034, mean(diffs)=0.97965\n",
      "eta going down: new_cost-cost=7.3048e-08 and jumptype='constrained'\n",
      "53: eta=1.00678e-05 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.0029389,   cost1=1.71889e-08, cost2=-0.00293892 :  mean(hits)=0.750131, mean(diffs)=0.979639\n",
      "About to break -- tol=1e-08, new_cost-cost=7.49351e-09, eta=1.00678e-05\n",
      "-- cost=-0.00293891,   cost1=6.13699e-09, cost2=-0.00293891 :  mean(hits)=0.749922, mean(diffs)=0.979638\n",
      "-- cost=-0.00293891,   cost1=6.13699e-09, cost2=-0.00293891 :  mean(hits)=0.749922, mean(diffs)=0.979638\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1×2 Array{Float64,2}:\n",
       " 0.487796  0.749922"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following sequence leads to a situation where having only [-0.8, -0.8] as the single finalFluxPoint \n",
    "# leads to the minimization getting stuck.  Adding further finalFluxPoints solves the problem\n",
    "\n",
    "dt = 0.02\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "mW = -4\n",
    "sW = 0.2\n",
    "noise = 0\n",
    "input = 0\n",
    "sigma = 0.1\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :sW=>sW, :mW=>mW, :W=>[sW mW ; mW sW], :nsteps=>nsteps, \n",
    ":noise=>noise, :input=>input, :sigma=>sigma, :const_add=>0, :init_add=>0)\n",
    "\n",
    "\n",
    "# WORKING gradient:\n",
    "# ForwardDiff.gradient((x)->JJ(startU; do_plot=true, nderivs=length(x), difforder=1, \n",
    "#    make_dict([[\"init_add\" 2], \"const_add\"], x, model_params)...), [2.9, -2.9, 0.1])\n",
    "\n",
    "\n",
    "\n",
    "# The backward and costfunc functions should turn a single-scalar parameter W into the matrix W\n",
    "# backward always runs with no within-forward noise, i.e., sigma=0\n",
    "backward = (endpoint; do_plot=false, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if ~haskey(pars, :sW) || ~haskey(pars, :mW) \n",
    "        error(\"Need both sW and mW to determine weight matrix\")\n",
    "    end\n",
    "    pars=make_dict([\"W\"], [[pars[:sW] pars[:mW] ; pars[:mW] pars[:sW]]], pars);\n",
    "    backwardsModel(endpoint; do_plot=do_plot, make_dict([\"sigma\"], [0], pars)...)[1]\n",
    "end\n",
    "\n",
    "forward = (startpoint; pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if ~haskey(pars, :sW) || ~haskey(pars, :mW) \n",
    "        error(\"Need both sW and mW to determine weight matrix\")\n",
    "    end\n",
    "    pars=make_dict([\"W\"], [[pars[:sW] pars[:mW] ; pars[:mW] pars[:sW]]], pars);\n",
    "    forwardsModel(startpoint; pars...)\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "costfunc = (startpoints; do_plot=false, verbose=false, beta=0.003, nderivs=0, difforder=0, sr=26, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if ~haskey(pars, :sW) || ~haskey(pars, :mW) \n",
    "        error(\"Need both sW and mW to determine weight matrix\")\n",
    "    end\n",
    "    pars=make_dict([\"W\"], [[pars[:sW] pars[:mW] ; pars[:mW] pars[:sW]]], pars);\n",
    "    JJ(startpoints; seedrand=sr, beta=beta, \n",
    "        do_plot=do_plot, verbose=verbose, nderivs=nderivs, difforder=difforder, pars...)\n",
    "end\n",
    "\n",
    "function JJ(initUs; theta1=0.15, theta2=0.2, beta=0.003, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, pre_string=\"\", zero_last_sigmas=0, seedrand=NaN, params...)\n",
    "\n",
    "    if ~isnan(seedrand); srand(seedrand); end\n",
    "    \n",
    "    Vend = ForwardDiffZeros(size(initUs,1), size(initUs,2), nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:size(initUs,1)\n",
    "        if false # i>size(initUs,1) - zero_last_sigmas\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; sigma=0, nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)            \n",
    "        else\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)\n",
    "        end\n",
    "        Vend[i,:] = Ve\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend[:,1]-Vend[:,2])/theta1))\n",
    "    diffs = tanh.((Vend[:,1]-Vend[:,2])/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "\n",
    "    if do_plot\n",
    "        title(@sprintf(\"mean(hits)=%g, mean(diffs)=%g\", convert(Float64, mean(hits)), convert(Float64, mean(diffs))))\n",
    "    end\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"%s\", pre_string)\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2, mean(hits), mean(diffs)\n",
    "end\n",
    "  \n",
    "\n",
    "\n",
    "beta = 0.003\n",
    "args = [[\"start_add\" 2], \"const_add\", \"mW\", \"sW\"] \n",
    "seed = [-0.2, 0.2, 0.2, -1.5, 0.1] \n",
    "walls = Dict(:start_add=>[-5.1, 5.1], :sW=>[0, 5.1], :mW=>[-5.1, 5.1]) # \n",
    "# sr =  gives \n",
    "\n",
    "\n",
    "new_random_seed = true; if new_random_seed\n",
    "    sr = convert(Int64, round(time()))\n",
    "else\n",
    "    sr = old_sr\n",
    "end\n",
    "# sr = 1504716566\n",
    "old_sr = sr\n",
    "\n",
    "srand(sr)\n",
    "\n",
    "startU=randn(50,2)-3\n",
    "\n",
    "\n",
    "clf()\n",
    "print(\"seed = \"); print_vector_g(seed); print(\"\\n\")\n",
    "ocost, omhits, omdiffs = costfunc(startU; do_plot=true, sr=sr, verbose=true, make_dict(args, seed, model_params)...)\n",
    "\n",
    "\n",
    "params, traj, zz, cpm_traj = bbox_Hessian_keyword_minimization(seed, args, walls,  \n",
    "(;params...) -> costfunc(startU; beta=beta, sr=sr, do_plot=true, verbose=true, merge(model_params, Dict(params))...)[1], \n",
    "verbose=true, start_eta=1, tol=1e-8, softbox=true, maxiter=400 )\n",
    "\n",
    "# params, cost, ptraj, gtraj = fluxSense(costfunc, backward, model_params, startU, fluxFinalPoint, args, seed; \n",
    "#    start_eta=0.01, tol=1e-15, maxiter=400, verbose=true, report_every=1, do_plot=false, cost_limit=cost_limit) # cost_limit=-0.000935) # for beta=0.01\n",
    "\n",
    "# And show the final position\n",
    "figure(1); clf()\n",
    "cost, mhits, mdiffs = \n",
    "    costfunc(startU; beta=beta, do_plot=true, sr=sr, verbose=true, make_dict(args, params, model_params)...)\n",
    "\n",
    "repeat_results_in_fig2 = true; if repeat_results_in_fig2\n",
    "    figure(2); clf()\n",
    "    costfunc(startU; beta=beta, do_plot=true, sr=sr, verbose=true, \n",
    "        make_dict(args, params, merge(Dict(:fignum=>2), model_params))...)\n",
    "    figure(1); \n",
    "end\n",
    "params'\n",
    "\n",
    "# For beta=0, and ntrials=20, we collected a bunch of results and observed that it failes about half the time\n",
    "# WHEN the initial mean)hits) is below 0.5.  It never fails if the initial mean(hits) is above 0.5. \n",
    "# Seems like when it starts below 0.5, the fastest way to increase mean(hits) is to push it to 0.5 and floor it there.\n",
    "# The results were collected in \"Results.mat\"\n",
    "# res = [res ; omhits mhits]\n",
    "\n",
    "[omhits mhits]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of successful minimization including self-connection weights\n",
    "\n",
    "Amazing. Adding self-connection weights seems to make the minimization process even more robust: we can now start from start_add = [-0.2, 0.2], which usually gives mean(hits) < 0.5, and **still** reach a successful solution.  Without the self-connection weights that rarely happened, mean(hits) < 0.5 was a bad start, and start_add = [-0.2, 0.2] most often did not lead to success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed = [-0.2, 0.2, 0.2, -1.5, 0.1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition JJ(Any) in module Main at In[47]:62 overwritten at In[48]:62.\n",
      "WARNING: Method definition #JJ(Array{Any, 1}, Main.#JJ, Any) in module Main overwritten.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- cost=0.0686324,   cost1=0.0687511, cost2=-0.00011865 :  mean(hits)=0.487796, mean(diffs)=0.0395499\n",
      "0: eta=1 ps=[-0.200, 0.200, 0.200, -1.500, 0.100]\n",
      "-- cost=0.0686324,   cost1=0.0687511, cost2=-0.00011865 :  mean(hits)=0.487796, mean(diffs)=0.0395499\n",
      "-- cost=0.0662608,   cost1=0.0663306, cost2=-6.98297e-05 :  mean(hits)=0.492453, mean(diffs)=0.0232766\n",
      "1: eta=1.1 cost=0.0662608 jtype=Newton costheta=-0.343 ps=[-0.112938, 0.349709, 0.101748, -1.26536, 0.103245]\n",
      "-- cost=0.0656768,   cost1=0.0657292, cost2=-5.24826e-05 :  mean(hits)=0.493623, mean(diffs)=0.0174942\n",
      "2: eta=1.21 cost=0.0656768 jtype=Newton costheta=-0.188 ps=[0.0772162, 0.616916, 0.0618108, -0.989811, 0.103702]\n",
      "-- cost=0.0658205,   cost1=0.0658659, cost2=-4.54283e-05 :  mean(hits)=0.493357, mean(diffs)=0.0151428\n",
      "eta going down: new_cost-cost=0.000143696 and jumptype='Newton'\n",
      "3: eta=0.605 cost=0.0656768 jtype=Newton costheta=NaN ps=[0.0772162, 0.616916, 0.0618108, -0.989811, 0.103702]\n",
      "-- cost=0.0658205,   cost1=0.0658659, cost2=-4.54283e-05 :  mean(hits)=0.493357, mean(diffs)=0.0151428\n",
      "eta going down: new_cost-cost=0.000143696 and jumptype='Newton'\n",
      "4: eta=0.3025 cost=0.0656768 jtype=Newton costheta=NaN ps=[0.0772162, 0.616916, 0.0618108, -0.989811, 0.103702]\n",
      "-- cost=0.0645488,   cost1=0.0646014, cost2=-5.25525e-05 :  mean(hits)=0.495832, mean(diffs)=0.0175175\n",
      "5: eta=0.33275 cost=0.0645488 jtype=constrained costheta=-0.719 ps=[0.274727, 0.41869, 0.0155098, -1.08603, 0.104242]\n",
      "-- cost=0.0614765,   cost1=0.0615494, cost2=-7.28586e-05 :  mean(hits)=0.501909, mean(diffs)=0.0242862\n",
      "6: eta=0.366025 cost=0.0614765 jtype=constrained costheta=-0.946 ps=[0.455695, 0.244242, 0.132879, -1.25726, 0.105988]\n",
      "-- cost=0.0486282,   cost1=0.0487824, cost2=-0.000154226 :  mean(hits)=0.529133, mean(diffs)=0.0514088\n",
      "7: eta=0.402628 cost=0.0486282 jtype=constrained costheta=-0.997 ps=[0.606886, 0.105449, 0.312739, -1.48064, 0.108562]\n",
      "-- cost=0.0133079,   cost1=0.0139258, cost2=-0.00061789 :  mean(hits)=0.631993, mean(diffs)=0.205963\n",
      "8: eta=0.44289 cost=0.0133079 jtype=constrained costheta=-1.000 ps=[0.755649, -0.026157, 0.504166, -1.74103, 0.111807]\n",
      "-- cost=0.000257413,   cost1=0.00210511, cost2=-0.00184769 :  mean(hits)=0.795881, mean(diffs)=0.615897\n",
      "9: eta=0.487179 cost=0.000257413 jtype=constrained costheta=-0.978 ps=[0.975464, -0.236981, 0.654068, -1.97877, 0.116327]\n",
      "-- cost=-0.00157669,   cost1=6.96102e-07, cost2=-0.00157738 :  mean(hits)=0.750834, mean(diffs)=0.525794\n",
      "10: eta=0.535897 cost=-0.00157669 jtype=Newton costheta=-0.587 ps=[1.02164, -0.0108635, 0.62631, -1.9228, 0.117996]\n",
      "-- cost=-0.00192434,   cost1=0.000676274, cost2=-0.00260062 :  mean(hits)=0.723995, mean(diffs)=0.866872\n",
      "11: eta=0.589487 cost=-0.00192434 jtype=constrained costheta=-0.796 ps=[0.821785, 0.247223, 0.818342, -2.23205, 0.123257]\n",
      "-- cost=-0.00271411,   cost1=6.39525e-05, cost2=-0.00277807 :  mean(hits)=0.742003, mean(diffs)=0.926022\n",
      "12: eta=0.648436 cost=-0.00271411 jtype=Newton costheta=-0.556 ps=[0.821891, 0.193543, 0.875512, -2.32724, 0.121511]\n",
      "-- cost=-0.00284202,   cost1=2.05373e-05, cost2=-0.00286256 :  mean(hits)=0.745468, mean(diffs)=0.954186\n",
      "13: eta=0.713279 cost=-0.00284202 jtype=Newton costheta=-0.248 ps=[0.72397, 0.0815275, 0.92699, -2.41553, 0.119098]\n",
      "-- cost=-0.00288506,   cost1=8.78345e-06, cost2=-0.00289385 :  mean(hits)=0.747036, mean(diffs)=0.964616\n",
      "14: eta=0.784607 cost=-0.00288506 jtype=Newton costheta=-0.210 ps=[0.603214, -0.0548918, 0.975406, -2.49512, 0.116614]\n",
      "-- cost=-0.00290096,   cost1=6.46895e-06, cost2=-0.00290743 :  mean(hits)=0.747457, mean(diffs)=0.969143\n",
      "15: eta=0.863068 cost=-0.00290096 jtype=Newton costheta=-0.147 ps=[0.375301, -0.293363, 1.03034, -2.58254, 0.1136]\n",
      "-- cost=-0.00291064,   cost1=1.10793e-05, cost2=-0.00292172 :  mean(hits)=0.746671, mean(diffs)=0.973907\n",
      "16: eta=0.949375 cost=-0.00291064 jtype=Newton costheta=-0.125 ps=[-0.101739, -0.779492, 1.10727, -2.71391, 0.109042]\n",
      "-- cost=-0.00209882,   cost1=0.000899802, cost2=-0.00299862 :  mean(hits)=0.779997, mean(diffs)=0.999541\n",
      "eta going down: new_cost-cost=0.000811819 and jumptype='constrained'\n",
      "17: eta=0.474687 cost=-0.00291064 jtype=constrained costheta=NaN ps=[-0.101739, -0.779492, 1.10727, -2.71391, 0.109042]\n",
      "-- cost=-0.00229665,   cost1=0.000648625, cost2=-0.00294527 :  mean(hits)=0.775468, mean(diffs)=0.981758\n",
      "eta going down: new_cost-cost=0.000613992 and jumptype='constrained'\n",
      "18: eta=0.237344 cost=-0.00291064 jtype=constrained costheta=NaN ps=[-0.101739, -0.779492, 1.10727, -2.71391, 0.109042]\n",
      "-- cost=-0.00286845,   cost1=4.55062e-05, cost2=-0.00291396 :  mean(hits)=0.756746, mean(diffs)=0.971319\n",
      "eta going down: new_cost-cost=4.21903e-05 and jumptype='constrained'\n",
      "19: eta=0.118672 cost=-0.00291064 jtype=constrained costheta=NaN ps=[-0.101739, -0.779492, 1.10727, -2.71391, 0.109042]\n",
      "-- cost=-0.00291069,   cost1=1.37804e-06, cost2=-0.00291207 :  mean(hits)=0.751174, mean(diffs)=0.97069\n",
      "20: eta=0.130539 cost=-0.00291069 jtype=constrained costheta=-0.090 ps=[-0.0962228, -0.781568, 1.15767, -2.78912, 0.110374]\n",
      "-- cost=-0.00292144,   cost1=7.15913e-06, cost2=-0.0029286 :  mean(hits)=0.747324, mean(diffs)=0.976199\n",
      "21: eta=0.143593 cost=-0.00292144 jtype=constrained costheta=-0.039 ps=[-0.0946473, -0.771467, 1.21975, -2.86746, 0.111792]\n",
      "-- cost=-0.00292227,   cost1=3.77572e-06, cost2=-0.00292604 :  mean(hits)=0.751943, mean(diffs)=0.975347\n",
      "22: eta=0.157952 cost=-0.00292227 jtype=constrained costheta=-0.035 ps=[-0.0923132, -0.770222, 1.27629, -2.95537, 0.11353]\n",
      "-- cost=-0.00292847,   cost1=9.24347e-06, cost2=-0.00293772 :  mean(hits)=0.74696, mean(diffs)=0.979239\n",
      "23: eta=0.173747 cost=-0.00292847 jtype=constrained costheta=-0.018 ps=[-0.0800473, -0.749842, 1.36128, -3.04012, 0.115212]\n",
      "-- cost=-0.00291325,   cost1=3.26152e-05, cost2=-0.00294587 :  mean(hits)=0.755711, mean(diffs)=0.981955\n",
      "eta going down: new_cost-cost=1.52223e-05 and jumptype='constrained'\n",
      "24: eta=0.0868737 cost=-0.00292847 jtype=constrained costheta=NaN ps=[-0.0800473, -0.749842, 1.36128, -3.04012, 0.115212]\n",
      "-- cost=-0.00292713,   cost1=9.25892e-06, cost2=-0.00293639 :  mean(hits)=0.753043, mean(diffs)=0.978795\n",
      "eta going down: new_cost-cost=1.34679e-06 and jumptype='constrained'\n",
      "25: eta=0.0434369 cost=-0.00292847 jtype=constrained costheta=NaN ps=[-0.0800473, -0.749842, 1.36128, -3.04012, 0.115212]\n",
      "-- cost=-0.00293064,   cost1=3.26848e-06, cost2=-0.00293391 :  mean(hits)=0.751808, mean(diffs)=0.97797\n",
      "26: eta=0.0477805 cost=-0.00293064 jtype=constrained costheta=-0.042 ps=[-0.0780774, -0.74942, 1.37648, -3.06588, 0.115771]\n",
      "-- cost=-0.00293452,   cost1=1.12552e-06, cost2=-0.00293565 :  mean(hits)=0.748939, mean(diffs)=0.97855\n",
      "27: eta=0.0525586 cost=-0.00293452 jtype=constrained costheta=-0.022 ps=[-0.0767747, -0.745522, 1.39617, -3.0931, 0.116353]\n",
      "-- cost=-0.00293575,   cost1=2.00775e-09, cost2=-0.00293575 :  mean(hits)=0.750045, mean(diffs)=0.978583\n",
      "28: eta=0.0578145 cost=-0.00293575 jtype=constrained costheta=-0.011 ps=[-0.0774677, -0.745455, 1.41562, -3.12338, 0.117064]\n",
      "-- cost=-0.00293654,   cost1=6.97725e-08, cost2=-0.00293661 :  mean(hits)=0.749736, mean(diffs)=0.978869\n",
      "29: eta=0.0635959 cost=-0.00293654 jtype=constrained costheta=-0.013 ps=[-0.0783256, -0.744805, 1.44056, -3.15536, 0.117805]\n",
      "-- cost=-0.00293718,   cost1=1.22753e-08, cost2=-0.00293719 :  mean(hits)=0.749889, mean(diffs)=0.979063\n",
      "30: eta=0.0699555 cost=-0.00293718 jtype=constrained costheta=-0.018 ps=[-0.0801053, -0.745193, 1.46612, -3.19054, 0.118677]\n",
      "-- cost=-0.00293768,   cost1=2.15906e-08, cost2=-0.0029377 :  mean(hits)=0.749853, mean(diffs)=0.979234\n",
      "31: eta=0.076951 cost=-0.00293768 jtype=constrained costheta=-0.096 ps=[-0.0821453, -0.745674, 1.49504, -3.22846, 0.119642]\n",
      "-- cost=-0.00293807,   cost1=1.7614e-08, cost2=-0.00293809 :  mean(hits)=0.749867, mean(diffs)=0.979363\n",
      "32: eta=0.0846461 cost=-0.00293807 jtype=constrained costheta=-0.009 ps=[-0.0852402, -0.747187, 1.52449, -3.26998, 0.120797]\n",
      "-- cost=-0.00293835,   cost1=2.79069e-08, cost2=-0.00293838 :  mean(hits)=0.749833, mean(diffs)=0.979461\n",
      "33: eta=0.0931108 cost=-0.00293835 jtype=constrained costheta=-0.005 ps=[-0.0894229, -0.749758, 1.55398, -3.31527, 0.122236]\n",
      "-- cost=-0.00293856,   cost1=3.56531e-08, cost2=-0.00293859 :  mean(hits)=0.749811, mean(diffs)=0.979531\n",
      "34: eta=0.102422 cost=-0.00293856 jtype=constrained costheta=-0.002 ps=[-0.0946094, -0.75342, 1.57897, -3.36357, 0.125003]\n",
      "-- cost=-0.00293869,   cost1=4.14224e-08, cost2=-0.00293873 :  mean(hits)=0.749796, mean(diffs)=0.979576\n",
      "35: eta=0.112664 cost=-0.00293869 jtype=Newton costheta=-0.001 ps=[-0.0998685, -0.757333, 1.60175, -3.41426, 0.121983]\n",
      "-- cost=-0.00293878,   cost1=8.36299e-09, cost2=-0.00293879 :  mean(hits)=0.749909, mean(diffs)=0.979596\n",
      "36: eta=0.12393 cost=-0.00293878 jtype=Newton costheta=-0.001 ps=[-0.103349, -0.759929, 1.61709, -3.44911, 0.119844]\n",
      "-- cost=-0.00293881,   cost1=8.37884e-08, cost2=-0.00293889 :  mean(hits)=0.749711, mean(diffs)=0.97963\n",
      "37: eta=0.136323 cost=-0.00293881 jtype=Newton costheta=-0.001 ps=[-0.108362, -0.763726, 1.63877, -3.49622, 0.116854]\n",
      "-- cost=-0.00293888,   cost1=4.20086e-12, cost2=-0.00293888 :  mean(hits)=0.749998, mean(diffs)=0.979628\n",
      "38: eta=0.149956 cost=-0.00293888 jtype=Newton costheta=-0.001 ps=[-0.109818, -0.764804, 1.64566, -3.51266, 0.115794]\n",
      "-- cost=-0.0029377,   cost1=1.92904e-06, cost2=-0.00293963 :  mean(hits)=0.748611, mean(diffs)=0.979877\n",
      "eta going down: new_cost-cost=1.18179e-06 and jumptype='constrained'\n",
      "39: eta=0.0749779 cost=-0.00293888 jtype=constrained costheta=NaN ps=[-0.109818, -0.764804, 1.64566, -3.51266, 0.115794]\n",
      "-- cost=-0.00293887,   cost1=1.04061e-07, cost2=-0.00293897 :  mean(hits)=0.749677, mean(diffs)=0.979657\n",
      "eta going down: new_cost-cost=1.78223e-08 and jumptype='constrained'\n",
      "40: eta=0.037489 cost=-0.00293888 jtype=constrained costheta=NaN ps=[-0.109818, -0.764804, 1.64566, -3.51266, 0.115794]\n",
      "-- cost=-0.00293891,   cost1=6.13699e-09, cost2=-0.00293891 :  mean(hits)=0.749922, mean(diffs)=0.979638\n",
      "41: eta=0.0412378 cost=-0.00293891 jtype=constrained costheta=-0.029 ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00209909,   cost1=0.000899869, cost2=-0.00299895 :  mean(hits)=0.779998, mean(diffs)=0.999652\n",
      "eta going down: new_cost-cost=0.000839821 and jumptype='constrained'\n",
      "42: eta=0.0206189 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00209908,   cost1=0.000899865, cost2=-0.00299894 :  mean(hits)=0.779998, mean(diffs)=0.999648\n",
      "eta going down: new_cost-cost=0.000839828 and jumptype='constrained'\n",
      "43: eta=0.0103095 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00209904,   cost1=0.000899838, cost2=-0.00299888 :  mean(hits)=0.779997, mean(diffs)=0.999625\n",
      "eta going down: new_cost-cost=0.000839869 and jumptype='constrained'\n",
      "44: eta=0.00515473 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00228001,   cost1=0.000672223, cost2=-0.00295223 :  mean(hits)=0.775927, mean(diffs)=0.984078\n",
      "eta going down: new_cost-cost=0.000658897 and jumptype='constrained'\n",
      "45: eta=0.00257737 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.0028976,   cost1=0.000100016, cost2=-0.00299761 :  mean(hits)=0.760001, mean(diffs)=0.999204\n",
      "eta going down: new_cost-cost=4.13094e-05 and jumptype='constrained'\n",
      "46: eta=0.00128868 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00289594,   cost1=9.69889e-05, cost2=-0.00299293 :  mean(hits)=0.759848, mean(diffs)=0.997645\n",
      "eta going down: new_cost-cost=4.29624e-05 and jumptype='constrained'\n",
      "47: eta=0.000644341 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00289846,   cost1=7.42642e-05, cost2=-0.00297273 :  mean(hits)=0.758618, mean(diffs)=0.990909\n",
      "eta going down: new_cost-cost=4.0444e-05 and jumptype='constrained'\n",
      "48: eta=0.000322171 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00291812,   cost1=3.3319e-05, cost2=-0.00295144 :  mean(hits)=0.755772, mean(diffs)=0.983815\n",
      "eta going down: new_cost-cost=2.07823e-05 and jumptype='constrained'\n",
      "49: eta=0.000161085 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00293241,   cost1=9.96215e-06, cost2=-0.00294237 :  mean(hits)=0.753156, mean(diffs)=0.980791\n",
      "eta going down: new_cost-cost=6.49675e-06 and jumptype='constrained'\n",
      "50: eta=8.05427e-05 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00293726,   cost1=2.5067e-06, cost2=-0.00293976 :  mean(hits)=0.751583, mean(diffs)=0.979921\n",
      "eta going down: new_cost-cost=1.65185e-06 and jumptype='constrained'\n",
      "51: eta=4.02713e-05 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00293853,   cost1=5.74493e-07, cost2=-0.0029391 :  mean(hits)=0.750758, mean(diffs)=0.979702\n",
      "eta going down: new_cost-cost=3.76922e-07 and jumptype='constrained'\n",
      "52: eta=2.01357e-05 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00293883,   cost1=1.15893e-07, cost2=-0.00293895 :  mean(hits)=0.75034, mean(diffs)=0.97965\n",
      "eta going down: new_cost-cost=7.3048e-08 and jumptype='constrained'\n",
      "53: eta=1.00678e-05 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.0029389,   cost1=1.71889e-08, cost2=-0.00293892 :  mean(hits)=0.750131, mean(diffs)=0.979639\n",
      "About to break -- tol=1e-08, new_cost-cost=7.49351e-09, eta=1.00678e-05\n",
      "-- cost=-0.00293891,   cost1=6.13699e-09, cost2=-0.00293891 :  mean(hits)=0.749922, mean(diffs)=0.979638\n",
      "-- cost=-0.00293891,   cost1=6.13699e-09, cost2=-0.00293891 :  mean(hits)=0.749922, mean(diffs)=0.979638\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1×2 Array{Float64,2}:\n",
       " 0.487796  0.749922"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following sequence leads to a situation where having only [-0.8, -0.8] as the single finalFluxPoint \n",
    "# leads to the minimization getting stuck.  Adding further finalFluxPoints solves the problem\n",
    "\n",
    "dt = 0.02\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "mW = -4\n",
    "sW = 0.2\n",
    "noise = 0\n",
    "input = 0\n",
    "sigma = 0.1\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :sW=>sW, :mW=>mW, :W=>[sW mW ; mW sW], :nsteps=>nsteps, \n",
    ":noise=>noise, :input=>input, :sigma=>sigma, :const_add=>0, :init_add=>0)\n",
    "\n",
    "\n",
    "# WORKING gradient:\n",
    "# ForwardDiff.gradient((x)->JJ(startU; do_plot=true, nderivs=length(x), difforder=1, \n",
    "#    make_dict([[\"init_add\" 2], \"const_add\"], x, model_params)...), [2.9, -2.9, 0.1])\n",
    "\n",
    "\n",
    "\n",
    "# The backward and costfunc functions should turn a single-scalar parameter W into the matrix W\n",
    "# backward always runs with no within-forward noise, i.e., sigma=0\n",
    "backward = (endpoint; do_plot=false, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if ~haskey(pars, :sW) || ~haskey(pars, :mW) \n",
    "        error(\"Need both sW and mW to determine weight matrix\")\n",
    "    end\n",
    "    pars=make_dict([\"W\"], [[pars[:sW] pars[:mW] ; pars[:mW] pars[:sW]]], pars);\n",
    "    backwardsModel(endpoint; do_plot=do_plot, make_dict([\"sigma\"], [0], pars)...)[1]\n",
    "end\n",
    "\n",
    "forward = (startpoint; pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if ~haskey(pars, :sW) || ~haskey(pars, :mW) \n",
    "        error(\"Need both sW and mW to determine weight matrix\")\n",
    "    end\n",
    "    pars=make_dict([\"W\"], [[pars[:sW] pars[:mW] ; pars[:mW] pars[:sW]]], pars);\n",
    "    forwardsModel(startpoint; pars...)\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "costfunc = (startpoints; do_plot=false, verbose=false, beta=0.003, nderivs=0, difforder=0, sr=26, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if ~haskey(pars, :sW) || ~haskey(pars, :mW) \n",
    "        error(\"Need both sW and mW to determine weight matrix\")\n",
    "    end\n",
    "    pars=make_dict([\"W\"], [[pars[:sW] pars[:mW] ; pars[:mW] pars[:sW]]], pars);\n",
    "    JJ(startpoints; seedrand=sr, beta=beta, \n",
    "        do_plot=do_plot, verbose=verbose, nderivs=nderivs, difforder=difforder, pars...)\n",
    "end\n",
    "\n",
    "function JJ(initUs; theta1=0.15, theta2=0.2, beta=0.003, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, pre_string=\"\", zero_last_sigmas=0, seedrand=NaN, params...)\n",
    "\n",
    "    if ~isnan(seedrand); srand(seedrand); end\n",
    "    \n",
    "    Vend = ForwardDiffZeros(size(initUs,1), size(initUs,2), nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:size(initUs,1)\n",
    "        if false # i>size(initUs,1) - zero_last_sigmas\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; sigma=0, nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)            \n",
    "        else\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)\n",
    "        end\n",
    "        Vend[i,:] = Ve\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend[:,1]-Vend[:,2])/theta1))\n",
    "    diffs = tanh.((Vend[:,1]-Vend[:,2])/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "\n",
    "    if do_plot\n",
    "        title(@sprintf(\"mean(hits)=%g, mean(diffs)=%g\", convert(Float64, mean(hits)), convert(Float64, mean(diffs))))\n",
    "    end\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"%s\", pre_string)\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2, mean(hits), mean(diffs)\n",
    "end\n",
    "  \n",
    "\n",
    "\n",
    "beta = 0.003\n",
    "args = [[\"start_add\" 2], \"const_add\", \"mW\", \"sW\"] \n",
    "seed = [-0.2, 0.2, 0.2, -1.5, 0.1] \n",
    "walls = Dict(:start_add=>[-5.1, 5.1], :sW=>[0, 5.1], :mW=>[-5.1, 5.1]) # \n",
    "# sr =  gives \n",
    "\n",
    "\n",
    "new_random_seed = true; if new_random_seed\n",
    "    sr = convert(Int64, round(time()))\n",
    "else\n",
    "    sr = old_sr\n",
    "end\n",
    "# sr = 1504716566\n",
    "old_sr = sr\n",
    "\n",
    "srand(sr)\n",
    "\n",
    "startU=randn(50,2)-3\n",
    "\n",
    "\n",
    "clf()\n",
    "print(\"seed = \"); print_vector_g(seed); print(\"\\n\")\n",
    "ocost, omhits, omdiffs = costfunc(startU; do_plot=true, sr=sr, verbose=true, make_dict(args, seed, model_params)...)\n",
    "\n",
    "\n",
    "params, traj, zz, cpm_traj = bbox_Hessian_keyword_minimization(seed, args, walls,  \n",
    "(;params...) -> costfunc(startU; beta=beta, sr=sr, do_plot=true, verbose=true, merge(model_params, Dict(params))...)[1], \n",
    "verbose=true, start_eta=1, tol=1e-8, softbox=true, maxiter=400 )\n",
    "\n",
    "# params, cost, ptraj, gtraj = fluxSense(costfunc, backward, model_params, startU, fluxFinalPoint, args, seed; \n",
    "#    start_eta=0.01, tol=1e-15, maxiter=400, verbose=true, report_every=1, do_plot=false, cost_limit=cost_limit) # cost_limit=-0.000935) # for beta=0.01\n",
    "\n",
    "# And show the final position\n",
    "figure(1); clf()\n",
    "cost, mhits, mdiffs = \n",
    "    costfunc(startU; beta=beta, do_plot=true, sr=sr, verbose=true, make_dict(args, params, model_params)...)\n",
    "\n",
    "repeat_results_in_fig2 = true; if repeat_results_in_fig2\n",
    "    figure(2); clf()\n",
    "    costfunc(startU; beta=beta, do_plot=true, sr=sr, verbose=true, \n",
    "        make_dict(args, params, merge(Dict(:fignum=>2), model_params))...)\n",
    "    figure(1); \n",
    "end\n",
    "params'\n",
    "\n",
    "# For beta=0, and ntrials=20, we collected a bunch of results and observed that it failes about half the time\n",
    "# WHEN the initial mean)hits) is below 0.5.  It never fails if the initial mean(hits) is above 0.5. \n",
    "# Seems like when it starts below 0.5, the fastest way to increase mean(hits) is to push it to 0.5 and floor it there.\n",
    "# The results were collected in \"Results.mat\"\n",
    "# res = [res ; omhits mhits]\n",
    "\n",
    "[omhits mhits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1504716566"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_sr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example that works about half the time-- seesm to follow the patter where initial mean(hits) < 0.5 means half the time go to mean(hits)=0.5 and get stuck there, whereas initial mean(hits)>=0.5 means success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed = [-0.2, 0.2, 0.2, -1.5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition JJ(Any) in module Main at In[17]:61 overwritten at In[19]:61.\n",
      "WARNING: Method definition #JJ(Array{Any, 1}, Main.#JJ, Any) in module Main overwritten.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- cost=0.0618702,   cost1=0.0619307, cost2=-6.04596e-05 :  mean(hits)=0.501141, mean(diffs)=0.0201532\n",
      "0: eta=1 ps=[-0.200, 0.200, 0.200, -1.500]\n",
      "-- cost=0.0618702,   cost1=0.0619307, cost2=-6.04596e-05 :  mean(hits)=0.501141, mean(diffs)=0.0201532\n",
      "-- cost=0.0609813,   cost1=0.0610149, cost2=-3.36142e-05 :  mean(hits)=0.502988, mean(diffs)=0.0112047\n",
      "1: eta=1.1 cost=0.0609813 jtype=Newton costheta=-0.368 ps=[-0.187942, 0.181484, -0.0195354, -1.39132]\n",
      "-- cost=0.0613172,   cost1=0.0613271, cost2=-9.90029e-06 :  mean(hits)=0.502357, mean(diffs)=0.0033001\n",
      "eta going down: new_cost-cost=0.000335981 and jumptype='Newton'\n",
      "2: eta=0.55 cost=0.0609813 jtype=Newton costheta=NaN ps=[-0.187942, 0.181484, -0.0195354, -1.39132]\n",
      "-- cost=0.0533101,   cost1=0.0533805, cost2=-7.04243e-05 :  mean(hits)=0.518958, mean(diffs)=0.0234748\n",
      "3: eta=0.605 cost=0.0533101 jtype=constrained costheta=-0.697 ps=[0.148346, -0.140224, 0.230154, -1.53175]\n",
      "-- cost=0.00648721,   cost1=0.00724268, cost2=-0.000755464 :  mean(hits)=0.664896, mean(diffs)=0.251821\n",
      "4: eta=0.6655 cost=0.00648721 jtype=constrained costheta=-0.996 ps=[0.353552, -0.315945, 0.585897, -1.89314]\n",
      "-- cost=-0.00125426,   cost1=0.000230058, cost2=-0.00148431 :  mean(hits)=0.765168, mean(diffs)=0.494771\n",
      "5: eta=0.73205 cost=-0.00125426 jtype=Newton costheta=-0.778 ps=[0.398471, -0.569932, 0.661777, -2.02273]\n",
      "-- cost=-0.00110447,   cost1=1.37151e-05, cost2=-0.00111819 :  mean(hits)=0.746297, mean(diffs)=0.372729\n",
      "eta going down: new_cost-cost=0.000149782 and jumptype='Newton'\n",
      "6: eta=0.366025 cost=-0.00125426 jtype=Newton costheta=NaN ps=[0.398471, -0.569932, 0.661777, -2.02273]\n",
      "-- cost=-0.00110447,   cost1=1.37151e-05, cost2=-0.00111819 :  mean(hits)=0.746297, mean(diffs)=0.372729\n",
      "eta going down: new_cost-cost=0.000149782 and jumptype='Newton'\n",
      "7: eta=0.183013 cost=-0.00125426 jtype=Newton costheta=NaN ps=[0.398471, -0.569932, 0.661777, -2.02273]\n",
      "-- cost=-0.00110447,   cost1=1.37151e-05, cost2=-0.00111819 :  mean(hits)=0.746297, mean(diffs)=0.372729\n",
      "eta going down: new_cost-cost=0.000149782 and jumptype='Newton'\n",
      "8: eta=0.0915063 cost=-0.00125426 jtype=Newton costheta=NaN ps=[0.398471, -0.569932, 0.661777, -2.02273]\n",
      "-- cost=-0.00154688,   cost1=4.1367e-05, cost2=-0.00158824 :  mean(hits)=0.756432, mean(diffs)=0.529414\n",
      "9: eta=0.100657 cost=-0.00154688 jtype=constrained costheta=-0.500 ps=[0.339325, -0.509753, 0.68099, -2.04598]\n",
      "-- cost=-0.001798,   cost1=1.4203e-05, cost2=-0.0018122 :  mean(hits)=0.753769, mean(diffs)=0.604068\n",
      "10: eta=0.110723 cost=-0.001798 jtype=constrained costheta=-0.954 ps=[0.286125, -0.453558, 0.715256, -2.09091]\n",
      "-- cost=-0.00207619,   cost1=2.66475e-06, cost2=-0.00207885 :  mean(hits)=0.751632, mean(diffs)=0.692952\n",
      "11: eta=0.121795 cost=-0.00207619 jtype=constrained costheta=-0.974 ps=[0.238178, -0.401162, 0.759385, -2.15064]\n",
      "-- cost=-0.00233075,   cost1=1.32111e-06, cost2=-0.00233207 :  mean(hits)=0.748851, mean(diffs)=0.777358\n",
      "12: eta=0.133974 cost=-0.00233075 jtype=constrained costheta=-0.922 ps=[0.200156, -0.35673, 0.813352, -2.22575]\n",
      "-- cost=-0.0025274,   cost1=6.29147e-06, cost2=-0.00253369 :  mean(hits)=0.747492, mean(diffs)=0.844564\n",
      "13: eta=0.147372 cost=-0.0025274 jtype=constrained costheta=-0.763 ps=[0.182843, -0.328748, 0.875797, -2.317]\n",
      "-- cost=-0.00267248,   cost1=2.55232e-06, cost2=-0.00267503 :  mean(hits)=0.748402, mean(diffs)=0.891678\n",
      "14: eta=0.162109 cost=-0.00267248 jtype=constrained costheta=-0.513 ps=[0.187217, -0.313812, 0.943893, -2.41878]\n",
      "-- cost=-0.00276642,   cost1=1.39128e-06, cost2=-0.00276781 :  mean(hits)=0.74882, mean(diffs)=0.922605\n",
      "15: eta=0.17832 cost=-0.00276642 jtype=constrained costheta=-0.414 ps=[0.199794, -0.303395, 1.02039, -2.52736]\n",
      "-- cost=-0.00282544,   cost1=9.90464e-09, cost2=-0.00282545 :  mean(hits)=0.7501, mean(diffs)=0.941817\n",
      "16: eta=0.196152 cost=-0.00282544 jtype=constrained costheta=-0.285 ps=[0.22659, -0.285909, 1.10701, -2.64072]\n",
      "-- cost=-0.00289147,   cost1=1.62338e-07, cost2=-0.00289163 :  mean(hits)=0.749597, mean(diffs)=0.963876\n",
      "17: eta=0.215767 cost=-0.00289147 jtype=constrained costheta=-0.790 ps=[0.249154, -0.27032, 1.20422, -2.76167]\n",
      "-- cost=-0.00293126,   cost1=1.47027e-07, cost2=-0.00293141 :  mean(hits)=0.750383, mean(diffs)=0.977137\n",
      "18: eta=0.237344 cost=-0.00293126 jtype=constrained costheta=-0.883 ps=[0.269609, -0.257936, 1.29073, -2.89745]\n",
      "-- cost=-0.00293542,   cost1=1.51272e-08, cost2=-0.00293543 :  mean(hits)=0.749877, mean(diffs)=0.978477\n",
      "19: eta=0.261078 cost=-0.00293542 jtype=Newton costheta=-0.069 ps=[0.3187, -0.211175, 1.31508, -2.9546]\n",
      "-- cost=-0.00293701,   cost1=6.57577e-09, cost2=-0.00293701 :  mean(hits)=0.750081, mean(diffs)=0.979005\n",
      "20: eta=0.287186 cost=-0.00293701 jtype=Newton costheta=-0.076 ps=[0.260131, -0.271745, 1.35753, -3.02038]\n",
      "-- cost=-0.00293771,   cost1=6.19265e-09, cost2=-0.00293772 :  mean(hits)=0.750079, mean(diffs)=0.979239\n",
      "21: eta=0.315904 cost=-0.00293771 jtype=Newton costheta=-0.030 ps=[0.325472, -0.209354, 1.39055, -3.08301]\n",
      "-- cost=-0.00293802,   cost1=2.11415e-07, cost2=-0.00293823 :  mean(hits)=0.75046, mean(diffs)=0.979411\n",
      "22: eta=0.347495 cost=-0.00293802 jtype=Newton costheta=-0.013 ps=[0.371607, -0.166556, 1.43658, -3.16823]\n",
      "-- cost=-0.00293832,   cost1=1.74097e-09, cost2=-0.00293832 :  mean(hits)=0.749958, mean(diffs)=0.979442\n",
      "23: eta=0.382244 cost=-0.00293832 jtype=Newton costheta=-0.002 ps=[0.383666, -0.155926, 1.45557, -3.22119]\n",
      "-- cost=-0.0029001,   cost1=6.93007e-05, cost2=-0.0029694 :  mean(hits)=0.758325, mean(diffs)=0.989799\n",
      "eta going down: new_cost-cost=3.82272e-05 and jumptype='constrained'\n",
      "24: eta=0.191122 cost=-0.00293832 jtype=constrained costheta=NaN ps=[0.383666, -0.155926, 1.45557, -3.22119]\n",
      "-- cost=-0.00293568,   cost1=4.40131e-06, cost2=-0.00294008 :  mean(hits)=0.752098, mean(diffs)=0.980028\n",
      "eta going down: new_cost-cost=2.64123e-06 and jumptype='constrained'\n",
      "25: eta=0.0955611 cost=-0.00293832 jtype=constrained costheta=NaN ps=[0.383666, -0.155926, 1.45557, -3.22119]\n",
      "-- cost=-0.00293834,   cost1=1.90498e-07, cost2=-0.00293853 :  mean(hits)=0.750436, mean(diffs)=0.979511\n",
      "26: eta=0.105117 cost=-0.00293834 jtype=constrained costheta=-0.005 ps=[0.38932, -0.152042, 1.51063, -3.2675]\n",
      "-- cost=-0.00293853,   cost1=1.13595e-09, cost2=-0.00293853 :  mean(hits)=0.749966, mean(diffs)=0.979509\n",
      "27: eta=0.115629 cost=-0.00293853 jtype=Newton costheta=-0.002 ps=[0.395298, -0.146756, 1.52057, -3.29715]\n",
      "-- cost=-0.00293821,   cost1=6.58622e-07, cost2=-0.00293887 :  mean(hits)=0.750812, mean(diffs)=0.979623\n",
      "eta going down: new_cost-cost=3.15695e-07 and jumptype='constrained'\n",
      "28: eta=0.0578145 cost=-0.00293853 jtype=constrained costheta=NaN ps=[0.395298, -0.146756, 1.52057, -3.29715]\n",
      "-- cost=-0.00293857,   cost1=3.24747e-08, cost2=-0.0029386 :  mean(hits)=0.75018, mean(diffs)=0.979534\n",
      "29: eta=0.0635959 cost=-0.00293857 jtype=constrained costheta=-0.003 ps=[0.39818, -0.144806, 1.55165, -3.32528]\n",
      "-- cost=-0.00293864,   cost1=6.45669e-09, cost2=-0.00293865 :  mean(hits)=0.75008, mean(diffs)=0.979549\n",
      "30: eta=0.0699555 cost=-0.00293864 jtype=Newton costheta=-0.001 ps=[0.405046, -0.138704, 1.56327, -3.35645]\n",
      "-- cost=-0.00293867,   cost1=4.14055e-08, cost2=-0.00293872 :  mean(hits)=0.750203, mean(diffs)=0.979572\n",
      "31: eta=0.076951 cost=-0.00293867 jtype=constrained costheta=-0.001 ps=[0.415647, -0.129059, 1.58082, -3.39364]\n",
      "-- cost=-0.00293873,   cost1=7.62131e-10, cost2=-0.00293873 :  mean(hits)=0.750028, mean(diffs)=0.979576\n",
      "32: eta=0.0846461 cost=-0.00293873 jtype=Newton costheta=-0.001 ps=[0.419668, -0.12547, 1.58841, -3.41297]\n",
      "-- cost=-0.00209856,   cost1=0.000900132, cost2=-0.0029987 :  mean(hits)=0.719998, mean(diffs)=0.999565\n",
      "eta going down: new_cost-cost=0.000840165 and jumptype='constrained'\n",
      "33: eta=0.0423231 cost=-0.00293873 jtype=constrained costheta=NaN ps=[0.419668, -0.12547, 1.58841, -3.41297]\n",
      "-- cost=-0.00293876,   cost1=6.45254e-09, cost2=-0.00293876 :  mean(hits)=0.75008, mean(diffs)=0.979587\n",
      "34: eta=0.0465554 cost=-0.00293876 jtype=constrained costheta=-0.002 ps=[0.414927, -0.130702, 1.60587, -3.43379]\n",
      "-- cost=-0.00289863,   cost1=0.000100054, cost2=-0.00299869 :  mean(hits)=0.739997, mean(diffs)=0.999562\n",
      "eta going down: new_cost-cost=4.01247e-05 and jumptype='constrained'\n",
      "35: eta=0.0232777 cost=-0.00293876 jtype=constrained costheta=NaN ps=[0.414927, -0.130702, 1.60587, -3.43379]\n",
      "-- cost=-0.00289868,   cost1=0.000100047, cost2=-0.00299872 :  mean(hits)=0.739998, mean(diffs)=0.999575\n",
      "eta going down: new_cost-cost=4.00782e-05 and jumptype='constrained'\n",
      "36: eta=0.0116388 cost=-0.00293876 jtype=constrained costheta=NaN ps=[0.414927, -0.130702, 1.60587, -3.43379]\n",
      "-- cost=-0.00289866,   cost1=0.000100045, cost2=-0.00299871 :  mean(hits)=0.739998, mean(diffs)=0.99957\n",
      "eta going down: new_cost-cost=4.00913e-05 and jumptype='constrained'\n",
      "37: eta=0.00581942 cost=-0.00293876 jtype=constrained costheta=NaN ps=[0.414927, -0.130702, 1.60587, -3.43379]\n",
      "-- cost=-0.00289858,   cost1=0.000100029, cost2=-0.00299861 :  mean(hits)=0.739999, mean(diffs)=0.999538\n",
      "eta going down: new_cost-cost=4.01714e-05 and jumptype='constrained'\n",
      "38: eta=0.00290971 cost=-0.00293876 jtype=constrained costheta=NaN ps=[0.414927, -0.130702, 1.60587, -3.43379]\n",
      "-- cost=-0.00289792,   cost1=9.97401e-05, cost2=-0.00299766 :  mean(hits)=0.740013, mean(diffs)=0.999219\n",
      "eta going down: new_cost-cost=4.08394e-05 and jumptype='constrained'\n",
      "39: eta=0.00145486 cost=-0.00293876 jtype=constrained costheta=NaN ps=[0.414927, -0.130702, 1.60587, -3.43379]\n",
      "-- cost=-0.00289509,   cost1=9.42936e-05, cost2=-0.00298938 :  mean(hits)=0.74029, mean(diffs)=0.996461\n",
      "eta going down: new_cost-cost=4.36676e-05 and jumptype='constrained'\n",
      "40: eta=0.000727428 cost=-0.00293876 jtype=constrained costheta=NaN ps=[0.414927, -0.130702, 1.60587, -3.43379]\n",
      "-- cost=-0.00290214,   cost1=6.428e-05, cost2=-0.00296642 :  mean(hits)=0.741983, mean(diffs)=0.988805\n",
      "eta going down: new_cost-cost=3.66203e-05 and jumptype='constrained'\n",
      "41: eta=0.000363714 cost=-0.00293876 jtype=constrained costheta=NaN ps=[0.414927, -0.130702, 1.60587, -3.43379]\n",
      "-- cost=-0.00292264,   cost1=2.53979e-05, cost2=-0.00294804 :  mean(hits)=0.74496, mean(diffs)=0.98268\n",
      "eta going down: new_cost-cost=1.61123e-05 and jumptype='constrained'\n",
      "42: eta=0.000181857 cost=-0.00293876 jtype=constrained costheta=NaN ps=[0.414927, -0.130702, 1.60587, -3.43379]\n",
      "-- cost=-0.00293407,   cost1=7.15022e-06, cost2=-0.00294122 :  mean(hits)=0.747326, mean(diffs)=0.980407\n",
      "eta going down: new_cost-cost=4.68551e-06 and jumptype='constrained'\n",
      "43: eta=9.09285e-05 cost=-0.00293876 jtype=constrained costheta=NaN ps=[0.414927, -0.130702, 1.60587, -3.43379]\n",
      "-- cost=-0.0029376,   cost1=1.75042e-06, cost2=-0.00293935 :  mean(hits)=0.748677, mean(diffs)=0.979784\n",
      "eta going down: new_cost-cost=1.15469e-06 and jumptype='constrained'\n",
      "44: eta=4.54642e-05 cost=-0.00293876 jtype=constrained costheta=NaN ps=[0.414927, -0.130702, 1.60587, -3.43379]\n",
      "-- cost=-0.0029385,   cost1=3.89949e-07, cost2=-0.00293889 :  mean(hits)=0.749376, mean(diffs)=0.97963\n",
      "eta going down: new_cost-cost=2.55089e-07 and jumptype='constrained'\n",
      "45: eta=2.27321e-05 cost=-0.00293876 jtype=constrained costheta=NaN ps=[0.414927, -0.130702, 1.60587, -3.43379]\n",
      "-- cost=-0.00293876,   cost1=1.51692e-12, cost2=-0.00293876 :  mean(hits)=0.750001, mean(diffs)=0.979587\n",
      "About to break -- tol=1e-08, new_cost-cost=-4.1584e-09, eta=2.27321e-05\n",
      "-- cost=-0.00293876,   cost1=6.45254e-09, cost2=-0.00293876 :  mean(hits)=0.75008, mean(diffs)=0.979587\n",
      "-- cost=-0.00293876,   cost1=6.45254e-09, cost2=-0.00293876 :  mean(hits)=0.75008, mean(diffs)=0.979587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1×2 Array{Float64,2}:\n",
       " 0.501141  0.75008"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following sequence leads to a situation where having only [-0.8, -0.8] as the single finalFluxPoint \n",
    "# leads to the minimization getting stuck.  Adding further finalFluxPoints solves the problem\n",
    "\n",
    "dt = 0.02\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "W = -4\n",
    "noise = 0\n",
    "input = 0\n",
    "sigma = 0.1\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    ":noise=>noise, :input=>input, :sigma=>sigma, :const_add=>0, :init_add=>0)\n",
    "\n",
    "\n",
    "# WORKING gradient:\n",
    "# ForwardDiff.gradient((x)->JJ(startU; do_plot=true, nderivs=length(x), difforder=1, \n",
    "#    make_dict([[\"init_add\" 2], \"const_add\"], x, model_params)...), [2.9, -2.9, 0.1])\n",
    "\n",
    "\n",
    "\n",
    "# The backward and costfunc functions should turn a single-scalar parameter W into the matrix W\n",
    "# backward always runs with no within-forward noise, i.e., sigma=0\n",
    "backward = (endpoint; do_plot=false, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;     \n",
    "    backwardsModel(endpoint; do_plot=do_plot, make_dict([\"sigma\"], [0], pars)...)[1]\n",
    "end\n",
    "\n",
    "forward = (startpoint; pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;     \n",
    "    forwardsModel(startpoint; pars...)\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "costfunc = (startpoints; do_plot=false, verbose=false, beta=0.003, nderivs=0, difforder=0, sr=26, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;         \n",
    "    JJ(startpoints; seedrand=sr, beta=beta, \n",
    "        do_plot=do_plot, verbose=verbose, nderivs=nderivs, difforder=difforder, pars...)\n",
    "end\n",
    "\n",
    "function JJ(initUs; theta1=0.15, theta2=0.2, beta=0.003, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, pre_string=\"\", zero_last_sigmas=0, seedrand=NaN, params...)\n",
    "\n",
    "    if ~isnan(seedrand); srand(seedrand); end\n",
    "    \n",
    "    Vend = ForwardDiffZeros(size(initUs,1), size(initUs,2), nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:size(initUs,1)\n",
    "        if false # i>size(initUs,1) - zero_last_sigmas\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; sigma=0, nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)            \n",
    "        else\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)\n",
    "        end\n",
    "        Vend[i,:] = Ve\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend[:,1]-Vend[:,2])/theta1))\n",
    "    diffs = tanh.((Vend[:,1]-Vend[:,2])/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "\n",
    "    if do_plot\n",
    "        title(@sprintf(\"mean(hits)=%g, mean(diffs)=%g\", convert(Float64, mean(hits)), convert(Float64, mean(diffs))))\n",
    "    end\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"%s\", pre_string)\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2, mean(hits), mean(diffs)\n",
    "end\n",
    "  \n",
    "\n",
    "\n",
    "fluxFinalPoint = [-0.8 -0.8; -0.6 -0.6 ; -0.4 -0.4; -0.2 -0.2; 0 0; 0.2 0.2]\n",
    "fluxFinalPoint = zeros(0,2);\n",
    "\n",
    "\n",
    "beta = 0.05\n",
    "args = [[\"start_add\" 2], \"const_add\", \"W\", \"sigma\"]\n",
    "seed = [0.1, 0.1, 2.1, -1, 0.1]\n",
    "walls = Dict(:start_add=>[-5.1, 5.1], :W=>[-5.1, 5.1], :sigma=>[-0.5, 0.5])\n",
    "# sr = 1504432803 causes a total mess with everything around the decision boundary; \n",
    "# sr = 1504432962 gets stuck at mean(hits)=0.66 but if we reduce the bounds of sigma, reaches 0.74\n",
    "\n",
    "\n",
    "beta = 0.0000001\n",
    "args = [[\"start_add\" 2], \"const_add\", \"W\"] \n",
    "seed = [0.1, 0.1, 2.1, -1] \n",
    "# This seed proves deadly, and always starts with mean(hits)<0.5:  seed = [-0.1, 0.1, 2.1, -1] \n",
    "walls = Dict(:start_add=>[-5.1, 5.1], :W=>[-5.1, 5.1]) # \n",
    "# sr = 1504433892 gives a mean(hits)=0.5 mess\n",
    "# sr = 1504433983 gives a mean(hits)=0.5 mess\n",
    "# sr = 1504434067 gives a mean(hits)=0.5 mess\n",
    "# sr = 1504434114 gives a mean(hits)=0.5 mess\n",
    "\n",
    "\n",
    "beta = 0.003\n",
    "args = [[\"start_add\" 2], \"const_add\", \"W\"] \n",
    "seed = [-0.2, 0.2, 0.2, -1.5] \n",
    "walls = Dict(:start_add=>[-5.1, 5.1], :W=>[-5.1, 5.1]) # \n",
    "# sr = 1504713552 gives a mean(hits)=0.5 mess\n",
    "# sr = 1504713626 gives a mean(hits)=0.5 mess\n",
    "# sr = 1504713708 gives a mean(hits)=0.5 mess\n",
    "# sr =  gives a mean(hits)=0.5 mess\n",
    "\n",
    "\n",
    "new_random_seed = true; if new_random_seed\n",
    "    sr = convert(Int64, round(time()))\n",
    "else\n",
    "    sr = old_sr\n",
    "end\n",
    "# sr = 1504649431\n",
    "old_sr = sr\n",
    "\n",
    "srand(sr)\n",
    "\n",
    "# THIS IS THE GOOD ONE FOR ALL THE COMMENTS ON sr NUMBERS ABOVE: startU=randn(50,2)-3\n",
    "startU=randn(50,2)-3\n",
    "\n",
    "\n",
    "clf()\n",
    "print(\"seed = \"); print_vector_g(seed); print(\"\\n\")\n",
    "ocost, omhits, omdiffs = costfunc(startU; do_plot=true, sr=sr, verbose=true, make_dict(args, seed, model_params)...)\n",
    "\n",
    "\n",
    "# :sigma=>[-0.3, 0.3] does fine but :sigma=>[-0.2, 0.2] gets stuck.\n",
    "# If we fix sigma at 0 it also gets stuck, but dynamics kind of odd, W a bit to big, or decrease dt\n",
    "params, traj, zz, cpm_traj = bbox_Hessian_keyword_minimization(seed, args, walls, # , :sigma=>[-0.2, 0.2]), \n",
    "(;params...) -> costfunc(startU; beta=beta, sr=sr, do_plot=false, verbose=true, merge(model_params, Dict(params))...)[1], \n",
    "verbose=true, start_eta=1, tol=1e-8, softbox=true, maxiter=400 )\n",
    "\n",
    "# params, cost, ptraj, gtraj = fluxSense(costfunc, backward, model_params, startU, fluxFinalPoint, args, seed; \n",
    "#    start_eta=0.01, tol=1e-15, maxiter=400, verbose=true, report_every=1, do_plot=false, cost_limit=cost_limit) # cost_limit=-0.000935) # for beta=0.01\n",
    "\n",
    "# And show the final position\n",
    "figure(1); clf()\n",
    "cost, mhits, mdiffs = \n",
    "    costfunc(startU; beta=beta, do_plot=true, sr=sr, verbose=true, make_dict(args, params, model_params)...)\n",
    "\n",
    "repeat_results_in_fig2 = true; if repeat_results_in_fig2\n",
    "    figure(2); clf()\n",
    "    costfunc(startU; beta=beta, do_plot=true, sr=sr, verbose=true, \n",
    "        make_dict(args, params, merge(Dict(:fignum=>2), model_params))...)\n",
    "    figure(1); \n",
    "end\n",
    "params'\n",
    "\n",
    "# For beta=0, and ntrials=20, we collected a bunch of results and observed that it failes about half the time\n",
    "# WHEN the initial mean)hits) is below 0.5.  It never fails if the initial mean(hits) is above 0.5. \n",
    "# Seems like when it starts below 0.5, the fastest way to increase mean(hits) is to push it to 0.5 and floor it there.\n",
    "# The results were collected in \"Results.mat\"\n",
    "# res = [res ; omhits mhits]\n",
    "\n",
    "[omhits mhits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1504713708"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Array{Any,1}:\n",
       " PyObject <matplotlib.lines.Line2D object at 0x33088fad0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "figure(2); clf();\n",
    "plot(cpm_traj[1,:], \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition param_path(Any) in module Main at In[258]:2 overwritten at In[260]:2.\n",
      "WARNING: Method definition #param_path(Array{Any, 1}, Main.#param_path, Any) in module Main overwritten.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "param_path (generic function with 1 method)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function param_path(ppath; k=1, do_plot=true, fignum=1, clearfig=true)\n",
    "    costhetas = zeros(1, size(ppath,2)-k-1)\n",
    "    deltas = ppath[:,2:end] - ppath[:,1:end-1]\n",
    "    for i=1:length(costhetas)\n",
    "        costhetas[i] = dot(deltas[:,i], deltas[:,i+k])/(norm(deltas[:,i])*norm(deltas[:,i+k]))\n",
    "    end;\n",
    "    \n",
    "    if do_plot\n",
    "        figure(fignum); if clearfig; clf(); end;\n",
    "        plot(costhetas', \".\")\n",
    "    end\n",
    "    return costhetas\n",
    "end\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×69 Array{Float64,2}:\n",
       " NaN  0.850328  0.775369  -0.952955  …  NaN  NaN  NaN  NaN  NaN  NaN  NaN"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_path(traj[3:end,:]; k=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpm: g (candidate indices) are : [1, 333, 444, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-7654187.550, -30616.750, 2518227.704, 7638879.175]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "1 : After searching for lambdas with efactor=3, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 361, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-30616750.199, 2449340.016, 30555516.699]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "2 : After searching for lambdas with efactor=12, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 340, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-122467000.798, 2081939.014, 122222066.796]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "3 : After searching for lambdas with efactor=48, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 335, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-489868003.190, 979736.006, 488888267.184]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "4 : After searching for lambdas with efactor=192, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 334, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-1959472012.760, -1959472.013, 1955553068.735]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "5 : After searching for lambdas with efactor=768, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 2, 12, 333, 656, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018, 0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-7837888051.040, -7814374386.887, -7579237745.356, -31351552.204, 7563561969.254, 7822212274.938]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "6 : After searching for lambdas with efactor=3072, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 1, 5, 10, 14, 18, 22, 26, 30, 34, 38, 41, 45, 48, 52, 55, 58, 62, 65, 68, 71, 73, 76, 79, 82, 84, 87, 89, 92, 94, 97, 99, 101, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 129, 131, 134, 136, 139, 142, 145, 148, 151, 155, 159, 165, 174, 333, 496, 504, 509, 513, 517, 520, 523, 526, 529, 531, 534, 536, 539, 541, 543, 545, 547, 549, 552, 555, 557, 559, 561, 563, 565, 568, 570, 572, 575, 577, 580, 582, 585, 588, 590, 593, 596, 599, 602, 605, 608, 612, 615, 618, 622, 625, 629, 633, 636, 640, 644, 648, 653, 657, 662, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-31351552204.161, -31351552204.161, -30975333577.711, -30505060294.648, -30128841668.198, -29752623041.748, -29376404415.298, -29000185788.849, -28623967162.399, -28247748535.949, -27871529909.499, -27589365939.661, -27213147313.211, -26930983343.374, -26554764716.924, -26272600747.087, -25990436777.249, -25614218150.799, -25332054180.962, -25049890211.124, -24767726241.287, -24579616928.062, -24297452958.224, -24015288988.387, -23733125018.550, -23545015705.325, -23262851735.487, -23074742422.262, -22792578452.425, -22604469139.200, -22322305169.362, -22134195856.137, -21946086542.912, -21663922573.075, -21475813259.850, -21287703946.625, -21099594633.400, -20911485320.175, -20723376006.950, -20535266693.725, -20347157380.500, -20159048067.275, -19970938754.050, -19782829440.825, -19594720127.600, -19312556157.763, -19124446844.538, -18842282874.700, -18654173561.476, -18372009591.638, -18089845621.801, -17807681651.963, -17525517682.126, -17243353712.288, -16867135085.838, -16490916459.388, -15926588519.714, -15080096610.201, -125406208.817, 15205502819.018, 15957940071.918, 16428213354.980, 16804431981.430, 17180650607.880, 17462814577.717, 17744978547.555, 18027142517.392, 18309306487.230, 18497415800.455, 18779579770.292, 18967689083.517, 19249853053.355, 19437962366.580, 19626071679.805, 19814180993.029, 20002290306.254, 20190399619.479, 20472563589.317, 20754727559.154, 20942836872.379, 21130946185.604, 21319055498.829, 21507164812.054, 21695274125.279, 21977438095.117, 22165547408.342, 22353656721.566, 22635820691.404, 22823930004.629, 23106093974.466, 23294203287.691, 23576367257.529, 23858531227.366, 24046640540.591, 24328804510.429, 24610968480.266, 24893132450.103, 25175296419.941, 25457460389.778, 25739624359.616, 26115842986.066, 26398006955.903, 26680170925.741, 27056389552.191, 27338553522.028, 27714772148.478, 28090990774.928, 28373154744.765, 28749373371.215, 29125591997.665, 29501810624.115, 29972083907.177, 30348302533.627, 30818575816.690, 31288849099.752]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "7 : After searching for lambdas with efactor=12288, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 12, 28, 43, 57, 69, 80, 91, 100, 109, 117, 125, 132, 139, 145, 151, 156, 161, 166, 171, 175, 179, 183, 187, 190, 194, 197, 200, 203, 206, 208, 211, 213, 216, 218, 220, 222, 226, 228, 230, 232, 235, 237, 241, 244, 249, 333, 419, 424, 427, 430, 433, 435, 438, 440, 442, 444, 446, 449, 451, 453, 456, 458, 461, 464, 467, 470, 473, 476, 480, 484, 488, 492, 496, 501, 506, 511, 516, 522, 528, 535, 542, 549, 558, 566, 576, 586, 598, 610, 623, 638, 655, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-125406208816.642, -121267803925.693, -115248305902.494, -109605026505.745, -104337965735.446, -99823342218.047, -95684937327.098, -91546532436.149, -88160564798.099, -84774597160.050, -81764848148.451, -78755099136.851, -76121568751.702, -73488038366.552, -71230726607.853, -68973414849.153, -67092321716.904, -65211228584.654, -63330135452.404, -61449042320.155, -59944167814.355, -58439293308.555, -56934418802.756, -55429544296.956, -54300888417.606, -52796013911.806, -51667358032.457, -50538702153.107, -49410046273.757, -48281390394.407, -47528953141.507, -46400297262.158, -45647860009.258, -44519204129.908, -43766766877.008, -43014329624.108, -42261892371.208, -40757017865.409, -40004580612.509, -39252143359.609, -38499706106.709, -37371050227.359, -36618612974.460, -35113738468.660, -33985082589.310, -32103989457.060, -501624835.267, 31853177039.427, 33734270171.677, 34862926051.027, 35991581930.376, 37120237809.726, 37872675062.626, 39001330941.976, 39753768194.876, 40506205447.775, 41258642700.675, 42011079953.575, 43139735832.925, 43892173085.825, 44644610338.725, 45773266218.074, 46525703470.974, 47654359350.324, 48783015229.674, 49911671109.024, 51040326988.373, 52168982867.723, 53297638747.073, 54802513252.873, 56307387758.672, 57812262264.472, 59317136770.272, 60822011276.071, 62703104408.321, 64584197540.571, 66465290672.820, 68346383805.070, 70603695563.770, 72861007322.469, 75494537707.619, 78128068092.768, 80761598477.918, 84147566115.967, 87157315127.566, 90919501392.066, 94681687656.565, 99196311173.964, 103710934691.363, 108601776835.212, 114245056231.961, 120640772881.610, 125155396399.009]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "8 : After searching for lambdas with efactor=49152, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 3, 63, 105, 135, 159, 177, 192, 204, 215, 224, 231, 238, 244, 249, 253, 257, 261, 265, 268, 270, 273, 275, 278, 280, 283, 285, 289, 333, 379, 382, 385, 387, 389, 391, 394, 396, 399, 402, 405, 409, 413, 418, 423, 429, 436, 443, 452, 462, 475, 490, 508, 531, 562, 604, 664, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-501624835266.569, -498615086254.969, -408322615906.987, -345117886663.399, -299971651489.408, -263854663350.215, -236766922245.820, -214193804658.825, -196135310589.228, -179581691025.432, -166037820473.234, -155503698932.636, -144969577392.038, -135940330357.240, -128415957828.242, -122396459805.043, -116376961781.844, -110357463758.645, -104337965735.446, -99823342218.047, -96813593206.448, -92298969689.049, -89289220677.449, -84774597160.050, -81764848148.451, -77250224631.052, -74240475619.452, -68220977596.253, -2006499341.066, 67217727925.720, 71732351443.119, 76246974960.519, 79256723972.118, 82266472983.717, 85276221995.317, 89790845512.716, 92800594524.315, 97315218041.714, 101829841559.114, 106344465076.513, 112363963099.711, 118383461122.910, 125907833651.909, 133432206180.907, 142461453215.706, 152995574756.303, 163529696296.901, 177073566849.099, 192122311907.096, 211685680482.492, 234258798069.488, 261346539173.882, 295958652807.276, 342609762487.067, 405814491730.654, 496106962078.637, 500621585596.036]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "9 : After searching for lambdas with efactor=196608, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 86, 185, 227, 251, 266, 277, 284, 290, 295, 298, 301, 304, 306, 308, 311, 333, 357, 359, 363, 365, 368, 372, 377, 382, 390, 401, 416, 439, 482, 581, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-2006499341066.275, -1494842009094.375, -898911704797.691, -646092787823.341, -501624835266.569, -411332364918.586, -345117886663.399, -302981400501.008, -266864412361.815, -236766922245.820, -218708428176.224, -200649934106.628, -182591440037.031, -170552443990.633, -158513447944.236, -140454953874.639, -8025997364.265, 136441955192.507, 148480951238.905, 172558943331.700, 184597939378.098, 202656433447.694, 226734425540.489, 256831915656.483, 286929405772.477, 335085389958.068, 401299868213.255, 491592338561.238, 630040793094.811, 888879208092.360, 1484809512389.044, 2002486342384.143]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "10 : After searching for lambdas with efactor=786432, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 148, 272, 297, 307, 313, 317, 321, 333, 347, 350, 354, 359, 370, 395, 519, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-8025997364265.102, -4486532526624.191, -1500861507117.573, -898911704797.691, -658131783869.738, -513663831312.966, -417351862941.785, -321039894570.604, -32103989457.060, 304987899842.074, 377221876120.459, 473533844491.641, 593923804955.618, 858781717976.366, 1460731520296.248, 4446402539802.866, 8009945369536.571]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "11 : After searching for lambdas with efactor=3.14573e+06, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 287, 318, 325, 327, 333, 342, 348, 379, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-32103989457060.406, -4558766502902.578, -1573095483395.958, -898911704797.692, -706287768055.330, -128415957828.242, 738391757512.394, 1316263567739.474, 4301934587246.098, 32039781478146.285]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "12 : After searching for lambdas with efactor=1.25829e+07, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 322, 330, 333, 337, 344, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-128415957828241.625, -4751390439644.944, -1669407451767.143, -513663831312.967, 1027327662625.934, 3724062777018.996, 128159125912585.141]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "13 : After searching for lambdas with efactor=5.03316e+07, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 331, 333, 336, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-513663831312966.500, -5136638313129.670, -2054655325251.868, 2568319156564.892, 512636503650340.563]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "14 : After searching for lambdas with efactor=2.01327e+08, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 333, 334, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-2054655325251866.000, -8218621301007.472, -2054655325251.868, 2050546014601362.250]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "15 : After searching for lambdas with efactor=8.05306e+08, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-8218621301007464.000, 8202184058405449.000]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "16 : After searching for lambdas with efactor=3.22123e+09, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-32874485204029856.000, 32808736233621796.000]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "17 : After searching for lambdas with efactor=1.28849e+10, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-131497940816119424.000, 131234944934487184.000]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "18 : After searching for lambdas with efactor=5.15396e+10, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-525991763264477696.000, 524939779737948736.000]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "19 : After searching for lambdas with efactor=2.06158e+11, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-2103967053057910784.000, 2099759118951794944.000]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "20 : After searching for lambdas with efactor=8.24634e+11, we found these : []\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "ArgumentError: collection must be non-empty",
     "output_type": "error",
     "traceback": [
      "ArgumentError: collection must be non-empty",
      "",
      " in findmin(::Array{Float64,1}) at ./array.jl:1254",
      " in indmin at ./array.jl:1294 [inlined]",
      " in #constrained_parabolic_minimization#777(::Float64, ::Bool, ::Bool, ::Bool, ::Float64, ::Int64, ::Float64, ::Float64, ::Float64, ::Int64, ::#constrained_parabolic_minimization, ::Array{Float64,2}, ::Array{Float64,2}, ::Float64) at ./In[227]:106",
      " in (::#kw##constrained_parabolic_minimization)(::Array{Any,1}, ::#constrained_parabolic_minimization, ::Array{Float64,2}, ::Array{Float64,2}, ::Float64) at ./<missing>:0"
     ]
    }
   ],
   "source": [
    "a = matread(\"error_report.mat\")\n",
    "hess = a[\"hess\"]\n",
    "eta = a[\"eta\"]\n",
    "grad = a[\"grad\"]\n",
    "\n",
    "chessdelta = constrained_parabolic_minimization(hess, grad'', eta, maxiter=500, max_efactor_tries=20, tol=1e-20, do_plot=true, verbose=true)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example doing a successful minimization of a 2d model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function JJ(initUs; theta1=0.15, theta2=0.2, beta=0.003, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, pre_string=\"\", zero_last_sigmas=0, seedrand=NaN, params...)\n",
    "\n",
    "    if ~isnan(seedrand); srand(seedrand); end\n",
    "    \n",
    "    Vend = ForwardDiffZeros(size(initUs,1), size(initUs,2), nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:size(initUs,1)\n",
    "        if i>size(initUs,1) - zero_last_sigmas\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; sigma=0, nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)            \n",
    "        else\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)\n",
    "        end\n",
    "        Vend[i,:] = Ve\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend[:,1]-Vend[:,2])/theta1))\n",
    "    diffs = tanh.((Vend[:,1]-Vend[:,2])/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"%s\", pre_string)\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The following sequence leads to a situation where having only [-0.8, -0.8] as the single finalFluxPoint \n",
    "# leads to the minimization getting stuck.  Adding further finalFluxPoints solves the problem\n",
    "#\n",
    "srand(12)   # 12 is perfect success; srand(11) gets stuck at mean(hits)=0.72\n",
    "startU=randn(100,2)-3\n",
    "startU=randn(100,2)-3\n",
    "sigma = 0\n",
    "\n",
    "# startU=0.1*randn(100,2)-3\n",
    "# startU=zeros(100,2)-3\n",
    "\n",
    "\n",
    "dt = 0.02\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "W = -4\n",
    "noise = 0\n",
    "input = 0\n",
    "sigma = 0\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    ":noise=>noise, :input=>input, :sigma=>sigma, :const_add=>0, :init_add=>0)\n",
    "\n",
    "\n",
    "# WORKING gradient:\n",
    "# ForwardDiff.gradient((x)->JJ(startU; do_plot=true, nderivs=length(x), difforder=1, \n",
    "#    make_dict([[\"init_add\" 2], \"const_add\"], x, model_params)...), [2.9, -2.9, 0.1])\n",
    "\n",
    "\n",
    "\n",
    "# The backward and costfunc functions should turn a single-scalar parameter W into the matrix W\n",
    "# backward always runs with no within-forward noise, i.e., sigma=0\n",
    "backward = (endpoint; do_plot=false, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;     \n",
    "    backwardsModel(endpoint; do_plot=do_plot, make_dict([\"sigma\"], [0], pars)...)[1]\n",
    "end\n",
    "\n",
    "\n",
    "beta = 0.003;\n",
    "\n",
    "costfunc = (startpoints; do_plot=false, verbose=false, nderivs=0, difforder=0, sr=26, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;         \n",
    "    JJ(startpoints; seedrand=sr, beta=beta, \n",
    "        do_plot=do_plot, verbose=verbose, nderivs=nderivs, difforder=difforder, pars...)\n",
    "end\n",
    "\n",
    "function JJ(initUs; theta1=0.15, theta2=0.2, beta=0.003, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, pre_string=\"\", zero_last_sigmas=0, seedrand=NaN, params...)\n",
    "\n",
    "    if ~isnan(seedrand); srand(seedrand); end\n",
    "    \n",
    "    Vend = ForwardDiffZeros(size(initUs,1), size(initUs,2), nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:size(initUs,1)\n",
    "        if false # i>size(initUs,1) - zero_last_sigmas\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; sigma=0, nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)            \n",
    "        else\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)\n",
    "        end\n",
    "        Vend[i,:] = Ve\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend[:,1]-Vend[:,2])/theta1))\n",
    "    diffs = tanh.((Vend[:,1]-Vend[:,2])/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"%s\", pre_string)\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "  \n",
    "\n",
    "\n",
    "if beta==0.003;     cost_limit = -0.00288\n",
    "elseif beta<0.001;  cost_limit = -0.0008\n",
    "elseif beta==0.001; cost_limit = -0.000935\n",
    "elseif beta==0.05;  cost_limit = -0.0485\n",
    "else\n",
    "    error(\"Don't know what cost limit goes with beta %g\\n\", beta)\n",
    "end\n",
    "\n",
    "fluxFinalPoint = [-0.8 -0.8; -0.6 -0.6 ; -0.4 -0.4; -0.2 -0.2; 0 0; 0.2 0.2]\n",
    "fluxFinalPoint = zeros(0,2);\n",
    "\n",
    "\n",
    "\n",
    "args = [[\"init_add\" 2], \"const_add\", \"W\", \"sigma\"]\n",
    "seed = [2, 2, 2.1, -1, 0.1]\n",
    "\n",
    "\n",
    "\n",
    "clf()\n",
    "print(\"seed = \"); print_vector_g(seed); print(\"\\n\")\n",
    "costfunc(startU; do_plot=true, verbose=true, make_dict(args, seed, model_params)...)\n",
    "\n",
    "# :sigma=>[-0.3, 0.3] does fine but :sigma=>[-0.2, 0.2] gets stuck.\n",
    "# If we fix sigma at 0 it also gets stuck, but dynamics kind of odd, W a bit to big, or decrease dt\n",
    "params, traj = bbox_Hessian_keyword_minimization(seed, args, Dict(:init_add=>[-5.1, 5.1], :sigma=>[-0.3, 0.3]), \n",
    "(;params...) -> costfunc(startU; do_plot=false, verbose=true, merge(model_params, Dict(params))...), \n",
    " verbose=true, start_eta=1, tol=1e-6, hardbox=true )\n",
    "\n",
    "# params, cost, ptraj, gtraj = fluxSense(costfunc, backward, model_params, startU, fluxFinalPoint, args, seed; \n",
    "#    start_eta=0.01, tol=1e-15, maxiter=400, verbose=true, report_every=1, do_plot=false, cost_limit=cost_limit) # cost_limit=-0.000935) # for beta=0.01\n",
    "\n",
    "# And show the final position\n",
    "clf()\n",
    "costfunc(startU; do_plot=true, verbose=true, make_dict(args, params, model_params)...)\n",
    "params'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf()\n",
    "startU=randn(4000,2)-3\n",
    "costfunc(startU; sr=NaN, do_plot=false, verbose=true, make_dict(args, params, model_params)...)\n",
    "params'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of getting stuck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function JJ(initUs; theta1=0.15, theta2=0.2, beta=0.003, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, pre_string=\"\", zero_last_sigmas=0, seedrand=NaN, params...)\n",
    "\n",
    "    if ~isnan(seedrand); srand(seedrand); end\n",
    "    \n",
    "    Vend = ForwardDiffZeros(size(initUs,1), size(initUs,2), nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:size(initUs,1)\n",
    "        if i>size(initUs,1) - zero_last_sigmas\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; sigma=0, nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)            \n",
    "        else\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)\n",
    "        end\n",
    "        Vend[i,:] = Ve\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend[:,1]-Vend[:,2])/theta1))\n",
    "    diffs = tanh.((Vend[:,1]-Vend[:,2])/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"%s\", pre_string)\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function JJ(initUs; theta1=0.15, theta2=0.2, beta=0.003, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, pre_string=\"\", zero_last_sigmas=0, seedrand=NaN, params...)\n",
    "\n",
    "    if ~isnan(seedrand); srand(seedrand); end\n",
    "    \n",
    "    Vend = ForwardDiffZeros(size(initUs,1), size(initUs,2), nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:size(initUs,1)\n",
    "        if false # i>size(initUs,1) - zero_last_sigmas\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; sigma=0, nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)            \n",
    "        else\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)\n",
    "        end\n",
    "        Vend[i,:] = Ve\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend[:,1]-Vend[:,2])/theta1))\n",
    "    diffs = tanh.((Vend[:,1]-Vend[:,2])/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"%s\", pre_string)\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "\n",
    "\n",
    "# The following sequence leads to a situation where having only [-0.8, -0.8] as the single finalFluxPoint \n",
    "# leads to the minimization getting stuck.  Adding further finalFluxPoints solves the problem\n",
    "#\n",
    "srand(11)\n",
    "startU=randn(100,2)-3\n",
    "startU=randn(100,2)-3\n",
    "\n",
    "dt = 0.02\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "W = -4\n",
    "noise = 0\n",
    "input = 0\n",
    "sigma = 0\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    ":noise=>noise, :input=>input, :sigma=>sigma, :const_add=>0, :init_add=>0)\n",
    "\n",
    "\n",
    "# WORKING gradient:\n",
    "# ForwardDiff.gradient((x)->JJ(startU; do_plot=true, nderivs=length(x), difforder=1, \n",
    "#    make_dict([[\"init_add\" 2], \"const_add\"], x, model_params)...), [2.9, -2.9, 0.1])\n",
    "\n",
    "\n",
    "\n",
    "# The backward and costfunc functions should turn a single-scalar parameter W into the matrix W\n",
    "# backward always runs with no within-forward noise, i.e., sigma=0\n",
    "backward = (endpoint; do_plot=false, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;     \n",
    "    backwardsModel(endpoint; do_plot=do_plot, make_dict([\"sigma\"], [0], pars)...)[1]\n",
    "end\n",
    "\n",
    "\n",
    "beta = 0.0001;\n",
    "beta = 0.003;\n",
    "beta = 0.003;\n",
    "beta=0\n",
    "\n",
    "costfunc = (startpoints; do_plot=false, verbose=false, nderivs=0, difforder=0, sr=26, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;         \n",
    "    JJ(startpoints; seedrand=sr, beta=beta, \n",
    "        do_plot=do_plot, verbose=verbose, nderivs=nderivs, difforder=difforder, pars...)\n",
    "end\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "if beta==0.003;     cost_limit = -0.00288\n",
    "elseif beta<0.001;  cost_limit = -0.0008\n",
    "elseif beta==0.001; cost_limit = -0.000935\n",
    "elseif beta==0.05;  cost_limit = -0.0485\n",
    "else\n",
    "    error(\"Don't know what cost limit goes with beta %g\\n\", beta)\n",
    "end\n",
    "\n",
    "fluxFinalPoint = [-0.8 -0.8; -0.6 -0.6 ; -0.4 -0.4; -0.2 -0.2; 0 0; 0.2 0.2]\n",
    "fluxFinalPoint = zeros(0,2);\n",
    "\n",
    "\n",
    "args = [[\"init_add\" 2], \"const_add\", \"W\"] # , \"sigma\"]\n",
    "seed = [2, 2, 2.1, -1] # , 0.1]\n",
    "\n",
    "\n",
    "\n",
    "clf()\n",
    "print(\"seed = \"); print_vector_g(seed); print(\"\\n\")\n",
    "costfunc(startU; do_plot=true, verbose=true, make_dict(args, seed, model_params)...)\n",
    "\n",
    "# :sigma=>[-0.3, 0.3] does fine but :sigma=>[-0.2, 0.2] gets stuck.\n",
    "# If we fix sigma at 0 it also gets stuck, but dynamics kind of odd, W a bit to big, or decrease dt\n",
    "params, traj = bbox_Hessian_keyword_minimization(seed, args, Dict(:init_add=>[-5.1, 5.1]), # , :sigma=>[-0.2, 0.2]), \n",
    "(;params...) -> costfunc(startU; do_plot=false, verbose=true, merge(model_params, Dict(params))...), \n",
    " verbose=true, start_eta=1, tol=1e-16, hardbox=true )\n",
    "\n",
    "# params, cost, ptraj, gtraj = fluxSense(costfunc, backward, model_params, startU, fluxFinalPoint, args, seed; \n",
    "#    start_eta=0.01, tol=1e-15, maxiter=400, verbose=true, report_every=1, do_plot=false, cost_limit=cost_limit) # cost_limit=-0.000935) # for beta=0.01\n",
    "\n",
    "# And show the final position\n",
    "clf()\n",
    "costfunc(startU; do_plot=true, verbose=true, make_dict(args, params, model_params)...)\n",
    "params'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function JJ(initUs; theta1=0.15, theta2=0.2, beta=0.003, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, pre_string=\"\", zero_last_sigmas=0, seedrand=NaN, params...)\n",
    "\n",
    "    if ~isnan(seedrand); srand(seedrand); end\n",
    "    \n",
    "    Vend = ForwardDiffZeros(size(initUs,1), size(initUs,2), nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:size(initUs,1)\n",
    "        if false # i>size(initUs,1) - zero_last_sigmas\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; sigma=0, nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)            \n",
    "        else\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)\n",
    "        end\n",
    "        Vend[i,:] = Ve\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend[:,1]-Vend[:,2])/theta1))\n",
    "    diffs = tanh.((Vend[:,1]-Vend[:,2])/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"%s\", pre_string)\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "\n",
    "\n",
    "# The following sequence leads to a situation where having only [-0.8, -0.8] as the single finalFluxPoint \n",
    "# leads to the minimization getting stuck.  Adding further finalFluxPoints solves the problem\n",
    "#\n",
    "srand(11)\n",
    "startU=randn(100,2)-3\n",
    "startU=randn(100,2)-3\n",
    "\n",
    "dt = 0.02\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "W = -4\n",
    "noise = 0\n",
    "input = 0\n",
    "sigma = 0\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    ":noise=>noise, :input=>input, :sigma=>sigma, :const_add=>0, :init_add=>0)\n",
    "\n",
    "\n",
    "# WORKING gradient:\n",
    "# ForwardDiff.gradient((x)->JJ(startU; do_plot=true, nderivs=length(x), difforder=1, \n",
    "#    make_dict([[\"init_add\" 2], \"const_add\"], x, model_params)...), [2.9, -2.9, 0.1])\n",
    "\n",
    "\n",
    "\n",
    "# The backward and costfunc functions should turn a single-scalar parameter W into the matrix W\n",
    "# backward always runs with no within-forward noise, i.e., sigma=0\n",
    "backward = (endpoint; do_plot=false, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;     \n",
    "    backwardsModel(endpoint; do_plot=do_plot, make_dict([\"sigma\"], [0], pars)...)[1]\n",
    "end\n",
    "\n",
    "\n",
    "beta = 0.0001;\n",
    "beta = 0.003;\n",
    "beta = 0.003;\n",
    "beta=0\n",
    "\n",
    "costfunc = (startpoints; do_plot=false, verbose=false, nderivs=0, difforder=0, sr=26, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;         \n",
    "    JJ(startpoints; seedrand=sr, beta=beta, \n",
    "        do_plot=do_plot, verbose=verbose, nderivs=nderivs, difforder=difforder, pars...)\n",
    "end\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "if beta==0.003;     cost_limit = -0.00288\n",
    "elseif beta<0.001;  cost_limit = -0.0008\n",
    "elseif beta==0.001; cost_limit = -0.000935\n",
    "elseif beta==0.05;  cost_limit = -0.0485\n",
    "else\n",
    "    error(\"Don't know what cost limit goes with beta %g\\n\", beta)\n",
    "end\n",
    "\n",
    "fluxFinalPoint = [-0.8 -0.8; -0.6 -0.6 ; -0.4 -0.4; -0.2 -0.2; 0 0; 0.2 0.2]\n",
    "fluxFinalPoint = zeros(0,2);\n",
    "\n",
    "\n",
    "args = [[\"init_add\" 2], \"const_add\", \"W\"] # , \"sigma\"]\n",
    "seed = [2, 2, 2.1, -1] # , 0.1]\n",
    "\n",
    "\n",
    "\n",
    "clf()\n",
    "print(\"seed = \"); print_vector_g(seed); print(\"\\n\")\n",
    "costfunc(startU; do_plot=true, verbose=true, make_dict(args, seed, model_params)...)\n",
    "\n",
    "# :sigma=>[-0.3, 0.3] does fine but :sigma=>[-0.2, 0.2] gets stuck.\n",
    "# If we fix sigma at 0 it also gets stuck, but dynamics kind of odd, W a bit to big, or decrease dt\n",
    "params, traj = bbox_Hessian_keyword_minimization(seed, args, Dict(:init_add=>[-5.1, 5.1], :W=>[-5.1, 5.1]), # , :sigma=>[-0.2, 0.2]), \n",
    "(;params...) -> costfunc(startU; do_plot=false, verbose=true, merge(model_params, Dict(params))...), \n",
    " verbose=true, start_eta=1, tol=1e-16, hardbox=true )\n",
    "\n",
    "# params, cost, ptraj, gtraj = fluxSense(costfunc, backward, model_params, startU, fluxFinalPoint, args, seed; \n",
    "#    start_eta=0.01, tol=1e-15, maxiter=400, verbose=true, report_every=1, do_plot=false, cost_limit=cost_limit) # cost_limit=-0.000935) # for beta=0.01\n",
    "\n",
    "# And show the final position\n",
    "clf()\n",
    "costfunc(startU; do_plot=true, verbose=true, make_dict(args, params, model_params)...)\n",
    "params'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(args)\n",
    "\n",
    "params'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf()\n",
    "costfunc(startU; do_plot=true, verbose=true, make_dict(args, [2, 1.7, 1.3, -1.4, -0.5], model_params)...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure(2)\n",
    "clf()\n",
    "plot(traj[1,:], \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf()\n",
    "cost, grad, hess = keyword_gradient((;params...) -> \n",
    "costfunc(startU; do_plot=true, verbose=true, merge(model_params, Dict(params))...), args, [0.6, -0.6, 1.5, -2])\n",
    "\n",
    "\n",
    "# costfunc(startU; do_plot=true, verbose=true, make_dict(args, [0.6, -0.6, 1.5, -2], model_params)...)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf()\n",
    "cost, gradB, hessB = keyword_vgh((;params...) -> \n",
    "costfunc(startU; do_plot=true, verbose=true, merge(merge(model_params, Dict(params)), Dict(:dt=>0.005, :nsteps=>201))...), \n",
    "args, [0.6, -0.6, 1.5, -2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[gradA gradB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf()\n",
    "costfunc(startU; do_plot=true, verbose=true, \n",
    "merge(make_dict(args, params, model_params), Dict(:dt=>0.005, :nsteps=>201))...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure(2);\n",
    "clf()\n",
    "subplot(2,1,1); plot(cost', \".\")\n",
    "subplot(2,1,2); \n",
    "guys = 4:5\n",
    "ng = sqrt(sum(gtraj[guys,:].*gtraj[guys,:],1))\n",
    "plot(sum(gtraj[guys,1:end-1].*gtraj[guys,2:end],1)'./(ng[1:end-1].*ng[2:end]), \".\")\n",
    "grid(true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ===  END of non-FLUX_SENSE() CODE ; START OF FLUX_SENSE() ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beginning of attempt at finding saddle points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt = 0.02\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "W = -4\n",
    "noise = 0\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    "    :noise=>noise, :noise=>noise, :const_add=>0, :init_add=>0)\n",
    "\n",
    "@time(trust_region_Hessian_minimization([-2.1, -2.1], \n",
    "    (x)->forwardModel(x; do_plot=false, nderivs=2, difforder=1, dUdt_mag_only=true, model_params...), \n",
    "verbose=false, start_eta=0.1, tol=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "backward = (endpoint; do_plot=false, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;     \n",
    "    backwardsModel(endpoint; do_plot=do_plot, make_dict([\"sigma\"], [0], pars)...)[1]\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hessian_fluxSense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "function bbox_Hessian_keyword_minimization(seed, args, bbox, func; wallwidth=NaN, start_eta=10, tol=1e-6, \n",
    "    maxiter=400, verbose=false)\n",
    "\n",
    "Like constrained_Hessian_minimization, but uses keyword_hessian!(). \n",
    "\n",
    "PARAMETERS:\n",
    "===========\n",
    "\n",
    "seed        column vector, representing the starting value of the parameters.\n",
    "\n",
    "args        List of strings identifying parameters for differentiation, e.g., [\"const_E\", \"w_self]\n",
    "\n",
    "bbox        An nargs-by-2 matrix indicating the range for each argument,\n",
    "            with the minima (first column) and maxima (second column).\n",
    "\n",
    "func        func must take only optional keyword args, and must \n",
    "            take nderivs=0, difforder=0  and declare any new matrices using ForwardDiffZeros() instead of zeros()\n",
    "\n",
    "\n",
    "OPTIONAL PARAMETERS:\n",
    "====================\n",
    "\n",
    "start_eta=10 Starting value of the radius.  It's good to start with somethibg biggish, if it is\n",
    "             too much, it'll quickly get cut down.\n",
    "\n",
    "tol=1e-6     Numerical tolerance. If a proposed jump produces a change in func that is less than\n",
    "             this, the minimization stops.\n",
    "\n",
    "maxiter=400  Maximum number of iterations to do before stopping\n",
    "\n",
    "verbose=false   If true, print out a report on each iteration of iteration number, radius size (eta),\n",
    "                what type jump was proposed (\"Newton\" means going straight to global min, \"constrained\" means jump has \n",
    "                norm eta, failed means that finding the minimum at a given radius somehow didn't work). Will also\n",
    "                print out the cosine of the angle between the proposed jump and the gradient.\n",
    "\n",
    "hardbox=false   If true, ignores wallwidth, and just rests parameter values to the bounding box if they go outside it.\n",
    "                If false, adds cost function \"walls\" to implement the bounding box.\n",
    "\n",
    "walldith=NaN     Used for putting up cost function \"walls\" that implement the bounding box limits. Can be NaN.\n",
    "                If it is NaN, then the wallwidth is a constant factor of the range width for each argument. If not NaN, must\n",
    "                be an nargs-long vector that indicates the actual wall widths.\n",
    "\n",
    "wallwidth_factor=0.18   Only relevant if wallwidth is NaN, otherwise ignored. For each arg, the wall width\n",
    "                is going to be wall_width_factor*(bbox[i,2] - bbox[i,1])\n",
    "\n",
    "\n",
    "RETURNS:\n",
    "========\n",
    "\n",
    "params       A vector the size of seed that has the last values of the minimizing parameters for func\n",
    "trajectory   A (2+length(params))-by-nsteps matrix. Each column corresponds to an iteration step, and contains\n",
    "                 the value of eta used, the cost, and the value of the parameters at that iteration\n",
    "cost         Final value of objective function\n",
    "\n",
    "\n",
    "EXAMPLE:\n",
    "========\n",
    "\n",
    "function tester(;x=5, y=10, z=20, nderivs=0, difforder=0)\n",
    "    return x^2*y + z/tanh(y)\n",
    "end\n",
    "\n",
    "params, trajectory = bbox_Hessian_keyword_minimization([0.5, 0.5], [\"x\", \"y\"], [1.1 2 ; 1.1 4], tester, \n",
    "    verbose=true, tol=1e-12, start_eta=1);\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# function bbox_Hessian_keyword_minimization(seed, args, bbox, func; \n",
    "    \n",
    "start_eta=10 \n",
    "tol=1e-6 \n",
    "maxiter=400\n",
    "verbose=false\n",
    "verbose_every=1 \n",
    "wallwidth=NaN \n",
    "wallwidth_factor=0.18\n",
    "hardbox=false\n",
    "\n",
    "\n",
    "    \n",
    "    traj_increment = 100\n",
    "    params = seed\n",
    "    eta = start_eta\n",
    "    trajectory = zeros(2+length(params), traj_increment)\n",
    "\n",
    "    if verbose\n",
    "        @printf \"%d: eta=%g ps=\" 0 eta \n",
    "        print_vector_g(params)\n",
    "        @printf \"\\n\"\n",
    "    end\n",
    "    \n",
    "    if hardbox\n",
    "        cost, grad, hess = keyword_vgh((;pars...) -> func(;pars...), args, params)\n",
    "    else\n",
    "        cost, grad, hess = keyword_vgh((;pars...) -> func(;pars...) + wall_cost(args, bbox; wallwidth=wallwidth, pars...),\n",
    "            args, params)        \n",
    "    end\n",
    "        \n",
    "    chessdelta = zeros(size(params))\n",
    "\n",
    "    for i=1:maxiter\n",
    "        if i > size(trajectory, 2)\n",
    "            trajectory = [trajectory zeros(2+length(params), traj_increment)]\n",
    "        end\n",
    "        trajectory[1:2, i]   = [eta;cost]\n",
    "        trajectory[3:end, i] = params\n",
    "        \n",
    "        hessdelta  = - inv(hess)*grad\n",
    "        try\n",
    "            chessdelta = constrained_parabolic_minimization(hess, grad'', eta)[1]\n",
    "            jumptype = \"not failed\"\n",
    "        catch y\n",
    "            jumptype = \"failed\"\n",
    "            if verbose\n",
    "                @printf \"Constrained parabolic minimization failed with error %s\\n\" y\n",
    "                @printf \"\\n\"\n",
    "                @printf \"eta was %g\\n\" eta\n",
    "                @printf \"grad was\\n\"\n",
    "                print_vector(grad)\n",
    "                @printf \"\\n\\nhess was\\n\"\n",
    "                for k in [1:length(grad);]\n",
    "                    print_vector(hess[k,:])\n",
    "                    @printf \"\\n\"\n",
    "                end\n",
    "                @printf \"\\n\"\n",
    "            end\n",
    "        end\n",
    "\n",
    "        if norm(hessdelta) <= eta\n",
    "            new_params = params + hessdelta\n",
    "            jumptype = \"Newton\"\n",
    "        elseif jumptype != \"failed\" \n",
    "            new_params = params + chessdelta\n",
    "            jumptype  = \"constrained\"\n",
    "        end\n",
    "\n",
    "        if jumptype != \"failed\"\n",
    "            if hardbox\n",
    "                for p in [1:length(new_params);]\n",
    "                    if new_params[p] < bbox[p,1]; new_params[p] = bbox[p,1]; end\n",
    "                    if bbox[p,2] < new_params[p]; new_params[p] = bbox[p,2]; end\n",
    "                 end        \n",
    "                \n",
    "                new_cost, new_grad, new_hess = keyword_vgh((;pars...) -> func(;pars...), args, new_params)\n",
    "            else\n",
    "                new_cost, new_grad, new_hess = keyword_vgh((;pars...) -> func(;pars...) + \n",
    "                        wall_cost(args, bbox; wallwidth=wallwidth, pars...),\n",
    "                    args, new_params)                \n",
    "            end\n",
    "            \n",
    "            if abs(new_cost - cost) < tol || eta < tol\n",
    "                trajectory = trajectory[:,1:i]\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "\n",
    "        if jumptype == \"failed\" || new_cost >= cost  \n",
    "            eta = eta/2\n",
    "            costheta = NaN\n",
    "            if eta < tol\n",
    "                trajectory = trajectory[:,1:i]\n",
    "                break\n",
    "            end\n",
    "        else\n",
    "            eta = eta*1.1\n",
    "            costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "\n",
    "            params = new_params\n",
    "            cost = new_cost\n",
    "            grad = new_grad\n",
    "            hess = new_hess\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            if rem(i, verbose_every)==0\n",
    "                @printf \"%d: eta=%g cost=%.4f jtype=%s costheta=%.3f ps=\" i eta cost jumptype costheta\n",
    "                print_vector(params)\n",
    "                @printf \"\\n\"\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return params, trajectory, cost\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    Given args, a list of string representing the arguments of interest, a bounding box for each,\n",
    "    and a Symbol=>value dictionary with the corresponding parameters, computes and returns a high cost for \n",
    "    being outside the bounding box\n",
    "    \"\"\"\n",
    "    function wall_cost(args, bbox; wallwidth=NaN, nderivs=0, difforder=0, pars...) \n",
    "        myparams = ForwardDiffZeros(length(pars), 1, nderivs=nderivs, difforder=difforder)\n",
    "        pars2 = Dict()\n",
    "        for i in [1:length(pars);]\n",
    "            pars2[string(pars[i][1])] = pars[i][2]\n",
    "        end\n",
    "        for i in [1:length(args);]\n",
    "            myparams[i] = pars2[args[i]]\n",
    "        end\n",
    "        \n",
    "        if isnan(wallwidth)\n",
    "            # We know that we're going to be taking hessian for params, so declare zeros accordingly:\n",
    "            wallwidth = ForwardDiffZeros(length(myparams), 1, nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "            for i in [1:length(myparams);]\n",
    "                wallwidth[i] = wallwidth_factor*(bbox[i,2]-bbox[i,1])\n",
    "            end\n",
    "        end\n",
    "\n",
    "        retval = 0\n",
    "        for i in [1:length(myparams);]\n",
    "            if myparams[i]<bbox[i,1]\n",
    "                retval += cosh((bbox[i,1]-myparams[i])/wallwidth[i])-1.0\n",
    "            elseif bbox[i,2] < myparams[i]\n",
    "                retval += cosh((myparams[i]-bbox[i,2])/wallwidth[i])-1.0                \n",
    "            end\n",
    "        end\n",
    "\n",
    "        return 2*retval\n",
    "    end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf()\n",
    "costfunc(startU; do_plot=true, verbose=true, make_dict([\"sigma\"], [0.1], make_dict(args, params, model_params))...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working on figuring out the weird trajectories. Probably a dt thing.  The fourth one is the weird one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forward = (startpoint; do_plot=false, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;     \n",
    "    forwardModel(startpoint; do_plot=do_plot, pars...)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTION DEFINITION: fluxSense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The main fluxSense() function containing the main minimization loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function fluxSense(costfunc, backward, paramsDict, startUs, ends, args, seed; start_eta=0.01, tol=1e-15, \n",
    "    maxiter=400, verbose=true, do_plot=false, cost_limit=[], report_fluxless_grad=false, report_every=1)\n",
    "   \n",
    "    if do_plot; clf(); end;\n",
    "\n",
    "    params = seed\n",
    "    eta    = start_eta\n",
    "\n",
    "    if ~(typeof(ends)<:Array); ends = [ends]; end\n",
    "    U0 = zeros(size(ends))\n",
    "    for j in 1:size(ends,1)\n",
    "        # @printf(\"model params is \"); print(model_params); print(\"\\n\")\n",
    "        # @printf(\"ends[j,:] is \"); print(ends[j,:]); print(\"\\n\")        \n",
    "        U0[j,:] = backward(ends[j,:]; tol=1e-25, do_plot=false, make_dict(args, params, model_params)...)'\n",
    "        # @printf(\"U0[j,:] is \"); print(U0[j,:]); print(\"\\n\")        \n",
    "    end\n",
    "    \n",
    "    if length(ends)>0\n",
    "        @printf(\"U0[end,:] is \"); print_vector_g(U0[end,:]); @printf(\"\\n\")\n",
    "    end\n",
    "    \n",
    "    cost, grad, hess = \n",
    "        vgh((x)->costfunc([startUs;U0]; do_plot=do_plot, nderivs=length(x), difforder=2, \n",
    "            make_dict(args, x, model_params)...), params)\n",
    "    \n",
    "    if verbose && report_fluxless_grad\n",
    "        fcost, fgrad, fhess = \n",
    "        vgh((x)->costfunc(startUs; do_plot=false, nderivs=length(x), difforder=2, \n",
    "                make_dict(args, x, model_params)...), params)\n",
    "        @printf(\"      ### grad without flux track = \"); print_vector_g(fgrad); @printf(\"\\n\")    \n",
    "    end\n",
    "\n",
    "    if verbose\n",
    "        @printf(\"Initial cost, grad, hess:\\n\")\n",
    "        @printf(\"   cost = %g\\n\", cost)\n",
    "        @printf(\"   grad = \"); print_vector_g(grad); print(\"\\n\")\n",
    "        @printf(\"   hess = \"); print_vector_g(hess); print(\"\\n\")\n",
    "    end\n",
    "\n",
    "    delta_params=0\n",
    "    ptrajectory = zeros(length(seed), maxiter); \n",
    "    gtrajectory = zeros(length(seed), maxiter); \n",
    "    ctrajectory = zeros(1, maxiter);\n",
    "    \n",
    "    for i in [1:maxiter;]         \n",
    "        my_verbose = verbose && rem(i, report_every)==0\n",
    "\n",
    "        new_params = params - eta*grad/(sqrt(sum(grad.*grad)))\n",
    "        delta_params = new_params - params\n",
    "\n",
    "        new_cost, new_grad, new_hess = \n",
    "        vgh((x)->costfunc([startUs;U0]; do_plot=false, verbose=my_verbose, pre_string=\"   newpars>> \",\n",
    "                zero_last_sigmas=size(U0,1), nderivs=length(x), difforder=2, make_dict(args, x, model_params)...), \n",
    "                new_params)\n",
    "\n",
    "        if my_verbose\n",
    "            @printf(\"delta_params=\"); print_vector_g(delta_params); @printf(\"\\n\"); \n",
    "            @printf(\"new_cost=%g  cost=%g   delta_cost=%g\\n\", new_cost, cost, new_cost-cost)\n",
    "        end\n",
    "        \n",
    "        if abs(new_cost - cost) < tol\n",
    "            @printf(\"\\n===\\nChange in cost was less than the tolerance %g\\n===\\n\", tol)\n",
    "            ptrajectory=ptrajectory[:,1:i-1]; gtrajectory=gtrajectory[:,1:i-1]; ctrajectory=ctrajectory[1:i-1]\n",
    "            break\n",
    "        end\n",
    "        if (length(cost_limit)>0 && cost < cost_limit)\n",
    "            @printf(\"\\n===\\nCost was less than the cost limit %g\\n===\\n\", cost_limit)\n",
    "            ptrajectory=ptrajectory[:,1:i-1]; gtrajectory=gtrajectory[:,1:i-1]; ctrajectory=ctrajectory[1:i-1]\n",
    "            break\n",
    "        end\n",
    "        \n",
    "        if new_cost >= cost\n",
    "            eta = eta/2\n",
    "            costheta = NaN\n",
    "        else\n",
    "            eta = eta*1.2\n",
    "            costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "\n",
    "            params = new_params\n",
    "    \n",
    "            for j in 1:size(ends,1)\n",
    "                U0[j,:] = backward(ends[j,:]; do_plot=false, make_dict(args, params, model_params)...)'\n",
    "            end\n",
    "            if my_verbose && length(ends)>0\n",
    "                @printf(\"U0[end,:] is \"); print_vector_g(U0[end,:]); @printf(\"\\n\")\n",
    "            end\n",
    "            cost, grad, hess = \n",
    "                vgh((x)->costfunc([startUs;U0]; do_plot=do_plot, verbose=my_verbose, nderivs=length(x), difforder=2, \n",
    "                    zero_last_sigmas=size(U0,1), make_dict(args, x, model_params)...), params)\n",
    "\n",
    "        end\n",
    "        \n",
    "        ptrajectory[:,i] = params\n",
    "        gtrajectory[:,i] = grad\n",
    "        ctrajectory[i]   = cost\n",
    "\n",
    "        if my_verbose\n",
    "            @printf \"%d: eta=%g cost=%g costheta=%g ps=\" i eta cost  costheta\n",
    "            print_vector(params)\n",
    "            @printf \"\\n\"\n",
    "            @printf(\"grad=\"); print_vector_g(grad); @printf(\"\\n\")\n",
    "            if report_fluxless_grad\n",
    "                fcost, fgrad, fhess = \n",
    "                vgh((x)->costfunc(startUs; do_plot=false, verbose=false, nderivs=length(x), difforder=2, \n",
    "                        make_dict(args, x, model_params)...), params)\n",
    "            @printf(\"      ### grad without flux track = \"); print_vector_g(fgrad); @printf(\"\\n\")    \n",
    "            end            \n",
    "        end\n",
    "    end    \n",
    "\n",
    "    return params, ctrajectory, ptrajectory, gtrajectory\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORKING EXAMPLE:   1-D example of using fluxSense()\n",
    "\n",
    "## after defining fluxSense(), run the next two cells in order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the cost function. It passes most keyword params down to the forward and backward models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function J(initUs; theta1=0.15, theta2=0.2, beta=0.01, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, params...)\n",
    "\n",
    "    Vend = ForwardDiffZeros(length(initUs), 1, nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:length(initUs)\n",
    "        Ue, Ve, U, V = forwardModel(initUs[i]; nderivs=nderivs, difforder=difforder, \n",
    "            do_plot=do_plot, clearfig=false, params...)\n",
    "        Vend[i] = Ve[1]\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend-0.5)/theta1))\n",
    "    diffs = tanh((Vend-0.5)/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now setup, run fluxSense(), and display results. Example is only 1-d so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "backward = (endpoint; do_plot=false, pars...) -> backwardsModel(endpoint; do_plot=do_plot, pars...)[1]\n",
    "\n",
    "costfunc = (startpoints; do_plot=false, verbose=false, nderivs=0, difforder=0, pars...) -> \n",
    "J(startpoints; do_plot=do_plot, verbose=verbose, beta=0.01, nderivs=nderivs, difforder=difforder, pars...)\n",
    "\n",
    "dt = 0.01\n",
    "t = 0:dt:2\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "noise = 0*randn(1, nsteps)\n",
    "noise = 0.02*sin(2*pi*3*t); noise=reshape(noise, 1, nsteps)\n",
    "\n",
    "W = 4.1\n",
    "const_add = -2\n",
    "init_add=0\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>W, :nsteps=>nsteps, \n",
    "    :noise=>noise, :noise=>noise, :const_add=>const_add, :init_add=>init_add)\n",
    "\n",
    "srand(20)  \n",
    "startUs = randn(20, 1)       # The starting values\n",
    "# startUs = [randn(10,1)+2;randn(10,1)-2]\n",
    "\n",
    "\n",
    "args = [\"init_add\", \"const_add\", \"W\"]\n",
    "seed = [0, -2, 4.1]\n",
    "\n",
    "# Do an initial run plotting to show the starting position\n",
    "clf()\n",
    "costfunc(startUs; do_plot=true, verbose=true, model_params...)\n",
    "\n",
    "\n",
    "fluxFinalPoint = convert(Float64, 0)  # The final value of the pinned output\n",
    "#\n",
    "# If you remove the fluxFinalPoint, by un-commenting the following line, it gets stuck. But\n",
    "# it is also true that if you make beta=0 (in the constfunc() definition in line 4 above) then t\n",
    "# that also solves the sticking problem.  If we had beta=0 until after our hits are what we want, \n",
    "# would we ever need fluxPoint?\n",
    "#\n",
    "fluxFinalPoint = [];\n",
    "\n",
    "params, cost = fluxSense(costfunc, backward, model_params, startUs, fluxFinalPoint, args, seed; \n",
    "start_eta=0.01, tol=1e-15, maxiter=400, verbose=true, report_fluxless_grad=false, do_plot=true, cost_limit=-0.00959)\n",
    "\n",
    "# And show the final position\n",
    "clf()\n",
    "costfunc(startUs; do_plot=true, verbose=true, make_dict(args, params, model_params)...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---  END --- 1d example of using fluxSense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of forwards and backwards models\n",
    "\n",
    "### Inverting time even through a sinusoidal noise, with added noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt = 0.01\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "noise = 0*randn(1, nsteps)\n",
    "noise = 0.2*sin(2*pi*3*t); noise=reshape(noise, 1, nsteps)\n",
    "W = [0.5]\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>W, :nsteps=>nsteps, \n",
    "    :noise=>noise, :noise=>noise, :const_add=>-0.15, :init_add=>0.3)\n",
    "clf();\n",
    "\n",
    "srand(10)\n",
    "\n",
    "startUs = -0.5\n",
    "Uend, Vend, U, V =forwardModel(startUs; do_plot=true, clearfig=false, model_params...)\n",
    "Ustart, Vstart = backwardsModel(Uend;  do_plot=true, clearfig=false, tol=1e-15, model_params...)\n",
    "\n",
    "[startUs Ustart]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD: scripts on the path to writing fluxSense() as a function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the cost function. It passes most keyword params down to the forward and backward models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function J(initUs; theta1=0.15, theta2=0.2, beta=0.01, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, params...)\n",
    "\n",
    "    Vend = ForwardDiffZeros(length(initUs), 1, nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:length(initUs)\n",
    "        Ue, Ve, U, V = forwardModel(initUs[i]; nderivs=nderivs, difforder=difforder, \n",
    "            do_plot=do_plot, clearfig=false, params...)\n",
    "        Vend[i] = Ve[1]\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend-0.5)/theta1))\n",
    "    diffs = tanh((Vend-0.5)/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An example of some code that does differentiation. This cell not necessary for running the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# An example of a standard setup which we'll try to modify to try to get 75% correct\n",
    "\n",
    "dt = 0.01\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "noise = 0*randn(1, nsteps)\n",
    "noise = 0.02*sin(2*pi*3*t); noise=reshape(noise, 1, nsteps)\n",
    "\n",
    "W = 4.1\n",
    "const_add = -2\n",
    "init_add=0\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>W, :nsteps=>nsteps, \n",
    ":noise=>noise, :noise=>noise, :const_add=>const_add, :init_add=>init_add)\n",
    "\n",
    "srand(10)\n",
    "startUs = randn(40, 1)\n",
    "J(startUs; do_plot=true, verbose=true, model_params...)\n",
    "\n",
    "# --- now while taking the derivative ---\n",
    "args = [\"init_add\", \"const_add\", \"W\"]\n",
    "seed = [init_add, const_add, W]\n",
    "\n",
    "ForwardDiff.gradient((x)->J(startUs; do_plot=true, nderivs=length(x), difforder=1, verbose=true, make_dict(args, x, model_params)...), seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main adaptive step with gradient and keywords loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################################\n",
    "#                                                   #\n",
    "#                                                   #\n",
    "#     ADAPTIVE GRADIENT FOR KEYWORD VERSION         #\n",
    "#                                                   #\n",
    "#                                                   #\n",
    "#####################################################\n",
    "\n",
    "\n",
    "# -----------------  FORWARD MODEL SETUP ---------------\n",
    "dt = 0.01\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "noise = 0*randn(1, nsteps)\n",
    "noise = 0.02*sin(2*pi*3*t); noise=reshape(noise, 1, nsteps)\n",
    "\n",
    "W = 4.1\n",
    "const_add = -2\n",
    "init_add=0\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>W, :nsteps=>nsteps, \n",
    ":noise=>noise, :noise=>noise, :const_add=>const_add, :init_add=>init_add)\n",
    "\n",
    "\n",
    "# ----------------  CRITICAL INDICATION OF PARAMETERS TO OPTIMIZE IS HERE: -----\n",
    "args = [\"init_add\", \"const_add\", \"W\"]\n",
    "seed = [0, -2, 4.1]\n",
    "\n",
    "fluxFinalPoint = convert(Float64, 0)  # The final value of the pinned output\n",
    "\n",
    "srand(10)  \n",
    "startUs = randn(200, 1)       # The starting values\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "clf()\n",
    "\n",
    "start_eta = 0.01\n",
    "tol = 1e-15\n",
    "maxiter = 400\n",
    "verbose = true\n",
    "do_plot=false\n",
    "\n",
    "# -------\n",
    "\n",
    "params = seed\n",
    "eta    = start_eta\n",
    "\n",
    "\n",
    "U0 = backwardsModel(fluxFinalPoint; do_plot=false, make_dict(args, params, model_params)...)[1]\n",
    "J([startUs;U0]; verbose=true, do_plot=true, make_dict(args, params, model_params)...)\n",
    "\n",
    "\n",
    "cost, grad, hess = \n",
    "    vgh((x)->J([startUs;U0]; do_plot=false, nderivs=length(x), difforder=2, make_dict(args, x, model_params)...), params)\n",
    "\n",
    "@printf(\"Initial cost, grad, hess:\\n\")\n",
    "print_vector_g(:cost)\n",
    "print_vector_g(:grad)\n",
    "print_vector_g(:hess)\n",
    "delta_params=0\n",
    "\n",
    "\n",
    "for i in 1:maxiter         \n",
    "        new_params = params - eta*grad/(sqrt(sum(grad.*grad)))\n",
    "        delta_params = new_params - params\n",
    "        print_vector_g(:delta_params)\n",
    "        new_cost, new_grad, new_hess = \n",
    "            vgh((x)->J([startUs;U0]; do_plot=false, verbose=false,\n",
    "                nderivs=length(x), difforder=2, make_dict(args, x, model_params)...), new_params)\n",
    "        @printf(\"new_cost=%g  cost=%g   delta_cost=%g\\n\", new_cost, cost, new_cost-cost)\n",
    "        if abs(new_cost - cost) < tol\n",
    "            break\n",
    "        end\n",
    "\n",
    "        if new_cost >= cost\n",
    "            eta = eta/2\n",
    "            costheta = NaN\n",
    "        else\n",
    "            eta = eta*1.2\n",
    "            costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "\n",
    "            params = new_params\n",
    "    \n",
    "            U0 = backwardsModel(fluxFinalPoint; do_plot=false, make_dict(args, params, model_params)...)[1]\n",
    "            cost, grad, hess = \n",
    "                vgh((x)->J([startUs;U0]; do_plot=do_plot, verbose=true,\n",
    "                    nderivs=length(x), difforder=2, make_dict(args, x, model_params)...), params)\n",
    "\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            @printf \"%d: eta=%g cost=%g costheta=%g ps=\" i eta cost  costheta\n",
    "            print_vector(params)\n",
    "            @printf \"\\n\"\n",
    "        end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A cell to plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# J(params; initUs=[startUs;U0], verbose=true, do_plot=true)\n",
    "clf()\n",
    "Ve = zeros(length(startUs),1)\n",
    "for i=1:length(startUs)\n",
    "    Ue, Vee, U, V = forwardModel(startUs[i]; do_plot=true, clearfig=false, make_dict(args, params, model_params)...)\n",
    "    Ve[i] = Vee[1]\n",
    "end\n",
    "\n",
    "@printf(\"\\n\\nFinal result produces %d hits out of %d trials for %.1f per cent correct\\n\\n\", length(find(Ve.>0.5)), \n",
    "    length(Ve), 100*length(find(Ve.>0.5))/length(Ve))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- END Complete keyword-driven adaptive gradient version of FluxSense minimizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete adaptive gradient version of FluxSense minimizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################################\n",
    "#                                                   #\n",
    "#                                                   #\n",
    "#     In this cell we define J(x::Vector[3])        #\n",
    "#     Not full keyword version yet.                 #\n",
    "#                                                   #\n",
    "#     Next cell has the adaptive gradient procedure #\n",
    "#                                                   #\n",
    "#     Run the third cell to see results             #\n",
    "#                                                   #\n",
    "#                                                   #\n",
    "#####################################################\n",
    "\n",
    "dt = 0.01\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "\n",
    "W = [4]\n",
    "k = -2\n",
    "init_k = 0\n",
    "\n",
    "noise = 0.2*sin(2*pi*3*t); noise = reshape(noise, 1, nsteps)\n",
    "\n",
    "mypars = Dict(:dt=>dt, :tau=>tau, :nsteps=>nsteps)\n",
    "\n",
    "srand(10)\n",
    "startUs = 2*randn(200,1)\n",
    "# for i=1:length(startUs)\n",
    "#    Uend, Vend, U, V = forwardModel(startUs[i]; noise=noise+k, W=W, do_plot=true, clearfig=false, params...)\n",
    "#end\n",
    "\n",
    "# backwardsModel([1.2*0]; do_plot=true, clearfig=false, params...)\n",
    "\n",
    "function J(x; initUs=startUs, theta1=0.15, theta2=0.2, beta=0.05, verbose=false,\n",
    "    nderivs=0, difforder=0, do_plot=true)\n",
    "    \n",
    "    Vend = ForwardDiffZeros(length(initUs), 1, nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    k = x[1]\n",
    "    W = x[2]\n",
    "    init_k = x[3]\n",
    "    \n",
    "    for i=1:length(initUs)\n",
    "        Ue, Ve, U, V = forwardModel(initUs[i]+init_k; noise=noise+k, W=[W], \n",
    "        nderivs=nderivs, difforder=difforder, do_plot=do_plot, clearfig=false, mypars...)\n",
    "        Vend[i] = Ve[1]\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend-0.5)/theta1))\n",
    "    diffs = tanh((Vend-0.5)/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "\n",
    "\n",
    "# WORKS:\n",
    "# ForwardDiff.gradient((x)->forwardModel(x[1]; noise=noise+k, W=[x[2]], \n",
    "#    do_plot=true, clearfig=true, nderivs=2, difforder=1, params...)[1][1], [-2.1, 4])\n",
    "\n",
    "ForwardDiff.gradient((x)->J(x; nderivs=3, difforder=1), [-2, 4.1, 0])\n",
    "# J([-2.1, 4])\n",
    "\n",
    "\n",
    "# ForwardDiff.derivative((x)->forwardModel(startUs[1]; noise=noise+k, W=[x], \n",
    "#    do_plot=true, clearfig=true, nderivs=1, difforder=1, params...)[1], 4.5995)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################################\n",
    "#                                                   #\n",
    "#                                                   #\n",
    "#     ADAPTIVE GRADIENT VERSION                     #\n",
    "#                                                   #\n",
    "#                                                   #\n",
    "#####################################################\n",
    "\n",
    "# This is all BEFORE makign J fully keyword-value driven\n",
    "\n",
    "seed = [-2, 4.1, 0]   # params are constant add, W, and init_add.\n",
    "start_eta = 0.01\n",
    "tol = 1e-15\n",
    "maxiter = 400\n",
    "verbose = true\n",
    "\n",
    "params = seed\n",
    "eta = start_eta\n",
    "\n",
    "U0 = backwardsModel([1.2*0]; noise=noise+params[1], W=[params[2]], do_plot=true, mypars...)[1] - params[3]\n",
    "\n",
    "J(params; initUs=[startUs;U0], verbose=true)\n",
    "\n",
    "cost, grad, hess = vgh((x)->J(x; initUs=[startUs;U0], nderivs=length(params), difforder=2, verbose=true), params)\n",
    "\n",
    "@printf(\"Initial cost, grad, hess:\\n\")\n",
    "print_vector_g(:cost)\n",
    "print_vector_g(:grad)\n",
    "print_vector_g(:hess)\n",
    "delta_params=0\n",
    "\n",
    "for i in 1:maxiter         \n",
    "        new_params = params - eta*grad/(sqrt(sum(grad.*grad)))\n",
    "        delta_params = new_params - params\n",
    "        print_vector_g(:delta_params)\n",
    "        new_cost, new_grad, new_hess = \n",
    "            vgh((x)->J(x; initUs=[startUs;U0], nderivs=length(params), difforder=2, verbose=false, do_plot=false), \n",
    "                new_params)        \n",
    "        @printf(\"new_cost=%g  cost=%g   delta_cost=%g\\n\", new_cost, cost, new_cost-cost)\n",
    "        if abs(new_cost - cost) < tol\n",
    "            break\n",
    "        end\n",
    "\n",
    "        if new_cost >= cost\n",
    "            eta = eta/2\n",
    "            costheta = NaN\n",
    "        else\n",
    "            eta = eta*1.2\n",
    "            costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "\n",
    "            params = new_params\n",
    "    \n",
    "            U0 = backwardsModel([1.2*0]; noise=noise+params[1], W=[params[2]], do_plot=false, mypars...)[1] - params[3]\n",
    "            cost, grad, hess = \n",
    "                vgh((x)->J(x; initUs=[startUs;U0], nderivs=length(params), difforder=2, verbose=true, do_plot=false), params)\n",
    "\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            @printf \"%d: eta=%g cost=%g costheta=%g ps=\" i eta cost  costheta\n",
    "            print_vector(params)\n",
    "            @printf \"\\n\"\n",
    "        end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# J(params; initUs=[startUs;U0], verbose=true, do_plot=true)\n",
    "clf()\n",
    "Ve = zeros(length(startUs),1)\n",
    "for i=1:length(startUs)\n",
    "    Ue, Vee, U, V = forwardModel(startUs[i]+params[3]; noise=params[1], W=[params[2]],     \n",
    "        do_plot=true, clearfig=false, tau=0.1, nsteps=201, dt=0.01)\n",
    "    Ve[i] = Vee[1]\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------END OF: complete adaptive gradient version of FluxSense minimizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf()\n",
    "J(params; initUs=[startUs;U0], verbose=true, do_plot=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "length(find(Ve.>0.5))/200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mypars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################################\n",
    "#                                                   #\n",
    "#                                                   #\n",
    "#     HESSIAN VERSION                               #\n",
    "#                                                   #\n",
    "#                                                   #\n",
    "#####################################################\n",
    "\n",
    "\n",
    "seed = [-2, 4.1]\n",
    "start_eta = 0.0000001\n",
    "tol = 1e-15\n",
    "maxiter = 400\n",
    "verbose = true\n",
    "\n",
    "params = seed\n",
    "eta = start_eta\n",
    "\n",
    "U0 = backwardsModel([1.2*0]; noise=noise+params[1], W=[params[2]], do_plot=true, mypars...)[1]\n",
    "\n",
    "J(params; initUs=[startUs;U0], verbose=true)\n",
    "\n",
    "cost, grad, hess = vgh((x)->J(x; initUs=[startUs;U0], nderivs=length(params), difforder=2, verbose=true), params)\n",
    "\n",
    "@printf(\"Initial cost, grad, hess:\\n\")\n",
    "print_vector_g(:cost)\n",
    "print_vector_g(:grad)\n",
    "print_vector_g(:hess)\n",
    "\n",
    "for i in 1:maxiter\n",
    "        hathess    = hess + eye(length(grad), length(grad))/eta        \n",
    "        new_params = params - inv(hathess)*grad\n",
    "        new_cost, new_grad, new_hess = \n",
    "            vgh((x)->J(x; initUs=[startUs;U0], nderivs=length(params), difforder=2, verbose=true), new_params)\n",
    "            \n",
    "        if abs(new_cost - cost) < tol\n",
    "            # break\n",
    "        end\n",
    "\n",
    "        if new_cost >= cost\n",
    "            eta = eta/2\n",
    "            costheta = NaN\n",
    "        else\n",
    "            eta = eta*1.1\n",
    "            costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "\n",
    "            params = new_params\n",
    "    \n",
    "            U0 = backwardsModel([1.2*0]; noise=noise+params[1], W=[params[2]], do_plot=false, mypars...)[1]\n",
    "            cost, grad, hess = \n",
    "                vgh((x)->J(x; initUs=[startUs;U0], nderivs=length(params), difforder=2, verbose=true), params)\n",
    "\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            @printf \"%d: eta=%g cost=%g costheta=%g ps=\" i eta cost  costheta\n",
    "            print_vector(params)\n",
    "            @printf \"\\n\"\n",
    "        end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = [-2, 4.1]\n",
    "start_eta = 10\n",
    "tol = 1e-15\n",
    "maxiter = 400\n",
    "\n",
    "params = seed\n",
    "eta = start_eta\n",
    "\n",
    "# backwardsModel([0.5]; noise=noise+params[1], W=params[2], params...)\n",
    "\n",
    "cost, grad, hess = vgh(func, params)\n",
    "\n",
    "\n",
    "    if verbose && verbose_level >= 2\n",
    "        @printf(\"Initial cost, grad, hess:\\n\")\n",
    "        print_vector_g(:cost)\n",
    "        print_vector_g(:grad)\n",
    "        print_vector_g(:hess)\n",
    "    end\n",
    "    \n",
    "\n",
    "    for i in [1:maxiter;]\n",
    "        hathess    = hess + eye(length(grad), length(grad))/eta        \n",
    "        new_params = params - inv(hathess)*grad\n",
    "        new_cost, new_grad, new_hess = vgh(func, new_params)\n",
    "            \n",
    "        if abs(new_cost - cost) < tol\n",
    "            break\n",
    "        end\n",
    "\n",
    "        if new_cost >= cost\n",
    "            eta = eta/2\n",
    "            costheta = NaN\n",
    "        else\n",
    "            eta = eta*1.1\n",
    "            costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "\n",
    "            params = new_params\n",
    "            cost = new_cost\n",
    "            grad = new_grad\n",
    "            hess = new_hess\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            @printf \"%d: eta=%.3f cost=%.4f costheta=%.3f ps=\" i eta cost  costheta\n",
    "            print_vector(params)\n",
    "            @printf \"\\n\"\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return params, cost\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trust_region_Hessian_minimization([-2, 4.1], (x)->J(x; nderivs=2, difforder=2, verbose=true), verbose=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt = 0.01\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "\n",
    "noise = 3.5*sin(2*pi*3*t); noise=reshape(noise, 1, nsteps)\n",
    "W = [0.5]\n",
    "\n",
    "params = Dict(:dt=>dt, :tau=>tau, :nsteps=>nsteps, :noise=>noise)\n",
    "\n",
    "function J(x; nderivs=0, difforder=0)\n",
    "    startU = x[1]\n",
    "    W = x[2]\n",
    "    \n",
    "    Uend, Vend, U, V = forwardModel(startU; do_plot=true, W=[W], nderivs=nderivs, difforder=difforder, params...)\n",
    "    \n",
    "    return (Vend[1]-0.5)^2\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trust_region_Hessian_minimization([-0.5, 0.5], (x) -> J(x;nderivs=2, difforder=2), verbose=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD STUFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- BEGIN --- Old example that gets stuck: too large beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The following sequence leads to a situation where having only [-0.8, -0.8] as the single finalFluxPoint \n",
    "# leads to the minimization getting stuck.  Adding further finalFluxPoints solves the problem.\n",
    "#\n",
    "# Reducing beta in the cost function J() from 0.01 to 0.003 also eliminated the problem.  \n",
    "#\n",
    "srand(10)\n",
    "startU=randn(100,2)-3\n",
    "startU=randn(100,2)-3\n",
    "\n",
    "# startU=randn(100,2)-3\n",
    "\n",
    "dt = 0.02\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "W = -4\n",
    "noise = 0\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    "    :noise=>noise, :noise=>noise, :const_add=>0, :init_add=>0)\n",
    "\n",
    "clf(); subplot(2,1,1)\n",
    "for i in 1:size(startU,1)\n",
    "    Uend, Vend, U, V = forwardModel(startU[i,:]; do_plot=true, clearfig=false, model_params...)\n",
    "end\n",
    "\n",
    "Ustarthat, Vstarthat, Uhatm, Vhat, costs = backwardsModel([-0.8, -0.8]; do_plot=true, clearfig=false, \n",
    "tol=1e-50, maxiter=800, model_params...)\n",
    "\n",
    "subplot(2,1,2)\n",
    "plot(t, costs, \".-\")\n",
    "\n",
    "\n",
    "function JJ(initUs; theta1=0.15, theta2=0.2, beta=0.01, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, pre_string=\"\", params...)\n",
    "\n",
    "    Vend = ForwardDiffZeros(size(initUs,1), size(initUs,2), nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:size(initUs,1)\n",
    "        Ue, Ve, U, V = forwardModel(initUs[i,:]; nderivs=nderivs, difforder=difforder, \n",
    "            do_plot=do_plot, clearfig=false, params...)\n",
    "        Vend[i,:] = Ve\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend[:,1]-Vend[:,2])/theta1))\n",
    "    diffs = tanh.((Vend[:,1]-Vend[:,2])/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"%s\", pre_string)\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "  \n",
    "clf();\n",
    "JJ(startU; do_plot=true, model_params...)\n",
    "\n",
    "# WORKING gradient:\n",
    "# ForwardDiff.gradient((x)->JJ(startU; do_plot=true, nderivs=length(x), difforder=1, \n",
    "#    make_dict([[\"init_add\" 2], \"const_add\"], x, model_params)...), [2.9, -2.9, 0.1])\n",
    "\n",
    "\n",
    "\n",
    "# The backward and costfunc functions should turn a single-scalar parameter W into the matrix W\n",
    "backward = (endpoint; do_plot=false, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;     \n",
    "    backwardsModel(endpoint; do_plot=do_plot, pars...)[1]\n",
    "end\n",
    "\n",
    "costfunc = (startpoints; do_plot=false, verbose=false, nderivs=0, difforder=0, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;         \n",
    "    JJ(startpoints; do_plot=do_plot, verbose=verbose, nderivs=nderivs, difforder=difforder, pars...)\n",
    "end\n",
    "\n",
    "\n",
    "fluxFinalPoint = [-0.8 -0.8]  # ; -0.6 -0.6 ; -0.4 -0.4; -0.2 -0.2; 0 0; 0.2 0.2]\n",
    "\n",
    "args = [[\"init_add\" 2], \"const_add\"] # , \"W\"]\n",
    "seed = [0.001, 0.001, 0] # , -4]\n",
    "# seed = [1.190, -1.178, 2.000]\n",
    "\n",
    "params, cost = fluxSense(costfunc, backward, model_params, startU, fluxFinalPoint, args, seed; \n",
    "start_eta=0.01, tol=1e-15, maxiter=400, verbose=true, do_plot=false, cost_limit=-0.00935) # cost_limit=-0.000935) # for beta=0.01\n",
    "\n",
    "# And show the final position\n",
    "clf()\n",
    "costfunc(startU; do_plot=true, verbose=true, make_dict(args, params, model_params)...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- END --- Old example that gets stuck: too large beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of getting stuck without a flux point even with beta=0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function JJ(initUs; theta1=0.15, theta2=0.2, beta=0.003, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, pre_string=\"\", zero_last_sigmas=0, seedrand=NaN, params...)\n",
    "\n",
    "    if ~isnan(seedrand); srand(seedrand); end\n",
    "    \n",
    "    Vend = ForwardDiffZeros(size(initUs,1), size(initUs,2), nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:size(initUs,1)\n",
    "        if i>size(initUs,1) - zero_last_sigmas\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; sigma=0, nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)            \n",
    "        else\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)\n",
    "        end\n",
    "        Vend[i,:] = Ve\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend[:,1]-Vend[:,2])/theta1))\n",
    "    diffs = tanh.((Vend[:,1]-Vend[:,2])/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"%s\", pre_string)\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "srand(11)\n",
    "startU=randn(100,2)-3\n",
    "startU=randn(100,2)-3\n",
    "\n",
    "\n",
    "# startU=0.1*randn(100,2)-3\n",
    "# startU=zeros(100,2)-3\n",
    "\n",
    "\n",
    "dt = 0.005  # If we go to dt=0.02, it doesn't get stuck\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "W = -4\n",
    "noise = 0\n",
    "input = 0\n",
    "sigma = 0.1\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    ":noise=>noise, :input=>input, :sigma=>sigma, :const_add=>0, :init_add=>0)\n",
    "\n",
    "\n",
    "\n",
    "# The backward and costfunc functions should turn a single-scalar parameter W into the matrix W\n",
    "# backward always runs with no within-forward noise, i.e., sigma=0\n",
    "backward = (endpoint; do_plot=false, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;     \n",
    "    backwardsModel(endpoint; do_plot=do_plot, make_dict([\"sigma\"], [0], pars)...)[1]\n",
    "end\n",
    "\n",
    "\n",
    "beta=0\n",
    "\n",
    "costfunc = (startpoints; do_plot=false, verbose=false, nderivs=0, difforder=0, sr=26, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;         \n",
    "    JJ(startpoints; seedrand=sr, beta=beta, \n",
    "        do_plot=do_plot, verbose=verbose, nderivs=nderivs, difforder=difforder, pars...)\n",
    "end\n",
    "\n",
    "if beta==0.003;     cost_limit = -0.00288\n",
    "elseif beta<0.001;  cost_limit = -0.0008\n",
    "elseif beta==0.001; cost_limit = -0.000935\n",
    "elseif beta==0.05;  cost_limit = -0.0485\n",
    "else\n",
    "    error(\"Don't know what cost limit goes with beta %g\\n\", beta)\n",
    "end\n",
    "\n",
    "fluxFinalPoint = zeros(0,2);\n",
    "\n",
    "args = [[\"init_add\" 2], \"const_add\", \"W\"]\n",
    "\n",
    "seed = [0.001, 0.001, 0, -4]\n",
    "\n",
    "\n",
    "# Alternatively, start right from the sticking point:\n",
    "seed = [4.74063,  -4.68228,  2.73165,  -5.6783]\n",
    "\n",
    "# Walls are big enough that we never hit them, so it is immaterial:\n",
    "bbox = [\n",
    "    -15        15  ;\n",
    "    -15        15  ;\n",
    "    -15        15  ;\n",
    "    -20.5  20.5  ; \n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clf()\n",
    "costfunc(startU; do_plot=true, verbose=true, make_dict(args, seed, model_params)...)\n",
    "\n",
    "\n",
    "# YOU CAN DO EITHER THIS:\n",
    "params, traj = bbox_Hessian_keyword_minimization(seed, args, bbox, \n",
    "(;params...) -> costfunc(startU; verbose=true, merge(model_params, Dict(params))...), \n",
    "verbose=true, start_eta=0.01, tol=1e-10, hardbox=true )\n",
    "\n",
    "# OR THIS:  (both get stuck)\n",
    "# fluxFinalPoint = [-0.1 -0.1]\n",
    "# params, cost, ptraj, gtraj = fluxSense(costfunc, backward, model_params, startU, fluxFinalPoint, args, seed; \n",
    "# start_eta=0.01, tol=1e-15, maxiter=400, verbose=true, report_every=1, do_plot=true, cost_limit=cost_limit) # cost_limit=-0.000935) # for beta=0.01\n",
    "\n",
    "# And show the final position\n",
    "clf()\n",
    "costfunc(startU; do_plot=true, verbose=true, make_dict(args, params, model_params)...)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.2",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
