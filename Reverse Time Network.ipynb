{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wed 2017-09-06 1pm:** Time to go to ProAnti. I don't think it's worth stopping to examine further whether fluxSense() is worth it, just doesn't seem the issue right now. Can revisit if necessary.\n",
    "\n",
    "**Wed 2017-09-06 11am:** Current status: everythign working, bbox_hessian_minimization fully debugged and report diagnostic info. Basic MGO minimizations are doing their thing.  Have now added self-connection weights, and can very successfully train the MGO network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">TABLE OF CONTENTS</h1>\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "**Updates to the table of contents are periodic, but run the cell below to first start or force an update.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://sites.google.com/site/brodylabhome/files/make_table_of_contents.js')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition @javascript_str(ANY<:Any) in module Main at In[1]:1 overwritten at In[49]:1.\n"
     ]
    }
   ],
   "source": [
    "macro javascript_str(s) display(\"text/javascript\", s); end\n",
    "\n",
    "javascript\"\"\"\n",
    "$.getScript('https://sites.google.com/site/brodylabhome/files/make_table_of_contents.js')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We define functions to convert Duals, the variable types used by ForwardDiff, \\nto Floats. This is useful if we want to print out the value of a variable \\n(since print doesn't know how to Duals). Note that after being converted to a Float, no\\ndifferentiation by ForwardDiff can happen!  e.g. after\\n    x = convert(Float64, y)\\nForwardDiff can still differentiate y, but it can't differentiate x\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "using PyCall\n",
    "using PyPlot\n",
    "using ForwardDiff\n",
    "using DiffBase\n",
    "using MAT\n",
    "\n",
    "pygui(true)\n",
    "\n",
    "import Base.convert\n",
    "convert(::Type{Float64}, x::ForwardDiff.Dual) = Float64(x.value)\n",
    "function convert(::Array{Float64}, x::Array{ForwardDiff.Dual}) \n",
    "    y = zeros(size(x)); \n",
    "    for i in 1:prod(size(x)) \n",
    "        y[i] = convert(Float64, x[i]) \n",
    "    end\n",
    "    return y\n",
    "end\n",
    "\n",
    "include(\"general_utils.jl\")\n",
    "include(\"hessian_utils.jl\")\n",
    "\n",
    "\"\"\"\n",
    "We define functions to convert Duals, the variable types used by ForwardDiff, \n",
    "to Floats. This is useful if we want to print out the value of a variable \n",
    "(since print doesn't know how to Duals). Note that after being converted to a Float, no\n",
    "differentiation by ForwardDiff can happen!  e.g. after\n",
    "    x = convert(Float64, y)\n",
    "ForwardDiff can still differentiate y, but it can't differentiate x\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup -- definitions of forwardModel() and backwardsModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition g(Any) in module Main at In[142]:5 overwritten at In[144]:5.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'g :: Tuple{Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition ginverse(Any) in module Main at In[142]:12 overwritten at In[144]:12.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'ginverse :: Tuple{Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition forwardModel(Any) in module Main at In[142]:78 overwritten at In[144]:78.\n",
      "WARNING: Method definition #forwardModel(Array{Any, 1}, Main.#forwardModel, Any) in module Main overwritten.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'forwardModel :: Tuple{Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition backwardsModel(Any) in module Main at In[142]:224 overwritten at In[144]:224.\n",
      "WARNING: Method definition #backwardsModel(Array{Any, 1}, Main.#backwardsModel, Any) in module Main overwritten.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'backwardsModel :: Tuple{Any}' in module 'Main'.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "backwardsModel"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "o = g(z)    squashing tanh function, running from 0 to 1, is equal to 0.5 when input is 0.\n",
    "\"\"\"\n",
    "function g(z)\n",
    "    return 0.5*tanh.(z)+0.5\n",
    "end\n",
    "    \n",
    "\"\"\"\n",
    "z = g^-1(o)    inverse of squashing tanh function, input must be in (0, 1), output is zero when passed 0.5.\n",
    "\"\"\"\n",
    "function ginverse(z)\n",
    "    return 0.5*log.(z./(1-z))\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "forwardModel(startU; dt=0.01, tau=0.1, nsteps=100, input=[0.1, 0], noise=[], W=[0 -5;-5 0], \n",
    "init_add=0, start_add=0, const_add=0, sigma=0, gleak=1, U_rest=0, \n",
    "    do_plot=false, nderivs=0, difforder=0, clearfig=true, fignum=1, dUdt_mag_only=false)\n",
    "\n",
    "Runs a tanh() style-network forwards in time, given its starting point, using simple Euler integration\n",
    "    tau dU/dt = -U + W*V + I\n",
    "    V = 0.5*tanh(U)+ 0.5\n",
    "\n",
    "**PARAMETERS:**\n",
    "\n",
    "startU     A column vector, nunits-by-1, indicating the values of U at time zero\n",
    "\n",
    "\n",
    "**OPTIONAL PARAMETERS**\n",
    "\n",
    "dt      Scalar, timestep size\n",
    "\n",
    "tau     Scalar, in seconds\n",
    "\n",
    "gleak   \n",
    "        dUdt will have a term equal to gleak*(U_rest - U)\n",
    "U_rest\n",
    "\n",
    "nsteps  Number of timesteps to run, including time=0.\n",
    "\n",
    "input   Either an nunits-by-1 vector, in which case inputs to each unit are constant\n",
    "        across time, or a matrix, nunits-by-nsteps, indicating input for each unit at each timepoint.\n",
    "\n",
    "W       Weight matrix, nunits-by-nunits\n",
    "\n",
    "init_add    DEPRECATED: Vector or scalar that gets added to the input current at very first timestep.\n",
    "            Deprecated because this made it dt-dependent. Replaced by start_add.\n",
    "\n",
    "start_add   Vector or scalar that gets added, once, to the initial U[:,1], before the integration process begins.\n",
    "\n",
    "const_add   Scalar that gets added to U after every timestep\n",
    "\n",
    "sigma       After each timestep, add sigma*sqrt(dt)*randn() to each element of U\n",
    "\n",
    "do_plot   Default false, if true, plots V of up to the first two dimensions\n",
    "\n",
    "fignum     Figure number on which to plot\n",
    "\n",
    "clrearfig  If true, the figure is first cleared, otherwise any plot ois overlaid\n",
    "\n",
    "nderivs, difforder     Required for making sure function can create its own arrays and \n",
    "                       still be differentiated\n",
    "\n",
    "dUdt_mag_only  If true, returns |dUdt|^2 from the first timestep only, then stops.\n",
    "\n",
    "** RETURNS:**\n",
    "\n",
    "Uend Vend       nunits-by-1 vectors representing the final values of U and V that were found.\n",
    "U, V            nunits-by-nsteps matrices containing the full trajectories\n",
    "\n",
    "\"\"\"\n",
    "function forwardModel(startU; dt=0.01, tau=0.1, nsteps=100, input=[], noise=[], W=[0 -5;-5 0], \n",
    "    init_add=0, start_add=0, const_add=0, do_plot=false, nderivs=0, difforder=0, clearfig=true, fignum=1,\n",
    "    dUdt_mag_only=false, sigma=0, g_leak=1, U_rest=0, theta=0, beta=1, \n",
    "    warn_if_unused_params=false, other_unused_params...)\n",
    "\n",
    "    if warn_if_unused_params && length(other_unused_params)>0\n",
    "        @printf(\"\\n\\n=== forwardModel warning, had unused params \")\n",
    "        for k in keys(Dict(other_unused_params))\n",
    "            @printf(\"%s, \", k)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    my_input = ForwardDiffZeros(size(input,1), size(input,2), nderivs=nderivs, difforder=difforder)\n",
    "    for i=1:prod(size(input)); my_input[i] = input[i]; end\n",
    "    input = my_input;\n",
    "    \n",
    "    nunits = length(startU)\n",
    "    if size(startU,2) > size(startU,1)\n",
    "        error(\"startU must be a column vector\")\n",
    "    end\n",
    "    \n",
    "    # --- formatting input ---\n",
    "    if ~(typeof(input)<:Array) || prod(size(input))==1  # was a scalar\n",
    "        input = input[1]*(1+ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    elseif length(input)==0 # was the empty matrix\n",
    "        input = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    elseif size(input,2)==1     # was a column vector\n",
    "        input = input*(1+ForwardDiffZeros(1, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    end    \n",
    "    # --- formatting noise ---\n",
    "    if ~(typeof(noise)<:Array) || prod(size(noise))==1  # was a scalar\n",
    "        noise = noise*(1+ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    elseif length(noise)==0 # was the empty matrix\n",
    "        noise = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    elseif size(noise,2)==1     # was a column vector\n",
    "        noise = noise*(1+ForwardDiffZeros(1, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    end    \n",
    "    \n",
    "    U = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    V = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    \n",
    "    if ~(typeof(W)<:Array); W = [W]; end\n",
    "\n",
    "    W     = reshape(W, nunits, nunits)\n",
    "    U     = reshape(U, nunits, nsteps)\n",
    "    V     = reshape(V, nunits, nsteps)\n",
    "    input = reshape(input, nunits, nsteps)\n",
    "    noise = reshape(noise, nunits, nsteps)\n",
    "\n",
    "    input[:,1] += init_add\n",
    "    input      += const_add\n",
    "\n",
    "    #@printf(\"size(U) is (%d,%d), and size(startU) is (%d,%d) and size(noise) is (%d,%d)\", \n",
    "    #    size(U,1), size(U,2), size(startU,1), size(startU,2), size(noise,1), size(noise,2))\n",
    "    # @printf(\"U[1]=%g, noise[1]=%g\\n\", startU, noise[1])\n",
    "    U[:,1] = startU + noise[:,1] + start_add; # @printf(\"Resulting U=%g\\n\", U[1])\n",
    "    V[:,1] = g((U[:,1]-theta)/beta); # @printf(\"Resulting V=%g\\n\", V[1])\n",
    "    \n",
    "    for i=2:nsteps\n",
    "        dUdt = g_leak*(U_rest -U[:,i-1]) + W*V[:,i-1] + input[:,i-1]\n",
    "        if dUdt_mag_only; return sum(dUdt.*dUdt); end;\n",
    "        # @printf(\"dUdt=%g\\n\", dUdt[1])\n",
    "        # @printf(\"i=%g\\n\", i)\n",
    "        # @printf(\"noise[2]=%g\\n\", noise[2])\n",
    "        U[:,i] = U[:,i-1] + (dt/tau)*dUdt + noise[:,i] + sigma*sqrt(dt)*randn(size(U,1),1)\n",
    "        # @printf(\"Resulting U[2]=%g\\n\", U[2])\n",
    "        V[:,i] = g((U[:,i]-theta)/beta)\n",
    "        # @printf(\"Resulting V[2]=%g\\n\", V[2])\n",
    "    end\n",
    "\n",
    "    if do_plot\n",
    "        figure(fignum)\n",
    "        if length(startU)==1\n",
    "            if clearfig; clf(); end;\n",
    "            t = (0:nsteps-1)*dt\n",
    "            plot(t, V[1,:], \"b-\")\n",
    "            plot(t[1], V[1,1], \"g.\")\n",
    "            plot(t[end], V[1,end], \"r.\")\n",
    "            xlabel(\"t\"); ylabel(\"V1\"); ylim([-0.01, 1.01])\n",
    "        elseif length(startU)>=2\n",
    "            if clearfig; clf(); end;\n",
    "            plot(V[1,:], V[2,:], \"b-\")\n",
    "            plot(V[1,1], V[2,1], \"g.\")\n",
    "            plot(V[1,end], V[2,end], \"r.\")\n",
    "            xlabel(\"V1\"); ylabel(\"V2\"); \n",
    "            xlim([-0.01, 1.01]); ylim([-0.01, 1.01])\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return U[:,end], V[:,end], U, V\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "backwardsModel(endU; dt=0.01, tau=0.1, nsteps=100, input=[0],noise=[],  W=[0 -5;-5 0], \n",
    "    do_plot=false, nderivs=0, difforder=0, clearfig=true, fignum=1, tol=1e-15, start_eta=10)\n",
    "\n",
    "Runs a tanh() style-network BACKWARDS in time, given its ending point, by making a backwards\n",
    "guess at each timepoint and then using Hessian minimization to find the backwards vector that correctly\n",
    "leads to the current timestep value.  Uses forwardModel() . The forwards equations are:\n",
    "\n",
    "    tau dU/dt = -U + W*V + I\n",
    "    V = 0.5*tanh(U)+ 0.5\n",
    "\n",
    "**PARAMETERS:**\n",
    "\n",
    "endU     A column vector, nunits-by-1, indicating the values of U at time=end\n",
    "\n",
    "\n",
    "**OPTIONAL PARAMETERS:**\n",
    "\n",
    "dt      Scalar, timestep size\n",
    "\n",
    "tau     Scalar, in seconds\n",
    "\n",
    "nsteps  Number of timesteps to run, including time=0.\n",
    "\n",
    "input   Either an nunits-by-1 vector, in which case inputs to each unit are constant\n",
    "        across time, or a matrix, nunits-by-nsteps, indicating input for each unit at each timepoint.\n",
    "\n",
    "W       Weight matrix, nunits-by-nunits\n",
    "\n",
    "do_plot   Default false, if true, plots V of up to the first two dimensions\n",
    "\n",
    "tol       Tolerance in the minimization procedure for finding each backwards timestep. Passed on\n",
    "          to trust_region_Hessian_minimization()\n",
    "\n",
    "start_eta   Passed on to trust_region_Hessian_minimization()\n",
    "\n",
    "fignum     Figure number on which to plot\n",
    "\n",
    "clrearfig  If true, the figure is first cleared, otherwise any plot ois overlaid\n",
    "\n",
    "nderivs, difforder     Required for making sure function can create its own arrays and \n",
    "                       still be differentiated\n",
    "\n",
    "\n",
    "\n",
    "** RETURNS:**\n",
    "\n",
    "Ustart Vstart   nunits-by-1 vectors representing the starting values of U and V that were found.\n",
    "U, V            nunits-by-nsteps matrices containing the full trajectories\n",
    "costs           1-by-nsteps vector with the final cost from the minimization procedure for each\n",
    "                timestep. This is the squared difference between the U[t+1] produced by the U[t] \n",
    "                guess and the actual U[t+1]\n",
    "\n",
    "\"\"\"\n",
    "function backwardsModel(endU; nsteps=100, start_eta=10, tol=1e-15, maxiter=400, \n",
    "    do_plot=false, init_add=0, start_add=0, dt=0.01, \n",
    "    input=[], noise=[], nderivs=0, difforder=0, clearfig=false, fignum=1, params...)    \n",
    "\n",
    "    nunits = length(endU)\n",
    "\n",
    "    # --- formatting input ---\n",
    "    if ~(typeof(input)<:Array) || prod(size(input))==1  # was a scalar\n",
    "        input = input[1]*(1+ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    elseif length(input)==0 # was the empty matrix\n",
    "        input = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    elseif size(input,2)==1     # was a column vector\n",
    "        input = input*(1+ForwardDiffZeros(1, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    end    \n",
    "    # --- formatting noise ---\n",
    "    if ~(typeof(noise)<:Array)  # was a scalar\n",
    "        noise = noise*(1+ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    elseif length(noise)==0 # was the empty matrix\n",
    "        noise = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    elseif size(noise,2)==1     # was a column vector\n",
    "        noise = noise*(1+ForwardDiffZeros(1, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    end    \n",
    "    \n",
    "    function J(U1, U2; nderivs=0, difforder=0, noise=[], inputs=[], pars...)\n",
    "        U2hat = forwardModel(U1; nsteps=2, noise=noise, input=input, nderivs=nderivs, difforder=difforder, pars...)[1]\n",
    "        U2hat = U2hat\n",
    "        DU = U2hat - U2\n",
    "    \n",
    "        return sum(DU.*DU)\n",
    "    end\n",
    "    \n",
    "    if length(noise)==0\n",
    "        noise = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    end\n",
    "\n",
    "    U = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    U = reshape(U, nunits, nsteps)\n",
    "    costs = ForwardDiffZeros(nsteps, 1, nderivs=nderivs, difforder=difforder)    \n",
    "    \n",
    "    U[:,end] = endU\n",
    "    for i=(nsteps-1):-1:1\n",
    "        if i==1\n",
    "            my_init_add = init_add\n",
    "            my_start_add = start_add\n",
    "        else\n",
    "            my_init_add = 0\n",
    "            my_start_add = 0\n",
    "        end\n",
    "                \n",
    "        U[:,i], costs[i] = trust_region_Hessian_minimization(U[:,i+1], \n",
    "            (x) -> J(x, U[:,i+1]; nderivs=length(endU), difforder=2, \n",
    "            input=input[:,i:i+1], noise = noise[:,i:i+1], \n",
    "            init_add=my_init_add, start_add=my_start_add, params...); \n",
    "            verbose=false, start_eta=start_eta, tol=tol, maxiter=maxiter)\n",
    "        if i>1; U[:,i] += noise[:,i]; end\n",
    "    end\n",
    "    \n",
    "    \n",
    "    V = g(U)\n",
    "    \n",
    "    if do_plot\n",
    "        figure(fignum)   \n",
    "        if typeof(params)<:Array; params = Dict(params); end;\n",
    "        if haskey(params, :dt);     dt     = params[:dt];     end\n",
    "        if haskey(params, :nsteps); nsteps = params[:nsteps]; end\n",
    "        if length(endU)==1\n",
    "            if clearfig; clf(); end;\n",
    "            t = (0:nsteps-1)*dt\n",
    "            plot(t, V[1,:], \"m-\")\n",
    "            plot(t[1], V[1,1], \"go\")\n",
    "            plot(t[end], V[1,end], \"ro\")            \n",
    "            ylim([-0.01, 1.01])\n",
    "        elseif length(endU)>=2\n",
    "            if clearfig; clf(); end;            \n",
    "            plot(V[1,:], V[2,:], \"m-\")\n",
    "            plot(V[1,1], V[2,1], \"go\")\n",
    "            plot(V[1,end], V[2,end], \"ro\")\n",
    "            xlim([-0.01, 1.01]); ylim([-0.01, 1.01])\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return U[:,1], V[:,1], U, V, costs\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing forward and backwards models with only 1 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ustart came back as 1.1\n"
     ]
    }
   ],
   "source": [
    "figure(1); clf();\n",
    "params = Dict(:noise => [0.1], :W => [-2], :nsteps=>10, :start_add=>-1.9)\n",
    "Uend = forwardModel([1.1]; do_plot=true, params...)[1]\n",
    "Ustart = backwardsModel(Uend; do_plot=true, tol=1e-30, params...)[1]\n",
    "@printf(\"Ustart came back as %g\\n\", Ustart[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing forward and backwards models now with 2 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ustart came back as : [0.1, 0.1]\n"
     ]
    }
   ],
   "source": [
    "nsteps=50\n",
    "params = Dict(:noise =>0.03*randn(2,nsteps) + [0.1,0]*ones(1,nsteps), :W => [0 -5; -5 0], :nsteps=>nsteps)\n",
    "\n",
    "Uend, Vend, U, V              = forwardModel([0.1,0.1]; do_plot=true, params...);\n",
    "Ustart, Vstart, bU, bV, costs = backwardsModel(Uend; do_plot=true, tol=1e-30, params...)\n",
    "\n",
    "@printf(\"Ustart came back as : \"); print_vector_g(Ustart); print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring dt-dependence of gradients and hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×5 Array{Float64,2}:\n",
       " 0.92859   -1.0449   1.11301  -1.12663  0.164904\n",
       " 0.918901  -1.01352  1.03901  -1.06345  0.512789"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# srand(111)\n",
    "startU=randn(100,2)-3\n",
    "startU=randn(100,2)-3\n",
    "sigma = 0\n",
    "\n",
    "# startU=0.1*randn(100,2)-3\n",
    "# startU=zeros(100,2)-3\n",
    "\n",
    "\n",
    "dt = 0.02\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "W = -4\n",
    "noise = 0\n",
    "input = 0\n",
    "sigma = 0\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    ":noise=>noise, :input=>input, :sigma=>sigma, :const_add=>0, :init_add=>0)\n",
    "\n",
    "forward = (startU; pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;     \n",
    "    forwardModel(startU; pars...)\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "# clf();\n",
    "# func = (;pars...) -> forward([-0.2, 0.3]; do_plot=true, merge(model_params, Dict(pars))...)\n",
    "# func(;W=-4)\n",
    "\n",
    "args = [\"W\", \"const_add\", [\"start_add\" 2], \"sigma\"]\n",
    "params = [-4.01, 0.5, 0.2, -0.2, 0.01]\n",
    "\n",
    "figure(1); clf();\n",
    "value1, grad1, hess1 = keyword_vgh((;pars...)->forward([-0.2, 0.3]; do_plot=true, merge(model_params, Dict(pars))...)[1][1], args, params)\n",
    "title(@sprintf(\"Running with dt=%g\", dt))\n",
    "\n",
    "dt = 0.005\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    ":noise=>noise, :input=>input, :sigma=>sigma, :const_add=>0, :init_add=>0)\n",
    "\n",
    "figure(2); clf();\n",
    "value2, grad2, hess2 = keyword_vgh((;pars...)->forward([-0.2, 0.3]; do_plot=true, fignum=2, merge(model_params, Dict(pars))...)[1][1], args, params)\n",
    "title(@sprintf(\"Running with dt=%g\", dt))\n",
    "\n",
    "[grad1[:]' ; grad2[:]']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO-DOs\n",
    "\n",
    "1. ~~Be able to use W as an optimizable parameter (including configs like \"all horizontal weights are the same\")~~ DONE!\n",
    "2. ~~Check out what is going on with the weird trajectories in the function-based MGO example~~  DONE: it's just the strong, single-timestep initial_add\n",
    "3. ~~Check out whether reducing beta solves the sticking issue even without extra finalFluxPoint locations~~. It does. Reducing beta from 0.01 to 0.003 was enough.  (We also needed dto change the cost_limit to -0.00288, since the range of costs changes when beta changes.)\n",
    "3. Find the saddle points and use those as the finalFluxPoint locations\n",
    "4. ===\n",
    "5. ~~Run a ProAnti model with noise only in initial conditions, and thus with the framework as we have it~~ (skipped, went straight to next step)\n",
    "6. ~~Make a cost function with frozen noise, and figure out how frozen noise will interact with the backwards trajectory in the minimizations~~\n",
    "7. ~~Make a forwards and backwards model with Urest, etc., just like in ProAnti()~~\n",
    "6. ===\n",
    "7. ~~Make sure that minimization procedures that use tanh() walls report the model parameter, not the control parameter~~ DONE\n",
    "7. ~~Figure out what is going on with the change in gradient and Hessian upon change of dt~~ DONE: it was just the init step\n",
    "7. ~~To really follow bbox_Hessian, printouts needs to be for the walled params, the trajectory should be for the walled params, and the seed should also take the walls into account.~~ DONE\n",
    "7. ~~Have one_d_minimization return the number of iterations it did and why it stopped; then have constrained_parabolic_minimization return the cost, maxiters, and stopping reason; then have bbox_Hessian_minimization return the trajectory of those, as a trace of what was going on.~~ DONE\n",
    "7. POSTPONED: Clean up examples of forward and backwards models and of 1-d use of fluxSense() function\n",
    "8. POSTPONED: Find a 2-d example where flux points are actually needed -- when beta=0, it is not so clear.\n",
    "8. POSTPONED: Measure gradient sensitivity to each of the endpoints in a set of trajectories, as a measure of whether fluxSense is needed or not.\n",
    "12. POSTPONED: Try to combinee fluxSense with bbox_Hessian_minimization9. ~~Fix the walls issue in bbox_Hessian_minimization using tanh encoding.~~\n",
    "\n",
    "\n",
    "1. ===\n",
    "2. ~~Optimize either an MGO or a ProAnti~~ DONE with MGO. Now on to ProAnti\n",
    "3. ~~Set up so we can easily change task period durations in JJ as we run the model to evaluate the results of model-fitting~~ DONE\n",
    "4. Have different task period durations while model-fitting\n",
    "5. Set up to do searches over parameter space\n",
    "6. Incorporate RT into fits?\n",
    "10. If fluxSense is needed in ProAnti, could try choosing the Anti unit endpoint values by maximizing the |dJ/dw|^2 over those values.\n",
    "11. Clean up the notebooks and write up what we've been doing!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting on ProAnti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition plot_PA(Any, Any, Any) in module Main at In[180]:9 overwritten at In[182]:9.\n",
      "WARNING: Method definition #plot_PA(Array{Any, 1}, Main.#plot_PA, Any, Any, Any) in module Main overwritten.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'plot_PA :: Tuple{Any,Any,Any}' in module 'Main'.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "plot_PA"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    plot_PA(t, U, V; fignum=1, clearfig=true, rule_and_delay_period=1, target_period=1, post_target_period=1,\n",
    "        other_unused_params...)\n",
    "\n",
    "Helper function for plotting ProAnti results\n",
    "\"\"\"\n",
    "function plot_PA(t, U, V; fignum=1, clearfig=true, rule_and_delay_period=1, target_period=1, post_target_period=1,\n",
    "    other_unused_params...)\n",
    "    figure(fignum)\n",
    "    if clearfig; clf(); end\n",
    "    \n",
    "    ax1 = subplot(3,1,1)\n",
    "    h = plot(t, V'); \n",
    "    setp(h[1], color=[0, 0, 1])\n",
    "    setp(h[2], color=[1, 0, 0])\n",
    "    setp(h[3], color=[1, 0.5, 0.5])\n",
    "    setp(h[4], color=[0, 1, 1])\n",
    "    ylabel(\"V\")\n",
    "\n",
    "    ax = gca()\n",
    "    yl = [ylim()[1], ylim()[2]]\n",
    "    vlines([rule_and_delay_period, \n",
    "            rule_and_delay_period+target_period,\n",
    "            rule_and_delay_period+target_period+post_target_period], \n",
    "            -0.05, 1.05, linewidth=2)\n",
    "    if yl[1]<0.02\n",
    "        yl[1] = -0.02\n",
    "    end\n",
    "    if yl[2]>0.98\n",
    "        yl[2] = 1.02\n",
    "    end\n",
    "    ylim(yl)\n",
    "    grid(true)\n",
    "    remove_xtick_labels(ax1)\n",
    "        \n",
    "    ax2 = subplot(3,1,2)\n",
    "    hu = plot(t, U')\n",
    "    setp(hu[1], color=[0, 0, 1])\n",
    "    setp(hu[2], color=[1, 0, 0])\n",
    "    setp(hu[3], color=[1, 0.5, 0.5])\n",
    "    setp(hu[4], color=[0, 1, 1])\n",
    "    ylabel(\"U\"); ylim(minimum(U[:])-0.1, maximum(U[:])+0.1)\n",
    "    vlines([rule_and_delay_period, \n",
    "            rule_and_delay_period+target_period,\n",
    "            rule_and_delay_period+target_period+post_target_period], \n",
    "            ylim()[1], ylim()[2], linewidth=2)\n",
    "    remove_xtick_labels(ax2)\n",
    "\n",
    "    grid(true)\n",
    "    \n",
    "    subplot(3,1,3)\n",
    "    delta = V[1,:] - V[4,:]\n",
    "    hr = plot(t, delta)\n",
    "    oldlims = [ylim()[1]+0.1, ylim()[2]-0.1]\n",
    "    ylim(minimum([delta[:];oldlims[1]])-0.1, maximum([delta[:];oldlims[2]])+0.1)\n",
    "    vlines([rule_and_delay_period, \n",
    "            rule_and_delay_period+target_period,\n",
    "            rule_and_delay_period+target_period+post_target_period], \n",
    "            ylim()[1], ylim()[2], linewidth=2)\n",
    "    xlabel(\"t\"); ylabel(\"Pro R - Pro L\")\n",
    "    grid(true)\n",
    "        \n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.366172 seconds (196.73 k allocations: 9.023 MB)\n",
      "Pro % correct = 90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition make_input(Any) in module Main at In[143]:39 overwritten at In[145]:39.\n",
      "WARNING: Method definition #make_input(Array{Any, 1}, Main.#make_input, Any) in module Main overwritten.\n",
      "WARNING: Method definition run_ntrials(Any, Any) in module Main at In[143]:63 overwritten at In[145]:63.\n",
      "WARNING: Method definition #run_ntrials(Array{Any, 1}, Main.#run_ntrials, Any, Any) in module Main overwritten.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anti % correct = 40% \n"
     ]
    }
   ],
   "source": [
    "model_params = Dict(\n",
    ":dt     =>  0.02, \n",
    ":tau    =>  0.1, \n",
    ":vW     =>  -1.7,\n",
    ":hW     =>  -1.7,\n",
    ":sW     =>  0.2,\n",
    ":dW     =>  0,\n",
    ":nsteps =>  2, \n",
    ":noise  =>  [], \n",
    ":sigma  =>  0.08, \n",
    ":input  =>  0, \n",
    ":g_leak =>  0.25, \n",
    ":U_rest =>  -1,\n",
    ":theta  =>  1, \n",
    ":beta   =>  1, \n",
    ":sw     =>  0.2,\n",
    ":hw     =>  -1.7,\n",
    ":vw     =>  -1.7,\n",
    ":constant_excitation      => 0.19, \n",
    ":anti_rule_strength       => 0.1,\n",
    ":pro_rule_strength        => 0.1, \n",
    ":target_period_excitation => 1,\n",
    ":right_light_excitation   => 0.5, \n",
    ":right_light_pro_extra    => 0,\n",
    ":const_add => 0, \n",
    ":init_add  => 0, \n",
    ":rule_and_delay_period    => 0.4,\n",
    ":target_period            => 0.1,\n",
    ":post_target_period       => 0.5,\n",
    ":const_pro_bias           => 0,\n",
    ")\n",
    "\n",
    "\n",
    "function make_input(trial_type; dt=0.02, nderivs=0, difforder=0, constant_excitation=0.19, anti_rule_strength=0.1, \n",
    "    pro_rule_strength=0.1, target_period_excitation=1, right_light_excitation=0.5, right_light_pro_extra=0, \n",
    "    rule_and_delay_period=0.4, target_period=0.1, post_target_period=0.4, const_pro_bias=0,\n",
    "    other_unused_params...)\n",
    "\n",
    "    T = rule_and_delay_period + target_period + post_target_period\n",
    "    t = 0:dt:T\n",
    "    nsteps = length(t)\n",
    "\n",
    "    input = constant_excitation + ForwardDiffZeros(4, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    if trial_type==\"Anti\"\n",
    "        input[2:3, t.<rule_and_delay_period] += anti_rule_strength\n",
    "    elseif trial_type==\"Pro\"\n",
    "        input[[1,4], t.<rule_and_delay_period] += pro_rule_strength\n",
    "    else\n",
    "        error(\"make_input: I don't recognize input type \\\"\" * trial_type * \"\\\"\")\n",
    "    end\n",
    "    \n",
    "    input[:,     (rule_and_delay_period.<=t) & (t.<rule_and_delay_period+target_period)] += target_period_excitation\n",
    "    input[1:2,   (rule_and_delay_period.<=t) & (t.<rule_and_delay_period+target_period)] += right_light_excitation\n",
    "    input[1,     (rule_and_delay_period.<=t) & (t.<rule_and_delay_period+target_period)] += right_light_pro_extra\n",
    "    \n",
    "    input[[1,4],:] += const_pro_bias\n",
    "    \n",
    "    return input, t, nsteps\n",
    "end\n",
    "\n",
    "\n",
    "function run_ntrials(nPro, nAnti; plot_list=[], nderivs=0, difforder=0, model_params...)\n",
    "    pro_input,  t, nsteps = make_input(\"Pro\" ; model_params...)\n",
    "    anti_input, t, nsteps = make_input(\"Anti\"; model_params...)\n",
    "\n",
    "    model_params = Dict(model_params)\n",
    "    sW = model_params[:sW]\n",
    "    hW = model_params[:hW]\n",
    "    vW = model_params[:vW]\n",
    "    dW = model_params[:dW]\n",
    "    model_params = make_dict([\"nsteps\", \"W\"], [nsteps, [sW vW dW hW; vW sW hW dW; dW hW sW vW; hW dW vW sW]], \n",
    "        model_params)\n",
    "    model_params = make_dict([\"nderivs\", \"difforder\"], [nderivs, difforder], model_params)\n",
    "    \n",
    "    proVs  = ForwardDiffZeros(4, nPro, nderivs=nderivs, difforder=difforder)\n",
    "    antiVs = ForwardDiffZeros(4, nAnti, nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    # --- PRO ---\n",
    "    figure(1); clf();\n",
    "    model_params = make_dict([\"input\"], [pro_input], model_params)\n",
    "    for i=1:nPro\n",
    "        startU = [-0.3, -0.7, -0.7, -0.3]\n",
    "        Uend, Vend, U, V = forwardModel(startU, do_plot=false; model_params...)\n",
    "        proVs[:,i] = Vend\n",
    "        if any(plot_list.==i) \n",
    "            plot_PA(t, U, V; fignum=1, clearfig=false, model_params...)\n",
    "            subplot(3,1,1); title(\"PRO\")\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # --- ANTI ---\n",
    "    figure(2); clf();\n",
    "    model_params = make_dict([\"input\"], [anti_input], model_params)\n",
    "    for i=1:nAnti\n",
    "        startU = [-0.7, -0.3, -0.3, -0.7]\n",
    "        Uend, Vend, U, V = forwardModel(startU, do_plot=false; model_params...)\n",
    "        antiVs[:,i] = Vend\n",
    "        if any(plot_list.==i) \n",
    "            plot_PA(t, U, V; fignum=2, clearfig=false, model_params...)\n",
    "            subplot(3,1,1); title(\"ANTI\")\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return proVs, antiVs\n",
    "end\n",
    "\n",
    "nPro = 10; nAnti = 5;\n",
    "proVs, antiVs = @time(run_ntrials(nPro, nAnti; plot_list=[1:5;], model_params...))\n",
    "\n",
    "@printf(\"Pro %% correct = %g%%\\n\", 100*length(find(proVs[1,:].>proVs[4,:]))/ntrials)\n",
    "@printf(\"Anti %% correct = %g%% \\n\", 100*length(find(antiVs[1,:].<antiVs[4,:]))/ntrials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition JJ(Any, Any) in module Main at In[250]:7 overwritten at In[251]:7.\n",
      "WARNING: Method definition #JJ(Array{Any, 1}, Main.#JJ, Any, Any) in module Main overwritten.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     -- cost=0.0145009,   cost1=0.0150714, cost2=-0.000570531\n",
      "     -- mean(hitsP)=0.649618, mean(diffsP)=0.116621 mean(hitsA)=0.625519, mean(diffsA)=0.204888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.014500908183137585"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function JJ(nPro, nAnti; pro_target=0.9, anti_target=0.7, \n",
    "    theta1=0.025, theta2=0.035, cbeta=0.003, verbose=false, \n",
    "    pre_string=\"\", zero_last_sigmas=0, seedrand=NaN, \n",
    "    rule_and_delay_periods = [0.4], target_periods = [0.1], post_target_periods = [0.5],\n",
    "    nderivs=0, difforder=0, model_params...)\n",
    "\n",
    "    nruns = length(rule_and_delay_periods)*length(target_periods)*length(post_target_periods)\n",
    "    \n",
    "    cost1s = ForwardDiffZeros(1, nruns, nderivs=nderivs, difforder=difforder)\n",
    "    cost2s = ForwardDiffZeros(1, nruns, nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if ~isnan(seedrand); srand(seedrand); end\n",
    "    \n",
    "    n = totHitsP = totHitsA = totDiffsP = totDiffsA = 0\n",
    "    for i in rule_and_delay_periods\n",
    "        for j in target_periods\n",
    "            for k = post_target_periods\n",
    "                n += 1\n",
    "                \n",
    "                my_params = make_dict([\"rule_and_delay_period\", \"target_period\", \"post_target_period\"],\n",
    "                [i, j, k], Dict(model_params))\n",
    "    \n",
    "                # print(\"model params is \" ); print(model_params); print(\"\\n\")\n",
    "                proVs, antiVs = run_ntrials(nPro, nAnti; nderivs=nderivs, difforder=difforder, my_params...)\n",
    "\n",
    "                hitsP  = 0.5*(1 + tanh.((proVs[1,:]-proVs[4,:,])/theta1))\n",
    "                diffsP = tanh.((proVs[1,:,]-proVs[4,:])/theta2).^2\n",
    "                hitsA  = 0.5*(1 + tanh.((antiVs[4,:]-antiVs[1,:,])/theta1))\n",
    "                diffsA = tanh.((antiVs[4,:,]-antiVs[1,:])/theta2).^2\n",
    "\n",
    "                if nPro>0 && nAnti>0\n",
    "                    cost1s[n] = (nPro*(mean(hitsP) - pro_target).^2  + nAnti*(mean(hitsA) - anti_target).^2)/(nPro+nAnti)\n",
    "                    cost2s[n] = -cbeta*(nPro*mean(diffsP) + nAnti*mean(diffsA))/(nPro+nAnti)\n",
    "                elseif nPro>0\n",
    "                    cost1s[n] = (mean(hitsP) - pro_target).^2\n",
    "                    cost2s[n] = -cbeta*mean(diffsP)\n",
    "                else\n",
    "                    cost1s[n] = (mean(hitsA) - anti_target).^2\n",
    "                    cost2s[n] = -cbeta*mean(diffsA)\n",
    "                end\n",
    "\n",
    "                totHitsP  += mean(hitsP);  totHitsA  += mean(hitsA); \n",
    "                totDiffsP += mean(diffsP); totDiffsA += mean(diffsA);\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    cost1 = mean(cost1s)\n",
    "    cost2 = mean(cost2s)\n",
    "\n",
    "    hitsP = totHitsP/n; hitsA = totHitsA/n; diffsP = totDiffsP/n; diffsA = totDiffsA/n\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"%s\", pre_string)\n",
    "        @printf(\"     -- cost=%g,   cost1=%g, cost2=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2))\n",
    "        if nPro>0 && nAnti>0\n",
    "            @printf(\"     -- mean(hitsP)=%g, mean(diffsP)=%g mean(hitsA)=%g, mean(diffsA)=%g\\n\", \n",
    "                convert(Float64, mean(hitsP)), convert(Float64, mean(diffsP)),\n",
    "                convert(Float64, mean(hitsA)), convert(Float64, mean(diffsA)))\n",
    "        elseif nPro>0\n",
    "            @printf(\"     -- mean(hitsP)=%g, mean(diffsP)=%g (nAnti=0)\\n\", \n",
    "                convert(Float64, mean(hitsP)), convert(Float64, mean(diffsP)))\n",
    "        else\n",
    "            @printf(\"     -- (nPro=0) mean(hitsA)=%g, mean(diffsA)=%g\\n\", \n",
    "                convert(Float64, mean(hitsA)), convert(Float64, mean(diffsA)))\n",
    "        end        \n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "JJ(2, 10; plot_list=1:5, verbose=true, model_params...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.10535699311851135,[0.02585,-0.0078272,-0.0213399,0.342989,-0.367236,0.0262403,0.650505],\n",
       "[-0.14336 0.0472937 … -0.0471095 -0.260981; 0.0472937 -0.0329522 … 0.0179526 0.0201735; … ; -0.0471095 0.0179526 … -0.00211476 -0.0168496; -0.260981 0.0201735 … -0.0168496 -10.5855])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func = (;params...) -> JJ(100, 0; rule_and_delay_periods = [0.4, 0.8], seedrand=30, cbeta=0.01, \n",
    "plot_list = 1:5, verbose=false, merge(model_params, Dict(params))...)\n",
    "\n",
    "cost, grad, hess = keyword_vgh(func, args, seed)\n",
    "\n",
    "# func(;make_dict(args, seed+ [1,0.2,0,0,0,0,0])...) - func(;make_dict(args, seed)...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: eta=0.01 ps=[0.200, -1.700, -1.700, 0.390, 0.150, 0.100, 0.100]\n",
      "     -- cost=0.0609354,   cost1=0.0702293, cost2=-0.0092939\n",
      "     -- mean(hitsP)=0.574693, mean(diffsP)=0.952638 mean(hitsA)=0.522901, mean(diffsA)=0.906143\n",
      "     -- cost=0.0594435,   cost1=0.0686243, cost2=-0.00918085\n",
      "     -- mean(hitsP)=0.57821, mean(diffsP)=0.941545 mean(hitsA)=0.525837, mean(diffsA)=0.894625\n",
      "1: eta=0.011 cost=0.0594435 jtype=constrained costheta=-0.985 ps=[0.199944, -1.70014, -1.69931, 0.382009, 0.150148, 0.0999991, 0.0947458]\n",
      "     -- cost=0.0586714,   cost1=0.067794, cost2=-0.0091226\n",
      "     -- mean(hitsP)=0.583192, mean(diffsP)=0.934959 mean(hitsA)=0.522194, mean(diffsA)=0.889561\n",
      "2: eta=0.0121 cost=0.0586714 jtype=constrained costheta=-0.428 ps=[0.200019, -1.70054, -1.69829, 0.373021, 0.150443, 0.0999975, 0.0994822]\n",
      "     -- cost=0.05822,   cost1=0.0672907, cost2=-0.00907067\n",
      "     -- mean(hitsP)=0.589098, mean(diffsP)=0.93218 mean(hitsA)=0.515129, mean(diffsA)=0.881954\n",
      "3: eta=0.01331 cost=0.05822 jtype=constrained costheta=-0.922 ps=[0.200186, -1.70123, -1.69672, 0.364247, 0.150863, 0.0999967, 0.105732]\n",
      "     -- cost=0.0577886,   cost1=0.0668117, cost2=-0.00902312\n",
      "     -- mean(hitsP)=0.587356, mean(diffsP)=0.92718 mean(hitsA)=0.521075, mean(diffsA)=0.877444\n",
      "4: eta=0.014641 cost=0.0577886 jtype=constrained costheta=-0.741 ps=[0.200991, -1.70386, -1.69409, 0.364611, 0.15185, 0.0999958, 0.099493]\n",
      "     -- cost=0.0562053,   cost1=0.0649936, cost2=-0.00878835\n",
      "     -- mean(hitsP)=0.591659, mean(diffsP)=0.910092 mean(hitsA)=0.523509, mean(diffsA)=0.847578\n",
      "5: eta=0.0161051 cost=0.0562053 jtype=constrained costheta=-0.808 ps=[0.201054, -1.70485, -1.69264, 0.352479, 0.152291, 0.099995, 0.0939078]\n",
      "     -- cost=0.0542893,   cost1=0.0627915, cost2=-0.0085022\n",
      "     -- mean(hitsP)=0.596231, mean(diffsP)=0.888019 mean(hitsA)=0.527329, mean(diffsA)=0.812421\n",
      "6: eta=0.0177156 cost=0.0542893 jtype=constrained costheta=-0.946 ps=[0.201112, -1.70606, -1.69014, 0.340528, 0.15284, 0.0999991, 0.0860907]\n",
      "     -- cost=0.0514922,   cost1=0.059548, cost2=-0.00805579\n",
      "     -- mean(hitsP)=0.60507, mean(diffsP)=0.84793 mean(hitsA)=0.529794, mean(diffsA)=0.763227\n",
      "7: eta=0.0194872 cost=0.0514922 jtype=constrained costheta=-0.975 ps=[0.20096, -1.70616, -1.68907, 0.325164, 0.152998, 0.0999991, 0.0784282]\n",
      "     -- cost=0.0500142,   cost1=0.0577104, cost2=-0.00769618\n",
      "     -- mean(hitsP)=0.610282, mean(diffsP)=0.816119 mean(hitsA)=0.531386, mean(diffsA)=0.723117\n",
      "8: eta=0.0214359 cost=0.0500142 jtype=constrained costheta=-0.772 ps=[0.20142, -1.70911, -1.68481, 0.314637, 0.154349, 0.100007, 0.0732291]\n",
      "     -- cost=0.0482316,   cost1=0.0551179, cost2=-0.00688629\n",
      "     -- mean(hitsP)=0.615989, mean(diffsP)=0.739217 mean(hitsA)=0.536287, mean(diffsA)=0.638042\n",
      "9: eta=0.0235795 cost=0.0482316 jtype=constrained costheta=-0.995 ps=[0.201365, -1.7099, -1.68284, 0.297695, 0.154873, 0.100009, 0.0621529]\n",
      "     -- cost=0.0471565,   cost1=0.0535924, cost2=-0.00643591\n",
      "     -- mean(hitsP)=0.62549, mean(diffsP)=0.697353 mean(hitsA)=0.529384, mean(diffsA)=0.58983\n",
      "10: eta=0.0259374 cost=0.0471565 jtype=constrained costheta=-0.862 ps=[0.201772, -1.71276, -1.67737, 0.283359, 0.15652, 0.100024, 0.0648333]\n",
      "     -- cost=0.0462898,   cost1=0.0525102, cost2=-0.00622037\n",
      "     -- mean(hitsP)=0.633864, mean(diffsP)=0.678969 mean(hitsA)=0.522557, mean(diffsA)=0.565105\n",
      "11: eta=0.0285312 cost=0.0462898 jtype=constrained costheta=-0.946 ps=[0.202394, -1.71672, -1.67017, 0.274286, 0.158693, 0.100046, 0.0689509]\n",
      "     -- cost=0.0454422,   cost1=0.0516201, cost2=-0.00617788\n",
      "     -- mean(hitsP)=0.638691, mean(diffsP)=0.676141 mean(hitsA)=0.520496, mean(diffsA)=0.559435\n",
      "12: eta=0.0313843 cost=0.0454422 jtype=constrained costheta=-0.980 ps=[0.203196, -1.72159, -1.66152, 0.271739, 0.161314, 0.100074, 0.0704131]\n",
      "     -- cost=0.0445102,   cost1=0.0506894, cost2=-0.00617924\n",
      "     -- mean(hitsP)=0.642204, mean(diffsP)=0.676909 mean(hitsA)=0.520735, mean(diffsA)=0.55894\n",
      "13: eta=0.0345227 cost=0.0445102 jtype=constrained costheta=-0.997 ps=[0.204112, -1.72699, -1.65186, 0.271489, 0.164277, 0.100106, 0.070395]\n",
      "     -- cost=0.0434651,   cost1=0.0496582, cost2=-0.00619306\n",
      "     -- mean(hitsP)=0.645484, mean(diffsP)=0.678678 mean(hitsA)=0.521964, mean(diffsA)=0.559934\n",
      "14: eta=0.037975 cost=0.0434651 jtype=constrained costheta=-1.000 ps=[0.205136, -1.7329, -1.64111, 0.272033, 0.167618, 0.10014, 0.0697537]\n",
      "     -- cost=0.0422893,   cost1=0.0485025, cost2=-0.00621321\n",
      "     -- mean(hitsP)=0.648952, mean(diffsP)=0.681036 mean(hitsA)=0.523723, mean(diffsA)=0.561607\n",
      "15: eta=0.0417725 cost=0.0422893 jtype=constrained costheta=-1.000 ps=[0.206279, -1.73932, -1.62913, 0.272926, 0.171395, 0.100177, 0.0688352]\n",
      "     -- cost=0.0409639,   cost1=0.0472024, cost2=-0.00623856\n",
      "     -- mean(hitsP)=0.652786, mean(diffsP)=0.683942 mean(hitsA)=0.5259, mean(diffsA)=0.56377\n",
      "16: eta=0.0459497 cost=0.0409639 jtype=constrained costheta=-1.000 ps=[0.207557, -1.7463, -1.61576, 0.274038, 0.175678, 0.100218, 0.067749]\n",
      "     -- cost=0.0394676,   cost1=0.045737, cost2=-0.00626941\n",
      "     -- mean(hitsP)=0.657084, mean(diffsP)=0.68746 mean(hitsA)=0.528521, mean(diffsA)=0.566422\n",
      "17: eta=0.0505447 cost=0.0394676 jtype=constrained costheta=-1.000 ps=[0.208988, -1.75387, -1.60082, 0.275353, 0.180547, 0.100263, 0.0665171]\n",
      "     -- cost=0.0377759,   cost1=0.0440826, cost2=-0.00630665\n",
      "     -- mean(hitsP)=0.661927, mean(diffsP)=0.691701 mean(hitsA)=0.531674, mean(diffsA)=0.569629\n",
      "18: eta=0.0555992 cost=0.0377759 jtype=constrained costheta=-1.000 ps=[0.210596, -1.76207, -1.58408, 0.276889, 0.1861, 0.100311, 0.065131]\n",
      "     -- cost=0.0358612,   cost1=0.0422127, cost2=-0.00635149\n",
      "     -- mean(hitsP)=0.667393, mean(diffsP)=0.696807 mean(hitsA)=0.535487, mean(diffsA)=0.573492\n",
      "19: eta=0.0611591 cost=0.0358612 jtype=constrained costheta=-1.000 ps=[0.212406, -1.77094, -1.56529, 0.27868, 0.192451, 0.100364, 0.0635724]\n",
      "     -- cost=0.0336923,   cost1=0.0400978, cost2=-0.00640551\n",
      "     -- mean(hitsP)=0.673578, mean(diffsP)=0.702959 mean(hitsA)=0.54013, mean(diffsA)=0.578142\n",
      "20: eta=0.067275 cost=0.0336923 jtype=constrained costheta=-1.000 ps=[0.214453, -1.78053, -1.54416, 0.280771, 0.199741, 0.100421, 0.0618224]\n",
      "     -- cost=0.0312357,   cost1=0.0377066, cost2=-0.00647087\n",
      "     -- mean(hitsP)=0.6806, mean(diffsP)=0.710404 mean(hitsA)=0.545803, mean(diffsA)=0.58377\n",
      "21: eta=0.0740025 cost=0.0312357 jtype=constrained costheta=-1.000 ps=[0.216774, -1.79087, -1.52031, 0.283206, 0.208136, 0.100482, 0.0598721]\n",
      "     -- cost=0.0284575,   cost1=0.0350083, cost2=-0.00655072\n",
      "     -- mean(hitsP)=0.68863, mean(diffsP)=0.719491 mean(hitsA)=0.552727, mean(diffsA)=0.590653\n",
      "22: eta=0.0814027 cost=0.0284575 jtype=constrained costheta=-1.000 ps=[0.219412, -1.802, -1.49332, 0.286027, 0.217838, 0.100548, 0.057743]\n",
      "     -- cost=0.0253283,   cost1=0.0319778, cost2=-0.00664945\n",
      "     -- mean(hitsP)=0.697923, mean(diffsP)=0.730709 mean(hitsA)=0.561099, mean(diffsA)=0.59918\n",
      "23: eta=0.089543 cost=0.0253283 jtype=constrained costheta=-1.000 ps=[0.222411, -1.81396, -1.46264, 0.289263, 0.229092, 0.100619, 0.055509]\n",
      "     -- cost=0.0218332,   cost1=0.0286046, cost2=-0.00677145\n",
      "     -- mean(hitsP)=0.708818, mean(diffsP)=0.744582 mean(hitsA)=0.571108, mean(diffsA)=0.609709\n",
      "24: eta=0.0984973 cost=0.0218332 jtype=constrained costheta=-1.000 ps=[0.225807, -1.82677, -1.42756, 0.29291, 0.242189, 0.100695, 0.0532735]\n",
      "     -- cost=0.0179877,   cost1=0.0249047, cost2=-0.00691704\n",
      "     -- mean(hitsP)=0.721609, mean(diffsP)=0.761285 mean(hitsA)=0.583118, mean(diffsA)=0.622124\n",
      "25: eta=0.108347 cost=0.0179877 jtype=constrained costheta=-1.000 ps=[0.229621, -1.84045, -1.38719, 0.296927, 0.257476, 0.100776, 0.0510433]\n",
      "     -- cost=0.0138569,   cost1=0.0209352, cost2=-0.00707832\n",
      "     -- mean(hitsP)=0.736401, mean(diffsP)=0.780279 mean(hitsA)=0.597919, mean(diffsA)=0.635385\n",
      "26: eta=0.119182 cost=0.0138569 jtype=constrained costheta=-1.000 ps=[0.233851, -1.85492, -1.34044, 0.301181, 0.275375, 0.100861, 0.0486268]\n",
      "     -- cost=0.00958037,   cost1=0.0168337, cost2=-0.00725335\n",
      "     -- mean(hitsP)=0.753209, mean(diffsP)=0.801542 mean(hitsA)=0.616505, mean(diffsA)=0.649128\n",
      "27: eta=0.1311 cost=0.00958037 jtype=constrained costheta=-1.000 ps=[0.238463, -1.87006, -1.28604, 0.305622, 0.296414, 0.100948, 0.0459271]\n",
      "     -- cost=0.00539569,   cost1=0.0128508, cost2=-0.00745513\n",
      "     -- mean(hitsP)=0.77264, mean(diffsP)=0.826271 mean(hitsA)=0.638777, mean(diffsA)=0.664755\n",
      "28: eta=0.14421 cost=0.00539569 jtype=constrained costheta=-0.999 ps=[0.243295, -1.88565, -1.22274, 0.310105, 0.321316, 0.101034, 0.043575]\n",
      "     -- cost=0.00150182,   cost1=0.00912849, cost2=-0.00762667\n",
      "     -- mean(hitsP)=0.798501, mean(diffsP)=0.85075 mean(hitsA)=0.658867, mean(diffsA)=0.674584\n",
      "29: eta=0.158631 cost=0.00150182 jtype=constrained costheta=-0.968 ps=[0.247664, -1.90177, -1.14772, 0.310831, 0.35067, 0.10112, 0.0441158]\n",
      "     -- cost=-0.00200285,   cost1=0.00522869, cost2=-0.00723154\n",
      "     -- mean(hitsP)=0.840649, mean(diffsP)=0.836904 mean(hitsA)=0.666135, mean(diffsA)=0.609404\n",
      "30: eta=0.174494 cost=-0.00200285 jtype=constrained costheta=-0.847 ps=[0.251135, -1.91867, -1.06159, 0.289926, 0.38545, 0.101249, 0.0476522]\n",
      "     -- cost=-0.00464681,   cost1=0.00321302, cost2=-0.00785983\n",
      "     -- mean(hitsP)=0.872273, mean(diffsP)=0.891122 mean(hitsA)=0.691104, mean(diffsA)=0.680843\n",
      "31: eta=0.191943 cost=-0.00464681 jtype=constrained costheta=-0.723 ps=[0.251201, -1.93568, -0.933554, 0.304367, 0.415443, 0.101362, 0.0543018]\n",
      "     -- cost=-0.00662375,   cost1=0.00203528, cost2=-0.00865903\n",
      "     -- mean(hitsP)=0.913901, mean(diffsP)=0.94802 mean(hitsA)=0.687489, mean(diffsA)=0.783787\n",
      "32: eta=0.211138 cost=-0.00662375 jtype=constrained costheta=-0.434 ps=[0.248672, -1.95381, -0.780385, 0.328251, 0.444378, 0.101443, 0.0749949]\n",
      "     -- cost=-0.00741861,   cost1=0.00158745, cost2=-0.00900605\n",
      "     -- mean(hitsP)=0.925545, mean(diffsP)=0.957664 mean(hitsA)=0.695482, mean(diffsA)=0.843547\n",
      "33: eta=0.232252 cost=-0.00741861 jtype=constrained costheta=-0.252 ps=[0.241041, -1.95471, -0.606836, 0.360614, 0.411492, 0.101328, 0.0864238]\n",
      "     -- cost=-0.00793573,   cost1=0.00113705, cost2=-0.00907278\n",
      "     -- mean(hitsP)=0.925389, mean(diffsP)=0.950064 mean(hitsA)=0.695924, mean(diffsA)=0.864492\n",
      "34: eta=0.255477 cost=-0.00793573 jtype=constrained costheta=-0.344 ps=[0.22985, -1.93176, -0.481803, 0.387116, 0.355151, 0.101254, 0.0913501]\n",
      "     -- cost=-0.00822581,   cost1=0.000855663, cost2=-0.00908148\n",
      "     -- mean(hitsP)=0.918392, mean(diffsP)=0.932211 mean(hitsA)=0.695062, mean(diffsA)=0.884084\n",
      "35: eta=0.281024 cost=-0.00822581 jtype=constrained costheta=-0.233 ps=[0.233444, -1.81757, -0.38867, 0.42007, 0.317438, 0.101183, 0.0935607]\n",
      "     -- cost=-0.00835962,   cost1=0.000686424, cost2=-0.00904605\n",
      "     -- mean(hitsP)=0.906756, mean(diffsP)=0.914815 mean(hitsA)=0.691834, mean(diffsA)=0.894394\n",
      "36: eta=0.309127 cost=-0.00835962 jtype=constrained costheta=-0.137 ps=[0.197063, -1.65883, -0.369865, 0.451357, 0.307688, 0.10116, 0.0880435]\n",
      "     -- cost=0.0193773,   cost1=0.0226079, cost2=-0.00323062\n",
      "     -- mean(hitsP)=0.75877, mean(diffsP)=0.33568 mean(hitsA)=0.624182, mean(diffsA)=0.310443\n",
      "eta going down: new_cost-cost=0.0277369 and jumptype='constrained'\n",
      "37: eta=0.154563 cost=-0.00835962 jtype=constrained costheta=NaN ps=[0.197063, -1.65883, -0.369865, 0.451357, 0.307688, 0.10116, 0.0880435]\n",
      "     -- cost=-0.00825202,   cost1=0.00091341, cost2=-0.00916543\n",
      "     -- mean(hitsP)=0.913658, mean(diffsP)=0.938501 mean(hitsA)=0.696693, mean(diffsA)=0.894585\n",
      "eta going down: new_cost-cost=0.000107605 and jumptype='constrained'\n",
      "38: eta=0.0772817 cost=-0.00835962 jtype=constrained costheta=NaN ps=[0.197063, -1.65883, -0.369865, 0.451357, 0.307688, 0.10116, 0.0880435]\n",
      "     -- cost=-0.00833144,   cost1=0.000768549, cost2=-0.00909999\n",
      "     -- mean(hitsP)=0.912202, mean(diffsP)=0.925342 mean(hitsA)=0.694187, mean(diffsA)=0.894655\n",
      "eta going down: new_cost-cost=2.81866e-05 and jumptype='constrained'\n",
      "39: eta=0.0386409 cost=-0.00835962 jtype=constrained costheta=NaN ps=[0.197063, -1.65883, -0.369865, 0.451357, 0.307688, 0.10116, 0.0880435]\n",
      "     -- cost=-0.00835418,   cost1=0.000710358, cost2=-0.00906454\n",
      "     -- mean(hitsP)=0.912379, mean(diffsP)=0.918457 mean(hitsA)=0.693433, mean(diffsA)=0.894451\n",
      "eta going down: new_cost-cost=5.44301e-06 and jumptype='constrained'\n",
      "40: eta=0.0193204 cost=-0.00835962 jtype=constrained costheta=NaN ps=[0.197063, -1.65883, -0.369865, 0.451357, 0.307688, 0.10116, 0.0880435]\n",
      "     -- cost=-0.00838045,   cost1=0.000645199, cost2=-0.00902564\n",
      "     -- mean(hitsP)=0.91108, mean(diffsP)=0.911341 mean(hitsA)=0.69247, mean(diffsA)=0.893788\n",
      "41: eta=0.0212525 cost=-0.00838045 jtype=constrained costheta=-0.583 ps=[0.195759, -1.6511, -0.355765, 0.454499, 0.307143, 0.101154, 0.0889379]\n",
      "     -- cost=-0.00838941,   cost1=0.000631523, cost2=-0.00902094\n",
      "     -- mean(hitsP)=0.910739, mean(diffsP)=0.909915 mean(hitsA)=0.692351, mean(diffsA)=0.894272\n",
      "42: eta=0.0233777 cost=-0.00838941 jtype=constrained costheta=-0.987 ps=[0.194237, -1.63729, -0.352334, 0.456777, 0.306618, 0.101143, 0.0886839]\n",
      "     -- cost=-0.00839885,   cost1=0.00062055, cost2=-0.00901939\n",
      "     -- mean(hitsP)=0.910367, mean(diffsP)=0.908862 mean(hitsA)=0.692294, mean(diffsA)=0.895017\n",
      "43: eta=0.0257155 cost=-0.00839885 jtype=constrained costheta=-0.990 ps=[0.192686, -1.62165, -0.349784, 0.459165, 0.306653, 0.10113, 0.0882969]\n",
      "     -- cost=-0.00840898,   cost1=0.000611572, cost2=-0.00902055\n",
      "     -- mean(hitsP)=0.910039, mean(diffsP)=0.908088 mean(hitsA)=0.692292, mean(diffsA)=0.896022\n",
      "44: eta=0.028287 cost=-0.00840898 jtype=constrained costheta=-0.990 ps=[0.191113, -1.60422, -0.347707, 0.46178, 0.307173, 0.101116, 0.0878144]\n",
      "     -- cost=-0.00841998,   cost1=0.000603798, cost2=-0.00902378\n",
      "     -- mean(hitsP)=0.909742, mean(diffsP)=0.907505 mean(hitsA)=0.692331, mean(diffsA)=0.89725\n",
      "45: eta=0.0311157 cost=-0.00841998 jtype=constrained costheta=-0.987 ps=[0.189512, -1.58487, -0.345868, 0.464666, 0.308094, 0.101102, 0.0872482]\n",
      "     -- cost=-0.00843196,   cost1=0.000596573, cost2=-0.00902854\n",
      "     -- mean(hitsP)=0.909453, mean(diffsP)=0.907033 mean(hitsA)=0.692395, mean(diffsA)=0.898674\n",
      "46: eta=0.0342273 cost=-0.00843196 jtype=constrained costheta=-0.982 ps=[0.187876, -1.5634, -0.344085, 0.467869, 0.309361, 0.101088, 0.0866021]\n",
      "     -- cost=-0.00844503,   cost1=0.000589391, cost2=-0.00903442\n",
      "     -- mean(hitsP)=0.909155, mean(diffsP)=0.906608 mean(hitsA)=0.692476, mean(diffsA)=0.900277\n",
      "47: eta=0.03765 cost=-0.00844503 jtype=constrained costheta=-0.973 ps=[0.186196, -1.53956, -0.342218, 0.471446, 0.310956, 0.101074, 0.0858743]\n",
      "     -- cost=-0.00845927,   cost1=0.000581859, cost2=-0.00904113\n",
      "     -- mean(hitsP)=0.908826, mean(diffsP)=0.906178 mean(hitsA)=0.692567, mean(diffsA)=0.902049\n",
      "48: eta=0.041415 cost=-0.00845927 jtype=constrained costheta=-0.960 ps=[0.184462, -1.51307, -0.34016, 0.475466, 0.312891, 0.101062, 0.0850583]\n",
      "     -- cost=-0.00847477,   cost1=0.000573641, cost2=-0.00904841\n",
      "     -- mean(hitsP)=0.90845, mean(diffsP)=0.905698 mean(hitsA)=0.692664, mean(diffsA)=0.903983\n",
      "49: eta=0.0455566 cost=-0.00847477 jtype=constrained costheta=-0.939 ps=[0.182659, -1.48364, -0.337829, 0.480012, 0.315209, 0.101052, 0.0841431]\n",
      "     -- cost=-0.0084916,   cost1=0.000564426, cost2=-0.00905602\n",
      "     -- mean(hitsP)=0.908005, mean(diffsP)=0.905128 mean(hitsA)=0.692764, mean(diffsA)=0.906076\n",
      "50: eta=0.0501122 cost=-0.0084916 jtype=constrained costheta=-0.908 ps=[0.180762, -1.45096, -0.335158, 0.485193, 0.317991, 0.101046, 0.0831128]\n",
      "     -- cost=-0.00850982,   cost1=0.000553917, cost2=-0.00906374\n",
      "     -- mean(hitsP)=0.907464, mean(diffsP)=0.904428 mean(hitsA)=0.692866, mean(diffsA)=0.90832\n",
      "51: eta=0.0551234 cost=-0.00850982 jtype=constrained costheta=-0.862 ps=[0.178738, -1.41472, -0.332098, 0.49114, 0.321362, 0.101045, 0.0819464]\n",
      "     -- cost=-0.00852947,   cost1=0.000541943, cost2=-0.00907141\n",
      "     -- mean(hitsP)=0.906799, mean(diffsP)=0.903578 mean(hitsA)=0.692972, mean(diffsA)=0.910704\n",
      "52: eta=0.0606358 cost=-0.00852947 jtype=constrained costheta=-0.800 ps=[0.176549, -1.37465, -0.328629, 0.498014, 0.325523, 0.101052, 0.0806182]\n",
      "     -- cost=-0.00855042,   cost1=0.000528841, cost2=-0.00907926\n",
      "     -- mean(hitsP)=0.905992, mean(diffsP)=0.902631 mean(hitsA)=0.693089, mean(diffsA)=0.913221\n",
      "53: eta=0.0666993 cost=-0.00855042 jtype=constrained costheta=-0.723 ps=[0.17419, -1.33055, -0.324795, 0.505982, 0.330808, 0.10107, 0.0791013]\n",
      "     -- cost=-0.00857216,   cost1=0.000516212, cost2=-0.00908837\n",
      "     -- mean(hitsP)=0.905073, mean(diffsP)=0.901801 mean(hitsA)=0.693228, mean(diffsA)=0.915874\n",
      "54: eta=0.0733693 cost=-0.00857216 jtype=constrained costheta=-0.637 ps=[0.171782, -1.2824, -0.320753, 0.515158, 0.337771, 0.101106, 0.0773806]\n",
      "     -- cost=-0.00859335,   cost1=0.000507433, cost2=-0.00910078\n",
      "     -- mean(hitsP)=0.90417, mean(diffsP)=0.901488 mean(hitsA)=0.693405, mean(diffsA)=0.918668\n",
      "55: eta=0.0807062 cost=-0.00859335 jtype=constrained costheta=-0.544 ps=[0.169698, -1.23051, -0.316732, 0.525511, 0.347203, 0.101173, 0.0754728]\n",
      "     -- cost=-0.00861135,   cost1=0.000506768, cost2=-0.00911811\n",
      "     -- mean(hitsP)=0.90345, mean(diffsP)=0.902045 mean(hitsA)=0.69363, mean(diffsA)=0.921577\n",
      "56: eta=0.0887768 cost=-0.00861135 jtype=constrained costheta=-0.419 ps=[0.168695, -1.1755, -0.312838, 0.536837, 0.359947, 0.101323, 0.0734326]\n",
      "     -- cost=-0.00862227,   cost1=0.000517539, cost2=-0.00913981\n",
      "     -- mean(hitsP)=0.903075, mean(diffsP)=0.903532 mean(hitsA)=0.693908, mean(diffsA)=0.924431\n",
      "57: eta=0.0976545 cost=-0.00862227 jtype=constrained costheta=-0.241 ps=[0.171065, -1.11862, -0.308772, 0.54802, 0.37604, 0.102049, 0.0714139]\n",
      "     -- cost=-0.00862543,   cost1=0.000526402, cost2=-0.00915183\n",
      "     -- mean(hitsP)=0.903728, mean(diffsP)=0.904751 mean(hitsA)=0.694461, mean(diffsA)=0.925615\n",
      "58: eta=0.10742 cost=-0.00862543 jtype=constrained costheta=-0.043 ps=[0.174508, -1.09753, -0.307258, 0.551589, 0.384293, 0.106895, 0.0706154]\n",
      "     -- cost=-0.00862698,   cost1=0.000528575, cost2=-0.00915555\n",
      "     -- mean(hitsP)=0.903749, mean(diffsP)=0.905152 mean(hitsA)=0.694587, mean(diffsA)=0.925959\n",
      "59: eta=0.118162 cost=-0.00862698 jtype=constrained costheta=-0.055 ps=[0.174406, -1.09239, -0.307263, 0.55279, 0.387637, 0.113284, 0.0702709]\n",
      "     -- cost=-0.00862878,   cost1=0.000528586, cost2=-0.00915737\n",
      "     -- mean(hitsP)=0.903714, mean(diffsP)=0.905408 mean(hitsA)=0.694616, mean(diffsA)=0.926066\n",
      "60: eta=0.129978 cost=-0.00862878 jtype=constrained costheta=-0.124 ps=[0.174252, -1.09083, -0.307453, 0.552895, 0.389596, 0.12117, 0.0700842]\n",
      "     -- cost=-0.00863101,   cost1=0.000528263, cost2=-0.00915927\n",
      "     -- mean(hitsP)=0.903652, mean(diffsP)=0.905697 mean(hitsA)=0.694631, mean(diffsA)=0.926158\n",
      "61: eta=0.142976 cost=-0.00863101 jtype=constrained costheta=-0.130 ps=[0.17408, -1.08944, -0.307711, 0.552854, 0.391786, 0.130961, 0.0698807]\n",
      "     -- cost=-0.0086338,   cost1=0.000527867, cost2=-0.00916167\n",
      "     -- mean(hitsP)=0.903567, mean(diffsP)=0.906071 mean(hitsA)=0.694643, mean(diffsA)=0.926262\n",
      "62: eta=0.157274 cost=-0.0086338 jtype=constrained costheta=-0.108 ps=[0.173862, -1.08779, -0.308058, 0.552792, 0.394583, 0.143249, 0.0696253]\n",
      "     -- cost=-0.00863732,   cost1=0.000527462, cost2=-0.00916479\n",
      "     -- mean(hitsP)=0.903446, mean(diffsP)=0.906571 mean(hitsA)=0.69465, mean(diffsA)=0.926386\n",
      "63: eta=0.173001 cost=-0.00863732 jtype=constrained costheta=-0.088 ps=[0.173565, -1.08576, -0.308537, 0.552734, 0.398277, 0.158857, 0.0692945]\n",
      "     -- cost=-0.00864184,   cost1=0.000527164, cost2=-0.009169\n",
      "     -- mean(hitsP)=0.903267, mean(diffsP)=0.907267 mean(hitsA)=0.694647, mean(diffsA)=0.926534\n",
      "64: eta=0.190301 cost=-0.00864184 jtype=constrained costheta=-0.071 ps=[0.173136, -1.08321, -0.309232, 0.552708, 0.403296, 0.178926, 0.0688555]\n",
      "     -- cost=-0.00864772,   cost1=0.000527292, cost2=-0.00917501\n",
      "     -- mean(hitsP)=0.902984, mean(diffsP)=0.908289 mean(hitsA)=0.694625, mean(diffsA)=0.926713\n",
      "65: eta=0.209331 cost=-0.00864772 jtype=constrained costheta=-0.056 ps=[0.172469, -1.08002, -0.310301, 0.552775, 0.410364, 0.205057, 0.0682554]\n",
      "     -- cost=-0.00865553,   cost1=0.000528816, cost2=-0.00918435\n",
      "     -- mean(hitsP)=0.9025, mean(diffsP)=0.909929 mean(hitsA)=0.694573, mean(diffsA)=0.926941\n",
      "66: eta=0.230264 cost=-0.00865553 jtype=constrained costheta=-0.045 ps=[0.171335, -1.07599, -0.312094, 0.553105, 0.420838, 0.239459, 0.0673977]\n",
      "     -- cost=-0.00866636,   cost1=0.000534583, cost2=-0.00920095\n",
      "     -- mean(hitsP)=0.901594, mean(diffsP)=0.912926 mean(hitsA)=0.694487, mean(diffsA)=0.927264\n",
      "67: eta=0.253291 cost=-0.00866636 jtype=constrained costheta=-0.035 ps=[0.169224, -1.07081, -0.315461, 0.554113, 0.437547, 0.285021, 0.0660907]\n",
      "     -- cost=-0.00865387,   cost1=0.00053053, cost2=-0.0091844\n",
      "     -- mean(hitsP)=0.902002, mean(diffsP)=0.909578 mean(hitsA)=0.694269, mean(diffsA)=0.927302\n",
      "eta going down: new_cost-cost=1.24895e-05 and jumptype='constrained'\n",
      "68: eta=0.126645 cost=-0.00866636 jtype=constrained costheta=NaN ps=[0.169224, -1.07081, -0.315461, 0.554113, 0.437547, 0.285021, 0.0660907]\n",
      "     -- cost=-0.00867462,   cost1=0.000541705, cost2=-0.00921632\n",
      "     -- mean(hitsP)=0.901543, mean(diffsP)=0.915945 mean(hitsA)=0.694962, mean(diffsA)=0.92732\n",
      "69: eta=0.13931 cost=-0.00867462 jtype=constrained costheta=-0.031 ps=[0.1677, -1.06714, -0.318452, 0.555081, 0.452073, 0.31329, 0.0650384]\n",
      "     -- cost=-0.00868438,   cost1=0.000540099, cost2=-0.00922448\n",
      "     -- mean(hitsP)=0.901077, mean(diffsP)=0.9177 mean(hitsA)=0.694781, mean(diffsA)=0.927195\n",
      "70: eta=0.153241 cost=-0.00868438 jtype=constrained costheta=-0.092 ps=[0.166237, -1.06203, -0.320171, 0.556495, 0.468491, 0.348049, 0.063935]\n",
      "     -- cost=-0.0082391,   cost1=0.000637738, cost2=-0.00887684\n",
      "     -- mean(hitsP)=0.889565, mean(diffsP)=0.871375 mean(hitsA)=0.678131, mean(diffsA)=0.903993\n",
      "eta going down: new_cost-cost=0.000445277 and jumptype='constrained'\n",
      "71: eta=0.0766204 cost=-0.00868438 jtype=constrained costheta=NaN ps=[0.166237, -1.06203, -0.320171, 0.556495, 0.468491, 0.348049, 0.063935]\n",
      "     -- cost=-0.00859043,   cost1=0.000463853, cost2=-0.00905428\n",
      "     -- mean(hitsP)=0.898284, mean(diffsP)=0.896115 mean(hitsA)=0.688542, mean(diffsA)=0.914741\n",
      "eta going down: new_cost-cost=9.39494e-05 and jumptype='constrained'\n",
      "72: eta=0.0383102 cost=-0.00868438 jtype=constrained costheta=NaN ps=[0.166237, -1.06203, -0.320171, 0.556495, 0.468491, 0.348049, 0.063935]\n",
      "     -- cost=-0.00865644,   cost1=0.0006144, cost2=-0.00927084\n",
      "     -- mean(hitsP)=0.902911, mean(diffsP)=0.925794 mean(hitsA)=0.697306, mean(diffsA)=0.928374\n",
      "eta going down: new_cost-cost=2.7933e-05 and jumptype='constrained'\n",
      "73: eta=0.0191551 cost=-0.00868438 jtype=constrained costheta=NaN ps=[0.166237, -1.06203, -0.320171, 0.556495, 0.468491, 0.348049, 0.063935]\n",
      "     -- cost=-0.00868613,   cost1=0.000538523, cost2=-0.00922465\n",
      "     -- mean(hitsP)=0.901576, mean(diffsP)=0.917957 mean(hitsA)=0.695103, mean(diffsA)=0.926973\n",
      "74: eta=0.0210706 cost=-0.00868613 jtype=constrained costheta=-0.073 ps=[0.166135, -1.06191, -0.320224, 0.556455, 0.471474, 0.352959, 0.0637711]\n",
      "     -- cost=-0.00868779,   cost1=0.000538162, cost2=-0.00922595\n",
      "     -- mean(hitsP)=0.901538, mean(diffsP)=0.91826 mean(hitsA)=0.695091, mean(diffsA)=0.92693\n",
      "75: eta=0.0231777 cost=-0.00868779 jtype=constrained costheta=-0.943 ps=[0.165955, -1.06096, -0.320486, 0.556726, 0.474525, 0.358514, 0.0635775]\n",
      "     -- cost=-0.00868966,   cost1=0.000537522, cost2=-0.00922718\n",
      "     -- mean(hitsP)=0.901503, mean(diffsP)=0.918572 mean(hitsA)=0.695074, mean(diffsA)=0.926864\n",
      "76: eta=0.0254954 cost=-0.00868966 jtype=constrained costheta=-0.888 ps=[0.165751, -1.06008, -0.320766, 0.556948, 0.477823, 0.36476, 0.0633738]\n",
      "     -- cost=-0.00869176,   cost1=0.000536641, cost2=-0.0092284\n",
      "     -- mean(hitsP)=0.901468, mean(diffsP)=0.918903 mean(hitsA)=0.695052, mean(diffsA)=0.926778\n",
      "77: eta=0.028045 cost=-0.00869176 jtype=constrained costheta=-0.851 ps=[0.165528, -1.05927, -0.32108, 0.557136, 0.481432, 0.371779, 0.0631556]\n",
      "     -- cost=-0.00869413,   cost1=0.000535497, cost2=-0.00922963\n",
      "     -- mean(hitsP)=0.901433, mean(diffsP)=0.919257 mean(hitsA)=0.695022, mean(diffsA)=0.926668\n",
      "78: eta=0.0308495 cost=-0.00869413 jtype=constrained costheta=-0.803 ps=[0.165287, -1.05854, -0.321436, 0.557292, 0.485423, 0.379671, 0.0629198]\n",
      "     -- cost=-0.00869681,   cost1=0.000534059, cost2=-0.00923087\n",
      "     -- mean(hitsP)=0.901398, mean(diffsP)=0.919641 mean(hitsA)=0.694985, mean(diffsA)=0.926533\n",
      "79: eta=0.0339344 cost=-0.00869681 jtype=constrained costheta=-0.744 ps=[0.165028, -1.05787, -0.321837, 0.557413, 0.489875, 0.388548, 0.0626628]\n",
      "     -- cost=-0.00869984,   cost1=0.000532287, cost2=-0.00923213\n",
      "     -- mean(hitsP)=0.901364, mean(diffsP)=0.92006 mean(hitsA)=0.694939, mean(diffsA)=0.926366\n",
      "80: eta=0.0373279 cost=-0.00869984 jtype=constrained costheta=-0.675 ps=[0.164752, -1.05727, -0.322293, 0.557496, 0.494886, 0.398544, 0.0623806]\n",
      "     -- cost=-0.00870327,   cost1=0.000530136, cost2=-0.00923341\n",
      "     -- mean(hitsP)=0.901331, mean(diffsP)=0.920522 mean(hitsA)=0.694882, mean(diffsA)=0.92616\n",
      "81: eta=0.0410607 cost=-0.00870327 jtype=constrained costheta=-0.600 ps=[0.16446, -1.05675, -0.322812, 0.557536, 0.500567, 0.40981, 0.0620689]\n",
      "     -- cost=-0.00870717,   cost1=0.000527551, cost2=-0.00923472\n",
      "     -- mean(hitsP)=0.901301, mean(diffsP)=0.921035 mean(hitsA)=0.694811, mean(diffsA)=0.925909\n",
      "82: eta=0.0451667 cost=-0.00870717 jtype=constrained costheta=-0.522 ps=[0.164151, -1.05633, -0.323411, 0.557522, 0.507055, 0.422525, 0.0617229]\n",
      "     -- cost=-0.00871159,   cost1=0.000524473, cost2=-0.00923606\n",
      "     -- mean(hitsP)=0.901274, mean(diffsP)=0.92161 mean(hitsA)=0.694724, mean(diffsA)=0.925603\n",
      "83: eta=0.0496834 cost=-0.00871159 jtype=constrained costheta=-0.445 ps=[0.163824, -1.05605, -0.324108, 0.557439, 0.51451, 0.436894, 0.0613372]\n",
      "     -- cost=-0.00871661,   cost1=0.00052084, cost2=-0.00923745\n",
      "     -- mean(hitsP)=0.901254, mean(diffsP)=0.922262 mean(hitsA)=0.694619, mean(diffsA)=0.925228\n",
      "84: eta=0.0546518 cost=-0.00871661 jtype=constrained costheta=-0.374 ps=[0.163476, -1.05596, -0.32493, 0.557269, 0.523132, 0.453153, 0.0609058]\n",
      "     -- cost=-0.00872229,   cost1=0.000516597, cost2=-0.00923889\n",
      "     -- mean(hitsP)=0.901241, mean(diffsP)=0.923007 mean(hitsA)=0.69449, mean(diffsA)=0.92477\n",
      "85: eta=0.0601169 cost=-0.00872229 jtype=constrained costheta=-0.309 ps=[0.1631, -1.05617, -0.325915, 0.556982, 0.53316, 0.471574, 0.060422]\n",
      "     -- cost=-0.0087287,   cost1=0.000511703, cost2=-0.0092404\n",
      "     -- mean(hitsP)=0.901238, mean(diffsP)=0.923869 mean(hitsA)=0.694334, mean(diffsA)=0.924212\n",
      "86: eta=0.0661286 cost=-0.0087287 jtype=constrained costheta=-0.253 ps=[0.162684, -1.05679, -0.327116, 0.556542, 0.544892, 0.492467, 0.0598783]\n",
      "     -- cost=-0.00873588,   cost1=0.000506157, cost2=-0.00924204\n",
      "     -- mean(hitsP)=0.901248, mean(diffsP)=0.924878 mean(hitsA)=0.694148, mean(diffsA)=0.923531\n",
      "87: eta=0.0727415 cost=-0.00873588 jtype=constrained costheta=-0.205 ps=[0.162208, -1.058, -0.328606, 0.555904, 0.558703, 0.51618, 0.0592662]\n",
      "     -- cost=-0.00874385,   cost1=0.000500031, cost2=-0.00924388\n",
      "     -- mean(hitsP)=0.901274, mean(diffsP)=0.926072 mean(hitsA)=0.693928, mean(diffsA)=0.922703\n",
      "88: eta=0.0800156 cost=-0.00874385 jtype=constrained costheta=-0.164 ps=[0.161633, -1.06005, -0.330485, 0.555012, 0.575064, 0.543092, 0.058576]\n",
      "     -- cost=-0.00875252,   cost1=0.000493517, cost2=-0.00924603\n",
      "     -- mean(hitsP)=0.901318, mean(diffsP)=0.927502 mean(hitsA)=0.693671, mean(diffsA)=0.921705\n",
      "89: eta=0.0880172 cost=-0.00875252 jtype=constrained costheta=-0.130 ps=[0.160895, -1.06327, -0.33289, 0.553807, 0.594588, 0.5736, 0.0577966]\n",
      "     -- cost=-0.00876171,   cost1=0.000486988, cost2=-0.0092487\n",
      "     -- mean(hitsP)=0.901386, mean(diffsP)=0.929226 mean(hitsA)=0.693378, mean(diffsA)=0.920514\n",
      "90: eta=0.0968189 cost=-0.00876171 jtype=constrained costheta=-0.101 ps=[0.159884, -1.06806, -0.336001, 0.552242, 0.618091, 0.608067, 0.0569154]\n",
      "     -- cost=-0.00877111,   cost1=0.000481043, cost2=-0.00925215\n",
      "     -- mean(hitsP)=0.901482, mean(diffsP)=0.93131 mean(hitsA)=0.693055, mean(diffsA)=0.91912\n",
      "91: eta=0.106501 cost=-0.00877111 jtype=constrained costheta=-0.077 ps=[0.158417, -1.07495, -0.340042, 0.550303, 0.646702, 0.646716, 0.0559179]\n",
      "     -- cost=-0.00878021,   cost1=0.000476488, cost2=-0.0092567\n",
      "     -- mean(hitsP)=0.901616, mean(diffsP)=0.933804 mean(hitsA)=0.692713, mean(diffsA)=0.917535\n",
      "92: eta=0.117151 cost=-0.00878021 jtype=constrained costheta=-0.058 ps=[0.15619, -1.08451, -0.345264, 0.548079, 0.682041, 0.68938, 0.0547864]\n",
      "     -- cost=-0.0087884,   cost1=0.000474239, cost2=-0.00926264\n",
      "     -- mean(hitsP)=0.901782, mean(diffsP)=0.936707 mean(hitsA)=0.692371, mean(diffsA)=0.915821\n",
      "93: eta=0.128866 cost=-0.0087884 jtype=constrained costheta=-0.041 ps=[0.152592, -1.09721, -0.351876, 0.545952, 0.726543, 0.734822, 0.0534878]\n",
      "     -- cost=-0.00879547,   cost1=0.000475438, cost2=-0.00927091\n",
      "     -- mean(hitsP)=0.901869, mean(diffsP)=0.939938 mean(hitsA)=0.692097, mean(diffsA)=0.914244\n",
      "94: eta=0.141753 cost=-0.00879547 jtype=constrained costheta=-0.028 ps=[0.145876, -1.11287, -0.359826, 0.545359, 0.783978, 0.777792, 0.0519421]\n",
      "     -- cost=-0.00880466,   cost1=0.000482583, cost2=-0.00928724\n",
      "     -- mean(hitsP)=0.901597, mean(diffsP)=0.944287 mean(hitsA)=0.691955, mean(diffsA)=0.913162\n",
      "95: eta=0.155928 cost=-0.00880466 jtype=constrained costheta=-0.022 ps=[0.136797, -1.12792, -0.368885, 0.547113, 0.859187, 0.810209, 0.0501208]\n",
      "     -- cost=-0.00881963,   cost1=0.000492044, cost2=-0.00931167\n",
      "     -- mean(hitsP)=0.900862, mean(diffsP)=0.950355 mean(hitsA)=0.691701, mean(diffsA)=0.911979\n",
      "96: eta=0.171521 cost=-0.00881963 jtype=constrained costheta=-0.032 ps=[0.129881, -1.14478, -0.379999, 0.547744, 0.954495, 0.841314, 0.0482654]\n",
      "     -- cost=-0.00883358,   cost1=0.000484316, cost2=-0.00931789\n",
      "     -- mean(hitsP)=0.900407, mean(diffsP)=0.95304 mean(hitsA)=0.691529, mean(diffsA)=0.910539\n",
      "97: eta=0.188673 cost=-0.00883358 jtype=constrained costheta=-0.026 ps=[0.115093, -1.17132, -0.389009, 0.550708, 1.05192, 0.868019, 0.0466511]\n",
      "     -- cost=-0.0088409,   cost1=0.000476404, cost2=-0.00931731\n",
      "     -- mean(hitsP)=0.90069, mean(diffsP)=0.953692 mean(hitsA)=0.691953, mean(diffsA)=0.909769\n",
      "98: eta=0.20754 cost=-0.0088409 jtype=Newton costheta=-0.017 ps=[0.0997248, -1.19305, -0.395077, 0.553976, 1.12593, 0.880166, 0.0456609]\n",
      "     -- cost=-0.00884303,   cost1=0.000471691, cost2=-0.00931472\n",
      "     -- mean(hitsP)=0.900651, mean(diffsP)=0.953967 mean(hitsA)=0.691985, mean(diffsA)=0.908977\n",
      "99: eta=0.228294 cost=-0.00884303 jtype=Newton costheta=-0.015 ps=[0.0898776, -1.2079, -0.399666, 0.555332, 1.18249, 0.889359, 0.0449695]\n",
      "     -- cost=-0.00884339,   cost1=0.000470208, cost2=-0.0093136\n",
      "     -- mean(hitsP)=0.900756, mean(diffsP)=0.953993 mean(hitsA)=0.692157, mean(diffsA)=0.908728\n",
      "100: eta=0.251123 cost=-0.00884339 jtype=Newton costheta=-0.010 ps=[0.0850058, -1.21456, -0.40164, 0.556126, 1.20712, 0.89199, 0.0446968]\n",
      "     -- cost=-0.00884341,   cost1=0.00046976, cost2=-0.00931317\n",
      "     -- mean(hitsP)=0.90076, mean(diffsP)=0.953983 mean(hitsA)=0.692179, mean(diffsA)=0.908651\n",
      "101: eta=0.276236 cost=-0.00884341 jtype=Newton costheta=-0.010 ps=[0.0835418, -1.21651, -0.402224, 0.556376, 1.21453, 0.892727, 0.0446154]\n",
      "     -- cost=-0.00884341,   cost1=0.000469734, cost2=-0.00931314\n",
      "     -- mean(hitsP)=0.900761, mean(diffsP)=0.953982 mean(hitsA)=0.692182, mean(diffsA)=0.908647\n",
      "102: eta=0.303859 cost=-0.00884341 jtype=Newton costheta=-0.007 ps=[0.0834454, -1.21662, -0.402257, 0.556394, 1.21496, 0.892756, 0.0446109]\n",
      "     -- cost=-0.00884341,   cost1=0.000469734, cost2=-0.00931314\n",
      "     -- mean(hitsP)=0.900761, mean(diffsP)=0.953982 mean(hitsA)=0.692182, mean(diffsA)=0.908647\n",
      "About to break -- tol=1e-12, new_cost-cost=-1.46064e-15, eta=0.303859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.08344536755256632"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = [\"sW\", \"vW\", \"hW\", \"constant_excitation\", \"right_light_excitation\", \"target_period_excitation\", \"sigma\"]\n",
    "seed = [0.2,   -1.7, -1.7,      0.19,                0.5,                       1,                       0.1]\n",
    "seed = [0.2,   0.17,  0.17,      0.19,                0.5,                       1,                       0.1]\n",
    "seed = [0.2,   -1.7, -1.7,      -0.19,                0.5,                       1,                       0.1]\n",
    "seed = [0.2,   -1.7, -1.7,      0.39,                0.15,                       0.1,                       0.1]\n",
    "bbox = Dict(:sW=>[0 3], :vW=>[-3 3], :hW=>[-3 3], :constant_excitation=>[-2 2],\n",
    ":right_light_excitation=>[0.05 4], :target_period_excitation=>[0.05 4], :sigma=>[0.05 1])\n",
    "model_params = merge(model_params, Dict(:post_target_period=>0.5))\n",
    "\n",
    "\n",
    "# Now with constant_pro_bias and a fixed sigma=0.1\n",
    "args = [\"sW\", \"vW\", \"hW\", \"constant_excitation\", \"right_light_excitation\", \"target_period_excitation\", \"const_pro_bias\"]\n",
    "seed = [0.2,   -1.7, -1.7,      0.39,                0.15,                       0.1,                       0.1]\n",
    "model_params = merge(model_params, Dict(:post_target_period=>0.5, :sigma=>0.1))\n",
    "bbox = Dict(:sW=>[0 3], :vW=>[-3 3], :hW=>[-3 3], :constant_excitation=>[-2 2],\n",
    ":right_light_excitation=>[0.05 4], :target_period_excitation=>[0.05 4], :const_pro_bias=>[-2 2])\n",
    "\n",
    "# ==========\n",
    "\n",
    "nPro=100; nAnti=100\n",
    "\n",
    "rule_and_delay_periods = [0.4, 0.8]\n",
    "post_target_periods    = [0.5, 1]\n",
    "\n",
    "pars, traj, cost, cpm_traj, Dlambda = bbox_Hessian_keyword_minimization(seed, args, bbox, \n",
    "(;params...) -> JJ(nPro, nAnti; rule_and_delay_periods=rule_and_delay_periods,\n",
    "post_target_periods=post_target_periods,\n",
    "seedrand=31, cbeta=0.01, verbose=true, merge(model_params, Dict(params))...),\n",
    "start_eta = 0.01, tol=1e-12, verbose=true)[1]\n",
    "\n",
    "pars'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     -- cost=-0.00884341,   cost1=0.000469729, cost2=-0.00931314\n",
      "     -- mean(hitsP)=0.90076, mean(diffsP)=0.953981 mean(hitsA)=0.69218, mean(diffsA)=0.908647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.008843411106774281"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func = (;params...) -> JJ(nPro, nAnti; rule_and_delay_periods=rule_and_delay_periods,\n",
    "post_target_periods=post_target_periods,\n",
    "seedrand=31, cbeta=0.01, verbose=true, merge(model_params, Dict(params))...)\n",
    "\n",
    "func(;make_dict(args, pars)...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8.740874 seconds (13.19 M allocations: 798.834 MB, 1.98% gc time)\n",
      "Pro % correct = 90.7%\n",
      "Anti % correct = 67.1% \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.0016377281161004287"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------\n",
    "# NOW EVALUATE RESULTS HERE\n",
    "# --------------------\n",
    "\n",
    "my_params = make_dict([args; \"plot_list\"; \"post_target_period\"; \"rule_and_delay_period\"; \"dt\"], \n",
    "[pars; [[1:10;]]; 1.5; 0.5; 0.02], model_params)\n",
    "\n",
    "run_factor = 10\n",
    "\n",
    "proVs, antiVs = @time(run_ntrials(nPro*run_factor, nAnti*run_factor; plot_list=[], my_params...))\n",
    "\n",
    "if nPro>0;  @printf(\"Pro %% correct = %g%%\\n\", 100*length(find(proVs[1,:].>proVs[4,:]))/(nPro*run_factor)); end;\n",
    "if nAnti>0; @printf(\"Anti %% correct = %g%% \\n\", 100*length(find(antiVs[1,:].<antiVs[4,:]))/(nAnti*run_factor)); end;\n",
    "\n",
    "\n",
    "figure(3); clf();\n",
    "ax1 = subplot(2,1,1)\n",
    "h = plt[:hist](proVs[1,:]-proVs[4,:],-1:0.02:1)\n",
    "title(\"PRO Vr - Vl\")\n",
    "remove_xtick_labels(ax1)\n",
    "vlines(0, ylim()[1], ylim()[2])\n",
    "\n",
    "ax2 = subplot(2,1,2)\n",
    "h = plt[:hist](antiVs[1,:]-antiVs[4,:],-1:0.02:1)\n",
    "title(\"ANTI Vr - Vl\")\n",
    "vlines(0, ylim()[1], ylim()[2])\n",
    "\n",
    "figure(1); clf(); figure(2); clf();\n",
    "\n",
    "JJ(nPro, nAnti; rule_and_delay_periods=my_params[:rule_and_delay_period], \n",
    "post_target_periods=my_params[:post_target_period], my_params...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Symbol,Any} with 29 entries:\n",
       "  :target_period          => 0.1\n",
       "  :right_light_pro_extra  => 0\n",
       "  :const_pro_bias         => 0\n",
       "  :beta                   => 1\n",
       "  :sigma                  => 0.08\n",
       "  :dW                     => 0\n",
       "  :anti_rule_strength     => 0.1\n",
       "  :init_add               => 0\n",
       "  :pro_rule_strength      => 0.1\n",
       "  :vw                     => -1.7\n",
       "  :sW                     => 0.2\n",
       "  :vW                     => -1.7\n",
       "  :noise                  => Any[]\n",
       "  :tau                    => 0.1\n",
       "  :theta                  => 1\n",
       "  :right_light_excitation => 0.5\n",
       "  :hW                     => -1.7\n",
       "  :hw                     => -1.7\n",
       "  :sw                     => 0.2\n",
       "  :constant_excitation    => 0.19\n",
       "  :rule_and_delay_period  => 0.4\n",
       "  :const_add              => 0\n",
       "  :nsteps                 => 2\n",
       "  :post_target_period     => 0.5\n",
       "  :input                  => 0\n",
       "  ⋮                       => ⋮"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3"
     ]
    }
   ],
   "source": [
    "for i in [3]\n",
    "    print(i)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7-element Array{Float64,1}:\n",
       "  0.764685\n",
       " -1.84917 \n",
       " -0.961235\n",
       "  0.399476\n",
       "  1.33592 \n",
       "  0.13114 \n",
       "  0.232522"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_pro_pars = pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition wallwrap(Any, Any) in module Main at In[90]:18 overwritten at In[92]:18.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'wallwrap :: Tuple{Any,Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition vector_wrap(Any, Any, Any) in module Main at In[90]:44 overwritten at In[92]:44.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'vector_wrap :: Tuple{Any,Any,Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition inverse_wall(Any, Any, Any) in module Main at In[90]:68 overwritten at In[92]:68.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'inverse_wall :: Tuple{Any,Any,Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition inverse_wall(Any, Any) in module Main at In[90]:91 overwritten at In[92]:91.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'inverse_wall :: Tuple{Any,Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition bbox_Hessian_keyword_minimization(Any, Any, Any, Any) in module Main at In[90]:193 overwritten at In[92]:193.\n",
      "WARNING: Method definition #bbox_Hessian_keyword_minimization(Array{Any, 1}, Main.#bbox_Hessian_keyword_minimization, Any, Any, Any, Any) in module Main overwritten.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'bbox_Hessian_keyword_minimization :: Tuple{Any,Any,Any,Any}' in module 'Main'.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bbox_Hessian_keyword_minimization"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################################\n",
    "#                                                    #\n",
    "#         BBOX_HESSIAN_KEYWORD_MINIMIZATION          #\n",
    "#                                                    #\n",
    "######################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "pdict = wallwrap(bdict, pdict)\n",
    "Given bdict, a dictionary of symbols to [minval, maxval] vectors, and pdict, a dictionary of symbols\n",
    "to values (or, alternatively, an Array of (Symbol, value) tuples], goes through each of the symbols in \n",
    "bdict and modifies the corresponding value in pdict putting it through a tanh so the final output lies \n",
    "within the limits in bdict.  Returns the new pdict.  Makes a copy of pdict so as not to modify the original.\n",
    "\"\"\"\n",
    "function wallwrap(bdict, epdict)\n",
    "    local pdict = two_level_copy(epdict)\n",
    "    if typeof(pdict)<:Array\n",
    "        pdict = Dict(pdict)\n",
    "    end\n",
    "\n",
    "    allkeys = keys(bdict)\n",
    "\n",
    "    for k in allkeys\n",
    "        local bbox = bdict[k]\n",
    "        d = 0.5*(bbox[2] - bbox[1])\n",
    "        m = 0.5*(bbox[2] + bbox[1])\n",
    "\n",
    "        pdict[k] = bbox[1] + d*(tanh((pdict[k]-m)/d)+1)\n",
    "    end\n",
    "    return pdict\n",
    "end\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "params = vector_wrap(bbox, args, eparams)\n",
    "Given bdict, a dictionary of symbols to [minval, maxval] vectors, args, an array of strings representing\n",
    "symbols, and params, an array of values corresponding to the args list, puts each param that has an entry \n",
    "in bdict through the tanh-walling mechanism, and returns the result. Does not modify the contents of the \n",
    "original params vector (or bdict or args).\n",
    "\"\"\"\n",
    "function vector_wrap(bbox, args, eparams)\n",
    "    local params = two_level_copy(eparams)\n",
    "    pdict = wallwrap(bbox, make_dict(args, params))\n",
    "    i=1; j=1\n",
    "    for i=1:length(args)\n",
    "        if typeof(args[i])<:Array\n",
    "            params[j:j+args[i][2]-1] = pdict[Symbol(args[i][1])]\n",
    "            j += args[i][2]-1\n",
    "        else\n",
    "            params[j] = pdict[Symbol(args[i])]\n",
    "        end\n",
    "    j = j+1\n",
    "    end\n",
    "    return params\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "params = inverse_wall(bdict, args, wparams)\n",
    "Given bdict, a dictionary of symbols to [minval, maxval] vectors, args, an array of strings representing\n",
    "symbols, and wparams, an array of values corresponding to the args list where each param that has an entry \n",
    "in bdict has alreadt been through the tanh-walling mechanism, UNwalls the ones that have a bdict entry and\n",
    "returns the result. Does not modify the contents of the original params vector (or bdict or args).\n",
    "\"\"\"\n",
    "function inverse_wall(bdict, args, wparams)\n",
    "    local params = two_level_copy(wparams)\n",
    "    pdict = inverse_wall(bdict, make_dict(args, params))\n",
    "    i=1; j=1\n",
    "    for i=1:length(args)\n",
    "        if typeof(args[i])<:Array\n",
    "            params[j:j+args[i][2]-1] = pdict[Symbol(args[i][1])]\n",
    "            j += args[i][2]-1\n",
    "        else\n",
    "            params[j] = pdict[Symbol(args[i])]\n",
    "        end\n",
    "        j = j+1\n",
    "    end\n",
    "    return params    \n",
    "end\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "pdict = inverse_wall(bdict, wdict)\n",
    "Given bdict, a dictionary of symbols to [minval, maxval] vectors, and wdict, a dictionary of symbols to values\n",
    "(or vectors of values)  UNwalls the ones that have a bdict entry and\n",
    "returns the result. Does not modify the contents of any dictionaries.\n",
    "\"\"\"\n",
    "function inverse_wall(bdict, wdict)\n",
    "    local pdict = two_level_copy(wdict)\n",
    "\n",
    "    allkeys = keys(bdict)\n",
    "    for k in allkeys\n",
    "        local bbox = bdict[k]\n",
    "        d = 0.5*(bbox[2] - bbox[1])\n",
    "        m = 0.5*(bbox[2] + bbox[1])\n",
    "\n",
    "        pdict[k] = m + d*0.5*log((pdict[k]-bbox[1])./(2*d - pdict[k] + bbox[1]))\n",
    "    end\n",
    "    return(pdict)\n",
    "end\n",
    "  \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "function bbox_Hessian_keyword_minimization(seed, args, bbox, func; wallwidth=NaN, start_eta=10, tol=1e-6, \n",
    "    maxiter=400, verbose=false)\n",
    "\n",
    "Like constrained_Hessian_minimization, but uses keyword_hessian!(). \n",
    "\n",
    "PARAMETERS:\n",
    "===========\n",
    "\n",
    "seed        column vector, representing the starting value of the parameters.\n",
    "\n",
    "args        List of strings identifying parameters for differentiation, e.g., [\"const_E\", \"w_self]\n",
    "\n",
    "bbox        If softbox=true (the default), should then be a Dict of Symbol=>[minval maxval] entries. An entry\n",
    "            in this Dict indicates that the corresponding parameter is to be bounded, as indicated by the associated \n",
    "            [minval maxval] vector. The bbox dictionary can have fewer entries than the number of parameters, and its\n",
    "            default value is Dict(), indicating an unbounded search.\n",
    "\n",
    "            If softbox=false, then bbox should be an nargs-by-2 matrix indicating the range for each argument,\n",
    "            with the minima (first column) and maxima (second column), and entries for ALL parameters.\n",
    "\n",
    "func        func must take only optional keyword args, and must \n",
    "            take nderivs=0, difforder=0  and declare any new matrices using ForwardDiffZeros() instead of zeros()\n",
    "\n",
    "\n",
    "OPTIONAL PARAMETERS:\n",
    "====================\n",
    "\n",
    "start_eta    Starting value of the radius.  It's good to start with somethibg biggish, if it is\n",
    "             too much, it'll quickly get cut down.\n",
    "\n",
    "tol=1e-6     Numerical tolerance. If a proposed jump produces a change in func that is less than\n",
    "             this, the minimization stops.\n",
    "\n",
    "maxiter=400  Maximum number of iterations to do before stopping\n",
    "\n",
    "verbose=false   If true, print out a report on each iteration of iteration number, radius size (eta),\n",
    "                what type jump was proposed (\"Newton\" means going straight to global min, \"constrained\" means jump has \n",
    "                norm eta, failed means that finding the minimum at a given radius somehow didn't work). Will also\n",
    "                print out the cosine of the angle between the proposed jump and the gradient.\n",
    "\n",
    "verbose_level   If less than 2, regular verbose output, if 2 or greater, very verbose, for debugging.\n",
    "\n",
    "softbox         If true, then bbox must be a Dict() and we use the tanh() mechanism for putting a fixed limit\n",
    "                on the parameters.\n",
    "\n",
    "hardbox=false   If true, ignores wallwidth, and just rests parameter values to the bounding box if they go outside it.\n",
    "                If false, adds cost function \"walls\" to implement the bounding box.\n",
    "\n",
    "walldith=NaN     Used for putting up cost function \"walls\" that implement the bounding box limits. Can be NaN.\n",
    "                If it is NaN, then the wallwidth is a constant factor of the range width for each argument. If not NaN, must\n",
    "                be an nargs-long vector that indicates the actual wall widths.\n",
    "\n",
    "wallwidth_factor=0.18   Only relevant if wallwidth is NaN, otherwise ignored. For each arg, the wall width\n",
    "                is going to be wall_width_factor*(bbox[i,2] - bbox[i,1])\n",
    "\n",
    "\n",
    "RETURNS:\n",
    "========\n",
    "\n",
    "params       A vector the size of seed that has the last values of the minimizing parameters for func\n",
    "trajectory   A (2+length(params))-by-nsteps matrix. Each column corresponds to an iteration step, and contains\n",
    "                 the value of eta used, the cost, and the value of the parameters at that iteration\n",
    "cost         Final value of objective function\n",
    "cpm_traj     A 2-by-nsteps matrix, containing reports from the contrained parabolic minimization at each timestep.\n",
    "             The first row is niters (how many iterations cpm's 1-d minimization ran for) and the second row is\n",
    "             Dlambda, the last change in the parameter being minimized in cpm's internal search\n",
    "\n",
    "\n",
    "EXAMPLE:\n",
    "========\n",
    "\n",
    "function tester(;x=5, y=10, z=20, nderivs=0, difforder=0)\n",
    "    return x^2*y + z/tanh(y)\n",
    "end\n",
    "\n",
    "params, trajectory = bbox_Hessian_keyword_minimization([0.5, 0.5], [\"x\", \"y\"], [1.1 2 ; 1.1 4], tester, \n",
    "    verbose=true, tol=1e-12, start_eta=1);\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "function bbox_Hessian_keyword_minimization(seed, args, bbox, func; start_eta=0.1, tol=1e-6, maxiter=400,\n",
    "    verbose=false, verbose_level=1, verbose_every=1, \n",
    "    softbox=true, hardbox=false, wallwidth=NaN, wallwidth_factor=0.18)\n",
    "\n",
    "      \n",
    "    \"\"\"\n",
    "    Given args, a list of string representing the arguments of interest, a bounding box for each,\n",
    "    and a Symbol=>value dictionary with the corresponding parameters, computes and returns a high cost for \n",
    "    being outside the bounding box\n",
    "    \"\"\"\n",
    "    function wall_cost(args, bbox; wallwidth=NaN, nderivs=0, difforder=0, pars...) \n",
    "        myparams = ForwardDiffZeros(length(pars), 1, nderivs=nderivs, difforder=difforder)\n",
    "        pars2 = Dict()\n",
    "        for i in [1:length(pars);]\n",
    "            pars2[string(pars[i][1])] = pars[i][2]\n",
    "        end\n",
    "        for i in [1:length(args);]\n",
    "            myparams[i] = pars2[args[i]]\n",
    "        end\n",
    "        \n",
    "        if isnan(wallwidth)\n",
    "            # We know that we're going to be taking hessian for params, so declare zeros accordingly:\n",
    "            wallwidth = ForwardDiffZeros(length(myparams), 1, nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "            for i in [1:length(myparams);]\n",
    "                wallwidth[i] = wallwidth_factor*(bbox[i,2]-bbox[i,1])\n",
    "            end\n",
    "        end\n",
    "\n",
    "        retval = 0\n",
    "        for i in [1:length(myparams);]\n",
    "            if myparams[i]<bbox[i,1]\n",
    "                retval += cosh((bbox[i,1]-myparams[i])/wallwidth[i])-1.0\n",
    "            elseif bbox[i,2] < myparams[i]\n",
    "                retval += cosh((myparams[i]-bbox[i,2])/wallwidth[i])-1.0                \n",
    "            end\n",
    "        end\n",
    "\n",
    "        return 2*retval\n",
    "    end\n",
    "\n",
    "    traj_increment = 100\n",
    "    params = 0  # Make sure to have this here so that params stays defined beyond the try/catch\n",
    "    if ( !(typeof(bbox)<:Dict) ); error(\"Currently only supporting softbox=true, bbox must be a Dict\"); end;\n",
    "    try\n",
    "        params = copy(inverse_wall(bbox, args, seed))\n",
    "    catch\n",
    "        error(\"Were all initial param values within the indicated walls?\")\n",
    "    end\n",
    "    eta = start_eta\n",
    "    trajectory = zeros(2+length(params), traj_increment); cpm_traj = zeros(2, traj_increment)\n",
    "\n",
    "    if verbose\n",
    "        @printf \"%d: eta=%g ps=\" 0 eta \n",
    "        print_vector(vector_wrap(bbox, args, params))\n",
    "        @printf \"\\n\"\n",
    "    end\n",
    "    \n",
    "    if softbox\n",
    "        if !(typeof(bbox)<:Dict); error(\"bhm: If softbox=true, then bbox must eb a Dict\"); end\n",
    "        cost, grad, hess = keyword_vgh((;pars...)->func(;wallwrap(bbox, pars)...), args, params)\n",
    "    elseif hardbox\n",
    "        cost, grad, hess = keyword_vgh((;pars...) -> func(;pars...), args, params)\n",
    "    else\n",
    "        cost, grad, hess = keyword_vgh((;pars...) -> func(;pars...) + wall_cost(args, bbox; wallwidth=wallwidth, pars...),\n",
    "            args, params)        \n",
    "    end\n",
    "        \n",
    "    chessdelta = zeros(size(params))\n",
    "\n",
    "    for i in [1:maxiter;]\n",
    "        if i > size(trajectory, 2)\n",
    "            trajectory = [trajectory zeros(2+length(params), traj_increment)]\n",
    "            cpm_traj   = [cpm_traj   zeros(2, traj_increment)]\n",
    "        end\n",
    "        trajectory[1:2, i]   = [eta;cost]\n",
    "        trajectory[3:end, i] = vector_wrap(bbox, args, params)\n",
    "        \n",
    "        hessdelta  = - inv(hess)*grad\n",
    "        try\n",
    "            if verbose && verbose_level >= 2\n",
    "                @printf(\"bhm: about to try cpm with grad : \"); print_vector_g(grad); print(\"\\n\")\n",
    "                @printf(\"bhm:   hess :\"); print_vector_g(hess[:]); print(\"\\n\");\n",
    "            end\n",
    "            if verbose && verbose_level >= 2\n",
    "                cpm_out = constrained_parabolic_minimization(hess, grad'', eta, \n",
    "                    maxiter=500, tol=1e-20, do_plot=true, verbose=true)                \n",
    "            else\n",
    "                cpm_out = constrained_parabolic_minimization(hess, grad'', eta, maxiter=500, tol=1e-20)\n",
    "            end\n",
    "            chessdelta = cpm_out[1]; cpm_traj[1,i] = cpm_out[5]; cpm_traj[2,i] = cpm_out[6]\n",
    "            jumptype = \"not failed\"\n",
    "        catch y\n",
    "            jumptype = \"failed\"\n",
    "            if verbose\n",
    "                @printf \"Constrained parabolic minimization failed with error %s\\n\" y\n",
    "                @printf \"\\n\"\n",
    "                @printf \"eta was %g\\n\" eta\n",
    "                @printf \"grad was\\n\"\n",
    "                print_vector(grad)\n",
    "                @printf \"\\n\\nhess was\\n\"\n",
    "                for k in [1:length(grad);]\n",
    "                    print_vector(hess[k,:])\n",
    "                    @printf \"\\n\"\n",
    "                end\n",
    "                @printf \"\\n\"\n",
    "                matwrite(\"error_report.mat\", Dict(\"grad\"=>grad, \"hess\"=>hess, \"eta\"=>eta))\n",
    "            end\n",
    "            break\n",
    "        end\n",
    "\n",
    "        if norm(hessdelta) <= eta\n",
    "            new_params = params + hessdelta\n",
    "            jumptype = \"Newton\"\n",
    "        elseif jumptype != \"failed\" \n",
    "            new_params = params + chessdelta\n",
    "            jumptype  = \"constrained\"\n",
    "        end\n",
    "\n",
    "        if jumptype != \"failed\"\n",
    "            if softbox\n",
    "                new_cost, new_grad, new_hess = \n",
    "                    keyword_vgh((;pars...) -> func(;wallwrap(bbox, pars)...), args, new_params)\n",
    "                if verbose && verbose_level >=2\n",
    "                    @printf(\"bhm: had new_params = : \"); print_vector_g(vector_wrap(bbox, args, params)); print(\"\\n\");\n",
    "                    @printf(\"bhm: and my bbox was : \"); print(bbox); print(\"\\n\")\n",
    "                    @printf(\"bhm: and my wallwrap output was : \"); print(wallwrap(bbox, make_dict(args, new_params))); print(\"\\n\")\n",
    "                    @printf(\"bhm: and this produced new_grad : \"); print_vector_g(new_grad); print(\"\\n\")\n",
    "                    @printf(\"bhm:   new_hess :\"); print_vector_g(new_hess[:]); print(\"\\n\");                                        \n",
    "                end\n",
    "            elseif hardbox\n",
    "                for p in [1:length(new_params);]\n",
    "                    if new_params[p] < bbox[p,1]; new_params[p] = bbox[p,1]; end\n",
    "                    if bbox[p,2] < new_params[p]; new_params[p] = bbox[p,2]; end\n",
    "                 end        \n",
    "                \n",
    "                new_cost, new_grad, new_hess = keyword_vgh((;pars...) -> func(;pars...), args, new_params)\n",
    "            else\n",
    "                new_cost, new_grad, new_hess = keyword_vgh((;pars...) -> func(;pars...) + \n",
    "                        wall_cost(args, bbox; wallwidth=wallwidth, pars...),\n",
    "                    args, new_params)                \n",
    "            end\n",
    "            \n",
    "            if abs(new_cost - cost) < tol || eta < tol\n",
    "                trajectory = trajectory[:,1:i]; cpm_traj = cpm_traj[:,1:i]\n",
    "                if verbose\n",
    "                    @printf(\"About to break -- tol=%g, new_cost-cost=%g, eta=%g\\n\", tol, new_cost-cost, eta)\n",
    "                end\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "\n",
    "        if jumptype == \"failed\" || new_cost >= cost  \n",
    "            if verbose\n",
    "                @printf(\"eta going down: new_cost-cost=%g and jumptype='%s'\\n\", new_cost-cost, jumptype)\n",
    "                if verbose_level >= 2\n",
    "                    nwp = vector_wrap(bbox, args, new_params); wp = vector_wrap(bbox, args, params)\n",
    "                    @printf(\"   vvv: proposed new params were : \"); print_vector_g(nwp); print(\"\\n\")\n",
    "                    @printf(\"   vvv: proposed delta params was : \"); print_vector_g(nwp-wp); print(\"\\n\")\n",
    "                    @printf(\"   vvv: grad was : \"); print_vector_g(grad); print(\"\\n\")\n",
    "                    costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "                    @printf(\"   vvv: costheta of proposed jump was %g\\n\", costheta)\n",
    "                end\n",
    "            end\n",
    "            eta = eta/2\n",
    "            costheta = NaN\n",
    "            if eta < tol\n",
    "                trajectory = trajectory[:,1:i]; cpm_traj = cpm_traj[:,1:i]\n",
    "                if verbose\n",
    "                    @printf(\"About to break -- tol=%g, new_cost-cost=%g, eta=%g\\n\", tol, new_cost-cost, eta)\n",
    "                end\n",
    "                break\n",
    "            end\n",
    "        else\n",
    "            eta = eta*1.1\n",
    "            costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "\n",
    "            params = new_params\n",
    "            cost = new_cost\n",
    "            grad = new_grad\n",
    "            hess = new_hess\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            if rem(i, verbose_every)==0\n",
    "                @printf \"%d: eta=%g cost=%g jtype=%s costheta=%.3f ps=\" i eta cost jumptype costheta\n",
    "                print_vector_g(vector_wrap(bbox, args, params))\n",
    "                @printf \"\\n\"\n",
    "                if verbose_level >= 3\n",
    "                    @printf \"    At this point, grad is =\"\n",
    "                    print_vector_g(grad)\n",
    "                    @printf \"\\n\"                \n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vector_wrap(bbox, args, params), trajectory, cost, cpm_traj\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# WORKS:\n",
    "# keyword_gradient((;params...) -> JJ(100; merge(model_params, Dict(params))...), [\"sw\", \"vw\", \"hw\"], [0.2, -1.7, -1.7])\n",
    "\n",
    "args = [\"sw\", \"vw\", \"hw\", \"constant_excitation\", \"right_light_excitation\", \"target_period_excitation\", \"sigma\"]\n",
    "seed = [0.2,   -1.7, -1.7,      0.19,                0.5,                       1,                       0.1]\n",
    "seed = [0.2,   0.17,  0.17,      0.19,                0.5,                       1,                       0.1]\n",
    "seed = [0.2,   -1.7, -1.7,      -0.19,                0.5,                       1,                       0.1]\n",
    "seed = [0.2,   -1.7, -1.7,      0.19,                0.15,                       0.1,                       0.1]\n",
    "\n",
    "bbox = [\n",
    "    -3    3 ;\n",
    "    -3    3 ; \n",
    "    -3    3 ;\n",
    "    0.1   0.4 ;\n",
    "    0.1   2.0 ;\n",
    "    0.1   2.0 ;\n",
    "    0.05  2.0 ;\n",
    "]\n",
    "\n",
    "pars = bbox_Hessian_keyword_minimization(seed, args, bbox, \n",
    "(;params...) -> JJ(100; seedrand=30, cbeta=0.001, verbose=true, merge(model_params, Dict(params))...),\n",
    "start_eta = 0.01, verbose=true)[1]\n",
    "\n",
    "# --------------------\n",
    "# NOW EVALUATE RESULTS\n",
    "# --------------------\n",
    "\n",
    "ntrials = 1000\n",
    "proVs, antiVs = @time(run_ntrials(ntrials; plot_list=[], make_dict(args, pars, model_params)...))\n",
    "\n",
    "@printf(\"Pro %% correct = %g%%\\n\", 100*length(find(proVs[1,:].>proVs[4,:]))/ntrials)\n",
    "@printf(\"Anti %% correct = %g%% \\n\", 100*length(find(antiVs[1,:].<antiVs[4,:]))/ntrials)\n",
    "\n",
    "\n",
    "figure(3); clf();\n",
    "ax1 = subplot(2,1,1)\n",
    "h = plt[:hist](proVs[1,:]-proVs[4,:],-1:0.02:0.1)\n",
    "title(\"PRO Vr - Vl\")\n",
    "remove_xtick_labels(ax1)\n",
    "vlines(0, ylim()[1], ylim()[2])\n",
    "\n",
    "ax2 = subplot(2,1,2)\n",
    "h = plt[:hist](antiVs[1,:]-antiVs[4,:],-1:0.02:1)\n",
    "title(\"ANTI Vr - Vl\")\n",
    "vlines(0, ylim()[1], ylim()[2])\n",
    "\n",
    "JJ(100; make_dict([args; \"plot_list\"], [pars; [[1:5;]]], model_params)...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[args pars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "ntrials = 1000\n",
    "proVs, antiVs = @time(run_ntrials(ntrials; plot_list=[], make_dict(args, pars, model_params)...))\n",
    "\n",
    "@printf(\"Pro %% correct = %g%%\\n\", 100*length(find(proVs[1,:].>proVs[4,:]))/ntrials)\n",
    "@printf(\"Anti %% correct = %g%% \\n\", 100*length(find(antiVs[1,:].<antiVs[4,:]))/ntrials)\n",
    "\n",
    "\n",
    "figure(3); clf();\n",
    "ax1 = subplot(2,1,1)\n",
    "h = plt[:hist](proVs[1,:]-proVs[4,:],-1:0.02:0.1)\n",
    "title(\"PRO Vr - Vl\")\n",
    "remove_xtick_labels(ax1)\n",
    "vlines(0, ylim()[1], ylim()[2])\n",
    "\n",
    "ax2 = subplot(2,1,2)\n",
    "h = plt[:hist](antiVs[1,:]-antiVs[4,:],-1:0.02:1)\n",
    "title(\"ANTI Vr - Vl\")\n",
    "vlines(0, ylim()[1], ylim()[2])\n",
    "\n",
    "JJ(100; make_dict([args; \"plot_list\"; \"dt\"], [pars; [[1:5;]]; 0.01], model_params)...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "JJ(100; make_dict([args; \"plot_list\"; \"dt\"], [pars; [[1:5;]]; 0.005], model_params)...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@time(ForwardDiff.hessian((x)->JJ(100; nderivs=length(x), difforder=2, \n",
    "make_dict([\"sw\", \"hw\", \"vw\", \"sigma\", \"gleak\", \"constant_excitation\", \"plot_list\"], \n",
    "[x[1], x[2], x[3], x[4], x[5], x[6], []], model_params)...), \n",
    "[0.2, -1.7, -1.7, 0.08, 0.25, 0.19]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@time(JJ(100; make_dict([\"sw\", \"hw\", \"vw\", \"plot_list\"], [0.2, -1.7, -1.7, []], model_params)...))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ntrials = 100\n",
    "proVs, antiVs = @time(run_ntrials(ntrials; plot_list=[], model_params...))\n",
    "\n",
    "@printf(\"Pro %% correct = %g%%\\n\", 100*length(find(proVs[1,:].>proVs[4,:]))/ntrials)\n",
    "@printf(\"Anti %% correct = %g%% \\n\", 100*length(find(antiVs[1,:].<antiVs[4,:]))/ntrials)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the distribution of VR - VL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ntrials = 500\n",
    "proVs, antiVs = @time(run_ntrials(ntrials; plot_list=[], model_params...))\n",
    "\n",
    "@printf(\"Pro %% correct = %g%%\\n\", 100*length(find(proVs[1,:].>proVs[4,:]))/ntrials)\n",
    "@printf(\"Anti %% correct = %g%% \\n\", 100*length(find(antiVs[1,:].<antiVs[4,:]))/ntrials)\n",
    "\n",
    "figure(1); clf();\n",
    "ax1 = subplot(2,1,1)\n",
    "h = plt[:hist](proVs[1,:]-proVs[4,:],-0.1:0.002:0.1)\n",
    "title(\"PRO Vr - Vl\")\n",
    "remove_xtick_labels(ax1)\n",
    "vlines(0, ylim()[1], ylim()[2])\n",
    "\n",
    "ax2 = subplot(2,1,2)\n",
    "h = plt[:hist](antiVs[1,:]-antiVs[4,:],-0.1:0.002:0.1)\n",
    "title(\"ANTI Vr - Vl\")\n",
    "vlines(0, ylim()[1], ylim()[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt = 0.0002\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "W = -4\n",
    "noise = 0\n",
    "sigma = 0.2\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    "    :noise=>0, :input=>0, :const_add=>0, :init_add=>0, :sigma=>sigma)\n",
    "\n",
    "clf()\n",
    "srand(20)\n",
    "startUs = randn(20,2)-3\n",
    "startUs = [-2 -4.1]\n",
    "\n",
    "for i=1:size(startUs,1)\n",
    "    forwardModel(startUs[i,:]; do_plot=true, clearfig=false, model_params...)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "typeof(model_params[:input])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- END of ProAnti section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing around with minimizing 2-d model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed = [-0.2, 0.2, 0.2, -1.5, 0.1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition JJ(Any) in module Main at In[47]:62 overwritten at In[48]:62.\n",
      "WARNING: Method definition #JJ(Array{Any, 1}, Main.#JJ, Any) in module Main overwritten.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- cost=0.0686324,   cost1=0.0687511, cost2=-0.00011865 :  mean(hits)=0.487796, mean(diffs)=0.0395499\n",
      "0: eta=1 ps=[-0.200, 0.200, 0.200, -1.500, 0.100]\n",
      "-- cost=0.0686324,   cost1=0.0687511, cost2=-0.00011865 :  mean(hits)=0.487796, mean(diffs)=0.0395499\n",
      "-- cost=0.0662608,   cost1=0.0663306, cost2=-6.98297e-05 :  mean(hits)=0.492453, mean(diffs)=0.0232766\n",
      "1: eta=1.1 cost=0.0662608 jtype=Newton costheta=-0.343 ps=[-0.112938, 0.349709, 0.101748, -1.26536, 0.103245]\n",
      "-- cost=0.0656768,   cost1=0.0657292, cost2=-5.24826e-05 :  mean(hits)=0.493623, mean(diffs)=0.0174942\n",
      "2: eta=1.21 cost=0.0656768 jtype=Newton costheta=-0.188 ps=[0.0772162, 0.616916, 0.0618108, -0.989811, 0.103702]\n",
      "-- cost=0.0658205,   cost1=0.0658659, cost2=-4.54283e-05 :  mean(hits)=0.493357, mean(diffs)=0.0151428\n",
      "eta going down: new_cost-cost=0.000143696 and jumptype='Newton'\n",
      "3: eta=0.605 cost=0.0656768 jtype=Newton costheta=NaN ps=[0.0772162, 0.616916, 0.0618108, -0.989811, 0.103702]\n",
      "-- cost=0.0658205,   cost1=0.0658659, cost2=-4.54283e-05 :  mean(hits)=0.493357, mean(diffs)=0.0151428\n",
      "eta going down: new_cost-cost=0.000143696 and jumptype='Newton'\n",
      "4: eta=0.3025 cost=0.0656768 jtype=Newton costheta=NaN ps=[0.0772162, 0.616916, 0.0618108, -0.989811, 0.103702]\n",
      "-- cost=0.0645488,   cost1=0.0646014, cost2=-5.25525e-05 :  mean(hits)=0.495832, mean(diffs)=0.0175175\n",
      "5: eta=0.33275 cost=0.0645488 jtype=constrained costheta=-0.719 ps=[0.274727, 0.41869, 0.0155098, -1.08603, 0.104242]\n",
      "-- cost=0.0614765,   cost1=0.0615494, cost2=-7.28586e-05 :  mean(hits)=0.501909, mean(diffs)=0.0242862\n",
      "6: eta=0.366025 cost=0.0614765 jtype=constrained costheta=-0.946 ps=[0.455695, 0.244242, 0.132879, -1.25726, 0.105988]\n",
      "-- cost=0.0486282,   cost1=0.0487824, cost2=-0.000154226 :  mean(hits)=0.529133, mean(diffs)=0.0514088\n",
      "7: eta=0.402628 cost=0.0486282 jtype=constrained costheta=-0.997 ps=[0.606886, 0.105449, 0.312739, -1.48064, 0.108562]\n",
      "-- cost=0.0133079,   cost1=0.0139258, cost2=-0.00061789 :  mean(hits)=0.631993, mean(diffs)=0.205963\n",
      "8: eta=0.44289 cost=0.0133079 jtype=constrained costheta=-1.000 ps=[0.755649, -0.026157, 0.504166, -1.74103, 0.111807]\n",
      "-- cost=0.000257413,   cost1=0.00210511, cost2=-0.00184769 :  mean(hits)=0.795881, mean(diffs)=0.615897\n",
      "9: eta=0.487179 cost=0.000257413 jtype=constrained costheta=-0.978 ps=[0.975464, -0.236981, 0.654068, -1.97877, 0.116327]\n",
      "-- cost=-0.00157669,   cost1=6.96102e-07, cost2=-0.00157738 :  mean(hits)=0.750834, mean(diffs)=0.525794\n",
      "10: eta=0.535897 cost=-0.00157669 jtype=Newton costheta=-0.587 ps=[1.02164, -0.0108635, 0.62631, -1.9228, 0.117996]\n",
      "-- cost=-0.00192434,   cost1=0.000676274, cost2=-0.00260062 :  mean(hits)=0.723995, mean(diffs)=0.866872\n",
      "11: eta=0.589487 cost=-0.00192434 jtype=constrained costheta=-0.796 ps=[0.821785, 0.247223, 0.818342, -2.23205, 0.123257]\n",
      "-- cost=-0.00271411,   cost1=6.39525e-05, cost2=-0.00277807 :  mean(hits)=0.742003, mean(diffs)=0.926022\n",
      "12: eta=0.648436 cost=-0.00271411 jtype=Newton costheta=-0.556 ps=[0.821891, 0.193543, 0.875512, -2.32724, 0.121511]\n",
      "-- cost=-0.00284202,   cost1=2.05373e-05, cost2=-0.00286256 :  mean(hits)=0.745468, mean(diffs)=0.954186\n",
      "13: eta=0.713279 cost=-0.00284202 jtype=Newton costheta=-0.248 ps=[0.72397, 0.0815275, 0.92699, -2.41553, 0.119098]\n",
      "-- cost=-0.00288506,   cost1=8.78345e-06, cost2=-0.00289385 :  mean(hits)=0.747036, mean(diffs)=0.964616\n",
      "14: eta=0.784607 cost=-0.00288506 jtype=Newton costheta=-0.210 ps=[0.603214, -0.0548918, 0.975406, -2.49512, 0.116614]\n",
      "-- cost=-0.00290096,   cost1=6.46895e-06, cost2=-0.00290743 :  mean(hits)=0.747457, mean(diffs)=0.969143\n",
      "15: eta=0.863068 cost=-0.00290096 jtype=Newton costheta=-0.147 ps=[0.375301, -0.293363, 1.03034, -2.58254, 0.1136]\n",
      "-- cost=-0.00291064,   cost1=1.10793e-05, cost2=-0.00292172 :  mean(hits)=0.746671, mean(diffs)=0.973907\n",
      "16: eta=0.949375 cost=-0.00291064 jtype=Newton costheta=-0.125 ps=[-0.101739, -0.779492, 1.10727, -2.71391, 0.109042]\n",
      "-- cost=-0.00209882,   cost1=0.000899802, cost2=-0.00299862 :  mean(hits)=0.779997, mean(diffs)=0.999541\n",
      "eta going down: new_cost-cost=0.000811819 and jumptype='constrained'\n",
      "17: eta=0.474687 cost=-0.00291064 jtype=constrained costheta=NaN ps=[-0.101739, -0.779492, 1.10727, -2.71391, 0.109042]\n",
      "-- cost=-0.00229665,   cost1=0.000648625, cost2=-0.00294527 :  mean(hits)=0.775468, mean(diffs)=0.981758\n",
      "eta going down: new_cost-cost=0.000613992 and jumptype='constrained'\n",
      "18: eta=0.237344 cost=-0.00291064 jtype=constrained costheta=NaN ps=[-0.101739, -0.779492, 1.10727, -2.71391, 0.109042]\n",
      "-- cost=-0.00286845,   cost1=4.55062e-05, cost2=-0.00291396 :  mean(hits)=0.756746, mean(diffs)=0.971319\n",
      "eta going down: new_cost-cost=4.21903e-05 and jumptype='constrained'\n",
      "19: eta=0.118672 cost=-0.00291064 jtype=constrained costheta=NaN ps=[-0.101739, -0.779492, 1.10727, -2.71391, 0.109042]\n",
      "-- cost=-0.00291069,   cost1=1.37804e-06, cost2=-0.00291207 :  mean(hits)=0.751174, mean(diffs)=0.97069\n",
      "20: eta=0.130539 cost=-0.00291069 jtype=constrained costheta=-0.090 ps=[-0.0962228, -0.781568, 1.15767, -2.78912, 0.110374]\n",
      "-- cost=-0.00292144,   cost1=7.15913e-06, cost2=-0.0029286 :  mean(hits)=0.747324, mean(diffs)=0.976199\n",
      "21: eta=0.143593 cost=-0.00292144 jtype=constrained costheta=-0.039 ps=[-0.0946473, -0.771467, 1.21975, -2.86746, 0.111792]\n",
      "-- cost=-0.00292227,   cost1=3.77572e-06, cost2=-0.00292604 :  mean(hits)=0.751943, mean(diffs)=0.975347\n",
      "22: eta=0.157952 cost=-0.00292227 jtype=constrained costheta=-0.035 ps=[-0.0923132, -0.770222, 1.27629, -2.95537, 0.11353]\n",
      "-- cost=-0.00292847,   cost1=9.24347e-06, cost2=-0.00293772 :  mean(hits)=0.74696, mean(diffs)=0.979239\n",
      "23: eta=0.173747 cost=-0.00292847 jtype=constrained costheta=-0.018 ps=[-0.0800473, -0.749842, 1.36128, -3.04012, 0.115212]\n",
      "-- cost=-0.00291325,   cost1=3.26152e-05, cost2=-0.00294587 :  mean(hits)=0.755711, mean(diffs)=0.981955\n",
      "eta going down: new_cost-cost=1.52223e-05 and jumptype='constrained'\n",
      "24: eta=0.0868737 cost=-0.00292847 jtype=constrained costheta=NaN ps=[-0.0800473, -0.749842, 1.36128, -3.04012, 0.115212]\n",
      "-- cost=-0.00292713,   cost1=9.25892e-06, cost2=-0.00293639 :  mean(hits)=0.753043, mean(diffs)=0.978795\n",
      "eta going down: new_cost-cost=1.34679e-06 and jumptype='constrained'\n",
      "25: eta=0.0434369 cost=-0.00292847 jtype=constrained costheta=NaN ps=[-0.0800473, -0.749842, 1.36128, -3.04012, 0.115212]\n",
      "-- cost=-0.00293064,   cost1=3.26848e-06, cost2=-0.00293391 :  mean(hits)=0.751808, mean(diffs)=0.97797\n",
      "26: eta=0.0477805 cost=-0.00293064 jtype=constrained costheta=-0.042 ps=[-0.0780774, -0.74942, 1.37648, -3.06588, 0.115771]\n",
      "-- cost=-0.00293452,   cost1=1.12552e-06, cost2=-0.00293565 :  mean(hits)=0.748939, mean(diffs)=0.97855\n",
      "27: eta=0.0525586 cost=-0.00293452 jtype=constrained costheta=-0.022 ps=[-0.0767747, -0.745522, 1.39617, -3.0931, 0.116353]\n",
      "-- cost=-0.00293575,   cost1=2.00775e-09, cost2=-0.00293575 :  mean(hits)=0.750045, mean(diffs)=0.978583\n",
      "28: eta=0.0578145 cost=-0.00293575 jtype=constrained costheta=-0.011 ps=[-0.0774677, -0.745455, 1.41562, -3.12338, 0.117064]\n",
      "-- cost=-0.00293654,   cost1=6.97725e-08, cost2=-0.00293661 :  mean(hits)=0.749736, mean(diffs)=0.978869\n",
      "29: eta=0.0635959 cost=-0.00293654 jtype=constrained costheta=-0.013 ps=[-0.0783256, -0.744805, 1.44056, -3.15536, 0.117805]\n",
      "-- cost=-0.00293718,   cost1=1.22753e-08, cost2=-0.00293719 :  mean(hits)=0.749889, mean(diffs)=0.979063\n",
      "30: eta=0.0699555 cost=-0.00293718 jtype=constrained costheta=-0.018 ps=[-0.0801053, -0.745193, 1.46612, -3.19054, 0.118677]\n",
      "-- cost=-0.00293768,   cost1=2.15906e-08, cost2=-0.0029377 :  mean(hits)=0.749853, mean(diffs)=0.979234\n",
      "31: eta=0.076951 cost=-0.00293768 jtype=constrained costheta=-0.096 ps=[-0.0821453, -0.745674, 1.49504, -3.22846, 0.119642]\n",
      "-- cost=-0.00293807,   cost1=1.7614e-08, cost2=-0.00293809 :  mean(hits)=0.749867, mean(diffs)=0.979363\n",
      "32: eta=0.0846461 cost=-0.00293807 jtype=constrained costheta=-0.009 ps=[-0.0852402, -0.747187, 1.52449, -3.26998, 0.120797]\n",
      "-- cost=-0.00293835,   cost1=2.79069e-08, cost2=-0.00293838 :  mean(hits)=0.749833, mean(diffs)=0.979461\n",
      "33: eta=0.0931108 cost=-0.00293835 jtype=constrained costheta=-0.005 ps=[-0.0894229, -0.749758, 1.55398, -3.31527, 0.122236]\n",
      "-- cost=-0.00293856,   cost1=3.56531e-08, cost2=-0.00293859 :  mean(hits)=0.749811, mean(diffs)=0.979531\n",
      "34: eta=0.102422 cost=-0.00293856 jtype=constrained costheta=-0.002 ps=[-0.0946094, -0.75342, 1.57897, -3.36357, 0.125003]\n",
      "-- cost=-0.00293869,   cost1=4.14224e-08, cost2=-0.00293873 :  mean(hits)=0.749796, mean(diffs)=0.979576\n",
      "35: eta=0.112664 cost=-0.00293869 jtype=Newton costheta=-0.001 ps=[-0.0998685, -0.757333, 1.60175, -3.41426, 0.121983]\n",
      "-- cost=-0.00293878,   cost1=8.36299e-09, cost2=-0.00293879 :  mean(hits)=0.749909, mean(diffs)=0.979596\n",
      "36: eta=0.12393 cost=-0.00293878 jtype=Newton costheta=-0.001 ps=[-0.103349, -0.759929, 1.61709, -3.44911, 0.119844]\n",
      "-- cost=-0.00293881,   cost1=8.37884e-08, cost2=-0.00293889 :  mean(hits)=0.749711, mean(diffs)=0.97963\n",
      "37: eta=0.136323 cost=-0.00293881 jtype=Newton costheta=-0.001 ps=[-0.108362, -0.763726, 1.63877, -3.49622, 0.116854]\n",
      "-- cost=-0.00293888,   cost1=4.20086e-12, cost2=-0.00293888 :  mean(hits)=0.749998, mean(diffs)=0.979628\n",
      "38: eta=0.149956 cost=-0.00293888 jtype=Newton costheta=-0.001 ps=[-0.109818, -0.764804, 1.64566, -3.51266, 0.115794]\n",
      "-- cost=-0.0029377,   cost1=1.92904e-06, cost2=-0.00293963 :  mean(hits)=0.748611, mean(diffs)=0.979877\n",
      "eta going down: new_cost-cost=1.18179e-06 and jumptype='constrained'\n",
      "39: eta=0.0749779 cost=-0.00293888 jtype=constrained costheta=NaN ps=[-0.109818, -0.764804, 1.64566, -3.51266, 0.115794]\n",
      "-- cost=-0.00293887,   cost1=1.04061e-07, cost2=-0.00293897 :  mean(hits)=0.749677, mean(diffs)=0.979657\n",
      "eta going down: new_cost-cost=1.78223e-08 and jumptype='constrained'\n",
      "40: eta=0.037489 cost=-0.00293888 jtype=constrained costheta=NaN ps=[-0.109818, -0.764804, 1.64566, -3.51266, 0.115794]\n",
      "-- cost=-0.00293891,   cost1=6.13699e-09, cost2=-0.00293891 :  mean(hits)=0.749922, mean(diffs)=0.979638\n",
      "41: eta=0.0412378 cost=-0.00293891 jtype=constrained costheta=-0.029 ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00209909,   cost1=0.000899869, cost2=-0.00299895 :  mean(hits)=0.779998, mean(diffs)=0.999652\n",
      "eta going down: new_cost-cost=0.000839821 and jumptype='constrained'\n",
      "42: eta=0.0206189 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00209908,   cost1=0.000899865, cost2=-0.00299894 :  mean(hits)=0.779998, mean(diffs)=0.999648\n",
      "eta going down: new_cost-cost=0.000839828 and jumptype='constrained'\n",
      "43: eta=0.0103095 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00209904,   cost1=0.000899838, cost2=-0.00299888 :  mean(hits)=0.779997, mean(diffs)=0.999625\n",
      "eta going down: new_cost-cost=0.000839869 and jumptype='constrained'\n",
      "44: eta=0.00515473 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00228001,   cost1=0.000672223, cost2=-0.00295223 :  mean(hits)=0.775927, mean(diffs)=0.984078\n",
      "eta going down: new_cost-cost=0.000658897 and jumptype='constrained'\n",
      "45: eta=0.00257737 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.0028976,   cost1=0.000100016, cost2=-0.00299761 :  mean(hits)=0.760001, mean(diffs)=0.999204\n",
      "eta going down: new_cost-cost=4.13094e-05 and jumptype='constrained'\n",
      "46: eta=0.00128868 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00289594,   cost1=9.69889e-05, cost2=-0.00299293 :  mean(hits)=0.759848, mean(diffs)=0.997645\n",
      "eta going down: new_cost-cost=4.29624e-05 and jumptype='constrained'\n",
      "47: eta=0.000644341 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00289846,   cost1=7.42642e-05, cost2=-0.00297273 :  mean(hits)=0.758618, mean(diffs)=0.990909\n",
      "eta going down: new_cost-cost=4.0444e-05 and jumptype='constrained'\n",
      "48: eta=0.000322171 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00291812,   cost1=3.3319e-05, cost2=-0.00295144 :  mean(hits)=0.755772, mean(diffs)=0.983815\n",
      "eta going down: new_cost-cost=2.07823e-05 and jumptype='constrained'\n",
      "49: eta=0.000161085 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00293241,   cost1=9.96215e-06, cost2=-0.00294237 :  mean(hits)=0.753156, mean(diffs)=0.980791\n",
      "eta going down: new_cost-cost=6.49675e-06 and jumptype='constrained'\n",
      "50: eta=8.05427e-05 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00293726,   cost1=2.5067e-06, cost2=-0.00293976 :  mean(hits)=0.751583, mean(diffs)=0.979921\n",
      "eta going down: new_cost-cost=1.65185e-06 and jumptype='constrained'\n",
      "51: eta=4.02713e-05 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00293853,   cost1=5.74493e-07, cost2=-0.0029391 :  mean(hits)=0.750758, mean(diffs)=0.979702\n",
      "eta going down: new_cost-cost=3.76922e-07 and jumptype='constrained'\n",
      "52: eta=2.01357e-05 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00293883,   cost1=1.15893e-07, cost2=-0.00293895 :  mean(hits)=0.75034, mean(diffs)=0.97965\n",
      "eta going down: new_cost-cost=7.3048e-08 and jumptype='constrained'\n",
      "53: eta=1.00678e-05 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.0029389,   cost1=1.71889e-08, cost2=-0.00293892 :  mean(hits)=0.750131, mean(diffs)=0.979639\n",
      "About to break -- tol=1e-08, new_cost-cost=7.49351e-09, eta=1.00678e-05\n",
      "-- cost=-0.00293891,   cost1=6.13699e-09, cost2=-0.00293891 :  mean(hits)=0.749922, mean(diffs)=0.979638\n",
      "-- cost=-0.00293891,   cost1=6.13699e-09, cost2=-0.00293891 :  mean(hits)=0.749922, mean(diffs)=0.979638\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1×2 Array{Float64,2}:\n",
       " 0.487796  0.749922"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following sequence leads to a situation where having only [-0.8, -0.8] as the single finalFluxPoint \n",
    "# leads to the minimization getting stuck.  Adding further finalFluxPoints solves the problem\n",
    "\n",
    "dt = 0.02\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "mW = -4\n",
    "sW = 0.2\n",
    "noise = 0\n",
    "input = 0\n",
    "sigma = 0.1\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :sW=>sW, :mW=>mW, :W=>[sW mW ; mW sW], :nsteps=>nsteps, \n",
    ":noise=>noise, :input=>input, :sigma=>sigma, :const_add=>0, :init_add=>0)\n",
    "\n",
    "\n",
    "# WORKING gradient:\n",
    "# ForwardDiff.gradient((x)->JJ(startU; do_plot=true, nderivs=length(x), difforder=1, \n",
    "#    make_dict([[\"init_add\" 2], \"const_add\"], x, model_params)...), [2.9, -2.9, 0.1])\n",
    "\n",
    "\n",
    "\n",
    "# The backward and costfunc functions should turn a single-scalar parameter W into the matrix W\n",
    "# backward always runs with no within-forward noise, i.e., sigma=0\n",
    "backward = (endpoint; do_plot=false, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if ~haskey(pars, :sW) || ~haskey(pars, :mW) \n",
    "        error(\"Need both sW and mW to determine weight matrix\")\n",
    "    end\n",
    "    pars=make_dict([\"W\"], [[pars[:sW] pars[:mW] ; pars[:mW] pars[:sW]]], pars);\n",
    "    backwardsModel(endpoint; do_plot=do_plot, make_dict([\"sigma\"], [0], pars)...)[1]\n",
    "end\n",
    "\n",
    "forward = (startpoint; pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if ~haskey(pars, :sW) || ~haskey(pars, :mW) \n",
    "        error(\"Need both sW and mW to determine weight matrix\")\n",
    "    end\n",
    "    pars=make_dict([\"W\"], [[pars[:sW] pars[:mW] ; pars[:mW] pars[:sW]]], pars);\n",
    "    forwardsModel(startpoint; pars...)\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "costfunc = (startpoints; do_plot=false, verbose=false, beta=0.003, nderivs=0, difforder=0, sr=26, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if ~haskey(pars, :sW) || ~haskey(pars, :mW) \n",
    "        error(\"Need both sW and mW to determine weight matrix\")\n",
    "    end\n",
    "    pars=make_dict([\"W\"], [[pars[:sW] pars[:mW] ; pars[:mW] pars[:sW]]], pars);\n",
    "    JJ(startpoints; seedrand=sr, beta=beta, \n",
    "        do_plot=do_plot, verbose=verbose, nderivs=nderivs, difforder=difforder, pars...)\n",
    "end\n",
    "\n",
    "function JJ(initUs; theta1=0.15, theta2=0.2, beta=0.003, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, pre_string=\"\", zero_last_sigmas=0, seedrand=NaN, params...)\n",
    "\n",
    "    if ~isnan(seedrand); srand(seedrand); end\n",
    "    \n",
    "    Vend = ForwardDiffZeros(size(initUs,1), size(initUs,2), nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:size(initUs,1)\n",
    "        if false # i>size(initUs,1) - zero_last_sigmas\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; sigma=0, nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)            \n",
    "        else\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)\n",
    "        end\n",
    "        Vend[i,:] = Ve\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend[:,1]-Vend[:,2])/theta1))\n",
    "    diffs = tanh.((Vend[:,1]-Vend[:,2])/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "\n",
    "    if do_plot\n",
    "        title(@sprintf(\"mean(hits)=%g, mean(diffs)=%g\", convert(Float64, mean(hits)), convert(Float64, mean(diffs))))\n",
    "    end\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"%s\", pre_string)\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2, mean(hits), mean(diffs)\n",
    "end\n",
    "  \n",
    "\n",
    "\n",
    "beta = 0.003\n",
    "args = [[\"start_add\" 2], \"const_add\", \"mW\", \"sW\"] \n",
    "seed = [-0.2, 0.2, 0.2, -1.5, 0.1] \n",
    "walls = Dict(:start_add=>[-5.1, 5.1], :sW=>[0, 5.1], :mW=>[-5.1, 5.1]) # \n",
    "# sr =  gives \n",
    "\n",
    "\n",
    "new_random_seed = true; if new_random_seed\n",
    "    sr = convert(Int64, round(time()))\n",
    "else\n",
    "    sr = old_sr\n",
    "end\n",
    "# sr = 1504716566\n",
    "old_sr = sr\n",
    "\n",
    "srand(sr)\n",
    "\n",
    "startU=randn(50,2)-3\n",
    "\n",
    "\n",
    "clf()\n",
    "print(\"seed = \"); print_vector_g(seed); print(\"\\n\")\n",
    "ocost, omhits, omdiffs = costfunc(startU; do_plot=true, sr=sr, verbose=true, make_dict(args, seed, model_params)...)\n",
    "\n",
    "\n",
    "params, traj, zz, cpm_traj = bbox_Hessian_keyword_minimization(seed, args, walls,  \n",
    "(;params...) -> costfunc(startU; beta=beta, sr=sr, do_plot=true, verbose=true, merge(model_params, Dict(params))...)[1], \n",
    "verbose=true, start_eta=1, tol=1e-8, softbox=true, maxiter=400 )\n",
    "\n",
    "# params, cost, ptraj, gtraj = fluxSense(costfunc, backward, model_params, startU, fluxFinalPoint, args, seed; \n",
    "#    start_eta=0.01, tol=1e-15, maxiter=400, verbose=true, report_every=1, do_plot=false, cost_limit=cost_limit) # cost_limit=-0.000935) # for beta=0.01\n",
    "\n",
    "# And show the final position\n",
    "figure(1); clf()\n",
    "cost, mhits, mdiffs = \n",
    "    costfunc(startU; beta=beta, do_plot=true, sr=sr, verbose=true, make_dict(args, params, model_params)...)\n",
    "\n",
    "repeat_results_in_fig2 = true; if repeat_results_in_fig2\n",
    "    figure(2); clf()\n",
    "    costfunc(startU; beta=beta, do_plot=true, sr=sr, verbose=true, \n",
    "        make_dict(args, params, merge(Dict(:fignum=>2), model_params))...)\n",
    "    figure(1); \n",
    "end\n",
    "params'\n",
    "\n",
    "# For beta=0, and ntrials=20, we collected a bunch of results and observed that it failes about half the time\n",
    "# WHEN the initial mean)hits) is below 0.5.  It never fails if the initial mean(hits) is above 0.5. \n",
    "# Seems like when it starts below 0.5, the fastest way to increase mean(hits) is to push it to 0.5 and floor it there.\n",
    "# The results were collected in \"Results.mat\"\n",
    "# res = [res ; omhits mhits]\n",
    "\n",
    "[omhits mhits]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of successful minimization including self-connection weights\n",
    "\n",
    "Amazing. Adding self-connection weights seems to make the minimization process even more robust: we can now start from start_add = [-0.2, 0.2], which usually gives mean(hits) < 0.5, and **still** reach a successful solution.  Without the self-connection weights that rarely happened, mean(hits) < 0.5 was a bad start, and start_add = [-0.2, 0.2] most often did not lead to success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed = [-0.2, 0.2, 0.2, -1.5, 0.1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition JJ(Any) in module Main at In[47]:62 overwritten at In[48]:62.\n",
      "WARNING: Method definition #JJ(Array{Any, 1}, Main.#JJ, Any) in module Main overwritten.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- cost=0.0686324,   cost1=0.0687511, cost2=-0.00011865 :  mean(hits)=0.487796, mean(diffs)=0.0395499\n",
      "0: eta=1 ps=[-0.200, 0.200, 0.200, -1.500, 0.100]\n",
      "-- cost=0.0686324,   cost1=0.0687511, cost2=-0.00011865 :  mean(hits)=0.487796, mean(diffs)=0.0395499\n",
      "-- cost=0.0662608,   cost1=0.0663306, cost2=-6.98297e-05 :  mean(hits)=0.492453, mean(diffs)=0.0232766\n",
      "1: eta=1.1 cost=0.0662608 jtype=Newton costheta=-0.343 ps=[-0.112938, 0.349709, 0.101748, -1.26536, 0.103245]\n",
      "-- cost=0.0656768,   cost1=0.0657292, cost2=-5.24826e-05 :  mean(hits)=0.493623, mean(diffs)=0.0174942\n",
      "2: eta=1.21 cost=0.0656768 jtype=Newton costheta=-0.188 ps=[0.0772162, 0.616916, 0.0618108, -0.989811, 0.103702]\n",
      "-- cost=0.0658205,   cost1=0.0658659, cost2=-4.54283e-05 :  mean(hits)=0.493357, mean(diffs)=0.0151428\n",
      "eta going down: new_cost-cost=0.000143696 and jumptype='Newton'\n",
      "3: eta=0.605 cost=0.0656768 jtype=Newton costheta=NaN ps=[0.0772162, 0.616916, 0.0618108, -0.989811, 0.103702]\n",
      "-- cost=0.0658205,   cost1=0.0658659, cost2=-4.54283e-05 :  mean(hits)=0.493357, mean(diffs)=0.0151428\n",
      "eta going down: new_cost-cost=0.000143696 and jumptype='Newton'\n",
      "4: eta=0.3025 cost=0.0656768 jtype=Newton costheta=NaN ps=[0.0772162, 0.616916, 0.0618108, -0.989811, 0.103702]\n",
      "-- cost=0.0645488,   cost1=0.0646014, cost2=-5.25525e-05 :  mean(hits)=0.495832, mean(diffs)=0.0175175\n",
      "5: eta=0.33275 cost=0.0645488 jtype=constrained costheta=-0.719 ps=[0.274727, 0.41869, 0.0155098, -1.08603, 0.104242]\n",
      "-- cost=0.0614765,   cost1=0.0615494, cost2=-7.28586e-05 :  mean(hits)=0.501909, mean(diffs)=0.0242862\n",
      "6: eta=0.366025 cost=0.0614765 jtype=constrained costheta=-0.946 ps=[0.455695, 0.244242, 0.132879, -1.25726, 0.105988]\n",
      "-- cost=0.0486282,   cost1=0.0487824, cost2=-0.000154226 :  mean(hits)=0.529133, mean(diffs)=0.0514088\n",
      "7: eta=0.402628 cost=0.0486282 jtype=constrained costheta=-0.997 ps=[0.606886, 0.105449, 0.312739, -1.48064, 0.108562]\n",
      "-- cost=0.0133079,   cost1=0.0139258, cost2=-0.00061789 :  mean(hits)=0.631993, mean(diffs)=0.205963\n",
      "8: eta=0.44289 cost=0.0133079 jtype=constrained costheta=-1.000 ps=[0.755649, -0.026157, 0.504166, -1.74103, 0.111807]\n",
      "-- cost=0.000257413,   cost1=0.00210511, cost2=-0.00184769 :  mean(hits)=0.795881, mean(diffs)=0.615897\n",
      "9: eta=0.487179 cost=0.000257413 jtype=constrained costheta=-0.978 ps=[0.975464, -0.236981, 0.654068, -1.97877, 0.116327]\n",
      "-- cost=-0.00157669,   cost1=6.96102e-07, cost2=-0.00157738 :  mean(hits)=0.750834, mean(diffs)=0.525794\n",
      "10: eta=0.535897 cost=-0.00157669 jtype=Newton costheta=-0.587 ps=[1.02164, -0.0108635, 0.62631, -1.9228, 0.117996]\n",
      "-- cost=-0.00192434,   cost1=0.000676274, cost2=-0.00260062 :  mean(hits)=0.723995, mean(diffs)=0.866872\n",
      "11: eta=0.589487 cost=-0.00192434 jtype=constrained costheta=-0.796 ps=[0.821785, 0.247223, 0.818342, -2.23205, 0.123257]\n",
      "-- cost=-0.00271411,   cost1=6.39525e-05, cost2=-0.00277807 :  mean(hits)=0.742003, mean(diffs)=0.926022\n",
      "12: eta=0.648436 cost=-0.00271411 jtype=Newton costheta=-0.556 ps=[0.821891, 0.193543, 0.875512, -2.32724, 0.121511]\n",
      "-- cost=-0.00284202,   cost1=2.05373e-05, cost2=-0.00286256 :  mean(hits)=0.745468, mean(diffs)=0.954186\n",
      "13: eta=0.713279 cost=-0.00284202 jtype=Newton costheta=-0.248 ps=[0.72397, 0.0815275, 0.92699, -2.41553, 0.119098]\n",
      "-- cost=-0.00288506,   cost1=8.78345e-06, cost2=-0.00289385 :  mean(hits)=0.747036, mean(diffs)=0.964616\n",
      "14: eta=0.784607 cost=-0.00288506 jtype=Newton costheta=-0.210 ps=[0.603214, -0.0548918, 0.975406, -2.49512, 0.116614]\n",
      "-- cost=-0.00290096,   cost1=6.46895e-06, cost2=-0.00290743 :  mean(hits)=0.747457, mean(diffs)=0.969143\n",
      "15: eta=0.863068 cost=-0.00290096 jtype=Newton costheta=-0.147 ps=[0.375301, -0.293363, 1.03034, -2.58254, 0.1136]\n",
      "-- cost=-0.00291064,   cost1=1.10793e-05, cost2=-0.00292172 :  mean(hits)=0.746671, mean(diffs)=0.973907\n",
      "16: eta=0.949375 cost=-0.00291064 jtype=Newton costheta=-0.125 ps=[-0.101739, -0.779492, 1.10727, -2.71391, 0.109042]\n",
      "-- cost=-0.00209882,   cost1=0.000899802, cost2=-0.00299862 :  mean(hits)=0.779997, mean(diffs)=0.999541\n",
      "eta going down: new_cost-cost=0.000811819 and jumptype='constrained'\n",
      "17: eta=0.474687 cost=-0.00291064 jtype=constrained costheta=NaN ps=[-0.101739, -0.779492, 1.10727, -2.71391, 0.109042]\n",
      "-- cost=-0.00229665,   cost1=0.000648625, cost2=-0.00294527 :  mean(hits)=0.775468, mean(diffs)=0.981758\n",
      "eta going down: new_cost-cost=0.000613992 and jumptype='constrained'\n",
      "18: eta=0.237344 cost=-0.00291064 jtype=constrained costheta=NaN ps=[-0.101739, -0.779492, 1.10727, -2.71391, 0.109042]\n",
      "-- cost=-0.00286845,   cost1=4.55062e-05, cost2=-0.00291396 :  mean(hits)=0.756746, mean(diffs)=0.971319\n",
      "eta going down: new_cost-cost=4.21903e-05 and jumptype='constrained'\n",
      "19: eta=0.118672 cost=-0.00291064 jtype=constrained costheta=NaN ps=[-0.101739, -0.779492, 1.10727, -2.71391, 0.109042]\n",
      "-- cost=-0.00291069,   cost1=1.37804e-06, cost2=-0.00291207 :  mean(hits)=0.751174, mean(diffs)=0.97069\n",
      "20: eta=0.130539 cost=-0.00291069 jtype=constrained costheta=-0.090 ps=[-0.0962228, -0.781568, 1.15767, -2.78912, 0.110374]\n",
      "-- cost=-0.00292144,   cost1=7.15913e-06, cost2=-0.0029286 :  mean(hits)=0.747324, mean(diffs)=0.976199\n",
      "21: eta=0.143593 cost=-0.00292144 jtype=constrained costheta=-0.039 ps=[-0.0946473, -0.771467, 1.21975, -2.86746, 0.111792]\n",
      "-- cost=-0.00292227,   cost1=3.77572e-06, cost2=-0.00292604 :  mean(hits)=0.751943, mean(diffs)=0.975347\n",
      "22: eta=0.157952 cost=-0.00292227 jtype=constrained costheta=-0.035 ps=[-0.0923132, -0.770222, 1.27629, -2.95537, 0.11353]\n",
      "-- cost=-0.00292847,   cost1=9.24347e-06, cost2=-0.00293772 :  mean(hits)=0.74696, mean(diffs)=0.979239\n",
      "23: eta=0.173747 cost=-0.00292847 jtype=constrained costheta=-0.018 ps=[-0.0800473, -0.749842, 1.36128, -3.04012, 0.115212]\n",
      "-- cost=-0.00291325,   cost1=3.26152e-05, cost2=-0.00294587 :  mean(hits)=0.755711, mean(diffs)=0.981955\n",
      "eta going down: new_cost-cost=1.52223e-05 and jumptype='constrained'\n",
      "24: eta=0.0868737 cost=-0.00292847 jtype=constrained costheta=NaN ps=[-0.0800473, -0.749842, 1.36128, -3.04012, 0.115212]\n",
      "-- cost=-0.00292713,   cost1=9.25892e-06, cost2=-0.00293639 :  mean(hits)=0.753043, mean(diffs)=0.978795\n",
      "eta going down: new_cost-cost=1.34679e-06 and jumptype='constrained'\n",
      "25: eta=0.0434369 cost=-0.00292847 jtype=constrained costheta=NaN ps=[-0.0800473, -0.749842, 1.36128, -3.04012, 0.115212]\n",
      "-- cost=-0.00293064,   cost1=3.26848e-06, cost2=-0.00293391 :  mean(hits)=0.751808, mean(diffs)=0.97797\n",
      "26: eta=0.0477805 cost=-0.00293064 jtype=constrained costheta=-0.042 ps=[-0.0780774, -0.74942, 1.37648, -3.06588, 0.115771]\n",
      "-- cost=-0.00293452,   cost1=1.12552e-06, cost2=-0.00293565 :  mean(hits)=0.748939, mean(diffs)=0.97855\n",
      "27: eta=0.0525586 cost=-0.00293452 jtype=constrained costheta=-0.022 ps=[-0.0767747, -0.745522, 1.39617, -3.0931, 0.116353]\n",
      "-- cost=-0.00293575,   cost1=2.00775e-09, cost2=-0.00293575 :  mean(hits)=0.750045, mean(diffs)=0.978583\n",
      "28: eta=0.0578145 cost=-0.00293575 jtype=constrained costheta=-0.011 ps=[-0.0774677, -0.745455, 1.41562, -3.12338, 0.117064]\n",
      "-- cost=-0.00293654,   cost1=6.97725e-08, cost2=-0.00293661 :  mean(hits)=0.749736, mean(diffs)=0.978869\n",
      "29: eta=0.0635959 cost=-0.00293654 jtype=constrained costheta=-0.013 ps=[-0.0783256, -0.744805, 1.44056, -3.15536, 0.117805]\n",
      "-- cost=-0.00293718,   cost1=1.22753e-08, cost2=-0.00293719 :  mean(hits)=0.749889, mean(diffs)=0.979063\n",
      "30: eta=0.0699555 cost=-0.00293718 jtype=constrained costheta=-0.018 ps=[-0.0801053, -0.745193, 1.46612, -3.19054, 0.118677]\n",
      "-- cost=-0.00293768,   cost1=2.15906e-08, cost2=-0.0029377 :  mean(hits)=0.749853, mean(diffs)=0.979234\n",
      "31: eta=0.076951 cost=-0.00293768 jtype=constrained costheta=-0.096 ps=[-0.0821453, -0.745674, 1.49504, -3.22846, 0.119642]\n",
      "-- cost=-0.00293807,   cost1=1.7614e-08, cost2=-0.00293809 :  mean(hits)=0.749867, mean(diffs)=0.979363\n",
      "32: eta=0.0846461 cost=-0.00293807 jtype=constrained costheta=-0.009 ps=[-0.0852402, -0.747187, 1.52449, -3.26998, 0.120797]\n",
      "-- cost=-0.00293835,   cost1=2.79069e-08, cost2=-0.00293838 :  mean(hits)=0.749833, mean(diffs)=0.979461\n",
      "33: eta=0.0931108 cost=-0.00293835 jtype=constrained costheta=-0.005 ps=[-0.0894229, -0.749758, 1.55398, -3.31527, 0.122236]\n",
      "-- cost=-0.00293856,   cost1=3.56531e-08, cost2=-0.00293859 :  mean(hits)=0.749811, mean(diffs)=0.979531\n",
      "34: eta=0.102422 cost=-0.00293856 jtype=constrained costheta=-0.002 ps=[-0.0946094, -0.75342, 1.57897, -3.36357, 0.125003]\n",
      "-- cost=-0.00293869,   cost1=4.14224e-08, cost2=-0.00293873 :  mean(hits)=0.749796, mean(diffs)=0.979576\n",
      "35: eta=0.112664 cost=-0.00293869 jtype=Newton costheta=-0.001 ps=[-0.0998685, -0.757333, 1.60175, -3.41426, 0.121983]\n",
      "-- cost=-0.00293878,   cost1=8.36299e-09, cost2=-0.00293879 :  mean(hits)=0.749909, mean(diffs)=0.979596\n",
      "36: eta=0.12393 cost=-0.00293878 jtype=Newton costheta=-0.001 ps=[-0.103349, -0.759929, 1.61709, -3.44911, 0.119844]\n",
      "-- cost=-0.00293881,   cost1=8.37884e-08, cost2=-0.00293889 :  mean(hits)=0.749711, mean(diffs)=0.97963\n",
      "37: eta=0.136323 cost=-0.00293881 jtype=Newton costheta=-0.001 ps=[-0.108362, -0.763726, 1.63877, -3.49622, 0.116854]\n",
      "-- cost=-0.00293888,   cost1=4.20086e-12, cost2=-0.00293888 :  mean(hits)=0.749998, mean(diffs)=0.979628\n",
      "38: eta=0.149956 cost=-0.00293888 jtype=Newton costheta=-0.001 ps=[-0.109818, -0.764804, 1.64566, -3.51266, 0.115794]\n",
      "-- cost=-0.0029377,   cost1=1.92904e-06, cost2=-0.00293963 :  mean(hits)=0.748611, mean(diffs)=0.979877\n",
      "eta going down: new_cost-cost=1.18179e-06 and jumptype='constrained'\n",
      "39: eta=0.0749779 cost=-0.00293888 jtype=constrained costheta=NaN ps=[-0.109818, -0.764804, 1.64566, -3.51266, 0.115794]\n",
      "-- cost=-0.00293887,   cost1=1.04061e-07, cost2=-0.00293897 :  mean(hits)=0.749677, mean(diffs)=0.979657\n",
      "eta going down: new_cost-cost=1.78223e-08 and jumptype='constrained'\n",
      "40: eta=0.037489 cost=-0.00293888 jtype=constrained costheta=NaN ps=[-0.109818, -0.764804, 1.64566, -3.51266, 0.115794]\n",
      "-- cost=-0.00293891,   cost1=6.13699e-09, cost2=-0.00293891 :  mean(hits)=0.749922, mean(diffs)=0.979638\n",
      "41: eta=0.0412378 cost=-0.00293891 jtype=constrained costheta=-0.029 ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00209909,   cost1=0.000899869, cost2=-0.00299895 :  mean(hits)=0.779998, mean(diffs)=0.999652\n",
      "eta going down: new_cost-cost=0.000839821 and jumptype='constrained'\n",
      "42: eta=0.0206189 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00209908,   cost1=0.000899865, cost2=-0.00299894 :  mean(hits)=0.779998, mean(diffs)=0.999648\n",
      "eta going down: new_cost-cost=0.000839828 and jumptype='constrained'\n",
      "43: eta=0.0103095 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00209904,   cost1=0.000899838, cost2=-0.00299888 :  mean(hits)=0.779997, mean(diffs)=0.999625\n",
      "eta going down: new_cost-cost=0.000839869 and jumptype='constrained'\n",
      "44: eta=0.00515473 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00228001,   cost1=0.000672223, cost2=-0.00295223 :  mean(hits)=0.775927, mean(diffs)=0.984078\n",
      "eta going down: new_cost-cost=0.000658897 and jumptype='constrained'\n",
      "45: eta=0.00257737 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.0028976,   cost1=0.000100016, cost2=-0.00299761 :  mean(hits)=0.760001, mean(diffs)=0.999204\n",
      "eta going down: new_cost-cost=4.13094e-05 and jumptype='constrained'\n",
      "46: eta=0.00128868 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00289594,   cost1=9.69889e-05, cost2=-0.00299293 :  mean(hits)=0.759848, mean(diffs)=0.997645\n",
      "eta going down: new_cost-cost=4.29624e-05 and jumptype='constrained'\n",
      "47: eta=0.000644341 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00289846,   cost1=7.42642e-05, cost2=-0.00297273 :  mean(hits)=0.758618, mean(diffs)=0.990909\n",
      "eta going down: new_cost-cost=4.0444e-05 and jumptype='constrained'\n",
      "48: eta=0.000322171 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00291812,   cost1=3.3319e-05, cost2=-0.00295144 :  mean(hits)=0.755772, mean(diffs)=0.983815\n",
      "eta going down: new_cost-cost=2.07823e-05 and jumptype='constrained'\n",
      "49: eta=0.000161085 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00293241,   cost1=9.96215e-06, cost2=-0.00294237 :  mean(hits)=0.753156, mean(diffs)=0.980791\n",
      "eta going down: new_cost-cost=6.49675e-06 and jumptype='constrained'\n",
      "50: eta=8.05427e-05 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00293726,   cost1=2.5067e-06, cost2=-0.00293976 :  mean(hits)=0.751583, mean(diffs)=0.979921\n",
      "eta going down: new_cost-cost=1.65185e-06 and jumptype='constrained'\n",
      "51: eta=4.02713e-05 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00293853,   cost1=5.74493e-07, cost2=-0.0029391 :  mean(hits)=0.750758, mean(diffs)=0.979702\n",
      "eta going down: new_cost-cost=3.76922e-07 and jumptype='constrained'\n",
      "52: eta=2.01357e-05 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00293883,   cost1=1.15893e-07, cost2=-0.00293895 :  mean(hits)=0.75034, mean(diffs)=0.97965\n",
      "eta going down: new_cost-cost=7.3048e-08 and jumptype='constrained'\n",
      "53: eta=1.00678e-05 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.0029389,   cost1=1.71889e-08, cost2=-0.00293892 :  mean(hits)=0.750131, mean(diffs)=0.979639\n",
      "About to break -- tol=1e-08, new_cost-cost=7.49351e-09, eta=1.00678e-05\n",
      "-- cost=-0.00293891,   cost1=6.13699e-09, cost2=-0.00293891 :  mean(hits)=0.749922, mean(diffs)=0.979638\n",
      "-- cost=-0.00293891,   cost1=6.13699e-09, cost2=-0.00293891 :  mean(hits)=0.749922, mean(diffs)=0.979638\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1×2 Array{Float64,2}:\n",
       " 0.487796  0.749922"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following sequence leads to a situation where having only [-0.8, -0.8] as the single finalFluxPoint \n",
    "# leads to the minimization getting stuck.  Adding further finalFluxPoints solves the problem\n",
    "\n",
    "dt = 0.02\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "mW = -4\n",
    "sW = 0.2\n",
    "noise = 0\n",
    "input = 0\n",
    "sigma = 0.1\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :sW=>sW, :mW=>mW, :W=>[sW mW ; mW sW], :nsteps=>nsteps, \n",
    ":noise=>noise, :input=>input, :sigma=>sigma, :const_add=>0, :init_add=>0)\n",
    "\n",
    "\n",
    "# WORKING gradient:\n",
    "# ForwardDiff.gradient((x)->JJ(startU; do_plot=true, nderivs=length(x), difforder=1, \n",
    "#    make_dict([[\"init_add\" 2], \"const_add\"], x, model_params)...), [2.9, -2.9, 0.1])\n",
    "\n",
    "\n",
    "\n",
    "# The backward and costfunc functions should turn a single-scalar parameter W into the matrix W\n",
    "# backward always runs with no within-forward noise, i.e., sigma=0\n",
    "backward = (endpoint; do_plot=false, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if ~haskey(pars, :sW) || ~haskey(pars, :mW) \n",
    "        error(\"Need both sW and mW to determine weight matrix\")\n",
    "    end\n",
    "    pars=make_dict([\"W\"], [[pars[:sW] pars[:mW] ; pars[:mW] pars[:sW]]], pars);\n",
    "    backwardsModel(endpoint; do_plot=do_plot, make_dict([\"sigma\"], [0], pars)...)[1]\n",
    "end\n",
    "\n",
    "forward = (startpoint; pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if ~haskey(pars, :sW) || ~haskey(pars, :mW) \n",
    "        error(\"Need both sW and mW to determine weight matrix\")\n",
    "    end\n",
    "    pars=make_dict([\"W\"], [[pars[:sW] pars[:mW] ; pars[:mW] pars[:sW]]], pars);\n",
    "    forwardsModel(startpoint; pars...)\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "costfunc = (startpoints; do_plot=false, verbose=false, beta=0.003, nderivs=0, difforder=0, sr=26, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if ~haskey(pars, :sW) || ~haskey(pars, :mW) \n",
    "        error(\"Need both sW and mW to determine weight matrix\")\n",
    "    end\n",
    "    pars=make_dict([\"W\"], [[pars[:sW] pars[:mW] ; pars[:mW] pars[:sW]]], pars);\n",
    "    JJ(startpoints; seedrand=sr, beta=beta, \n",
    "        do_plot=do_plot, verbose=verbose, nderivs=nderivs, difforder=difforder, pars...)\n",
    "end\n",
    "\n",
    "function JJ(initUs; theta1=0.15, theta2=0.2, beta=0.003, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, pre_string=\"\", zero_last_sigmas=0, seedrand=NaN, params...)\n",
    "\n",
    "    if ~isnan(seedrand); srand(seedrand); end\n",
    "    \n",
    "    Vend = ForwardDiffZeros(size(initUs,1), size(initUs,2), nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:size(initUs,1)\n",
    "        if false # i>size(initUs,1) - zero_last_sigmas\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; sigma=0, nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)            \n",
    "        else\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)\n",
    "        end\n",
    "        Vend[i,:] = Ve\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend[:,1]-Vend[:,2])/theta1))\n",
    "    diffs = tanh.((Vend[:,1]-Vend[:,2])/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "\n",
    "    if do_plot\n",
    "        title(@sprintf(\"mean(hits)=%g, mean(diffs)=%g\", convert(Float64, mean(hits)), convert(Float64, mean(diffs))))\n",
    "    end\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"%s\", pre_string)\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2, mean(hits), mean(diffs)\n",
    "end\n",
    "  \n",
    "\n",
    "\n",
    "beta = 0.003\n",
    "args = [[\"start_add\" 2], \"const_add\", \"mW\", \"sW\"] \n",
    "seed = [-0.2, 0.2, 0.2, -1.5, 0.1] \n",
    "walls = Dict(:start_add=>[-5.1, 5.1], :sW=>[0, 5.1], :mW=>[-5.1, 5.1]) # \n",
    "# sr =  gives \n",
    "\n",
    "\n",
    "new_random_seed = true; if new_random_seed\n",
    "    sr = convert(Int64, round(time()))\n",
    "else\n",
    "    sr = old_sr\n",
    "end\n",
    "# sr = 1504716566\n",
    "old_sr = sr\n",
    "\n",
    "srand(sr)\n",
    "\n",
    "startU=randn(50,2)-3\n",
    "\n",
    "\n",
    "clf()\n",
    "print(\"seed = \"); print_vector_g(seed); print(\"\\n\")\n",
    "ocost, omhits, omdiffs = costfunc(startU; do_plot=true, sr=sr, verbose=true, make_dict(args, seed, model_params)...)\n",
    "\n",
    "\n",
    "params, traj, zz, cpm_traj = bbox_Hessian_keyword_minimization(seed, args, walls,  \n",
    "(;params...) -> costfunc(startU; beta=beta, sr=sr, do_plot=true, verbose=true, merge(model_params, Dict(params))...)[1], \n",
    "verbose=true, start_eta=1, tol=1e-8, softbox=true, maxiter=400 )\n",
    "\n",
    "# params, cost, ptraj, gtraj = fluxSense(costfunc, backward, model_params, startU, fluxFinalPoint, args, seed; \n",
    "#    start_eta=0.01, tol=1e-15, maxiter=400, verbose=true, report_every=1, do_plot=false, cost_limit=cost_limit) # cost_limit=-0.000935) # for beta=0.01\n",
    "\n",
    "# And show the final position\n",
    "figure(1); clf()\n",
    "cost, mhits, mdiffs = \n",
    "    costfunc(startU; beta=beta, do_plot=true, sr=sr, verbose=true, make_dict(args, params, model_params)...)\n",
    "\n",
    "repeat_results_in_fig2 = true; if repeat_results_in_fig2\n",
    "    figure(2); clf()\n",
    "    costfunc(startU; beta=beta, do_plot=true, sr=sr, verbose=true, \n",
    "        make_dict(args, params, merge(Dict(:fignum=>2), model_params))...)\n",
    "    figure(1); \n",
    "end\n",
    "params'\n",
    "\n",
    "# For beta=0, and ntrials=20, we collected a bunch of results and observed that it failes about half the time\n",
    "# WHEN the initial mean)hits) is below 0.5.  It never fails if the initial mean(hits) is above 0.5. \n",
    "# Seems like when it starts below 0.5, the fastest way to increase mean(hits) is to push it to 0.5 and floor it there.\n",
    "# The results were collected in \"Results.mat\"\n",
    "# res = [res ; omhits mhits]\n",
    "\n",
    "[omhits mhits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1504716566"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_sr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example that works about half the time-- seesm to follow the patter where initial mean(hits) < 0.5 means half the time go to mean(hits)=0.5 and get stuck there, whereas initial mean(hits)>=0.5 means success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed = [-0.2, 0.2, 0.2, -1.5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition JJ(Any) in module Main at In[17]:61 overwritten at In[19]:61.\n",
      "WARNING: Method definition #JJ(Array{Any, 1}, Main.#JJ, Any) in module Main overwritten.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- cost=0.0618702,   cost1=0.0619307, cost2=-6.04596e-05 :  mean(hits)=0.501141, mean(diffs)=0.0201532\n",
      "0: eta=1 ps=[-0.200, 0.200, 0.200, -1.500]\n",
      "-- cost=0.0618702,   cost1=0.0619307, cost2=-6.04596e-05 :  mean(hits)=0.501141, mean(diffs)=0.0201532\n",
      "-- cost=0.0609813,   cost1=0.0610149, cost2=-3.36142e-05 :  mean(hits)=0.502988, mean(diffs)=0.0112047\n",
      "1: eta=1.1 cost=0.0609813 jtype=Newton costheta=-0.368 ps=[-0.187942, 0.181484, -0.0195354, -1.39132]\n",
      "-- cost=0.0613172,   cost1=0.0613271, cost2=-9.90029e-06 :  mean(hits)=0.502357, mean(diffs)=0.0033001\n",
      "eta going down: new_cost-cost=0.000335981 and jumptype='Newton'\n",
      "2: eta=0.55 cost=0.0609813 jtype=Newton costheta=NaN ps=[-0.187942, 0.181484, -0.0195354, -1.39132]\n",
      "-- cost=0.0533101,   cost1=0.0533805, cost2=-7.04243e-05 :  mean(hits)=0.518958, mean(diffs)=0.0234748\n",
      "3: eta=0.605 cost=0.0533101 jtype=constrained costheta=-0.697 ps=[0.148346, -0.140224, 0.230154, -1.53175]\n",
      "-- cost=0.00648721,   cost1=0.00724268, cost2=-0.000755464 :  mean(hits)=0.664896, mean(diffs)=0.251821\n",
      "4: eta=0.6655 cost=0.00648721 jtype=constrained costheta=-0.996 ps=[0.353552, -0.315945, 0.585897, -1.89314]\n",
      "-- cost=-0.00125426,   cost1=0.000230058, cost2=-0.00148431 :  mean(hits)=0.765168, mean(diffs)=0.494771\n",
      "5: eta=0.73205 cost=-0.00125426 jtype=Newton costheta=-0.778 ps=[0.398471, -0.569932, 0.661777, -2.02273]\n",
      "-- cost=-0.00110447,   cost1=1.37151e-05, cost2=-0.00111819 :  mean(hits)=0.746297, mean(diffs)=0.372729\n",
      "eta going down: new_cost-cost=0.000149782 and jumptype='Newton'\n",
      "6: eta=0.366025 cost=-0.00125426 jtype=Newton costheta=NaN ps=[0.398471, -0.569932, 0.661777, -2.02273]\n",
      "-- cost=-0.00110447,   cost1=1.37151e-05, cost2=-0.00111819 :  mean(hits)=0.746297, mean(diffs)=0.372729\n",
      "eta going down: new_cost-cost=0.000149782 and jumptype='Newton'\n",
      "7: eta=0.183013 cost=-0.00125426 jtype=Newton costheta=NaN ps=[0.398471, -0.569932, 0.661777, -2.02273]\n",
      "-- cost=-0.00110447,   cost1=1.37151e-05, cost2=-0.00111819 :  mean(hits)=0.746297, mean(diffs)=0.372729\n",
      "eta going down: new_cost-cost=0.000149782 and jumptype='Newton'\n",
      "8: eta=0.0915063 cost=-0.00125426 jtype=Newton costheta=NaN ps=[0.398471, -0.569932, 0.661777, -2.02273]\n",
      "-- cost=-0.00154688,   cost1=4.1367e-05, cost2=-0.00158824 :  mean(hits)=0.756432, mean(diffs)=0.529414\n",
      "9: eta=0.100657 cost=-0.00154688 jtype=constrained costheta=-0.500 ps=[0.339325, -0.509753, 0.68099, -2.04598]\n",
      "-- cost=-0.001798,   cost1=1.4203e-05, cost2=-0.0018122 :  mean(hits)=0.753769, mean(diffs)=0.604068\n",
      "10: eta=0.110723 cost=-0.001798 jtype=constrained costheta=-0.954 ps=[0.286125, -0.453558, 0.715256, -2.09091]\n",
      "-- cost=-0.00207619,   cost1=2.66475e-06, cost2=-0.00207885 :  mean(hits)=0.751632, mean(diffs)=0.692952\n",
      "11: eta=0.121795 cost=-0.00207619 jtype=constrained costheta=-0.974 ps=[0.238178, -0.401162, 0.759385, -2.15064]\n",
      "-- cost=-0.00233075,   cost1=1.32111e-06, cost2=-0.00233207 :  mean(hits)=0.748851, mean(diffs)=0.777358\n",
      "12: eta=0.133974 cost=-0.00233075 jtype=constrained costheta=-0.922 ps=[0.200156, -0.35673, 0.813352, -2.22575]\n",
      "-- cost=-0.0025274,   cost1=6.29147e-06, cost2=-0.00253369 :  mean(hits)=0.747492, mean(diffs)=0.844564\n",
      "13: eta=0.147372 cost=-0.0025274 jtype=constrained costheta=-0.763 ps=[0.182843, -0.328748, 0.875797, -2.317]\n",
      "-- cost=-0.00267248,   cost1=2.55232e-06, cost2=-0.00267503 :  mean(hits)=0.748402, mean(diffs)=0.891678\n",
      "14: eta=0.162109 cost=-0.00267248 jtype=constrained costheta=-0.513 ps=[0.187217, -0.313812, 0.943893, -2.41878]\n",
      "-- cost=-0.00276642,   cost1=1.39128e-06, cost2=-0.00276781 :  mean(hits)=0.74882, mean(diffs)=0.922605\n",
      "15: eta=0.17832 cost=-0.00276642 jtype=constrained costheta=-0.414 ps=[0.199794, -0.303395, 1.02039, -2.52736]\n",
      "-- cost=-0.00282544,   cost1=9.90464e-09, cost2=-0.00282545 :  mean(hits)=0.7501, mean(diffs)=0.941817\n",
      "16: eta=0.196152 cost=-0.00282544 jtype=constrained costheta=-0.285 ps=[0.22659, -0.285909, 1.10701, -2.64072]\n",
      "-- cost=-0.00289147,   cost1=1.62338e-07, cost2=-0.00289163 :  mean(hits)=0.749597, mean(diffs)=0.963876\n",
      "17: eta=0.215767 cost=-0.00289147 jtype=constrained costheta=-0.790 ps=[0.249154, -0.27032, 1.20422, -2.76167]\n",
      "-- cost=-0.00293126,   cost1=1.47027e-07, cost2=-0.00293141 :  mean(hits)=0.750383, mean(diffs)=0.977137\n",
      "18: eta=0.237344 cost=-0.00293126 jtype=constrained costheta=-0.883 ps=[0.269609, -0.257936, 1.29073, -2.89745]\n",
      "-- cost=-0.00293542,   cost1=1.51272e-08, cost2=-0.00293543 :  mean(hits)=0.749877, mean(diffs)=0.978477\n",
      "19: eta=0.261078 cost=-0.00293542 jtype=Newton costheta=-0.069 ps=[0.3187, -0.211175, 1.31508, -2.9546]\n",
      "-- cost=-0.00293701,   cost1=6.57577e-09, cost2=-0.00293701 :  mean(hits)=0.750081, mean(diffs)=0.979005\n",
      "20: eta=0.287186 cost=-0.00293701 jtype=Newton costheta=-0.076 ps=[0.260131, -0.271745, 1.35753, -3.02038]\n",
      "-- cost=-0.00293771,   cost1=6.19265e-09, cost2=-0.00293772 :  mean(hits)=0.750079, mean(diffs)=0.979239\n",
      "21: eta=0.315904 cost=-0.00293771 jtype=Newton costheta=-0.030 ps=[0.325472, -0.209354, 1.39055, -3.08301]\n",
      "-- cost=-0.00293802,   cost1=2.11415e-07, cost2=-0.00293823 :  mean(hits)=0.75046, mean(diffs)=0.979411\n",
      "22: eta=0.347495 cost=-0.00293802 jtype=Newton costheta=-0.013 ps=[0.371607, -0.166556, 1.43658, -3.16823]\n",
      "-- cost=-0.00293832,   cost1=1.74097e-09, cost2=-0.00293832 :  mean(hits)=0.749958, mean(diffs)=0.979442\n",
      "23: eta=0.382244 cost=-0.00293832 jtype=Newton costheta=-0.002 ps=[0.383666, -0.155926, 1.45557, -3.22119]\n",
      "-- cost=-0.0029001,   cost1=6.93007e-05, cost2=-0.0029694 :  mean(hits)=0.758325, mean(diffs)=0.989799\n",
      "eta going down: new_cost-cost=3.82272e-05 and jumptype='constrained'\n",
      "24: eta=0.191122 cost=-0.00293832 jtype=constrained costheta=NaN ps=[0.383666, -0.155926, 1.45557, -3.22119]\n",
      "-- cost=-0.00293568,   cost1=4.40131e-06, cost2=-0.00294008 :  mean(hits)=0.752098, mean(diffs)=0.980028\n",
      "eta going down: new_cost-cost=2.64123e-06 and jumptype='constrained'\n",
      "25: eta=0.0955611 cost=-0.00293832 jtype=constrained costheta=NaN ps=[0.383666, -0.155926, 1.45557, -3.22119]\n",
      "-- cost=-0.00293834,   cost1=1.90498e-07, cost2=-0.00293853 :  mean(hits)=0.750436, mean(diffs)=0.979511\n",
      "26: eta=0.105117 cost=-0.00293834 jtype=constrained costheta=-0.005 ps=[0.38932, -0.152042, 1.51063, -3.2675]\n",
      "-- cost=-0.00293853,   cost1=1.13595e-09, cost2=-0.00293853 :  mean(hits)=0.749966, mean(diffs)=0.979509\n",
      "27: eta=0.115629 cost=-0.00293853 jtype=Newton costheta=-0.002 ps=[0.395298, -0.146756, 1.52057, -3.29715]\n",
      "-- cost=-0.00293821,   cost1=6.58622e-07, cost2=-0.00293887 :  mean(hits)=0.750812, mean(diffs)=0.979623\n",
      "eta going down: new_cost-cost=3.15695e-07 and jumptype='constrained'\n",
      "28: eta=0.0578145 cost=-0.00293853 jtype=constrained costheta=NaN ps=[0.395298, -0.146756, 1.52057, -3.29715]\n",
      "-- cost=-0.00293857,   cost1=3.24747e-08, cost2=-0.0029386 :  mean(hits)=0.75018, mean(diffs)=0.979534\n",
      "29: eta=0.0635959 cost=-0.00293857 jtype=constrained costheta=-0.003 ps=[0.39818, -0.144806, 1.55165, -3.32528]\n",
      "-- cost=-0.00293864,   cost1=6.45669e-09, cost2=-0.00293865 :  mean(hits)=0.75008, mean(diffs)=0.979549\n",
      "30: eta=0.0699555 cost=-0.00293864 jtype=Newton costheta=-0.001 ps=[0.405046, -0.138704, 1.56327, -3.35645]\n",
      "-- cost=-0.00293867,   cost1=4.14055e-08, cost2=-0.00293872 :  mean(hits)=0.750203, mean(diffs)=0.979572\n",
      "31: eta=0.076951 cost=-0.00293867 jtype=constrained costheta=-0.001 ps=[0.415647, -0.129059, 1.58082, -3.39364]\n",
      "-- cost=-0.00293873,   cost1=7.62131e-10, cost2=-0.00293873 :  mean(hits)=0.750028, mean(diffs)=0.979576\n",
      "32: eta=0.0846461 cost=-0.00293873 jtype=Newton costheta=-0.001 ps=[0.419668, -0.12547, 1.58841, -3.41297]\n",
      "-- cost=-0.00209856,   cost1=0.000900132, cost2=-0.0029987 :  mean(hits)=0.719998, mean(diffs)=0.999565\n",
      "eta going down: new_cost-cost=0.000840165 and jumptype='constrained'\n",
      "33: eta=0.0423231 cost=-0.00293873 jtype=constrained costheta=NaN ps=[0.419668, -0.12547, 1.58841, -3.41297]\n",
      "-- cost=-0.00293876,   cost1=6.45254e-09, cost2=-0.00293876 :  mean(hits)=0.75008, mean(diffs)=0.979587\n",
      "34: eta=0.0465554 cost=-0.00293876 jtype=constrained costheta=-0.002 ps=[0.414927, -0.130702, 1.60587, -3.43379]\n",
      "-- cost=-0.00289863,   cost1=0.000100054, cost2=-0.00299869 :  mean(hits)=0.739997, mean(diffs)=0.999562\n",
      "eta going down: new_cost-cost=4.01247e-05 and jumptype='constrained'\n",
      "35: eta=0.0232777 cost=-0.00293876 jtype=constrained costheta=NaN ps=[0.414927, -0.130702, 1.60587, -3.43379]\n",
      "-- cost=-0.00289868,   cost1=0.000100047, cost2=-0.00299872 :  mean(hits)=0.739998, mean(diffs)=0.999575\n",
      "eta going down: new_cost-cost=4.00782e-05 and jumptype='constrained'\n",
      "36: eta=0.0116388 cost=-0.00293876 jtype=constrained costheta=NaN ps=[0.414927, -0.130702, 1.60587, -3.43379]\n",
      "-- cost=-0.00289866,   cost1=0.000100045, cost2=-0.00299871 :  mean(hits)=0.739998, mean(diffs)=0.99957\n",
      "eta going down: new_cost-cost=4.00913e-05 and jumptype='constrained'\n",
      "37: eta=0.00581942 cost=-0.00293876 jtype=constrained costheta=NaN ps=[0.414927, -0.130702, 1.60587, -3.43379]\n",
      "-- cost=-0.00289858,   cost1=0.000100029, cost2=-0.00299861 :  mean(hits)=0.739999, mean(diffs)=0.999538\n",
      "eta going down: new_cost-cost=4.01714e-05 and jumptype='constrained'\n",
      "38: eta=0.00290971 cost=-0.00293876 jtype=constrained costheta=NaN ps=[0.414927, -0.130702, 1.60587, -3.43379]\n",
      "-- cost=-0.00289792,   cost1=9.97401e-05, cost2=-0.00299766 :  mean(hits)=0.740013, mean(diffs)=0.999219\n",
      "eta going down: new_cost-cost=4.08394e-05 and jumptype='constrained'\n",
      "39: eta=0.00145486 cost=-0.00293876 jtype=constrained costheta=NaN ps=[0.414927, -0.130702, 1.60587, -3.43379]\n",
      "-- cost=-0.00289509,   cost1=9.42936e-05, cost2=-0.00298938 :  mean(hits)=0.74029, mean(diffs)=0.996461\n",
      "eta going down: new_cost-cost=4.36676e-05 and jumptype='constrained'\n",
      "40: eta=0.000727428 cost=-0.00293876 jtype=constrained costheta=NaN ps=[0.414927, -0.130702, 1.60587, -3.43379]\n",
      "-- cost=-0.00290214,   cost1=6.428e-05, cost2=-0.00296642 :  mean(hits)=0.741983, mean(diffs)=0.988805\n",
      "eta going down: new_cost-cost=3.66203e-05 and jumptype='constrained'\n",
      "41: eta=0.000363714 cost=-0.00293876 jtype=constrained costheta=NaN ps=[0.414927, -0.130702, 1.60587, -3.43379]\n",
      "-- cost=-0.00292264,   cost1=2.53979e-05, cost2=-0.00294804 :  mean(hits)=0.74496, mean(diffs)=0.98268\n",
      "eta going down: new_cost-cost=1.61123e-05 and jumptype='constrained'\n",
      "42: eta=0.000181857 cost=-0.00293876 jtype=constrained costheta=NaN ps=[0.414927, -0.130702, 1.60587, -3.43379]\n",
      "-- cost=-0.00293407,   cost1=7.15022e-06, cost2=-0.00294122 :  mean(hits)=0.747326, mean(diffs)=0.980407\n",
      "eta going down: new_cost-cost=4.68551e-06 and jumptype='constrained'\n",
      "43: eta=9.09285e-05 cost=-0.00293876 jtype=constrained costheta=NaN ps=[0.414927, -0.130702, 1.60587, -3.43379]\n",
      "-- cost=-0.0029376,   cost1=1.75042e-06, cost2=-0.00293935 :  mean(hits)=0.748677, mean(diffs)=0.979784\n",
      "eta going down: new_cost-cost=1.15469e-06 and jumptype='constrained'\n",
      "44: eta=4.54642e-05 cost=-0.00293876 jtype=constrained costheta=NaN ps=[0.414927, -0.130702, 1.60587, -3.43379]\n",
      "-- cost=-0.0029385,   cost1=3.89949e-07, cost2=-0.00293889 :  mean(hits)=0.749376, mean(diffs)=0.97963\n",
      "eta going down: new_cost-cost=2.55089e-07 and jumptype='constrained'\n",
      "45: eta=2.27321e-05 cost=-0.00293876 jtype=constrained costheta=NaN ps=[0.414927, -0.130702, 1.60587, -3.43379]\n",
      "-- cost=-0.00293876,   cost1=1.51692e-12, cost2=-0.00293876 :  mean(hits)=0.750001, mean(diffs)=0.979587\n",
      "About to break -- tol=1e-08, new_cost-cost=-4.1584e-09, eta=2.27321e-05\n",
      "-- cost=-0.00293876,   cost1=6.45254e-09, cost2=-0.00293876 :  mean(hits)=0.75008, mean(diffs)=0.979587\n",
      "-- cost=-0.00293876,   cost1=6.45254e-09, cost2=-0.00293876 :  mean(hits)=0.75008, mean(diffs)=0.979587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1×2 Array{Float64,2}:\n",
       " 0.501141  0.75008"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following sequence leads to a situation where having only [-0.8, -0.8] as the single finalFluxPoint \n",
    "# leads to the minimization getting stuck.  Adding further finalFluxPoints solves the problem\n",
    "\n",
    "dt = 0.02\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "W = -4\n",
    "noise = 0\n",
    "input = 0\n",
    "sigma = 0.1\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    ":noise=>noise, :input=>input, :sigma=>sigma, :const_add=>0, :init_add=>0)\n",
    "\n",
    "\n",
    "# WORKING gradient:\n",
    "# ForwardDiff.gradient((x)->JJ(startU; do_plot=true, nderivs=length(x), difforder=1, \n",
    "#    make_dict([[\"init_add\" 2], \"const_add\"], x, model_params)...), [2.9, -2.9, 0.1])\n",
    "\n",
    "\n",
    "\n",
    "# The backward and costfunc functions should turn a single-scalar parameter W into the matrix W\n",
    "# backward always runs with no within-forward noise, i.e., sigma=0\n",
    "backward = (endpoint; do_plot=false, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;     \n",
    "    backwardsModel(endpoint; do_plot=do_plot, make_dict([\"sigma\"], [0], pars)...)[1]\n",
    "end\n",
    "\n",
    "forward = (startpoint; pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;     \n",
    "    forwardsModel(startpoint; pars...)\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "costfunc = (startpoints; do_plot=false, verbose=false, beta=0.003, nderivs=0, difforder=0, sr=26, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;         \n",
    "    JJ(startpoints; seedrand=sr, beta=beta, \n",
    "        do_plot=do_plot, verbose=verbose, nderivs=nderivs, difforder=difforder, pars...)\n",
    "end\n",
    "\n",
    "function JJ(initUs; theta1=0.15, theta2=0.2, beta=0.003, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, pre_string=\"\", zero_last_sigmas=0, seedrand=NaN, params...)\n",
    "\n",
    "    if ~isnan(seedrand); srand(seedrand); end\n",
    "    \n",
    "    Vend = ForwardDiffZeros(size(initUs,1), size(initUs,2), nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:size(initUs,1)\n",
    "        if false # i>size(initUs,1) - zero_last_sigmas\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; sigma=0, nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)            \n",
    "        else\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)\n",
    "        end\n",
    "        Vend[i,:] = Ve\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend[:,1]-Vend[:,2])/theta1))\n",
    "    diffs = tanh.((Vend[:,1]-Vend[:,2])/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "\n",
    "    if do_plot\n",
    "        title(@sprintf(\"mean(hits)=%g, mean(diffs)=%g\", convert(Float64, mean(hits)), convert(Float64, mean(diffs))))\n",
    "    end\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"%s\", pre_string)\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2, mean(hits), mean(diffs)\n",
    "end\n",
    "  \n",
    "\n",
    "\n",
    "fluxFinalPoint = [-0.8 -0.8; -0.6 -0.6 ; -0.4 -0.4; -0.2 -0.2; 0 0; 0.2 0.2]\n",
    "fluxFinalPoint = zeros(0,2);\n",
    "\n",
    "\n",
    "beta = 0.05\n",
    "args = [[\"start_add\" 2], \"const_add\", \"W\", \"sigma\"]\n",
    "seed = [0.1, 0.1, 2.1, -1, 0.1]\n",
    "walls = Dict(:start_add=>[-5.1, 5.1], :W=>[-5.1, 5.1], :sigma=>[-0.5, 0.5])\n",
    "# sr = 1504432803 causes a total mess with everything around the decision boundary; \n",
    "# sr = 1504432962 gets stuck at mean(hits)=0.66 but if we reduce the bounds of sigma, reaches 0.74\n",
    "\n",
    "\n",
    "beta = 0.0000001\n",
    "args = [[\"start_add\" 2], \"const_add\", \"W\"] \n",
    "seed = [0.1, 0.1, 2.1, -1] \n",
    "# This seed proves deadly, and always starts with mean(hits)<0.5:  seed = [-0.1, 0.1, 2.1, -1] \n",
    "walls = Dict(:start_add=>[-5.1, 5.1], :W=>[-5.1, 5.1]) # \n",
    "# sr = 1504433892 gives a mean(hits)=0.5 mess\n",
    "# sr = 1504433983 gives a mean(hits)=0.5 mess\n",
    "# sr = 1504434067 gives a mean(hits)=0.5 mess\n",
    "# sr = 1504434114 gives a mean(hits)=0.5 mess\n",
    "\n",
    "\n",
    "beta = 0.003\n",
    "args = [[\"start_add\" 2], \"const_add\", \"W\"] \n",
    "seed = [-0.2, 0.2, 0.2, -1.5] \n",
    "walls = Dict(:start_add=>[-5.1, 5.1], :W=>[-5.1, 5.1]) # \n",
    "# sr = 1504713552 gives a mean(hits)=0.5 mess\n",
    "# sr = 1504713626 gives a mean(hits)=0.5 mess\n",
    "# sr = 1504713708 gives a mean(hits)=0.5 mess\n",
    "# sr =  gives a mean(hits)=0.5 mess\n",
    "\n",
    "\n",
    "new_random_seed = true; if new_random_seed\n",
    "    sr = convert(Int64, round(time()))\n",
    "else\n",
    "    sr = old_sr\n",
    "end\n",
    "# sr = 1504649431\n",
    "old_sr = sr\n",
    "\n",
    "srand(sr)\n",
    "\n",
    "# THIS IS THE GOOD ONE FOR ALL THE COMMENTS ON sr NUMBERS ABOVE: startU=randn(50,2)-3\n",
    "startU=randn(50,2)-3\n",
    "\n",
    "\n",
    "clf()\n",
    "print(\"seed = \"); print_vector_g(seed); print(\"\\n\")\n",
    "ocost, omhits, omdiffs = costfunc(startU; do_plot=true, sr=sr, verbose=true, make_dict(args, seed, model_params)...)\n",
    "\n",
    "\n",
    "# :sigma=>[-0.3, 0.3] does fine but :sigma=>[-0.2, 0.2] gets stuck.\n",
    "# If we fix sigma at 0 it also gets stuck, but dynamics kind of odd, W a bit to big, or decrease dt\n",
    "params, traj, zz, cpm_traj = bbox_Hessian_keyword_minimization(seed, args, walls, # , :sigma=>[-0.2, 0.2]), \n",
    "(;params...) -> costfunc(startU; beta=beta, sr=sr, do_plot=false, verbose=true, merge(model_params, Dict(params))...)[1], \n",
    "verbose=true, start_eta=1, tol=1e-8, softbox=true, maxiter=400 )\n",
    "\n",
    "# params, cost, ptraj, gtraj = fluxSense(costfunc, backward, model_params, startU, fluxFinalPoint, args, seed; \n",
    "#    start_eta=0.01, tol=1e-15, maxiter=400, verbose=true, report_every=1, do_plot=false, cost_limit=cost_limit) # cost_limit=-0.000935) # for beta=0.01\n",
    "\n",
    "# And show the final position\n",
    "figure(1); clf()\n",
    "cost, mhits, mdiffs = \n",
    "    costfunc(startU; beta=beta, do_plot=true, sr=sr, verbose=true, make_dict(args, params, model_params)...)\n",
    "\n",
    "repeat_results_in_fig2 = true; if repeat_results_in_fig2\n",
    "    figure(2); clf()\n",
    "    costfunc(startU; beta=beta, do_plot=true, sr=sr, verbose=true, \n",
    "        make_dict(args, params, merge(Dict(:fignum=>2), model_params))...)\n",
    "    figure(1); \n",
    "end\n",
    "params'\n",
    "\n",
    "# For beta=0, and ntrials=20, we collected a bunch of results and observed that it failes about half the time\n",
    "# WHEN the initial mean)hits) is below 0.5.  It never fails if the initial mean(hits) is above 0.5. \n",
    "# Seems like when it starts below 0.5, the fastest way to increase mean(hits) is to push it to 0.5 and floor it there.\n",
    "# The results were collected in \"Results.mat\"\n",
    "# res = [res ; omhits mhits]\n",
    "\n",
    "[omhits mhits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1504713708"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Array{Any,1}:\n",
       " PyObject <matplotlib.lines.Line2D object at 0x33088fad0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "figure(2); clf();\n",
    "plot(cpm_traj[1,:], \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition param_path(Any) in module Main at In[258]:2 overwritten at In[260]:2.\n",
      "WARNING: Method definition #param_path(Array{Any, 1}, Main.#param_path, Any) in module Main overwritten.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "param_path (generic function with 1 method)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function param_path(ppath; k=1, do_plot=true, fignum=1, clearfig=true)\n",
    "    costhetas = zeros(1, size(ppath,2)-k-1)\n",
    "    deltas = ppath[:,2:end] - ppath[:,1:end-1]\n",
    "    for i=1:length(costhetas)\n",
    "        costhetas[i] = dot(deltas[:,i], deltas[:,i+k])/(norm(deltas[:,i])*norm(deltas[:,i+k]))\n",
    "    end;\n",
    "    \n",
    "    if do_plot\n",
    "        figure(fignum); if clearfig; clf(); end;\n",
    "        plot(costhetas', \".\")\n",
    "    end\n",
    "    return costhetas\n",
    "end\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×69 Array{Float64,2}:\n",
       " NaN  0.850328  0.775369  -0.952955  …  NaN  NaN  NaN  NaN  NaN  NaN  NaN"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_path(traj[3:end,:]; k=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpm: g (candidate indices) are : [1, 333, 444, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-7654187.550, -30616.750, 2518227.704, 7638879.175]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "1 : After searching for lambdas with efactor=3, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 361, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-30616750.199, 2449340.016, 30555516.699]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "2 : After searching for lambdas with efactor=12, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 340, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-122467000.798, 2081939.014, 122222066.796]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "3 : After searching for lambdas with efactor=48, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 335, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-489868003.190, 979736.006, 488888267.184]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "4 : After searching for lambdas with efactor=192, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 334, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-1959472012.760, -1959472.013, 1955553068.735]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "5 : After searching for lambdas with efactor=768, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 2, 12, 333, 656, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018, 0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-7837888051.040, -7814374386.887, -7579237745.356, -31351552.204, 7563561969.254, 7822212274.938]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "6 : After searching for lambdas with efactor=3072, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 1, 5, 10, 14, 18, 22, 26, 30, 34, 38, 41, 45, 48, 52, 55, 58, 62, 65, 68, 71, 73, 76, 79, 82, 84, 87, 89, 92, 94, 97, 99, 101, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 129, 131, 134, 136, 139, 142, 145, 148, 151, 155, 159, 165, 174, 333, 496, 504, 509, 513, 517, 520, 523, 526, 529, 531, 534, 536, 539, 541, 543, 545, 547, 549, 552, 555, 557, 559, 561, 563, 565, 568, 570, 572, 575, 577, 580, 582, 585, 588, 590, 593, 596, 599, 602, 605, 608, 612, 615, 618, 622, 625, 629, 633, 636, 640, 644, 648, 653, 657, 662, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-31351552204.161, -31351552204.161, -30975333577.711, -30505060294.648, -30128841668.198, -29752623041.748, -29376404415.298, -29000185788.849, -28623967162.399, -28247748535.949, -27871529909.499, -27589365939.661, -27213147313.211, -26930983343.374, -26554764716.924, -26272600747.087, -25990436777.249, -25614218150.799, -25332054180.962, -25049890211.124, -24767726241.287, -24579616928.062, -24297452958.224, -24015288988.387, -23733125018.550, -23545015705.325, -23262851735.487, -23074742422.262, -22792578452.425, -22604469139.200, -22322305169.362, -22134195856.137, -21946086542.912, -21663922573.075, -21475813259.850, -21287703946.625, -21099594633.400, -20911485320.175, -20723376006.950, -20535266693.725, -20347157380.500, -20159048067.275, -19970938754.050, -19782829440.825, -19594720127.600, -19312556157.763, -19124446844.538, -18842282874.700, -18654173561.476, -18372009591.638, -18089845621.801, -17807681651.963, -17525517682.126, -17243353712.288, -16867135085.838, -16490916459.388, -15926588519.714, -15080096610.201, -125406208.817, 15205502819.018, 15957940071.918, 16428213354.980, 16804431981.430, 17180650607.880, 17462814577.717, 17744978547.555, 18027142517.392, 18309306487.230, 18497415800.455, 18779579770.292, 18967689083.517, 19249853053.355, 19437962366.580, 19626071679.805, 19814180993.029, 20002290306.254, 20190399619.479, 20472563589.317, 20754727559.154, 20942836872.379, 21130946185.604, 21319055498.829, 21507164812.054, 21695274125.279, 21977438095.117, 22165547408.342, 22353656721.566, 22635820691.404, 22823930004.629, 23106093974.466, 23294203287.691, 23576367257.529, 23858531227.366, 24046640540.591, 24328804510.429, 24610968480.266, 24893132450.103, 25175296419.941, 25457460389.778, 25739624359.616, 26115842986.066, 26398006955.903, 26680170925.741, 27056389552.191, 27338553522.028, 27714772148.478, 28090990774.928, 28373154744.765, 28749373371.215, 29125591997.665, 29501810624.115, 29972083907.177, 30348302533.627, 30818575816.690, 31288849099.752]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "7 : After searching for lambdas with efactor=12288, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 12, 28, 43, 57, 69, 80, 91, 100, 109, 117, 125, 132, 139, 145, 151, 156, 161, 166, 171, 175, 179, 183, 187, 190, 194, 197, 200, 203, 206, 208, 211, 213, 216, 218, 220, 222, 226, 228, 230, 232, 235, 237, 241, 244, 249, 333, 419, 424, 427, 430, 433, 435, 438, 440, 442, 444, 446, 449, 451, 453, 456, 458, 461, 464, 467, 470, 473, 476, 480, 484, 488, 492, 496, 501, 506, 511, 516, 522, 528, 535, 542, 549, 558, 566, 576, 586, 598, 610, 623, 638, 655, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-125406208816.642, -121267803925.693, -115248305902.494, -109605026505.745, -104337965735.446, -99823342218.047, -95684937327.098, -91546532436.149, -88160564798.099, -84774597160.050, -81764848148.451, -78755099136.851, -76121568751.702, -73488038366.552, -71230726607.853, -68973414849.153, -67092321716.904, -65211228584.654, -63330135452.404, -61449042320.155, -59944167814.355, -58439293308.555, -56934418802.756, -55429544296.956, -54300888417.606, -52796013911.806, -51667358032.457, -50538702153.107, -49410046273.757, -48281390394.407, -47528953141.507, -46400297262.158, -45647860009.258, -44519204129.908, -43766766877.008, -43014329624.108, -42261892371.208, -40757017865.409, -40004580612.509, -39252143359.609, -38499706106.709, -37371050227.359, -36618612974.460, -35113738468.660, -33985082589.310, -32103989457.060, -501624835.267, 31853177039.427, 33734270171.677, 34862926051.027, 35991581930.376, 37120237809.726, 37872675062.626, 39001330941.976, 39753768194.876, 40506205447.775, 41258642700.675, 42011079953.575, 43139735832.925, 43892173085.825, 44644610338.725, 45773266218.074, 46525703470.974, 47654359350.324, 48783015229.674, 49911671109.024, 51040326988.373, 52168982867.723, 53297638747.073, 54802513252.873, 56307387758.672, 57812262264.472, 59317136770.272, 60822011276.071, 62703104408.321, 64584197540.571, 66465290672.820, 68346383805.070, 70603695563.770, 72861007322.469, 75494537707.619, 78128068092.768, 80761598477.918, 84147566115.967, 87157315127.566, 90919501392.066, 94681687656.565, 99196311173.964, 103710934691.363, 108601776835.212, 114245056231.961, 120640772881.610, 125155396399.009]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "8 : After searching for lambdas with efactor=49152, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 3, 63, 105, 135, 159, 177, 192, 204, 215, 224, 231, 238, 244, 249, 253, 257, 261, 265, 268, 270, 273, 275, 278, 280, 283, 285, 289, 333, 379, 382, 385, 387, 389, 391, 394, 396, 399, 402, 405, 409, 413, 418, 423, 429, 436, 443, 452, 462, 475, 490, 508, 531, 562, 604, 664, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-501624835266.569, -498615086254.969, -408322615906.987, -345117886663.399, -299971651489.408, -263854663350.215, -236766922245.820, -214193804658.825, -196135310589.228, -179581691025.432, -166037820473.234, -155503698932.636, -144969577392.038, -135940330357.240, -128415957828.242, -122396459805.043, -116376961781.844, -110357463758.645, -104337965735.446, -99823342218.047, -96813593206.448, -92298969689.049, -89289220677.449, -84774597160.050, -81764848148.451, -77250224631.052, -74240475619.452, -68220977596.253, -2006499341.066, 67217727925.720, 71732351443.119, 76246974960.519, 79256723972.118, 82266472983.717, 85276221995.317, 89790845512.716, 92800594524.315, 97315218041.714, 101829841559.114, 106344465076.513, 112363963099.711, 118383461122.910, 125907833651.909, 133432206180.907, 142461453215.706, 152995574756.303, 163529696296.901, 177073566849.099, 192122311907.096, 211685680482.492, 234258798069.488, 261346539173.882, 295958652807.276, 342609762487.067, 405814491730.654, 496106962078.637, 500621585596.036]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "9 : After searching for lambdas with efactor=196608, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 86, 185, 227, 251, 266, 277, 284, 290, 295, 298, 301, 304, 306, 308, 311, 333, 357, 359, 363, 365, 368, 372, 377, 382, 390, 401, 416, 439, 482, 581, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-2006499341066.275, -1494842009094.375, -898911704797.691, -646092787823.341, -501624835266.569, -411332364918.586, -345117886663.399, -302981400501.008, -266864412361.815, -236766922245.820, -218708428176.224, -200649934106.628, -182591440037.031, -170552443990.633, -158513447944.236, -140454953874.639, -8025997364.265, 136441955192.507, 148480951238.905, 172558943331.700, 184597939378.098, 202656433447.694, 226734425540.489, 256831915656.483, 286929405772.477, 335085389958.068, 401299868213.255, 491592338561.238, 630040793094.811, 888879208092.360, 1484809512389.044, 2002486342384.143]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "10 : After searching for lambdas with efactor=786432, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 148, 272, 297, 307, 313, 317, 321, 333, 347, 350, 354, 359, 370, 395, 519, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-8025997364265.102, -4486532526624.191, -1500861507117.573, -898911704797.691, -658131783869.738, -513663831312.966, -417351862941.785, -321039894570.604, -32103989457.060, 304987899842.074, 377221876120.459, 473533844491.641, 593923804955.618, 858781717976.366, 1460731520296.248, 4446402539802.866, 8009945369536.571]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "11 : After searching for lambdas with efactor=3.14573e+06, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 287, 318, 325, 327, 333, 342, 348, 379, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-32103989457060.406, -4558766502902.578, -1573095483395.958, -898911704797.692, -706287768055.330, -128415957828.242, 738391757512.394, 1316263567739.474, 4301934587246.098, 32039781478146.285]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "12 : After searching for lambdas with efactor=1.25829e+07, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 322, 330, 333, 337, 344, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-128415957828241.625, -4751390439644.944, -1669407451767.143, -513663831312.967, 1027327662625.934, 3724062777018.996, 128159125912585.141]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "13 : After searching for lambdas with efactor=5.03316e+07, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 331, 333, 336, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-513663831312966.500, -5136638313129.670, -2054655325251.868, 2568319156564.892, 512636503650340.563]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "14 : After searching for lambdas with efactor=2.01327e+08, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 333, 334, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-2054655325251866.000, -8218621301007.472, -2054655325251.868, 2050546014601362.250]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "15 : After searching for lambdas with efactor=8.05306e+08, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-8218621301007464.000, 8202184058405449.000]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "16 : After searching for lambdas with efactor=3.22123e+09, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-32874485204029856.000, 32808736233621796.000]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "17 : After searching for lambdas with efactor=1.28849e+10, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-131497940816119424.000, 131234944934487184.000]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "18 : After searching for lambdas with efactor=5.15396e+10, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-525991763264477696.000, 524939779737948736.000]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "19 : After searching for lambdas with efactor=2.06158e+11, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-2103967053057910784.000, 2099759118951794944.000]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "20 : After searching for lambdas with efactor=8.24634e+11, we found these : []\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "ArgumentError: collection must be non-empty",
     "output_type": "error",
     "traceback": [
      "ArgumentError: collection must be non-empty",
      "",
      " in findmin(::Array{Float64,1}) at ./array.jl:1254",
      " in indmin at ./array.jl:1294 [inlined]",
      " in #constrained_parabolic_minimization#777(::Float64, ::Bool, ::Bool, ::Bool, ::Float64, ::Int64, ::Float64, ::Float64, ::Float64, ::Int64, ::#constrained_parabolic_minimization, ::Array{Float64,2}, ::Array{Float64,2}, ::Float64) at ./In[227]:106",
      " in (::#kw##constrained_parabolic_minimization)(::Array{Any,1}, ::#constrained_parabolic_minimization, ::Array{Float64,2}, ::Array{Float64,2}, ::Float64) at ./<missing>:0"
     ]
    }
   ],
   "source": [
    "a = matread(\"error_report.mat\")\n",
    "hess = a[\"hess\"]\n",
    "eta = a[\"eta\"]\n",
    "grad = a[\"grad\"]\n",
    "\n",
    "chessdelta = constrained_parabolic_minimization(hess, grad'', eta, maxiter=500, max_efactor_tries=20, tol=1e-20, do_plot=true, verbose=true)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example doing a successful minimization of a 2d model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function JJ(initUs; theta1=0.15, theta2=0.2, beta=0.003, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, pre_string=\"\", zero_last_sigmas=0, seedrand=NaN, params...)\n",
    "\n",
    "    if ~isnan(seedrand); srand(seedrand); end\n",
    "    \n",
    "    Vend = ForwardDiffZeros(size(initUs,1), size(initUs,2), nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:size(initUs,1)\n",
    "        if i>size(initUs,1) - zero_last_sigmas\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; sigma=0, nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)            \n",
    "        else\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)\n",
    "        end\n",
    "        Vend[i,:] = Ve\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend[:,1]-Vend[:,2])/theta1))\n",
    "    diffs = tanh.((Vend[:,1]-Vend[:,2])/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"%s\", pre_string)\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The following sequence leads to a situation where having only [-0.8, -0.8] as the single finalFluxPoint \n",
    "# leads to the minimization getting stuck.  Adding further finalFluxPoints solves the problem\n",
    "#\n",
    "srand(12)   # 12 is perfect success; srand(11) gets stuck at mean(hits)=0.72\n",
    "startU=randn(100,2)-3\n",
    "startU=randn(100,2)-3\n",
    "sigma = 0\n",
    "\n",
    "# startU=0.1*randn(100,2)-3\n",
    "# startU=zeros(100,2)-3\n",
    "\n",
    "\n",
    "dt = 0.02\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "W = -4\n",
    "noise = 0\n",
    "input = 0\n",
    "sigma = 0\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    ":noise=>noise, :input=>input, :sigma=>sigma, :const_add=>0, :init_add=>0)\n",
    "\n",
    "\n",
    "# WORKING gradient:\n",
    "# ForwardDiff.gradient((x)->JJ(startU; do_plot=true, nderivs=length(x), difforder=1, \n",
    "#    make_dict([[\"init_add\" 2], \"const_add\"], x, model_params)...), [2.9, -2.9, 0.1])\n",
    "\n",
    "\n",
    "\n",
    "# The backward and costfunc functions should turn a single-scalar parameter W into the matrix W\n",
    "# backward always runs with no within-forward noise, i.e., sigma=0\n",
    "backward = (endpoint; do_plot=false, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;     \n",
    "    backwardsModel(endpoint; do_plot=do_plot, make_dict([\"sigma\"], [0], pars)...)[1]\n",
    "end\n",
    "\n",
    "\n",
    "beta = 0.003;\n",
    "\n",
    "costfunc = (startpoints; do_plot=false, verbose=false, nderivs=0, difforder=0, sr=26, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;         \n",
    "    JJ(startpoints; seedrand=sr, beta=beta, \n",
    "        do_plot=do_plot, verbose=verbose, nderivs=nderivs, difforder=difforder, pars...)\n",
    "end\n",
    "\n",
    "function JJ(initUs; theta1=0.15, theta2=0.2, beta=0.003, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, pre_string=\"\", zero_last_sigmas=0, seedrand=NaN, params...)\n",
    "\n",
    "    if ~isnan(seedrand); srand(seedrand); end\n",
    "    \n",
    "    Vend = ForwardDiffZeros(size(initUs,1), size(initUs,2), nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:size(initUs,1)\n",
    "        if false # i>size(initUs,1) - zero_last_sigmas\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; sigma=0, nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)            \n",
    "        else\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)\n",
    "        end\n",
    "        Vend[i,:] = Ve\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend[:,1]-Vend[:,2])/theta1))\n",
    "    diffs = tanh.((Vend[:,1]-Vend[:,2])/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"%s\", pre_string)\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "  \n",
    "\n",
    "\n",
    "if beta==0.003;     cost_limit = -0.00288\n",
    "elseif beta<0.001;  cost_limit = -0.0008\n",
    "elseif beta==0.001; cost_limit = -0.000935\n",
    "elseif beta==0.05;  cost_limit = -0.0485\n",
    "else\n",
    "    error(\"Don't know what cost limit goes with beta %g\\n\", beta)\n",
    "end\n",
    "\n",
    "fluxFinalPoint = [-0.8 -0.8; -0.6 -0.6 ; -0.4 -0.4; -0.2 -0.2; 0 0; 0.2 0.2]\n",
    "fluxFinalPoint = zeros(0,2);\n",
    "\n",
    "\n",
    "\n",
    "args = [[\"init_add\" 2], \"const_add\", \"W\", \"sigma\"]\n",
    "seed = [2, 2, 2.1, -1, 0.1]\n",
    "\n",
    "\n",
    "\n",
    "clf()\n",
    "print(\"seed = \"); print_vector_g(seed); print(\"\\n\")\n",
    "costfunc(startU; do_plot=true, verbose=true, make_dict(args, seed, model_params)...)\n",
    "\n",
    "# :sigma=>[-0.3, 0.3] does fine but :sigma=>[-0.2, 0.2] gets stuck.\n",
    "# If we fix sigma at 0 it also gets stuck, but dynamics kind of odd, W a bit to big, or decrease dt\n",
    "params, traj = bbox_Hessian_keyword_minimization(seed, args, Dict(:init_add=>[-5.1, 5.1], :sigma=>[-0.3, 0.3]), \n",
    "(;params...) -> costfunc(startU; do_plot=false, verbose=true, merge(model_params, Dict(params))...), \n",
    " verbose=true, start_eta=1, tol=1e-6, hardbox=true )\n",
    "\n",
    "# params, cost, ptraj, gtraj = fluxSense(costfunc, backward, model_params, startU, fluxFinalPoint, args, seed; \n",
    "#    start_eta=0.01, tol=1e-15, maxiter=400, verbose=true, report_every=1, do_plot=false, cost_limit=cost_limit) # cost_limit=-0.000935) # for beta=0.01\n",
    "\n",
    "# And show the final position\n",
    "clf()\n",
    "costfunc(startU; do_plot=true, verbose=true, make_dict(args, params, model_params)...)\n",
    "params'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf()\n",
    "startU=randn(4000,2)-3\n",
    "costfunc(startU; sr=NaN, do_plot=false, verbose=true, make_dict(args, params, model_params)...)\n",
    "params'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of getting stuck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function JJ(initUs; theta1=0.15, theta2=0.2, beta=0.003, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, pre_string=\"\", zero_last_sigmas=0, seedrand=NaN, params...)\n",
    "\n",
    "    if ~isnan(seedrand); srand(seedrand); end\n",
    "    \n",
    "    Vend = ForwardDiffZeros(size(initUs,1), size(initUs,2), nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:size(initUs,1)\n",
    "        if i>size(initUs,1) - zero_last_sigmas\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; sigma=0, nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)            \n",
    "        else\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)\n",
    "        end\n",
    "        Vend[i,:] = Ve\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend[:,1]-Vend[:,2])/theta1))\n",
    "    diffs = tanh.((Vend[:,1]-Vend[:,2])/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"%s\", pre_string)\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function JJ(initUs; theta1=0.15, theta2=0.2, beta=0.003, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, pre_string=\"\", zero_last_sigmas=0, seedrand=NaN, params...)\n",
    "\n",
    "    if ~isnan(seedrand); srand(seedrand); end\n",
    "    \n",
    "    Vend = ForwardDiffZeros(size(initUs,1), size(initUs,2), nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:size(initUs,1)\n",
    "        if false # i>size(initUs,1) - zero_last_sigmas\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; sigma=0, nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)            \n",
    "        else\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)\n",
    "        end\n",
    "        Vend[i,:] = Ve\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend[:,1]-Vend[:,2])/theta1))\n",
    "    diffs = tanh.((Vend[:,1]-Vend[:,2])/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"%s\", pre_string)\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "\n",
    "\n",
    "# The following sequence leads to a situation where having only [-0.8, -0.8] as the single finalFluxPoint \n",
    "# leads to the minimization getting stuck.  Adding further finalFluxPoints solves the problem\n",
    "#\n",
    "srand(11)\n",
    "startU=randn(100,2)-3\n",
    "startU=randn(100,2)-3\n",
    "\n",
    "dt = 0.02\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "W = -4\n",
    "noise = 0\n",
    "input = 0\n",
    "sigma = 0\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    ":noise=>noise, :input=>input, :sigma=>sigma, :const_add=>0, :init_add=>0)\n",
    "\n",
    "\n",
    "# WORKING gradient:\n",
    "# ForwardDiff.gradient((x)->JJ(startU; do_plot=true, nderivs=length(x), difforder=1, \n",
    "#    make_dict([[\"init_add\" 2], \"const_add\"], x, model_params)...), [2.9, -2.9, 0.1])\n",
    "\n",
    "\n",
    "\n",
    "# The backward and costfunc functions should turn a single-scalar parameter W into the matrix W\n",
    "# backward always runs with no within-forward noise, i.e., sigma=0\n",
    "backward = (endpoint; do_plot=false, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;     \n",
    "    backwardsModel(endpoint; do_plot=do_plot, make_dict([\"sigma\"], [0], pars)...)[1]\n",
    "end\n",
    "\n",
    "\n",
    "beta = 0.0001;\n",
    "beta = 0.003;\n",
    "beta = 0.003;\n",
    "beta=0\n",
    "\n",
    "costfunc = (startpoints; do_plot=false, verbose=false, nderivs=0, difforder=0, sr=26, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;         \n",
    "    JJ(startpoints; seedrand=sr, beta=beta, \n",
    "        do_plot=do_plot, verbose=verbose, nderivs=nderivs, difforder=difforder, pars...)\n",
    "end\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "if beta==0.003;     cost_limit = -0.00288\n",
    "elseif beta<0.001;  cost_limit = -0.0008\n",
    "elseif beta==0.001; cost_limit = -0.000935\n",
    "elseif beta==0.05;  cost_limit = -0.0485\n",
    "else\n",
    "    error(\"Don't know what cost limit goes with beta %g\\n\", beta)\n",
    "end\n",
    "\n",
    "fluxFinalPoint = [-0.8 -0.8; -0.6 -0.6 ; -0.4 -0.4; -0.2 -0.2; 0 0; 0.2 0.2]\n",
    "fluxFinalPoint = zeros(0,2);\n",
    "\n",
    "\n",
    "args = [[\"init_add\" 2], \"const_add\", \"W\"] # , \"sigma\"]\n",
    "seed = [2, 2, 2.1, -1] # , 0.1]\n",
    "\n",
    "\n",
    "\n",
    "clf()\n",
    "print(\"seed = \"); print_vector_g(seed); print(\"\\n\")\n",
    "costfunc(startU; do_plot=true, verbose=true, make_dict(args, seed, model_params)...)\n",
    "\n",
    "# :sigma=>[-0.3, 0.3] does fine but :sigma=>[-0.2, 0.2] gets stuck.\n",
    "# If we fix sigma at 0 it also gets stuck, but dynamics kind of odd, W a bit to big, or decrease dt\n",
    "params, traj = bbox_Hessian_keyword_minimization(seed, args, Dict(:init_add=>[-5.1, 5.1]), # , :sigma=>[-0.2, 0.2]), \n",
    "(;params...) -> costfunc(startU; do_plot=false, verbose=true, merge(model_params, Dict(params))...), \n",
    " verbose=true, start_eta=1, tol=1e-16, hardbox=true )\n",
    "\n",
    "# params, cost, ptraj, gtraj = fluxSense(costfunc, backward, model_params, startU, fluxFinalPoint, args, seed; \n",
    "#    start_eta=0.01, tol=1e-15, maxiter=400, verbose=true, report_every=1, do_plot=false, cost_limit=cost_limit) # cost_limit=-0.000935) # for beta=0.01\n",
    "\n",
    "# And show the final position\n",
    "clf()\n",
    "costfunc(startU; do_plot=true, verbose=true, make_dict(args, params, model_params)...)\n",
    "params'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function JJ(initUs; theta1=0.15, theta2=0.2, beta=0.003, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, pre_string=\"\", zero_last_sigmas=0, seedrand=NaN, params...)\n",
    "\n",
    "    if ~isnan(seedrand); srand(seedrand); end\n",
    "    \n",
    "    Vend = ForwardDiffZeros(size(initUs,1), size(initUs,2), nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:size(initUs,1)\n",
    "        if false # i>size(initUs,1) - zero_last_sigmas\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; sigma=0, nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)            \n",
    "        else\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)\n",
    "        end\n",
    "        Vend[i,:] = Ve\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend[:,1]-Vend[:,2])/theta1))\n",
    "    diffs = tanh.((Vend[:,1]-Vend[:,2])/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"%s\", pre_string)\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "\n",
    "\n",
    "# The following sequence leads to a situation where having only [-0.8, -0.8] as the single finalFluxPoint \n",
    "# leads to the minimization getting stuck.  Adding further finalFluxPoints solves the problem\n",
    "#\n",
    "srand(11)\n",
    "startU=randn(100,2)-3\n",
    "startU=randn(100,2)-3\n",
    "\n",
    "dt = 0.02\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "W = -4\n",
    "noise = 0\n",
    "input = 0\n",
    "sigma = 0\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    ":noise=>noise, :input=>input, :sigma=>sigma, :const_add=>0, :init_add=>0)\n",
    "\n",
    "\n",
    "# WORKING gradient:\n",
    "# ForwardDiff.gradient((x)->JJ(startU; do_plot=true, nderivs=length(x), difforder=1, \n",
    "#    make_dict([[\"init_add\" 2], \"const_add\"], x, model_params)...), [2.9, -2.9, 0.1])\n",
    "\n",
    "\n",
    "\n",
    "# The backward and costfunc functions should turn a single-scalar parameter W into the matrix W\n",
    "# backward always runs with no within-forward noise, i.e., sigma=0\n",
    "backward = (endpoint; do_plot=false, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;     \n",
    "    backwardsModel(endpoint; do_plot=do_plot, make_dict([\"sigma\"], [0], pars)...)[1]\n",
    "end\n",
    "\n",
    "\n",
    "beta = 0.0001;\n",
    "beta = 0.003;\n",
    "beta = 0.003;\n",
    "beta=0\n",
    "\n",
    "costfunc = (startpoints; do_plot=false, verbose=false, nderivs=0, difforder=0, sr=26, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;         \n",
    "    JJ(startpoints; seedrand=sr, beta=beta, \n",
    "        do_plot=do_plot, verbose=verbose, nderivs=nderivs, difforder=difforder, pars...)\n",
    "end\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "if beta==0.003;     cost_limit = -0.00288\n",
    "elseif beta<0.001;  cost_limit = -0.0008\n",
    "elseif beta==0.001; cost_limit = -0.000935\n",
    "elseif beta==0.05;  cost_limit = -0.0485\n",
    "else\n",
    "    error(\"Don't know what cost limit goes with beta %g\\n\", beta)\n",
    "end\n",
    "\n",
    "fluxFinalPoint = [-0.8 -0.8; -0.6 -0.6 ; -0.4 -0.4; -0.2 -0.2; 0 0; 0.2 0.2]\n",
    "fluxFinalPoint = zeros(0,2);\n",
    "\n",
    "\n",
    "args = [[\"init_add\" 2], \"const_add\", \"W\"] # , \"sigma\"]\n",
    "seed = [2, 2, 2.1, -1] # , 0.1]\n",
    "\n",
    "\n",
    "\n",
    "clf()\n",
    "print(\"seed = \"); print_vector_g(seed); print(\"\\n\")\n",
    "costfunc(startU; do_plot=true, verbose=true, make_dict(args, seed, model_params)...)\n",
    "\n",
    "# :sigma=>[-0.3, 0.3] does fine but :sigma=>[-0.2, 0.2] gets stuck.\n",
    "# If we fix sigma at 0 it also gets stuck, but dynamics kind of odd, W a bit to big, or decrease dt\n",
    "params, traj = bbox_Hessian_keyword_minimization(seed, args, Dict(:init_add=>[-5.1, 5.1], :W=>[-5.1, 5.1]), # , :sigma=>[-0.2, 0.2]), \n",
    "(;params...) -> costfunc(startU; do_plot=false, verbose=true, merge(model_params, Dict(params))...), \n",
    " verbose=true, start_eta=1, tol=1e-16, hardbox=true )\n",
    "\n",
    "# params, cost, ptraj, gtraj = fluxSense(costfunc, backward, model_params, startU, fluxFinalPoint, args, seed; \n",
    "#    start_eta=0.01, tol=1e-15, maxiter=400, verbose=true, report_every=1, do_plot=false, cost_limit=cost_limit) # cost_limit=-0.000935) # for beta=0.01\n",
    "\n",
    "# And show the final position\n",
    "clf()\n",
    "costfunc(startU; do_plot=true, verbose=true, make_dict(args, params, model_params)...)\n",
    "params'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(args)\n",
    "\n",
    "params'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf()\n",
    "costfunc(startU; do_plot=true, verbose=true, make_dict(args, [2, 1.7, 1.3, -1.4, -0.5], model_params)...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure(2)\n",
    "clf()\n",
    "plot(traj[1,:], \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf()\n",
    "cost, grad, hess = keyword_gradient((;params...) -> \n",
    "costfunc(startU; do_plot=true, verbose=true, merge(model_params, Dict(params))...), args, [0.6, -0.6, 1.5, -2])\n",
    "\n",
    "\n",
    "# costfunc(startU; do_plot=true, verbose=true, make_dict(args, [0.6, -0.6, 1.5, -2], model_params)...)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf()\n",
    "cost, gradB, hessB = keyword_vgh((;params...) -> \n",
    "costfunc(startU; do_plot=true, verbose=true, merge(merge(model_params, Dict(params)), Dict(:dt=>0.005, :nsteps=>201))...), \n",
    "args, [0.6, -0.6, 1.5, -2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[gradA gradB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf()\n",
    "costfunc(startU; do_plot=true, verbose=true, \n",
    "merge(make_dict(args, params, model_params), Dict(:dt=>0.005, :nsteps=>201))...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure(2);\n",
    "clf()\n",
    "subplot(2,1,1); plot(cost', \".\")\n",
    "subplot(2,1,2); \n",
    "guys = 4:5\n",
    "ng = sqrt(sum(gtraj[guys,:].*gtraj[guys,:],1))\n",
    "plot(sum(gtraj[guys,1:end-1].*gtraj[guys,2:end],1)'./(ng[1:end-1].*ng[2:end]), \".\")\n",
    "grid(true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beginning of attempt at finding saddle points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt = 0.02\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "W = -4\n",
    "noise = 0\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    "    :noise=>noise, :noise=>noise, :const_add=>0, :init_add=>0)\n",
    "\n",
    "@time(trust_region_Hessian_minimization([-2.1, -2.1], \n",
    "    (x)->forwardModel(x; do_plot=false, nderivs=2, difforder=1, dUdt_mag_only=true, model_params...), \n",
    "verbose=false, start_eta=0.1, tol=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "backward = (endpoint; do_plot=false, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;     \n",
    "    backwardsModel(endpoint; do_plot=do_plot, make_dict([\"sigma\"], [0], pars)...)[1]\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hessian_fluxSense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "function bbox_Hessian_keyword_minimization(seed, args, bbox, func; wallwidth=NaN, start_eta=10, tol=1e-6, \n",
    "    maxiter=400, verbose=false)\n",
    "\n",
    "Like constrained_Hessian_minimization, but uses keyword_hessian!(). \n",
    "\n",
    "PARAMETERS:\n",
    "===========\n",
    "\n",
    "seed        column vector, representing the starting value of the parameters.\n",
    "\n",
    "args        List of strings identifying parameters for differentiation, e.g., [\"const_E\", \"w_self]\n",
    "\n",
    "bbox        An nargs-by-2 matrix indicating the range for each argument,\n",
    "            with the minima (first column) and maxima (second column).\n",
    "\n",
    "func        func must take only optional keyword args, and must \n",
    "            take nderivs=0, difforder=0  and declare any new matrices using ForwardDiffZeros() instead of zeros()\n",
    "\n",
    "\n",
    "OPTIONAL PARAMETERS:\n",
    "====================\n",
    "\n",
    "start_eta=10 Starting value of the radius.  It's good to start with somethibg biggish, if it is\n",
    "             too much, it'll quickly get cut down.\n",
    "\n",
    "tol=1e-6     Numerical tolerance. If a proposed jump produces a change in func that is less than\n",
    "             this, the minimization stops.\n",
    "\n",
    "maxiter=400  Maximum number of iterations to do before stopping\n",
    "\n",
    "verbose=false   If true, print out a report on each iteration of iteration number, radius size (eta),\n",
    "                what type jump was proposed (\"Newton\" means going straight to global min, \"constrained\" means jump has \n",
    "                norm eta, failed means that finding the minimum at a given radius somehow didn't work). Will also\n",
    "                print out the cosine of the angle between the proposed jump and the gradient.\n",
    "\n",
    "hardbox=false   If true, ignores wallwidth, and just rests parameter values to the bounding box if they go outside it.\n",
    "                If false, adds cost function \"walls\" to implement the bounding box.\n",
    "\n",
    "walldith=NaN     Used for putting up cost function \"walls\" that implement the bounding box limits. Can be NaN.\n",
    "                If it is NaN, then the wallwidth is a constant factor of the range width for each argument. If not NaN, must\n",
    "                be an nargs-long vector that indicates the actual wall widths.\n",
    "\n",
    "wallwidth_factor=0.18   Only relevant if wallwidth is NaN, otherwise ignored. For each arg, the wall width\n",
    "                is going to be wall_width_factor*(bbox[i,2] - bbox[i,1])\n",
    "\n",
    "\n",
    "RETURNS:\n",
    "========\n",
    "\n",
    "params       A vector the size of seed that has the last values of the minimizing parameters for func\n",
    "trajectory   A (2+length(params))-by-nsteps matrix. Each column corresponds to an iteration step, and contains\n",
    "                 the value of eta used, the cost, and the value of the parameters at that iteration\n",
    "cost         Final value of objective function\n",
    "\n",
    "\n",
    "EXAMPLE:\n",
    "========\n",
    "\n",
    "function tester(;x=5, y=10, z=20, nderivs=0, difforder=0)\n",
    "    return x^2*y + z/tanh(y)\n",
    "end\n",
    "\n",
    "params, trajectory = bbox_Hessian_keyword_minimization([0.5, 0.5], [\"x\", \"y\"], [1.1 2 ; 1.1 4], tester, \n",
    "    verbose=true, tol=1e-12, start_eta=1);\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# function bbox_Hessian_keyword_minimization(seed, args, bbox, func; \n",
    "    \n",
    "start_eta=10 \n",
    "tol=1e-6 \n",
    "maxiter=400\n",
    "verbose=false\n",
    "verbose_every=1 \n",
    "wallwidth=NaN \n",
    "wallwidth_factor=0.18\n",
    "hardbox=false\n",
    "\n",
    "\n",
    "    \n",
    "    traj_increment = 100\n",
    "    params = seed\n",
    "    eta = start_eta\n",
    "    trajectory = zeros(2+length(params), traj_increment)\n",
    "\n",
    "    if verbose\n",
    "        @printf \"%d: eta=%g ps=\" 0 eta \n",
    "        print_vector_g(params)\n",
    "        @printf \"\\n\"\n",
    "    end\n",
    "    \n",
    "    if hardbox\n",
    "        cost, grad, hess = keyword_vgh((;pars...) -> func(;pars...), args, params)\n",
    "    else\n",
    "        cost, grad, hess = keyword_vgh((;pars...) -> func(;pars...) + wall_cost(args, bbox; wallwidth=wallwidth, pars...),\n",
    "            args, params)        \n",
    "    end\n",
    "        \n",
    "    chessdelta = zeros(size(params))\n",
    "\n",
    "    for i=1:maxiter\n",
    "        if i > size(trajectory, 2)\n",
    "            trajectory = [trajectory zeros(2+length(params), traj_increment)]\n",
    "        end\n",
    "        trajectory[1:2, i]   = [eta;cost]\n",
    "        trajectory[3:end, i] = params\n",
    "        \n",
    "        hessdelta  = - inv(hess)*grad\n",
    "        try\n",
    "            chessdelta = constrained_parabolic_minimization(hess, grad'', eta)[1]\n",
    "            jumptype = \"not failed\"\n",
    "        catch y\n",
    "            jumptype = \"failed\"\n",
    "            if verbose\n",
    "                @printf \"Constrained parabolic minimization failed with error %s\\n\" y\n",
    "                @printf \"\\n\"\n",
    "                @printf \"eta was %g\\n\" eta\n",
    "                @printf \"grad was\\n\"\n",
    "                print_vector(grad)\n",
    "                @printf \"\\n\\nhess was\\n\"\n",
    "                for k in [1:length(grad);]\n",
    "                    print_vector(hess[k,:])\n",
    "                    @printf \"\\n\"\n",
    "                end\n",
    "                @printf \"\\n\"\n",
    "            end\n",
    "        end\n",
    "\n",
    "        if norm(hessdelta) <= eta\n",
    "            new_params = params + hessdelta\n",
    "            jumptype = \"Newton\"\n",
    "        elseif jumptype != \"failed\" \n",
    "            new_params = params + chessdelta\n",
    "            jumptype  = \"constrained\"\n",
    "        end\n",
    "\n",
    "        if jumptype != \"failed\"\n",
    "            if hardbox\n",
    "                for p in [1:length(new_params);]\n",
    "                    if new_params[p] < bbox[p,1]; new_params[p] = bbox[p,1]; end\n",
    "                    if bbox[p,2] < new_params[p]; new_params[p] = bbox[p,2]; end\n",
    "                 end        \n",
    "                \n",
    "                new_cost, new_grad, new_hess = keyword_vgh((;pars...) -> func(;pars...), args, new_params)\n",
    "            else\n",
    "                new_cost, new_grad, new_hess = keyword_vgh((;pars...) -> func(;pars...) + \n",
    "                        wall_cost(args, bbox; wallwidth=wallwidth, pars...),\n",
    "                    args, new_params)                \n",
    "            end\n",
    "            \n",
    "            if abs(new_cost - cost) < tol || eta < tol\n",
    "                trajectory = trajectory[:,1:i]\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "\n",
    "        if jumptype == \"failed\" || new_cost >= cost  \n",
    "            eta = eta/2\n",
    "            costheta = NaN\n",
    "            if eta < tol\n",
    "                trajectory = trajectory[:,1:i]\n",
    "                break\n",
    "            end\n",
    "        else\n",
    "            eta = eta*1.1\n",
    "            costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "\n",
    "            params = new_params\n",
    "            cost = new_cost\n",
    "            grad = new_grad\n",
    "            hess = new_hess\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            if rem(i, verbose_every)==0\n",
    "                @printf \"%d: eta=%g cost=%.4f jtype=%s costheta=%.3f ps=\" i eta cost jumptype costheta\n",
    "                print_vector(params)\n",
    "                @printf \"\\n\"\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return params, trajectory, cost\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    Given args, a list of string representing the arguments of interest, a bounding box for each,\n",
    "    and a Symbol=>value dictionary with the corresponding parameters, computes and returns a high cost for \n",
    "    being outside the bounding box\n",
    "    \"\"\"\n",
    "    function wall_cost(args, bbox; wallwidth=NaN, nderivs=0, difforder=0, pars...) \n",
    "        myparams = ForwardDiffZeros(length(pars), 1, nderivs=nderivs, difforder=difforder)\n",
    "        pars2 = Dict()\n",
    "        for i in [1:length(pars);]\n",
    "            pars2[string(pars[i][1])] = pars[i][2]\n",
    "        end\n",
    "        for i in [1:length(args);]\n",
    "            myparams[i] = pars2[args[i]]\n",
    "        end\n",
    "        \n",
    "        if isnan(wallwidth)\n",
    "            # We know that we're going to be taking hessian for params, so declare zeros accordingly:\n",
    "            wallwidth = ForwardDiffZeros(length(myparams), 1, nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "            for i in [1:length(myparams);]\n",
    "                wallwidth[i] = wallwidth_factor*(bbox[i,2]-bbox[i,1])\n",
    "            end\n",
    "        end\n",
    "\n",
    "        retval = 0\n",
    "        for i in [1:length(myparams);]\n",
    "            if myparams[i]<bbox[i,1]\n",
    "                retval += cosh((bbox[i,1]-myparams[i])/wallwidth[i])-1.0\n",
    "            elseif bbox[i,2] < myparams[i]\n",
    "                retval += cosh((myparams[i]-bbox[i,2])/wallwidth[i])-1.0                \n",
    "            end\n",
    "        end\n",
    "\n",
    "        return 2*retval\n",
    "    end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf()\n",
    "costfunc(startU; do_plot=true, verbose=true, make_dict([\"sigma\"], [0.1], make_dict(args, params, model_params))...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working on figuring out the weird trajectories. Probably a dt thing.  The fourth one is the weird one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forward = (startpoint; do_plot=false, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;     \n",
    "    forwardModel(startpoint; do_plot=do_plot, pars...)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTION DEFINITION: fluxSense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The main fluxSense() function containing the main minimization loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function fluxSense(costfunc, backward, paramsDict, startUs, ends, args, seed; start_eta=0.01, tol=1e-15, \n",
    "    maxiter=400, verbose=true, do_plot=false, cost_limit=[], report_fluxless_grad=false, report_every=1)\n",
    "   \n",
    "    if do_plot; clf(); end;\n",
    "\n",
    "    params = seed\n",
    "    eta    = start_eta\n",
    "\n",
    "    if ~(typeof(ends)<:Array); ends = [ends]; end\n",
    "    U0 = zeros(size(ends))\n",
    "    for j in 1:size(ends,1)\n",
    "        # @printf(\"model params is \"); print(model_params); print(\"\\n\")\n",
    "        # @printf(\"ends[j,:] is \"); print(ends[j,:]); print(\"\\n\")        \n",
    "        U0[j,:] = backward(ends[j,:]; tol=1e-25, do_plot=false, make_dict(args, params, model_params)...)'\n",
    "        # @printf(\"U0[j,:] is \"); print(U0[j,:]); print(\"\\n\")        \n",
    "    end\n",
    "    \n",
    "    if length(ends)>0\n",
    "        @printf(\"U0[end,:] is \"); print_vector_g(U0[end,:]); @printf(\"\\n\")\n",
    "    end\n",
    "    \n",
    "    cost, grad, hess = \n",
    "        vgh((x)->costfunc([startUs;U0]; do_plot=do_plot, nderivs=length(x), difforder=2, \n",
    "            make_dict(args, x, model_params)...), params)\n",
    "    \n",
    "    if verbose && report_fluxless_grad\n",
    "        fcost, fgrad, fhess = \n",
    "        vgh((x)->costfunc(startUs; do_plot=false, nderivs=length(x), difforder=2, \n",
    "                make_dict(args, x, model_params)...), params)\n",
    "        @printf(\"      ### grad without flux track = \"); print_vector_g(fgrad); @printf(\"\\n\")    \n",
    "    end\n",
    "\n",
    "    if verbose\n",
    "        @printf(\"Initial cost, grad, hess:\\n\")\n",
    "        @printf(\"   cost = %g\\n\", cost)\n",
    "        @printf(\"   grad = \"); print_vector_g(grad); print(\"\\n\")\n",
    "        @printf(\"   hess = \"); print_vector_g(hess); print(\"\\n\")\n",
    "    end\n",
    "\n",
    "    delta_params=0\n",
    "    ptrajectory = zeros(length(seed), maxiter); \n",
    "    gtrajectory = zeros(length(seed), maxiter); \n",
    "    ctrajectory = zeros(1, maxiter);\n",
    "    \n",
    "    for i in [1:maxiter;]         \n",
    "        my_verbose = verbose && rem(i, report_every)==0\n",
    "\n",
    "        new_params = params - eta*grad/(sqrt(sum(grad.*grad)))\n",
    "        delta_params = new_params - params\n",
    "\n",
    "        new_cost, new_grad, new_hess = \n",
    "        vgh((x)->costfunc([startUs;U0]; do_plot=false, verbose=my_verbose, pre_string=\"   newpars>> \",\n",
    "                zero_last_sigmas=size(U0,1), nderivs=length(x), difforder=2, make_dict(args, x, model_params)...), \n",
    "                new_params)\n",
    "\n",
    "        if my_verbose\n",
    "            @printf(\"delta_params=\"); print_vector_g(delta_params); @printf(\"\\n\"); \n",
    "            @printf(\"new_cost=%g  cost=%g   delta_cost=%g\\n\", new_cost, cost, new_cost-cost)\n",
    "        end\n",
    "        \n",
    "        if abs(new_cost - cost) < tol\n",
    "            @printf(\"\\n===\\nChange in cost was less than the tolerance %g\\n===\\n\", tol)\n",
    "            ptrajectory=ptrajectory[:,1:i-1]; gtrajectory=gtrajectory[:,1:i-1]; ctrajectory=ctrajectory[1:i-1]\n",
    "            break\n",
    "        end\n",
    "        if (length(cost_limit)>0 && cost < cost_limit)\n",
    "            @printf(\"\\n===\\nCost was less than the cost limit %g\\n===\\n\", cost_limit)\n",
    "            ptrajectory=ptrajectory[:,1:i-1]; gtrajectory=gtrajectory[:,1:i-1]; ctrajectory=ctrajectory[1:i-1]\n",
    "            break\n",
    "        end\n",
    "        \n",
    "        if new_cost >= cost\n",
    "            eta = eta/2\n",
    "            costheta = NaN\n",
    "        else\n",
    "            eta = eta*1.2\n",
    "            costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "\n",
    "            params = new_params\n",
    "    \n",
    "            for j in 1:size(ends,1)\n",
    "                U0[j,:] = backward(ends[j,:]; do_plot=false, make_dict(args, params, model_params)...)'\n",
    "            end\n",
    "            if my_verbose && length(ends)>0\n",
    "                @printf(\"U0[end,:] is \"); print_vector_g(U0[end,:]); @printf(\"\\n\")\n",
    "            end\n",
    "            cost, grad, hess = \n",
    "                vgh((x)->costfunc([startUs;U0]; do_plot=do_plot, verbose=my_verbose, nderivs=length(x), difforder=2, \n",
    "                    zero_last_sigmas=size(U0,1), make_dict(args, x, model_params)...), params)\n",
    "\n",
    "        end\n",
    "        \n",
    "        ptrajectory[:,i] = params\n",
    "        gtrajectory[:,i] = grad\n",
    "        ctrajectory[i]   = cost\n",
    "\n",
    "        if my_verbose\n",
    "            @printf \"%d: eta=%g cost=%g costheta=%g ps=\" i eta cost  costheta\n",
    "            print_vector(params)\n",
    "            @printf \"\\n\"\n",
    "            @printf(\"grad=\"); print_vector_g(grad); @printf(\"\\n\")\n",
    "            if report_fluxless_grad\n",
    "                fcost, fgrad, fhess = \n",
    "                vgh((x)->costfunc(startUs; do_plot=false, verbose=false, nderivs=length(x), difforder=2, \n",
    "                        make_dict(args, x, model_params)...), params)\n",
    "            @printf(\"      ### grad without flux track = \"); print_vector_g(fgrad); @printf(\"\\n\")    \n",
    "            end            \n",
    "        end\n",
    "    end    \n",
    "\n",
    "    return params, ctrajectory, ptrajectory, gtrajectory\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORKING EXAMPLE:   1-D example of using fluxSense()\n",
    "\n",
    "## after defining fluxSense(), run the next two cells in order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the cost function. It passes most keyword params down to the forward and backward models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function J(initUs; theta1=0.15, theta2=0.2, beta=0.01, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, params...)\n",
    "\n",
    "    Vend = ForwardDiffZeros(length(initUs), 1, nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:length(initUs)\n",
    "        Ue, Ve, U, V = forwardModel(initUs[i]; nderivs=nderivs, difforder=difforder, \n",
    "            do_plot=do_plot, clearfig=false, params...)\n",
    "        Vend[i] = Ve[1]\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend-0.5)/theta1))\n",
    "    diffs = tanh((Vend-0.5)/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now setup, run fluxSense(), and display results. Example is only 1-d so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "backward = (endpoint; do_plot=false, pars...) -> backwardsModel(endpoint; do_plot=do_plot, pars...)[1]\n",
    "\n",
    "costfunc = (startpoints; do_plot=false, verbose=false, nderivs=0, difforder=0, pars...) -> \n",
    "J(startpoints; do_plot=do_plot, verbose=verbose, beta=0.01, nderivs=nderivs, difforder=difforder, pars...)\n",
    "\n",
    "dt = 0.01\n",
    "t = 0:dt:2\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "noise = 0*randn(1, nsteps)\n",
    "noise = 0.02*sin(2*pi*3*t); noise=reshape(noise, 1, nsteps)\n",
    "\n",
    "W = 4.1\n",
    "const_add = -2\n",
    "init_add=0\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>W, :nsteps=>nsteps, \n",
    "    :noise=>noise, :noise=>noise, :const_add=>const_add, :init_add=>init_add)\n",
    "\n",
    "srand(20)  \n",
    "startUs = randn(20, 1)       # The starting values\n",
    "# startUs = [randn(10,1)+2;randn(10,1)-2]\n",
    "\n",
    "\n",
    "args = [\"init_add\", \"const_add\", \"W\"]\n",
    "seed = [0, -2, 4.1]\n",
    "\n",
    "# Do an initial run plotting to show the starting position\n",
    "clf()\n",
    "costfunc(startUs; do_plot=true, verbose=true, model_params...)\n",
    "\n",
    "\n",
    "fluxFinalPoint = convert(Float64, 0)  # The final value of the pinned output\n",
    "#\n",
    "# If you remove the fluxFinalPoint, by un-commenting the following line, it gets stuck. But\n",
    "# it is also true that if you make beta=0 (in the constfunc() definition in line 4 above) then t\n",
    "# that also solves the sticking problem.  If we had beta=0 until after our hits are what we want, \n",
    "# would we ever need fluxPoint?\n",
    "#\n",
    "fluxFinalPoint = [];\n",
    "\n",
    "params, cost = fluxSense(costfunc, backward, model_params, startUs, fluxFinalPoint, args, seed; \n",
    "start_eta=0.01, tol=1e-15, maxiter=400, verbose=true, report_fluxless_grad=false, do_plot=true, cost_limit=-0.00959)\n",
    "\n",
    "# And show the final position\n",
    "clf()\n",
    "costfunc(startUs; do_plot=true, verbose=true, make_dict(args, params, model_params)...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---  END --- 1d example of using fluxSense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of forwards and backwards models\n",
    "\n",
    "### Inverting time even through a sinusoidal noise, with added noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt = 0.01\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "noise = 0*randn(1, nsteps)\n",
    "noise = 0.2*sin(2*pi*3*t); noise=reshape(noise, 1, nsteps)\n",
    "W = [0.5]\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>W, :nsteps=>nsteps, \n",
    "    :noise=>noise, :noise=>noise, :const_add=>-0.15, :init_add=>0.3)\n",
    "clf();\n",
    "\n",
    "srand(10)\n",
    "\n",
    "startUs = -0.5\n",
    "Uend, Vend, U, V =forwardModel(startUs; do_plot=true, clearfig=false, model_params...)\n",
    "Ustart, Vstart = backwardsModel(Uend;  do_plot=true, clearfig=false, tol=1e-15, model_params...)\n",
    "\n",
    "[startUs Ustart]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD: scripts on the path to writing fluxSense() as a function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the cost function. It passes most keyword params down to the forward and backward models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function J(initUs; theta1=0.15, theta2=0.2, beta=0.01, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, params...)\n",
    "\n",
    "    Vend = ForwardDiffZeros(length(initUs), 1, nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:length(initUs)\n",
    "        Ue, Ve, U, V = forwardModel(initUs[i]; nderivs=nderivs, difforder=difforder, \n",
    "            do_plot=do_plot, clearfig=false, params...)\n",
    "        Vend[i] = Ve[1]\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend-0.5)/theta1))\n",
    "    diffs = tanh((Vend-0.5)/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An example of some code that does differentiation. This cell not necessary for running the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# An example of a standard setup which we'll try to modify to try to get 75% correct\n",
    "\n",
    "dt = 0.01\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "noise = 0*randn(1, nsteps)\n",
    "noise = 0.02*sin(2*pi*3*t); noise=reshape(noise, 1, nsteps)\n",
    "\n",
    "W = 4.1\n",
    "const_add = -2\n",
    "init_add=0\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>W, :nsteps=>nsteps, \n",
    ":noise=>noise, :noise=>noise, :const_add=>const_add, :init_add=>init_add)\n",
    "\n",
    "srand(10)\n",
    "startUs = randn(40, 1)\n",
    "J(startUs; do_plot=true, verbose=true, model_params...)\n",
    "\n",
    "# --- now while taking the derivative ---\n",
    "args = [\"init_add\", \"const_add\", \"W\"]\n",
    "seed = [init_add, const_add, W]\n",
    "\n",
    "ForwardDiff.gradient((x)->J(startUs; do_plot=true, nderivs=length(x), difforder=1, verbose=true, make_dict(args, x, model_params)...), seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main adaptive step with gradient and keywords loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################################\n",
    "#                                                   #\n",
    "#                                                   #\n",
    "#     ADAPTIVE GRADIENT FOR KEYWORD VERSION         #\n",
    "#                                                   #\n",
    "#                                                   #\n",
    "#####################################################\n",
    "\n",
    "\n",
    "# -----------------  FORWARD MODEL SETUP ---------------\n",
    "dt = 0.01\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "noise = 0*randn(1, nsteps)\n",
    "noise = 0.02*sin(2*pi*3*t); noise=reshape(noise, 1, nsteps)\n",
    "\n",
    "W = 4.1\n",
    "const_add = -2\n",
    "init_add=0\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>W, :nsteps=>nsteps, \n",
    ":noise=>noise, :noise=>noise, :const_add=>const_add, :init_add=>init_add)\n",
    "\n",
    "\n",
    "# ----------------  CRITICAL INDICATION OF PARAMETERS TO OPTIMIZE IS HERE: -----\n",
    "args = [\"init_add\", \"const_add\", \"W\"]\n",
    "seed = [0, -2, 4.1]\n",
    "\n",
    "fluxFinalPoint = convert(Float64, 0)  # The final value of the pinned output\n",
    "\n",
    "srand(10)  \n",
    "startUs = randn(200, 1)       # The starting values\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "clf()\n",
    "\n",
    "start_eta = 0.01\n",
    "tol = 1e-15\n",
    "maxiter = 400\n",
    "verbose = true\n",
    "do_plot=false\n",
    "\n",
    "# -------\n",
    "\n",
    "params = seed\n",
    "eta    = start_eta\n",
    "\n",
    "\n",
    "U0 = backwardsModel(fluxFinalPoint; do_plot=false, make_dict(args, params, model_params)...)[1]\n",
    "J([startUs;U0]; verbose=true, do_plot=true, make_dict(args, params, model_params)...)\n",
    "\n",
    "\n",
    "cost, grad, hess = \n",
    "    vgh((x)->J([startUs;U0]; do_plot=false, nderivs=length(x), difforder=2, make_dict(args, x, model_params)...), params)\n",
    "\n",
    "@printf(\"Initial cost, grad, hess:\\n\")\n",
    "print_vector_g(:cost)\n",
    "print_vector_g(:grad)\n",
    "print_vector_g(:hess)\n",
    "delta_params=0\n",
    "\n",
    "\n",
    "for i in 1:maxiter         \n",
    "        new_params = params - eta*grad/(sqrt(sum(grad.*grad)))\n",
    "        delta_params = new_params - params\n",
    "        print_vector_g(:delta_params)\n",
    "        new_cost, new_grad, new_hess = \n",
    "            vgh((x)->J([startUs;U0]; do_plot=false, verbose=false,\n",
    "                nderivs=length(x), difforder=2, make_dict(args, x, model_params)...), new_params)\n",
    "        @printf(\"new_cost=%g  cost=%g   delta_cost=%g\\n\", new_cost, cost, new_cost-cost)\n",
    "        if abs(new_cost - cost) < tol\n",
    "            break\n",
    "        end\n",
    "\n",
    "        if new_cost >= cost\n",
    "            eta = eta/2\n",
    "            costheta = NaN\n",
    "        else\n",
    "            eta = eta*1.2\n",
    "            costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "\n",
    "            params = new_params\n",
    "    \n",
    "            U0 = backwardsModel(fluxFinalPoint; do_plot=false, make_dict(args, params, model_params)...)[1]\n",
    "            cost, grad, hess = \n",
    "                vgh((x)->J([startUs;U0]; do_plot=do_plot, verbose=true,\n",
    "                    nderivs=length(x), difforder=2, make_dict(args, x, model_params)...), params)\n",
    "\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            @printf \"%d: eta=%g cost=%g costheta=%g ps=\" i eta cost  costheta\n",
    "            print_vector(params)\n",
    "            @printf \"\\n\"\n",
    "        end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A cell to plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# J(params; initUs=[startUs;U0], verbose=true, do_plot=true)\n",
    "clf()\n",
    "Ve = zeros(length(startUs),1)\n",
    "for i=1:length(startUs)\n",
    "    Ue, Vee, U, V = forwardModel(startUs[i]; do_plot=true, clearfig=false, make_dict(args, params, model_params)...)\n",
    "    Ve[i] = Vee[1]\n",
    "end\n",
    "\n",
    "@printf(\"\\n\\nFinal result produces %d hits out of %d trials for %.1f per cent correct\\n\\n\", length(find(Ve.>0.5)), \n",
    "    length(Ve), 100*length(find(Ve.>0.5))/length(Ve))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- END Complete keyword-driven adaptive gradient version of FluxSense minimizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete adaptive gradient version of FluxSense minimizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################################\n",
    "#                                                   #\n",
    "#                                                   #\n",
    "#     In this cell we define J(x::Vector[3])        #\n",
    "#     Not full keyword version yet.                 #\n",
    "#                                                   #\n",
    "#     Next cell has the adaptive gradient procedure #\n",
    "#                                                   #\n",
    "#     Run the third cell to see results             #\n",
    "#                                                   #\n",
    "#                                                   #\n",
    "#####################################################\n",
    "\n",
    "dt = 0.01\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "\n",
    "W = [4]\n",
    "k = -2\n",
    "init_k = 0\n",
    "\n",
    "noise = 0.2*sin(2*pi*3*t); noise = reshape(noise, 1, nsteps)\n",
    "\n",
    "mypars = Dict(:dt=>dt, :tau=>tau, :nsteps=>nsteps)\n",
    "\n",
    "srand(10)\n",
    "startUs = 2*randn(200,1)\n",
    "# for i=1:length(startUs)\n",
    "#    Uend, Vend, U, V = forwardModel(startUs[i]; noise=noise+k, W=W, do_plot=true, clearfig=false, params...)\n",
    "#end\n",
    "\n",
    "# backwardsModel([1.2*0]; do_plot=true, clearfig=false, params...)\n",
    "\n",
    "function J(x; initUs=startUs, theta1=0.15, theta2=0.2, beta=0.05, verbose=false,\n",
    "    nderivs=0, difforder=0, do_plot=true)\n",
    "    \n",
    "    Vend = ForwardDiffZeros(length(initUs), 1, nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    k = x[1]\n",
    "    W = x[2]\n",
    "    init_k = x[3]\n",
    "    \n",
    "    for i=1:length(initUs)\n",
    "        Ue, Ve, U, V = forwardModel(initUs[i]+init_k; noise=noise+k, W=[W], \n",
    "        nderivs=nderivs, difforder=difforder, do_plot=do_plot, clearfig=false, mypars...)\n",
    "        Vend[i] = Ve[1]\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend-0.5)/theta1))\n",
    "    diffs = tanh((Vend-0.5)/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "\n",
    "\n",
    "# WORKS:\n",
    "# ForwardDiff.gradient((x)->forwardModel(x[1]; noise=noise+k, W=[x[2]], \n",
    "#    do_plot=true, clearfig=true, nderivs=2, difforder=1, params...)[1][1], [-2.1, 4])\n",
    "\n",
    "ForwardDiff.gradient((x)->J(x; nderivs=3, difforder=1), [-2, 4.1, 0])\n",
    "# J([-2.1, 4])\n",
    "\n",
    "\n",
    "# ForwardDiff.derivative((x)->forwardModel(startUs[1]; noise=noise+k, W=[x], \n",
    "#    do_plot=true, clearfig=true, nderivs=1, difforder=1, params...)[1], 4.5995)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################################\n",
    "#                                                   #\n",
    "#                                                   #\n",
    "#     ADAPTIVE GRADIENT VERSION                     #\n",
    "#                                                   #\n",
    "#                                                   #\n",
    "#####################################################\n",
    "\n",
    "# This is all BEFORE makign J fully keyword-value driven\n",
    "\n",
    "seed = [-2, 4.1, 0]   # params are constant add, W, and init_add.\n",
    "start_eta = 0.01\n",
    "tol = 1e-15\n",
    "maxiter = 400\n",
    "verbose = true\n",
    "\n",
    "params = seed\n",
    "eta = start_eta\n",
    "\n",
    "U0 = backwardsModel([1.2*0]; noise=noise+params[1], W=[params[2]], do_plot=true, mypars...)[1] - params[3]\n",
    "\n",
    "J(params; initUs=[startUs;U0], verbose=true)\n",
    "\n",
    "cost, grad, hess = vgh((x)->J(x; initUs=[startUs;U0], nderivs=length(params), difforder=2, verbose=true), params)\n",
    "\n",
    "@printf(\"Initial cost, grad, hess:\\n\")\n",
    "print_vector_g(:cost)\n",
    "print_vector_g(:grad)\n",
    "print_vector_g(:hess)\n",
    "delta_params=0\n",
    "\n",
    "for i in 1:maxiter         \n",
    "        new_params = params - eta*grad/(sqrt(sum(grad.*grad)))\n",
    "        delta_params = new_params - params\n",
    "        print_vector_g(:delta_params)\n",
    "        new_cost, new_grad, new_hess = \n",
    "            vgh((x)->J(x; initUs=[startUs;U0], nderivs=length(params), difforder=2, verbose=false, do_plot=false), \n",
    "                new_params)        \n",
    "        @printf(\"new_cost=%g  cost=%g   delta_cost=%g\\n\", new_cost, cost, new_cost-cost)\n",
    "        if abs(new_cost - cost) < tol\n",
    "            break\n",
    "        end\n",
    "\n",
    "        if new_cost >= cost\n",
    "            eta = eta/2\n",
    "            costheta = NaN\n",
    "        else\n",
    "            eta = eta*1.2\n",
    "            costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "\n",
    "            params = new_params\n",
    "    \n",
    "            U0 = backwardsModel([1.2*0]; noise=noise+params[1], W=[params[2]], do_plot=false, mypars...)[1] - params[3]\n",
    "            cost, grad, hess = \n",
    "                vgh((x)->J(x; initUs=[startUs;U0], nderivs=length(params), difforder=2, verbose=true, do_plot=false), params)\n",
    "\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            @printf \"%d: eta=%g cost=%g costheta=%g ps=\" i eta cost  costheta\n",
    "            print_vector(params)\n",
    "            @printf \"\\n\"\n",
    "        end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# J(params; initUs=[startUs;U0], verbose=true, do_plot=true)\n",
    "clf()\n",
    "Ve = zeros(length(startUs),1)\n",
    "for i=1:length(startUs)\n",
    "    Ue, Vee, U, V = forwardModel(startUs[i]+params[3]; noise=params[1], W=[params[2]],     \n",
    "        do_plot=true, clearfig=false, tau=0.1, nsteps=201, dt=0.01)\n",
    "    Ve[i] = Vee[1]\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------END OF: complete adaptive gradient version of FluxSense minimizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf()\n",
    "J(params; initUs=[startUs;U0], verbose=true, do_plot=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "length(find(Ve.>0.5))/200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mypars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################################\n",
    "#                                                   #\n",
    "#                                                   #\n",
    "#     HESSIAN VERSION                               #\n",
    "#                                                   #\n",
    "#                                                   #\n",
    "#####################################################\n",
    "\n",
    "\n",
    "seed = [-2, 4.1]\n",
    "start_eta = 0.0000001\n",
    "tol = 1e-15\n",
    "maxiter = 400\n",
    "verbose = true\n",
    "\n",
    "params = seed\n",
    "eta = start_eta\n",
    "\n",
    "U0 = backwardsModel([1.2*0]; noise=noise+params[1], W=[params[2]], do_plot=true, mypars...)[1]\n",
    "\n",
    "J(params; initUs=[startUs;U0], verbose=true)\n",
    "\n",
    "cost, grad, hess = vgh((x)->J(x; initUs=[startUs;U0], nderivs=length(params), difforder=2, verbose=true), params)\n",
    "\n",
    "@printf(\"Initial cost, grad, hess:\\n\")\n",
    "print_vector_g(:cost)\n",
    "print_vector_g(:grad)\n",
    "print_vector_g(:hess)\n",
    "\n",
    "for i in 1:maxiter\n",
    "        hathess    = hess + eye(length(grad), length(grad))/eta        \n",
    "        new_params = params - inv(hathess)*grad\n",
    "        new_cost, new_grad, new_hess = \n",
    "            vgh((x)->J(x; initUs=[startUs;U0], nderivs=length(params), difforder=2, verbose=true), new_params)\n",
    "            \n",
    "        if abs(new_cost - cost) < tol\n",
    "            # break\n",
    "        end\n",
    "\n",
    "        if new_cost >= cost\n",
    "            eta = eta/2\n",
    "            costheta = NaN\n",
    "        else\n",
    "            eta = eta*1.1\n",
    "            costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "\n",
    "            params = new_params\n",
    "    \n",
    "            U0 = backwardsModel([1.2*0]; noise=noise+params[1], W=[params[2]], do_plot=false, mypars...)[1]\n",
    "            cost, grad, hess = \n",
    "                vgh((x)->J(x; initUs=[startUs;U0], nderivs=length(params), difforder=2, verbose=true), params)\n",
    "\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            @printf \"%d: eta=%g cost=%g costheta=%g ps=\" i eta cost  costheta\n",
    "            print_vector(params)\n",
    "            @printf \"\\n\"\n",
    "        end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = [-2, 4.1]\n",
    "start_eta = 10\n",
    "tol = 1e-15\n",
    "maxiter = 400\n",
    "\n",
    "params = seed\n",
    "eta = start_eta\n",
    "\n",
    "# backwardsModel([0.5]; noise=noise+params[1], W=params[2], params...)\n",
    "\n",
    "cost, grad, hess = vgh(func, params)\n",
    "\n",
    "\n",
    "    if verbose && verbose_level >= 2\n",
    "        @printf(\"Initial cost, grad, hess:\\n\")\n",
    "        print_vector_g(:cost)\n",
    "        print_vector_g(:grad)\n",
    "        print_vector_g(:hess)\n",
    "    end\n",
    "    \n",
    "\n",
    "    for i in [1:maxiter;]\n",
    "        hathess    = hess + eye(length(grad), length(grad))/eta        \n",
    "        new_params = params - inv(hathess)*grad\n",
    "        new_cost, new_grad, new_hess = vgh(func, new_params)\n",
    "            \n",
    "        if abs(new_cost - cost) < tol\n",
    "            break\n",
    "        end\n",
    "\n",
    "        if new_cost >= cost\n",
    "            eta = eta/2\n",
    "            costheta = NaN\n",
    "        else\n",
    "            eta = eta*1.1\n",
    "            costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "\n",
    "            params = new_params\n",
    "            cost = new_cost\n",
    "            grad = new_grad\n",
    "            hess = new_hess\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            @printf \"%d: eta=%.3f cost=%.4f costheta=%.3f ps=\" i eta cost  costheta\n",
    "            print_vector(params)\n",
    "            @printf \"\\n\"\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return params, cost\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trust_region_Hessian_minimization([-2, 4.1], (x)->J(x; nderivs=2, difforder=2, verbose=true), verbose=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt = 0.01\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "\n",
    "noise = 3.5*sin(2*pi*3*t); noise=reshape(noise, 1, nsteps)\n",
    "W = [0.5]\n",
    "\n",
    "params = Dict(:dt=>dt, :tau=>tau, :nsteps=>nsteps, :noise=>noise)\n",
    "\n",
    "function J(x; nderivs=0, difforder=0)\n",
    "    startU = x[1]\n",
    "    W = x[2]\n",
    "    \n",
    "    Uend, Vend, U, V = forwardModel(startU; do_plot=true, W=[W], nderivs=nderivs, difforder=difforder, params...)\n",
    "    \n",
    "    return (Vend[1]-0.5)^2\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trust_region_Hessian_minimization([-0.5, 0.5], (x) -> J(x;nderivs=2, difforder=2), verbose=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD STUFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- BEGIN --- Old example that gets stuck: too large beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The following sequence leads to a situation where having only [-0.8, -0.8] as the single finalFluxPoint \n",
    "# leads to the minimization getting stuck.  Adding further finalFluxPoints solves the problem.\n",
    "#\n",
    "# Reducing beta in the cost function J() from 0.01 to 0.003 also eliminated the problem.  \n",
    "#\n",
    "srand(10)\n",
    "startU=randn(100,2)-3\n",
    "startU=randn(100,2)-3\n",
    "\n",
    "# startU=randn(100,2)-3\n",
    "\n",
    "dt = 0.02\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "W = -4\n",
    "noise = 0\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    "    :noise=>noise, :noise=>noise, :const_add=>0, :init_add=>0)\n",
    "\n",
    "clf(); subplot(2,1,1)\n",
    "for i in 1:size(startU,1)\n",
    "    Uend, Vend, U, V = forwardModel(startU[i,:]; do_plot=true, clearfig=false, model_params...)\n",
    "end\n",
    "\n",
    "Ustarthat, Vstarthat, Uhatm, Vhat, costs = backwardsModel([-0.8, -0.8]; do_plot=true, clearfig=false, \n",
    "tol=1e-50, maxiter=800, model_params...)\n",
    "\n",
    "subplot(2,1,2)\n",
    "plot(t, costs, \".-\")\n",
    "\n",
    "\n",
    "function JJ(initUs; theta1=0.15, theta2=0.2, beta=0.01, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, pre_string=\"\", params...)\n",
    "\n",
    "    Vend = ForwardDiffZeros(size(initUs,1), size(initUs,2), nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:size(initUs,1)\n",
    "        Ue, Ve, U, V = forwardModel(initUs[i,:]; nderivs=nderivs, difforder=difforder, \n",
    "            do_plot=do_plot, clearfig=false, params...)\n",
    "        Vend[i,:] = Ve\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend[:,1]-Vend[:,2])/theta1))\n",
    "    diffs = tanh.((Vend[:,1]-Vend[:,2])/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"%s\", pre_string)\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "  \n",
    "clf();\n",
    "JJ(startU; do_plot=true, model_params...)\n",
    "\n",
    "# WORKING gradient:\n",
    "# ForwardDiff.gradient((x)->JJ(startU; do_plot=true, nderivs=length(x), difforder=1, \n",
    "#    make_dict([[\"init_add\" 2], \"const_add\"], x, model_params)...), [2.9, -2.9, 0.1])\n",
    "\n",
    "\n",
    "\n",
    "# The backward and costfunc functions should turn a single-scalar parameter W into the matrix W\n",
    "backward = (endpoint; do_plot=false, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;     \n",
    "    backwardsModel(endpoint; do_plot=do_plot, pars...)[1]\n",
    "end\n",
    "\n",
    "costfunc = (startpoints; do_plot=false, verbose=false, nderivs=0, difforder=0, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;         \n",
    "    JJ(startpoints; do_plot=do_plot, verbose=verbose, nderivs=nderivs, difforder=difforder, pars...)\n",
    "end\n",
    "\n",
    "\n",
    "fluxFinalPoint = [-0.8 -0.8]  # ; -0.6 -0.6 ; -0.4 -0.4; -0.2 -0.2; 0 0; 0.2 0.2]\n",
    "\n",
    "args = [[\"init_add\" 2], \"const_add\"] # , \"W\"]\n",
    "seed = [0.001, 0.001, 0] # , -4]\n",
    "# seed = [1.190, -1.178, 2.000]\n",
    "\n",
    "params, cost = fluxSense(costfunc, backward, model_params, startU, fluxFinalPoint, args, seed; \n",
    "start_eta=0.01, tol=1e-15, maxiter=400, verbose=true, do_plot=false, cost_limit=-0.00935) # cost_limit=-0.000935) # for beta=0.01\n",
    "\n",
    "# And show the final position\n",
    "clf()\n",
    "costfunc(startU; do_plot=true, verbose=true, make_dict(args, params, model_params)...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- END --- Old example that gets stuck: too large beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of getting stuck without a flux point even with beta=0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function JJ(initUs; theta1=0.15, theta2=0.2, beta=0.003, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, pre_string=\"\", zero_last_sigmas=0, seedrand=NaN, params...)\n",
    "\n",
    "    if ~isnan(seedrand); srand(seedrand); end\n",
    "    \n",
    "    Vend = ForwardDiffZeros(size(initUs,1), size(initUs,2), nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:size(initUs,1)\n",
    "        if i>size(initUs,1) - zero_last_sigmas\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; sigma=0, nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)            \n",
    "        else\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)\n",
    "        end\n",
    "        Vend[i,:] = Ve\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend[:,1]-Vend[:,2])/theta1))\n",
    "    diffs = tanh.((Vend[:,1]-Vend[:,2])/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"%s\", pre_string)\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "srand(11)\n",
    "startU=randn(100,2)-3\n",
    "startU=randn(100,2)-3\n",
    "\n",
    "\n",
    "# startU=0.1*randn(100,2)-3\n",
    "# startU=zeros(100,2)-3\n",
    "\n",
    "\n",
    "dt = 0.005  # If we go to dt=0.02, it doesn't get stuck\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "W = -4\n",
    "noise = 0\n",
    "input = 0\n",
    "sigma = 0.1\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    ":noise=>noise, :input=>input, :sigma=>sigma, :const_add=>0, :init_add=>0)\n",
    "\n",
    "\n",
    "\n",
    "# The backward and costfunc functions should turn a single-scalar parameter W into the matrix W\n",
    "# backward always runs with no within-forward noise, i.e., sigma=0\n",
    "backward = (endpoint; do_plot=false, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;     \n",
    "    backwardsModel(endpoint; do_plot=do_plot, make_dict([\"sigma\"], [0], pars)...)[1]\n",
    "end\n",
    "\n",
    "\n",
    "beta=0\n",
    "\n",
    "costfunc = (startpoints; do_plot=false, verbose=false, nderivs=0, difforder=0, sr=26, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;         \n",
    "    JJ(startpoints; seedrand=sr, beta=beta, \n",
    "        do_plot=do_plot, verbose=verbose, nderivs=nderivs, difforder=difforder, pars...)\n",
    "end\n",
    "\n",
    "if beta==0.003;     cost_limit = -0.00288\n",
    "elseif beta<0.001;  cost_limit = -0.0008\n",
    "elseif beta==0.001; cost_limit = -0.000935\n",
    "elseif beta==0.05;  cost_limit = -0.0485\n",
    "else\n",
    "    error(\"Don't know what cost limit goes with beta %g\\n\", beta)\n",
    "end\n",
    "\n",
    "fluxFinalPoint = zeros(0,2);\n",
    "\n",
    "args = [[\"init_add\" 2], \"const_add\", \"W\"]\n",
    "\n",
    "seed = [0.001, 0.001, 0, -4]\n",
    "\n",
    "\n",
    "# Alternatively, start right from the sticking point:\n",
    "seed = [4.74063,  -4.68228,  2.73165,  -5.6783]\n",
    "\n",
    "# Walls are big enough that we never hit them, so it is immaterial:\n",
    "bbox = [\n",
    "    -15        15  ;\n",
    "    -15        15  ;\n",
    "    -15        15  ;\n",
    "    -20.5  20.5  ; \n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clf()\n",
    "costfunc(startU; do_plot=true, verbose=true, make_dict(args, seed, model_params)...)\n",
    "\n",
    "\n",
    "# YOU CAN DO EITHER THIS:\n",
    "params, traj = bbox_Hessian_keyword_minimization(seed, args, bbox, \n",
    "(;params...) -> costfunc(startU; verbose=true, merge(model_params, Dict(params))...), \n",
    "verbose=true, start_eta=0.01, tol=1e-10, hardbox=true )\n",
    "\n",
    "# OR THIS:  (both get stuck)\n",
    "# fluxFinalPoint = [-0.1 -0.1]\n",
    "# params, cost, ptraj, gtraj = fluxSense(costfunc, backward, model_params, startU, fluxFinalPoint, args, seed; \n",
    "# start_eta=0.01, tol=1e-15, maxiter=400, verbose=true, report_every=1, do_plot=true, cost_limit=cost_limit) # cost_limit=-0.000935) # for beta=0.01\n",
    "\n",
    "# And show the final position\n",
    "clf()\n",
    "costfunc(startU; do_plot=true, verbose=true, make_dict(args, params, model_params)...)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.2",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
