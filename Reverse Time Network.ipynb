{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wed 2017-09-06 1pm:** Time to go to ProAnti. I don't think it's worth stopping to examine further whether fluxSense() is worth it, just doesn't seem the issue right now. Can revisit if necessary.\n",
    "\n",
    "**Wed 2017-09-06 11am:** Current status: everythign working, bbox_hessian_minimization fully debugged and report diagnostic info. Basic MGO minimizations are doing their thing.  Have now added self-connection weights, and can very successfully train the MGO network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">TABLE OF CONTENTS</h1>\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "**Updates to the table of contents are periodic, but run the cell below to first start or force an update.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://sites.google.com/site/brodylabhome/files/make_table_of_contents.js')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition @javascript_str(ANY<:Any) in module Main at In[1]:1 overwritten at In[49]:1.\n"
     ]
    }
   ],
   "source": [
    "macro javascript_str(s) display(\"text/javascript\", s); end\n",
    "\n",
    "javascript\"\"\"\n",
    "$.getScript('https://sites.google.com/site/brodylabhome/files/make_table_of_contents.js')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We define functions to convert Duals, the variable types used by ForwardDiff, \\nto Floats. This is useful if we want to print out the value of a variable \\n(since print doesn't know how to Duals). Note that after being converted to a Float, no\\ndifferentiation by ForwardDiff can happen!  e.g. after\\n    x = convert(Float64, y)\\nForwardDiff can still differentiate y, but it can't differentiate x\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "using PyCall\n",
    "using PyPlot\n",
    "using ForwardDiff\n",
    "using DiffBase\n",
    "using MAT\n",
    "\n",
    "pygui(true)\n",
    "\n",
    "import Base.convert\n",
    "convert(::Type{Float64}, x::ForwardDiff.Dual) = Float64(x.value)\n",
    "function convert(::Array{Float64}, x::Array{ForwardDiff.Dual}) \n",
    "    y = zeros(size(x)); \n",
    "    for i in 1:prod(size(x)) \n",
    "        y[i] = convert(Float64, x[i]) \n",
    "    end\n",
    "    return y\n",
    "end\n",
    "\n",
    "include(\"general_utils.jl\")\n",
    "include(\"hessian_utils.jl\")\n",
    "\n",
    "\"\"\"\n",
    "We define functions to convert Duals, the variable types used by ForwardDiff, \n",
    "to Floats. This is useful if we want to print out the value of a variable \n",
    "(since print doesn't know how to Duals). Note that after being converted to a Float, no\n",
    "differentiation by ForwardDiff can happen!  e.g. after\n",
    "    x = convert(Float64, y)\n",
    "ForwardDiff can still differentiate y, but it can't differentiate x\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup -- definitions of forwardModel() and backwardsModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition g(Any) in module Main at In[142]:5 overwritten at In[144]:5.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'g :: Tuple{Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition ginverse(Any) in module Main at In[142]:12 overwritten at In[144]:12.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'ginverse :: Tuple{Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition forwardModel(Any) in module Main at In[142]:78 overwritten at In[144]:78.\n",
      "WARNING: Method definition #forwardModel(Array{Any, 1}, Main.#forwardModel, Any) in module Main overwritten.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'forwardModel :: Tuple{Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition backwardsModel(Any) in module Main at In[142]:224 overwritten at In[144]:224.\n",
      "WARNING: Method definition #backwardsModel(Array{Any, 1}, Main.#backwardsModel, Any) in module Main overwritten.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'backwardsModel :: Tuple{Any}' in module 'Main'.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "backwardsModel"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "o = g(z)    squashing tanh function, running from 0 to 1, is equal to 0.5 when input is 0.\n",
    "\"\"\"\n",
    "function g(z)\n",
    "    return 0.5*tanh.(z)+0.5\n",
    "end\n",
    "    \n",
    "\"\"\"\n",
    "z = g^-1(o)    inverse of squashing tanh function, input must be in (0, 1), output is zero when passed 0.5.\n",
    "\"\"\"\n",
    "function ginverse(z)\n",
    "    return 0.5*log.(z./(1-z))\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "forwardModel(startU; dt=0.01, tau=0.1, nsteps=100, input=[0.1, 0], noise=[], W=[0 -5;-5 0], \n",
    "init_add=0, start_add=0, const_add=0, sigma=0, gleak=1, U_rest=0, \n",
    "    do_plot=false, nderivs=0, difforder=0, clearfig=true, fignum=1, dUdt_mag_only=false)\n",
    "\n",
    "Runs a tanh() style-network forwards in time, given its starting point, using simple Euler integration\n",
    "    tau dU/dt = -U + W*V + I\n",
    "    V = 0.5*tanh(U)+ 0.5\n",
    "\n",
    "**PARAMETERS:**\n",
    "\n",
    "startU     A column vector, nunits-by-1, indicating the values of U at time zero\n",
    "\n",
    "\n",
    "**OPTIONAL PARAMETERS**\n",
    "\n",
    "dt      Scalar, timestep size\n",
    "\n",
    "tau     Scalar, in seconds\n",
    "\n",
    "gleak   \n",
    "        dUdt will have a term equal to gleak*(U_rest - U)\n",
    "U_rest\n",
    "\n",
    "nsteps  Number of timesteps to run, including time=0.\n",
    "\n",
    "input   Either an nunits-by-1 vector, in which case inputs to each unit are constant\n",
    "        across time, or a matrix, nunits-by-nsteps, indicating input for each unit at each timepoint.\n",
    "\n",
    "W       Weight matrix, nunits-by-nunits\n",
    "\n",
    "init_add    DEPRECATED: Vector or scalar that gets added to the input current at very first timestep.\n",
    "            Deprecated because this made it dt-dependent. Replaced by start_add.\n",
    "\n",
    "start_add   Vector or scalar that gets added, once, to the initial U[:,1], before the integration process begins.\n",
    "\n",
    "const_add   Scalar that gets added to U after every timestep\n",
    "\n",
    "sigma       After each timestep, add sigma*sqrt(dt)*randn() to each element of U\n",
    "\n",
    "do_plot   Default false, if true, plots V of up to the first two dimensions\n",
    "\n",
    "fignum     Figure number on which to plot\n",
    "\n",
    "clrearfig  If true, the figure is first cleared, otherwise any plot ois overlaid\n",
    "\n",
    "nderivs, difforder     Required for making sure function can create its own arrays and \n",
    "                       still be differentiated\n",
    "\n",
    "dUdt_mag_only  If true, returns |dUdt|^2 from the first timestep only, then stops.\n",
    "\n",
    "** RETURNS:**\n",
    "\n",
    "Uend Vend       nunits-by-1 vectors representing the final values of U and V that were found.\n",
    "U, V            nunits-by-nsteps matrices containing the full trajectories\n",
    "\n",
    "\"\"\"\n",
    "function forwardModel(startU; dt=0.01, tau=0.1, nsteps=100, input=[], noise=[], W=[0 -5;-5 0], \n",
    "    init_add=0, start_add=0, const_add=0, do_plot=false, nderivs=0, difforder=0, clearfig=true, fignum=1,\n",
    "    dUdt_mag_only=false, sigma=0, g_leak=1, U_rest=0, theta=0, beta=1, \n",
    "    warn_if_unused_params=false, other_unused_params...)\n",
    "\n",
    "    if warn_if_unused_params && length(other_unused_params)>0\n",
    "        @printf(\"\\n\\n=== forwardModel warning, had unused params \")\n",
    "        for k in keys(Dict(other_unused_params))\n",
    "            @printf(\"%s, \", k)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    my_input = ForwardDiffZeros(size(input,1), size(input,2), nderivs=nderivs, difforder=difforder)\n",
    "    for i=1:prod(size(input)); my_input[i] = input[i]; end\n",
    "    input = my_input;\n",
    "    \n",
    "    nunits = length(startU)\n",
    "    if size(startU,2) > size(startU,1)\n",
    "        error(\"startU must be a column vector\")\n",
    "    end\n",
    "    \n",
    "    # --- formatting input ---\n",
    "    if ~(typeof(input)<:Array) || prod(size(input))==1  # was a scalar\n",
    "        input = input[1]*(1+ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    elseif length(input)==0 # was the empty matrix\n",
    "        input = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    elseif size(input,2)==1     # was a column vector\n",
    "        input = input*(1+ForwardDiffZeros(1, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    end    \n",
    "    # --- formatting noise ---\n",
    "    if ~(typeof(noise)<:Array) || prod(size(noise))==1  # was a scalar\n",
    "        noise = noise*(1+ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    elseif length(noise)==0 # was the empty matrix\n",
    "        noise = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    elseif size(noise,2)==1     # was a column vector\n",
    "        noise = noise*(1+ForwardDiffZeros(1, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    end    \n",
    "    \n",
    "    U = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    V = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    \n",
    "    if ~(typeof(W)<:Array); W = [W]; end\n",
    "\n",
    "    W     = reshape(W, nunits, nunits)\n",
    "    U     = reshape(U, nunits, nsteps)\n",
    "    V     = reshape(V, nunits, nsteps)\n",
    "    input = reshape(input, nunits, nsteps)\n",
    "    noise = reshape(noise, nunits, nsteps)\n",
    "\n",
    "    input[:,1] += init_add\n",
    "    input      += const_add\n",
    "\n",
    "    #@printf(\"size(U) is (%d,%d), and size(startU) is (%d,%d) and size(noise) is (%d,%d)\", \n",
    "    #    size(U,1), size(U,2), size(startU,1), size(startU,2), size(noise,1), size(noise,2))\n",
    "    # @printf(\"U[1]=%g, noise[1]=%g\\n\", startU, noise[1])\n",
    "    U[:,1] = startU + noise[:,1] + start_add; # @printf(\"Resulting U=%g\\n\", U[1])\n",
    "    V[:,1] = g((U[:,1]-theta)/beta); # @printf(\"Resulting V=%g\\n\", V[1])\n",
    "    \n",
    "    for i=2:nsteps\n",
    "        dUdt = g_leak*(U_rest -U[:,i-1]) + W*V[:,i-1] + input[:,i-1]\n",
    "        if dUdt_mag_only; return sum(dUdt.*dUdt); end;\n",
    "        # @printf(\"dUdt=%g\\n\", dUdt[1])\n",
    "        # @printf(\"i=%g\\n\", i)\n",
    "        # @printf(\"noise[2]=%g\\n\", noise[2])\n",
    "        U[:,i] = U[:,i-1] + (dt/tau)*dUdt + noise[:,i] + sigma*sqrt(dt)*randn(size(U,1),1)\n",
    "        # @printf(\"Resulting U[2]=%g\\n\", U[2])\n",
    "        V[:,i] = g((U[:,i]-theta)/beta)\n",
    "        # @printf(\"Resulting V[2]=%g\\n\", V[2])\n",
    "    end\n",
    "\n",
    "    if do_plot\n",
    "        figure(fignum)\n",
    "        if length(startU)==1\n",
    "            if clearfig; clf(); end;\n",
    "            t = (0:nsteps-1)*dt\n",
    "            plot(t, V[1,:], \"b-\")\n",
    "            plot(t[1], V[1,1], \"g.\")\n",
    "            plot(t[end], V[1,end], \"r.\")\n",
    "            xlabel(\"t\"); ylabel(\"V1\"); ylim([-0.01, 1.01])\n",
    "        elseif length(startU)>=2\n",
    "            if clearfig; clf(); end;\n",
    "            plot(V[1,:], V[2,:], \"b-\")\n",
    "            plot(V[1,1], V[2,1], \"g.\")\n",
    "            plot(V[1,end], V[2,end], \"r.\")\n",
    "            xlabel(\"V1\"); ylabel(\"V2\"); \n",
    "            xlim([-0.01, 1.01]); ylim([-0.01, 1.01])\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return U[:,end], V[:,end], U, V\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "backwardsModel(endU; dt=0.01, tau=0.1, nsteps=100, input=[0],noise=[],  W=[0 -5;-5 0], \n",
    "    do_plot=false, nderivs=0, difforder=0, clearfig=true, fignum=1, tol=1e-15, start_eta=10)\n",
    "\n",
    "Runs a tanh() style-network BACKWARDS in time, given its ending point, by making a backwards\n",
    "guess at each timepoint and then using Hessian minimization to find the backwards vector that correctly\n",
    "leads to the current timestep value.  Uses forwardModel() . The forwards equations are:\n",
    "\n",
    "    tau dU/dt = -U + W*V + I\n",
    "    V = 0.5*tanh(U)+ 0.5\n",
    "\n",
    "**PARAMETERS:**\n",
    "\n",
    "endU     A column vector, nunits-by-1, indicating the values of U at time=end\n",
    "\n",
    "\n",
    "**OPTIONAL PARAMETERS:**\n",
    "\n",
    "dt      Scalar, timestep size\n",
    "\n",
    "tau     Scalar, in seconds\n",
    "\n",
    "nsteps  Number of timesteps to run, including time=0.\n",
    "\n",
    "input   Either an nunits-by-1 vector, in which case inputs to each unit are constant\n",
    "        across time, or a matrix, nunits-by-nsteps, indicating input for each unit at each timepoint.\n",
    "\n",
    "W       Weight matrix, nunits-by-nunits\n",
    "\n",
    "do_plot   Default false, if true, plots V of up to the first two dimensions\n",
    "\n",
    "tol       Tolerance in the minimization procedure for finding each backwards timestep. Passed on\n",
    "          to trust_region_Hessian_minimization()\n",
    "\n",
    "start_eta   Passed on to trust_region_Hessian_minimization()\n",
    "\n",
    "fignum     Figure number on which to plot\n",
    "\n",
    "clrearfig  If true, the figure is first cleared, otherwise any plot ois overlaid\n",
    "\n",
    "nderivs, difforder     Required for making sure function can create its own arrays and \n",
    "                       still be differentiated\n",
    "\n",
    "\n",
    "\n",
    "** RETURNS:**\n",
    "\n",
    "Ustart Vstart   nunits-by-1 vectors representing the starting values of U and V that were found.\n",
    "U, V            nunits-by-nsteps matrices containing the full trajectories\n",
    "costs           1-by-nsteps vector with the final cost from the minimization procedure for each\n",
    "                timestep. This is the squared difference between the U[t+1] produced by the U[t] \n",
    "                guess and the actual U[t+1]\n",
    "\n",
    "\"\"\"\n",
    "function backwardsModel(endU; nsteps=100, start_eta=10, tol=1e-15, maxiter=400, \n",
    "    do_plot=false, init_add=0, start_add=0, dt=0.01, \n",
    "    input=[], noise=[], nderivs=0, difforder=0, clearfig=false, fignum=1, params...)    \n",
    "\n",
    "    nunits = length(endU)\n",
    "\n",
    "    # --- formatting input ---\n",
    "    if ~(typeof(input)<:Array) || prod(size(input))==1  # was a scalar\n",
    "        input = input[1]*(1+ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    elseif length(input)==0 # was the empty matrix\n",
    "        input = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    elseif size(input,2)==1     # was a column vector\n",
    "        input = input*(1+ForwardDiffZeros(1, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    end    \n",
    "    # --- formatting noise ---\n",
    "    if ~(typeof(noise)<:Array)  # was a scalar\n",
    "        noise = noise*(1+ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    elseif length(noise)==0 # was the empty matrix\n",
    "        noise = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    elseif size(noise,2)==1     # was a column vector\n",
    "        noise = noise*(1+ForwardDiffZeros(1, nsteps, nderivs=nderivs, difforder=difforder))\n",
    "    end    \n",
    "    \n",
    "    function J(U1, U2; nderivs=0, difforder=0, noise=[], inputs=[], pars...)\n",
    "        U2hat = forwardModel(U1; nsteps=2, noise=noise, input=input, nderivs=nderivs, difforder=difforder, pars...)[1]\n",
    "        U2hat = U2hat\n",
    "        DU = U2hat - U2\n",
    "    \n",
    "        return sum(DU.*DU)\n",
    "    end\n",
    "    \n",
    "    if length(noise)==0\n",
    "        noise = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    end\n",
    "\n",
    "    U = ForwardDiffZeros(nunits, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    U = reshape(U, nunits, nsteps)\n",
    "    costs = ForwardDiffZeros(nsteps, 1, nderivs=nderivs, difforder=difforder)    \n",
    "    \n",
    "    U[:,end] = endU\n",
    "    for i=(nsteps-1):-1:1\n",
    "        if i==1\n",
    "            my_init_add = init_add\n",
    "            my_start_add = start_add\n",
    "        else\n",
    "            my_init_add = 0\n",
    "            my_start_add = 0\n",
    "        end\n",
    "                \n",
    "        U[:,i], costs[i] = trust_region_Hessian_minimization(U[:,i+1], \n",
    "            (x) -> J(x, U[:,i+1]; nderivs=length(endU), difforder=2, \n",
    "            input=input[:,i:i+1], noise = noise[:,i:i+1], \n",
    "            init_add=my_init_add, start_add=my_start_add, params...); \n",
    "            verbose=false, start_eta=start_eta, tol=tol, maxiter=maxiter)\n",
    "        if i>1; U[:,i] += noise[:,i]; end\n",
    "    end\n",
    "    \n",
    "    \n",
    "    V = g(U)\n",
    "    \n",
    "    if do_plot\n",
    "        figure(fignum)   \n",
    "        if typeof(params)<:Array; params = Dict(params); end;\n",
    "        if haskey(params, :dt);     dt     = params[:dt];     end\n",
    "        if haskey(params, :nsteps); nsteps = params[:nsteps]; end\n",
    "        if length(endU)==1\n",
    "            if clearfig; clf(); end;\n",
    "            t = (0:nsteps-1)*dt\n",
    "            plot(t, V[1,:], \"m-\")\n",
    "            plot(t[1], V[1,1], \"go\")\n",
    "            plot(t[end], V[1,end], \"ro\")            \n",
    "            ylim([-0.01, 1.01])\n",
    "        elseif length(endU)>=2\n",
    "            if clearfig; clf(); end;            \n",
    "            plot(V[1,:], V[2,:], \"m-\")\n",
    "            plot(V[1,1], V[2,1], \"go\")\n",
    "            plot(V[1,end], V[2,end], \"ro\")\n",
    "            xlim([-0.01, 1.01]); ylim([-0.01, 1.01])\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return U[:,1], V[:,1], U, V, costs\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing forward and backwards models with only 1 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ustart came back as 1.1\n"
     ]
    }
   ],
   "source": [
    "figure(1); clf();\n",
    "params = Dict(:noise => [0.1], :W => [-2], :nsteps=>10, :start_add=>-1.9)\n",
    "Uend = forwardModel([1.1]; do_plot=true, params...)[1]\n",
    "Ustart = backwardsModel(Uend; do_plot=true, tol=1e-30, params...)[1]\n",
    "@printf(\"Ustart came back as %g\\n\", Ustart[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing forward and backwards models now with 2 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ustart came back as : [0.1, 0.1]\n"
     ]
    }
   ],
   "source": [
    "nsteps=50\n",
    "params = Dict(:noise =>0.03*randn(2,nsteps) + [0.1,0]*ones(1,nsteps), :W => [0 -5; -5 0], :nsteps=>nsteps)\n",
    "\n",
    "Uend, Vend, U, V              = forwardModel([0.1,0.1]; do_plot=true, params...);\n",
    "Ustart, Vstart, bU, bV, costs = backwardsModel(Uend; do_plot=true, tol=1e-30, params...)\n",
    "\n",
    "@printf(\"Ustart came back as : \"); print_vector_g(Ustart); print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring dt-dependence of gradients and hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×5 Array{Float64,2}:\n",
       " 0.92859   -1.0449   1.11301  -1.12663  0.164904\n",
       " 0.918901  -1.01352  1.03901  -1.06345  0.512789"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# srand(111)\n",
    "startU=randn(100,2)-3\n",
    "startU=randn(100,2)-3\n",
    "sigma = 0\n",
    "\n",
    "# startU=0.1*randn(100,2)-3\n",
    "# startU=zeros(100,2)-3\n",
    "\n",
    "\n",
    "dt = 0.02\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "W = -4\n",
    "noise = 0\n",
    "input = 0\n",
    "sigma = 0\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    ":noise=>noise, :input=>input, :sigma=>sigma, :const_add=>0, :init_add=>0)\n",
    "\n",
    "forward = (startU; pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;     \n",
    "    forwardModel(startU; pars...)\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "# clf();\n",
    "# func = (;pars...) -> forward([-0.2, 0.3]; do_plot=true, merge(model_params, Dict(pars))...)\n",
    "# func(;W=-4)\n",
    "\n",
    "args = [\"W\", \"const_add\", [\"start_add\" 2], \"sigma\"]\n",
    "params = [-4.01, 0.5, 0.2, -0.2, 0.01]\n",
    "\n",
    "figure(1); clf();\n",
    "value1, grad1, hess1 = keyword_vgh((;pars...)->forward([-0.2, 0.3]; do_plot=true, merge(model_params, Dict(pars))...)[1][1], args, params)\n",
    "title(@sprintf(\"Running with dt=%g\", dt))\n",
    "\n",
    "dt = 0.005\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    ":noise=>noise, :input=>input, :sigma=>sigma, :const_add=>0, :init_add=>0)\n",
    "\n",
    "figure(2); clf();\n",
    "value2, grad2, hess2 = keyword_vgh((;pars...)->forward([-0.2, 0.3]; do_plot=true, fignum=2, merge(model_params, Dict(pars))...)[1][1], args, params)\n",
    "title(@sprintf(\"Running with dt=%g\", dt))\n",
    "\n",
    "[grad1[:]' ; grad2[:]']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO-DOs\n",
    "\n",
    "1. ~~Be able to use W as an optimizable parameter (including configs like \"all horizontal weights are the same\")~~ DONE!\n",
    "2. ~~Check out what is going on with the weird trajectories in the function-based MGO example~~  DONE: it's just the strong, single-timestep initial_add\n",
    "3. ~~Check out whether reducing beta solves the sticking issue even without extra finalFluxPoint locations~~. It does. Reducing beta from 0.01 to 0.003 was enough.  (We also needed dto change the cost_limit to -0.00288, since the range of costs changes when beta changes.)\n",
    "3. Find the saddle points and use those as the finalFluxPoint locations\n",
    "4. ===\n",
    "5. ~~Run a ProAnti model with noise only in initial conditions, and thus with the framework as we have it~~ (skipped, went straight to next step)\n",
    "6. ~~Make a cost function with frozen noise, and figure out how frozen noise will interact with the backwards trajectory in the minimizations~~\n",
    "7. ~~Make a forwards and backwards model with Urest, etc., just like in ProAnti()~~\n",
    "6. ===\n",
    "7. ~~Make sure that minimization procedures that use tanh() walls report the model parameter, not the control parameter~~ DONE\n",
    "7. ~~Figure out what is going on with the change in gradient and Hessian upon change of dt~~ DONE: it was just the init step\n",
    "7. ~~To really follow bbox_Hessian, printouts needs to be for the walled params, the trajectory should be for the walled params, and the seed should also take the walls into account.~~ DONE\n",
    "7. ~~Have one_d_minimization return the number of iterations it did and why it stopped; then have constrained_parabolic_minimization return the cost, maxiters, and stopping reason; then have bbox_Hessian_minimization return the trajectory of those, as a trace of what was going on.~~ DONE\n",
    "7. POSTPONED: Clean up examples of forward and backwards models and of 1-d use of fluxSense() function\n",
    "8. POSTPONED: Find a 2-d example where flux points are actually needed -- when beta=0, it is not so clear.\n",
    "8. POSTPONED: Measure gradient sensitivity to each of the endpoints in a set of trajectories, as a measure of whether fluxSense is needed or not.\n",
    "12. POSTPONED: Try to combinee fluxSense with bbox_Hessian_minimization9. ~~Fix the walls issue in bbox_Hessian_minimization using tanh encoding.~~\n",
    "\n",
    "\n",
    "1. ===\n",
    "2. ~~Optimize either an MGO or a ProAnti~~ DONE with MGO. Now on to ProAnti\n",
    "3. ~~Set up so we can easily change task period durations in JJ as we run the model to evaluate the results of model-fitting~~ DONE\n",
    "4. Have different task period durations while model-fitting\n",
    "5. Set up to do searches over parameter space\n",
    "6. Incorporate RT into fits?\n",
    "10. If fluxSense is needed in ProAnti, could try choosing the Anti unit endpoint values by maximizing the |dJ/dw|^2 over those values.\n",
    "11. Clean up the notebooks and write up what we've been doing!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting on ProAnti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition plot_PA(Any, Any, Any) in module Main at In[180]:9 overwritten at In[182]:9.\n",
      "WARNING: Method definition #plot_PA(Array{Any, 1}, Main.#plot_PA, Any, Any, Any) in module Main overwritten.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'plot_PA :: Tuple{Any,Any,Any}' in module 'Main'.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "plot_PA"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    plot_PA(t, U, V; fignum=1, clearfig=true, rule_and_delay_period=1, target_period=1, post_target_period=1,\n",
    "        other_unused_params...)\n",
    "\n",
    "Helper function for plotting ProAnti results\n",
    "\"\"\"\n",
    "function plot_PA(t, U, V; fignum=1, clearfig=true, rule_and_delay_period=1, target_period=1, post_target_period=1,\n",
    "    other_unused_params...)\n",
    "    figure(fignum)\n",
    "    if clearfig; clf(); end\n",
    "    \n",
    "    ax1 = subplot(3,1,1)\n",
    "    h = plot(t, V'); \n",
    "    setp(h[1], color=[0, 0, 1])\n",
    "    setp(h[2], color=[1, 0, 0])\n",
    "    setp(h[3], color=[1, 0.5, 0.5])\n",
    "    setp(h[4], color=[0, 1, 1])\n",
    "    ylabel(\"V\")\n",
    "\n",
    "    ax = gca()\n",
    "    yl = [ylim()[1], ylim()[2]]\n",
    "    vlines([rule_and_delay_period, \n",
    "            rule_and_delay_period+target_period,\n",
    "            rule_and_delay_period+target_period+post_target_period], \n",
    "            -0.05, 1.05, linewidth=2)\n",
    "    if yl[1]<0.02\n",
    "        yl[1] = -0.02\n",
    "    end\n",
    "    if yl[2]>0.98\n",
    "        yl[2] = 1.02\n",
    "    end\n",
    "    ylim(yl)\n",
    "    grid(true)\n",
    "    remove_xtick_labels(ax1)\n",
    "        \n",
    "    ax2 = subplot(3,1,2)\n",
    "    hu = plot(t, U')\n",
    "    setp(hu[1], color=[0, 0, 1])\n",
    "    setp(hu[2], color=[1, 0, 0])\n",
    "    setp(hu[3], color=[1, 0.5, 0.5])\n",
    "    setp(hu[4], color=[0, 1, 1])\n",
    "    ylabel(\"U\"); ylim(minimum(U[:])-0.1, maximum(U[:])+0.1)\n",
    "    vlines([rule_and_delay_period, \n",
    "            rule_and_delay_period+target_period,\n",
    "            rule_and_delay_period+target_period+post_target_period], \n",
    "            ylim()[1], ylim()[2], linewidth=2)\n",
    "    remove_xtick_labels(ax2)\n",
    "\n",
    "    grid(true)\n",
    "    \n",
    "    subplot(3,1,3)\n",
    "    delta = V[1,:] - V[4,:]\n",
    "    hr = plot(t, delta)\n",
    "    oldlims = [ylim()[1]+0.1, ylim()[2]-0.1]\n",
    "    ylim(minimum([delta[:];oldlims[1]])-0.1, maximum([delta[:];oldlims[2]])+0.1)\n",
    "    vlines([rule_and_delay_period, \n",
    "            rule_and_delay_period+target_period,\n",
    "            rule_and_delay_period+target_period+post_target_period], \n",
    "            ylim()[1], ylim()[2], linewidth=2)\n",
    "    xlabel(\"t\"); ylabel(\"Pro R - Pro L\")\n",
    "    grid(true)\n",
    "        \n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.366172 seconds (196.73 k allocations: 9.023 MB)\n",
      "Pro % correct = 90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition make_input(Any) in module Main at In[143]:39 overwritten at In[145]:39.\n",
      "WARNING: Method definition #make_input(Array{Any, 1}, Main.#make_input, Any) in module Main overwritten.\n",
      "WARNING: Method definition run_ntrials(Any, Any) in module Main at In[143]:63 overwritten at In[145]:63.\n",
      "WARNING: Method definition #run_ntrials(Array{Any, 1}, Main.#run_ntrials, Any, Any) in module Main overwritten.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anti % correct = 40% \n"
     ]
    }
   ],
   "source": [
    "model_params = Dict(\n",
    ":dt     =>  0.02, \n",
    ":tau    =>  0.1, \n",
    ":vW     =>  -1.7,\n",
    ":hW     =>  -1.7,\n",
    ":sW     =>  0.2,\n",
    ":dW     =>  0,\n",
    ":nsteps =>  2, \n",
    ":noise  =>  [], \n",
    ":sigma  =>  0.08, \n",
    ":input  =>  0, \n",
    ":g_leak =>  0.25, \n",
    ":U_rest =>  -1,\n",
    ":theta  =>  1, \n",
    ":beta   =>  1, \n",
    ":sw     =>  0.2,\n",
    ":hw     =>  -1.7,\n",
    ":vw     =>  -1.7,\n",
    ":constant_excitation      => 0.19, \n",
    ":anti_rule_strength       => 0.1,\n",
    ":pro_rule_strength        => 0.1, \n",
    ":target_period_excitation => 1,\n",
    ":right_light_excitation   => 0.5, \n",
    ":right_light_pro_extra    => 0,\n",
    ":const_add => 0, \n",
    ":init_add  => 0, \n",
    ":rule_and_delay_period    => 0.4,\n",
    ":target_period            => 0.1,\n",
    ":post_target_period       => 0.5,\n",
    ":const_pro_bias           => 0,\n",
    ")\n",
    "\n",
    "\n",
    "function make_input(trial_type; dt=0.02, nderivs=0, difforder=0, constant_excitation=0.19, anti_rule_strength=0.1, \n",
    "    pro_rule_strength=0.1, target_period_excitation=1, right_light_excitation=0.5, right_light_pro_extra=0, \n",
    "    rule_and_delay_period=0.4, target_period=0.1, post_target_period=0.4, const_pro_bias=0,\n",
    "    other_unused_params...)\n",
    "\n",
    "    T = rule_and_delay_period + target_period + post_target_period\n",
    "    t = 0:dt:T\n",
    "    nsteps = length(t)\n",
    "\n",
    "    input = constant_excitation + ForwardDiffZeros(4, nsteps, nderivs=nderivs, difforder=difforder)\n",
    "    if trial_type==\"Anti\"\n",
    "        input[2:3, t.<rule_and_delay_period] += anti_rule_strength\n",
    "    elseif trial_type==\"Pro\"\n",
    "        input[[1,4], t.<rule_and_delay_period] += pro_rule_strength\n",
    "    else\n",
    "        error(\"make_input: I don't recognize input type \\\"\" * trial_type * \"\\\"\")\n",
    "    end\n",
    "    \n",
    "    input[:,     (rule_and_delay_period.<=t) & (t.<rule_and_delay_period+target_period)] += target_period_excitation\n",
    "    input[1:2,   (rule_and_delay_period.<=t) & (t.<rule_and_delay_period+target_period)] += right_light_excitation\n",
    "    input[1,     (rule_and_delay_period.<=t) & (t.<rule_and_delay_period+target_period)] += right_light_pro_extra\n",
    "    \n",
    "    input[[1,4],:] += const_pro_bias\n",
    "    \n",
    "    return input, t, nsteps\n",
    "end\n",
    "\n",
    "\n",
    "function run_ntrials(nPro, nAnti; plot_list=[], nderivs=0, difforder=0, model_params...)\n",
    "    pro_input,  t, nsteps = make_input(\"Pro\" ; model_params...)\n",
    "    anti_input, t, nsteps = make_input(\"Anti\"; model_params...)\n",
    "\n",
    "    model_params = Dict(model_params)\n",
    "    sW = model_params[:sW]\n",
    "    hW = model_params[:hW]\n",
    "    vW = model_params[:vW]\n",
    "    dW = model_params[:dW]\n",
    "    model_params = make_dict([\"nsteps\", \"W\"], [nsteps, [sW vW dW hW; vW sW hW dW; dW hW sW vW; hW dW vW sW]], \n",
    "        model_params)\n",
    "    model_params = make_dict([\"nderivs\", \"difforder\"], [nderivs, difforder], model_params)\n",
    "    \n",
    "    proVs  = ForwardDiffZeros(4, nPro, nderivs=nderivs, difforder=difforder)\n",
    "    antiVs = ForwardDiffZeros(4, nAnti, nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    # --- PRO ---\n",
    "    figure(1); clf();\n",
    "    model_params = make_dict([\"input\"], [pro_input], model_params)\n",
    "    for i=1:nPro\n",
    "        startU = [-0.3, -0.7, -0.7, -0.3]\n",
    "        Uend, Vend, U, V = forwardModel(startU, do_plot=false; model_params...)\n",
    "        proVs[:,i] = Vend\n",
    "        if any(plot_list.==i) \n",
    "            plot_PA(t, U, V; fignum=1, clearfig=false, model_params...)\n",
    "            subplot(3,1,1); title(\"PRO\")\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # --- ANTI ---\n",
    "    figure(2); clf();\n",
    "    model_params = make_dict([\"input\"], [anti_input], model_params)\n",
    "    for i=1:nAnti\n",
    "        startU = [-0.7, -0.3, -0.3, -0.7]\n",
    "        Uend, Vend, U, V = forwardModel(startU, do_plot=false; model_params...)\n",
    "        antiVs[:,i] = Vend\n",
    "        if any(plot_list.==i) \n",
    "            plot_PA(t, U, V; fignum=2, clearfig=false, model_params...)\n",
    "            subplot(3,1,1); title(\"ANTI\")\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return proVs, antiVs\n",
    "end\n",
    "\n",
    "nPro = 10; nAnti = 5;\n",
    "proVs, antiVs = @time(run_ntrials(nPro, nAnti; plot_list=[1:5;], model_params...))\n",
    "\n",
    "@printf(\"Pro %% correct = %g%%\\n\", 100*length(find(proVs[1,:].>proVs[4,:]))/ntrials)\n",
    "@printf(\"Anti %% correct = %g%% \\n\", 100*length(find(antiVs[1,:].<antiVs[4,:]))/ntrials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     -- cost=0.00112205,   cost1=0.00190748, cost2=-0.000785422\n",
      "     -- mean(hitsP)=0.992813, mean(diffsP)=0.893347 mean(hitsA)=0.676207, mean(diffsA)=0.1355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition JJ(Any, Any) in module Main at In[134]:5 overwritten at In[195]:5.\n",
      "WARNING: Method definition #JJ(Array{Any, 1}, Main.#JJ, Any, Any) in module Main overwritten.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0011220538560254845"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function JJ(nPro, nAnti; pro_target=0.9, anti_target=0.7, \n",
    "    theta1=0.025, theta2=0.035, cbeta=0.003, verbose=false, \n",
    "    pre_string=\"\", zero_last_sigmas=0, seedrand=NaN, model_params...)\n",
    "\n",
    "    if ~isnan(seedrand); srand(seedrand); end\n",
    "    \n",
    "    proVs, antiVs = run_ntrials(nPro, nAnti; model_params...)\n",
    "    \n",
    "    hitsP  = 0.5*(1 + tanh.((proVs[1,:]-proVs[4,:,])/theta1))\n",
    "    diffsP = tanh.((proVs[1,:,]-proVs[4,:])/theta2).^2\n",
    "    hitsA  = 0.5*(1 + tanh.((antiVs[4,:]-antiVs[1,:,])/theta1))\n",
    "    diffsA = tanh.((antiVs[4,:,]-antiVs[1,:])/theta2).^2\n",
    "    \n",
    "    if nPro>0 && nAnti>0\n",
    "        cost1 = (nPro*(mean(hitsP) - pro_target).^2  + nAnti*(mean(hitsA) - anti_target).^2)/(nPro+nAnti)\n",
    "        cost2 = -cbeta*(nPro*mean(diffsP) + nAnti*mean(diffsA))/(nPro+nAnti)\n",
    "    elseif nPro>0\n",
    "        cost1 = (mean(hitsP) - pro_target).^2\n",
    "        cost2 = -cbeta*mean(diffsP)\n",
    "    else\n",
    "        cost1 = (mean(hitsA) - anti_target).^2\n",
    "        cost2 = -cbeta*mean(diffsA)\n",
    "    end\n",
    "\n",
    "    if verbose\n",
    "        @printf(\"%s\", pre_string)\n",
    "        @printf(\"     -- cost=%g,   cost1=%g, cost2=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2))\n",
    "        if nPro>0 && nAnti>0\n",
    "            @printf(\"     -- mean(hitsP)=%g, mean(diffsP)=%g mean(hitsA)=%g, mean(diffsA)=%g\\n\", \n",
    "                convert(Float64, mean(hitsP)), convert(Float64, mean(diffsP)),\n",
    "                convert(Float64, mean(hitsA)), convert(Float64, mean(diffsA)))\n",
    "        elseif nPro>0\n",
    "            @printf(\"     -- mean(hitsP)=%g, mean(diffsP)=%g (nAnti=0)\\n\", \n",
    "                convert(Float64, mean(hitsP)), convert(Float64, mean(diffsP)))\n",
    "        else\n",
    "            @printf(\"     -- (nPro=0) mean(hitsA)=%g, mean(diffsA)=%g\\n\", \n",
    "                convert(Float64, mean(hitsA)), convert(Float64, mean(diffsA)))\n",
    "        end        \n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "JJ(2, 10; plot_list=1:5, verbose=true, model_params...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.09639370901714994,[-0.0451448,-0.00832748,0.00163073,-0.252927,-0.394026,-0.028209,0.178825],\n",
       "[-0.0143819 -0.0043839 … -0.0227375 0.355415; -0.0043839 -0.0149854 … 0.00051473 -0.0163409; … ; -0.0227375 0.00051473 … 0.000772025 0.258721; 0.355415 -0.0163409 … 0.258721 1.07321])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func = (;params...) -> JJ(100, 0; seedrand=30, cbeta=0.003, verbose=false, merge(model_params, Dict(params))...)\n",
    "cost, grad, hess = keyword_vgh(func, args, seed)\n",
    "\n",
    "# func(;make_dict(args, seed+ [1,0.2,0,0,0,0,0])...) - func(;make_dict(args, seed)...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: eta=0.01 ps=[0.200, -1.700, -1.700, 0.190, 0.150, 0.100, 0.100]\n",
      "     -- cost=0.070106,   cost1=0.0703077, cost2=-0.000201714\n",
      "     -- mean(hitsP)=0.589085, mean(diffsP)=0.0914605 mean(hitsA)=0.490364, mean(diffsA)=0.0430155\n",
      "     -- cost=0.0684932,   cost1=0.0687241, cost2=-0.000230865\n",
      "     -- mean(hitsP)=0.593228, mean(diffsP)=0.104359 mean(hitsA)=0.49182, mean(diffsA)=0.0495505\n",
      "1: eta=0.011 cost=0.0684932 jtype=constrained costheta=-1.000 ps=[0.200105, -1.70011, -1.69989, 0.19961, 0.150456, 0.100013, 0.0997651]\n",
      "     -- cost=0.066748,   cost1=0.067015, cost2=-0.000267011\n",
      "     -- mean(hitsP)=0.597588, mean(diffsP)=0.120136 mean(hitsA)=0.493657, mean(diffsA)=0.0578714\n",
      "2: eta=0.0121 cost=0.066748 jtype=constrained costheta=-0.999 ps=[0.200227, -1.70024, -1.69974, 0.21011, 0.151003, 0.100028, 0.0994481]\n",
      "     -- cost=0.0649009,   cost1=0.0652122, cost2=-0.000311388\n",
      "     -- mean(hitsP)=0.60203, mean(diffsP)=0.139181 mean(hitsA)=0.495945, mean(diffsA)=0.0684116\n",
      "3: eta=0.01331 cost=0.0649009 jtype=constrained costheta=-0.999 ps=[0.20037, -1.70041, -1.69954, 0.221438, 0.151677, 0.100046, 0.0990093]\n",
      "     -- cost=0.0629964,   cost1=0.063361, cost2=-0.000364628\n",
      "     -- mean(hitsP)=0.606378, mean(diffsP)=0.161573 mean(hitsA)=0.498734, mean(diffsA)=0.0815125\n",
      "4: eta=0.014641 cost=0.0629964 jtype=constrained costheta=-0.998 ps=[0.200541, -1.70064, -1.69924, 0.233416, 0.152537, 0.100066, 0.0983898]\n",
      "     -- cost=0.0610823,   cost1=0.0615081, cost2=-0.000425768\n",
      "     -- mean(hitsP)=0.610483, mean(diffsP)=0.186712 mean(hitsA)=0.50202, mean(diffsA)=0.0971335\n",
      "5: eta=0.0161051 cost=0.0610823 jtype=constrained costheta=-0.995 ps=[0.200746, -1.70093, -1.6988, 0.245673, 0.153665, 0.100089, 0.0975097]\n",
      "     -- cost=0.0591862,   cost1=0.0596774, cost2=-0.000491198\n",
      "     -- mean(hitsP)=0.614331, mean(diffsP)=0.212992 mean(hitsA)=0.505712, mean(diffsA)=0.114474\n",
      "6: eta=0.0177156 cost=0.0591862 jtype=constrained costheta=-0.991 ps=[0.200992, -1.70131, -1.69817, 0.257621, 0.155156, 0.100116, 0.0962892]\n",
      "     -- cost=0.0572913,   cost1=0.0578466, cost2=-0.000555281\n",
      "     -- mean(hitsP)=0.618118, mean(diffsP)=0.23818 mean(hitsA)=0.509643, mean(diffsA)=0.132007\n",
      "7: eta=0.0194872 cost=0.0572913 jtype=constrained costheta=-0.989 ps=[0.201279, -1.70181, -1.69729, 0.268618, 0.157089, 0.100146, 0.0946965]\n",
      "     -- cost=0.0553385,   cost1=0.0559519, cost2=-0.000613411\n",
      "     -- mean(hitsP)=0.62217, mean(diffsP)=0.260631 mean(hitsA)=0.513682, mean(diffsA)=0.148309\n",
      "8: eta=0.0214359 cost=0.0553385 jtype=constrained costheta=-0.990 ps=[0.201606, -1.70241, -1.69615, 0.278316, 0.159502, 0.10018, 0.0927753]\n",
      "     -- cost=0.0532539,   cost1=0.053918, cost2=-0.000664145\n",
      "     -- mean(hitsP)=0.626789, mean(diffsP)=0.279983 mean(hitsA)=0.517814, mean(diffsA)=0.162781\n",
      "9: eta=0.0235795 cost=0.0532539 jtype=constrained costheta=-0.993 ps=[0.201971, -1.70312, -1.69474, 0.286774, 0.162415, 0.100216, 0.0906094]\n",
      "     -- cost=0.0509717,   cost1=0.0516802, cost2=-0.000708524\n",
      "     -- mean(hitsP)=0.632188, mean(diffsP)=0.296781 mean(hitsA)=0.522131, mean(diffsA)=0.175569\n",
      "10: eta=0.0259374 cost=0.0509717 jtype=constrained costheta=-0.996 ps=[0.202378, -1.70395, -1.69307, 0.294268, 0.165868, 0.100255, 0.0882809]\n",
      "     -- cost=0.0484386,   cost1=0.0491871, cost2=-0.000748594\n",
      "     -- mean(hitsP)=0.638525, mean(diffsP)=0.311887 mean(hitsA)=0.52678, mean(diffsA)=0.187176\n",
      "11: eta=0.0285312 cost=0.0484386 jtype=constrained costheta=-0.998 ps=[0.202833, -1.70493, -1.6911, 0.301103, 0.169928, 0.100298, 0.0858553]\n",
      "     -- cost=0.0456092,   cost1=0.0463958, cost2=-0.000786523\n",
      "     -- mean(hitsP)=0.645942, mean(diffsP)=0.326158 mean(hitsA)=0.531935, mean(diffsA)=0.198191\n",
      "12: eta=0.0313843 cost=0.0456092 jtype=constrained costheta=-0.999 ps=[0.203345, -1.70605, -1.68883, 0.307535, 0.174693, 0.100345, 0.0833847]\n",
      "     -- cost=0.0424431,   cost1=0.0432674, cost2=-0.000824267\n",
      "     -- mean(hitsP)=0.654599, mean(diffsP)=0.340343 mean(hitsA)=0.537786, mean(diffsA)=0.209169\n",
      "13: eta=0.0345227 cost=0.0424431 jtype=constrained costheta=-0.999 ps=[0.203926, -1.70736, -1.68621, 0.313748, 0.180295, 0.100396, 0.0809112]\n",
      "     -- cost=0.0389029,   cost1=0.0397665, cost2=-0.000863529\n",
      "     -- mean(hitsP)=0.664698, mean(diffsP)=0.35508 mean(hitsA)=0.544547, mean(diffsA)=0.220606\n",
      "14: eta=0.037975 cost=0.0389029 jtype=constrained costheta=-0.999 ps=[0.204589, -1.70888, -1.6832, 0.319862, 0.186907, 0.100454, 0.0784713]\n",
      "     -- cost=0.0349569,   cost1=0.0358627, cost2=-0.000905813\n",
      "     -- mean(hitsP)=0.676491, mean(diffsP)=0.370936 mean(hitsA)=0.552457, mean(diffsA)=0.23294\n",
      "15: eta=0.0417725 cost=0.0349569 jtype=constrained costheta=-0.999 ps=[0.205348, -1.71064, -1.67975, 0.325947, 0.194742, 0.100517, 0.0760965]\n",
      "     -- cost=0.0305847,   cost1=0.0315373, cost2=-0.000952542\n",
      "     -- mean(hitsP)=0.6903, mean(diffsP)=0.388457 mean(hitsA)=0.561795, mean(diffsA)=0.246571\n",
      "16: eta=0.0459497 cost=0.0305847 jtype=constrained costheta=-0.999 ps=[0.206221, -1.71269, -1.67581, 0.332025, 0.204075, 0.100589, 0.073814]\n",
      "     -- cost=0.0257896,   cost1=0.0267948, cost2=-0.00100519\n",
      "     -- mean(hitsP)=0.706524, mean(diffsP)=0.408241 mean(hitsA)=0.572891, mean(diffsA)=0.261886\n",
      "17: eta=0.0505447 cost=0.0257896 jtype=constrained costheta=-0.999 ps=[0.20723, -1.71505, -1.67131, 0.338086, 0.215247, 0.100668, 0.0716462]\n",
      "     -- cost=0.0206187,   cost1=0.0216842, cost2=-0.00106547\n",
      "     -- mean(hitsP)=0.725635, mean(diffsP)=0.431029 mean(hitsA)=0.586135, mean(diffsA)=0.279283\n",
      "18: eta=0.0555992 cost=0.0206187 jtype=constrained costheta=-0.999 ps=[0.208399, -1.71778, -1.66616, 0.344085, 0.228695, 0.100757, 0.0696104]\n",
      "     -- cost=0.0151958,   cost1=0.0163313, cost2=-0.00113553\n",
      "     -- mean(hitsP)=0.748153, mean(diffsP)=0.457821 mean(hitsA)=0.601995, mean(diffsA)=0.2992\n",
      "19: eta=0.0611591 cost=0.0151958 jtype=constrained costheta=-0.999 ps=[0.209761, -1.72093, -1.66029, 0.349947, 0.244971, 0.100857, 0.0677182]\n",
      "     -- cost=0.0097667,   cost1=0.0109849, cost2=-0.00121825\n",
      "     -- mean(hitsP)=0.774578, mean(diffsP)=0.490006 mean(hitsA)=0.621012, mean(diffsA)=0.322158\n",
      "20: eta=0.067275 cost=0.0097667 jtype=constrained costheta=-0.999 ps=[0.211355, -1.72456, -1.65359, 0.355564, 0.264784, 0.100969, 0.0659761]\n",
      "     -- cost=0.00475166,   cost1=0.00606902, cost2=-0.00131736\n",
      "     -- mean(hitsP)=0.805242, mean(diffsP)=0.529451 mean(hitsA)=0.643795, mean(diffsA)=0.348791\n",
      "21: eta=0.0740025 cost=0.00475166 jtype=constrained costheta=-0.999 ps=[0.21323, -1.7287, -1.64593, 0.360782, 0.289059, 0.101093, 0.0643867]\n",
      "     -- cost=0.000778933,   cost1=0.00221621, cost2=-0.00143728\n",
      "     -- mean(hitsP)=0.840102, mean(diffsP)=0.578377 mean(hitsA)=0.670936, mean(diffsA)=0.37981\n",
      "22: eta=0.0814027 cost=0.000778933 jtype=constrained costheta=-0.999 ps=[0.215458, -1.73325, -1.63728, 0.365369, 0.319065, 0.101227, 0.0629589]\n",
      "     -- cost=-0.00134784,   cost1=0.000232359, cost2=-0.0015802\n",
      "     -- mean(hitsP)=0.878547, mean(diffsP)=0.638693 mean(hitsA)=0.702119, mean(diffsA)=0.414776\n",
      "23: eta=0.089543 cost=-0.00134784 jtype=constrained costheta=-0.992 ps=[0.218297, -1.73581, -1.62919, 0.368552, 0.357347, 0.101343, 0.06184]\n",
      "     -- cost=-0.00162919,   cost1=3.5065e-05, cost2=-0.00166426\n",
      "     -- mean(hitsP)=0.895538, mean(diffsP)=0.673836 mean(hitsA)=0.707087, mean(diffsA)=0.435668\n",
      "24: eta=0.0984973 cost=-0.00162919 jtype=constrained costheta=-0.450 ps=[0.220901, -1.69589, -1.65893, 0.370474, 0.384691, 0.101121, 0.0622803]\n",
      "     -- cost=-0.00175124,   cost1=1.72115e-05, cost2=-0.00176845\n",
      "     -- mean(hitsP)=0.896469, mean(diffsP)=0.700277 mean(hitsA)=0.704686, mean(diffsA)=0.478688\n",
      "25: eta=0.108347 cost=-0.00175124 jtype=constrained costheta=-0.829 ps=[0.223362, -1.65012, -1.6906, 0.378226, 0.402114, 0.10102, 0.0641615]\n",
      "     -- cost=-0.00188073,   cost1=7.48604e-06, cost2=-0.00188821\n",
      "     -- mean(hitsP)=0.897174, mean(diffsP)=0.73072 mean(hitsA)=0.702643, mean(diffsA)=0.528089\n",
      "26: eta=0.119182 cost=-0.00188073 jtype=constrained costheta=-0.711 ps=[0.228497, -1.60573, -1.72039, 0.385382, 0.427362, 0.101137, 0.067135]\n",
      "     -- cost=-0.00201277,   cost1=5.31677e-06, cost2=-0.00201809\n",
      "     -- mean(hitsP)=0.896965, mean(diffsP)=0.763716 mean(hitsA)=0.701191, mean(diffsA)=0.581678\n",
      "27: eta=0.1311 cost=-0.00201277 jtype=constrained costheta=-0.616 ps=[0.237698, -1.56721, -1.74649, 0.390457, 0.463943, 0.101535, 0.0720388]\n",
      "     -- cost=-0.0021462,   cost1=7.9533e-06, cost2=-0.00215415\n",
      "     -- mean(hitsP)=0.896026, mean(diffsP)=0.798775 mean(hitsA)=0.70034, mean(diffsA)=0.637327\n",
      "28: eta=0.14421 cost=-0.0021462 jtype=constrained costheta=-0.496 ps=[0.251323, -1.53175, -1.76958, 0.393571, 0.514941, 0.102151, 0.0791131]\n",
      "     -- cost=-0.00227872,   cost1=9.90877e-06, cost2=-0.00228863\n",
      "     -- mean(hitsP)=0.895553, mean(diffsP)=0.834377 mean(hitsA)=0.699803, mean(diffsA)=0.691379\n",
      "29: eta=0.158631 cost=-0.00227872 jtype=constrained costheta=-0.376 ps=[0.270763, -1.49292, -1.79167, 0.395585, 0.581524, 0.10295, 0.0877212]\n",
      "     -- cost=-0.00240102,   cost1=1.08524e-05, cost2=-0.00241187\n",
      "     -- mean(hitsP)=0.895374, mean(diffsP)=0.866862 mean(hitsA)=0.699444, mean(diffsA)=0.74105\n",
      "30: eta=0.174494 cost=-0.00240102 jtype=constrained costheta=-0.295 ps=[0.299369, -1.44863, -1.81209, 0.396923, 0.661849, 0.103953, 0.097108]\n",
      "     -- cost=-0.00250554,   cost1=9.86846e-06, cost2=-0.0025154\n",
      "     -- mean(hitsP)=0.895585, mean(diffsP)=0.89258 mean(hitsA)=0.699506, mean(diffsA)=0.784357\n",
      "31: eta=0.191943 cost=-0.00250554 jtype=constrained costheta=-0.223 ps=[0.342557, -1.39661, -1.82911, 0.397836, 0.748025, 0.105145, 0.105643]\n",
      "     -- cost=-0.00258968,   cost1=8.62415e-06, cost2=-0.00259831\n",
      "     -- mean(hitsP)=0.895852, mean(diffsP)=0.910726 mean(hitsA)=0.699799, mean(diffsA)=0.821478\n",
      "32: eta=0.211138 cost=-0.00258968 jtype=constrained costheta=-0.171 ps=[0.40463, -1.33436, -1.84215, 0.39846, 0.83167, 0.106422, 0.11203]\n",
      "     -- cost=-0.00265864,   cost1=1.30315e-05, cost2=-0.00267168\n",
      "     -- mean(hitsP)=0.894929, mean(diffsP)=0.926724 mean(hitsA)=0.700586, mean(diffsA)=0.854393\n",
      "33: eta=0.232252 cost=-0.00265864 jtype=constrained costheta=-0.137 ps=[0.489437, -1.26099, -1.8571, 0.398871, 0.910181, 0.107695, 0.115669]\n",
      "     -- cost=-0.00271708,   cost1=1.74052e-05, cost2=-0.00273448\n",
      "     -- mean(hitsP)=0.894235, mean(diffsP)=0.941723 mean(hitsA)=0.701254, mean(diffsA)=0.881264\n",
      "34: eta=0.255477 cost=-0.00271708 jtype=constrained costheta=-0.111 ps=[0.592011, -1.15911, -1.85337, 0.399425, 0.975315, 0.109246, 0.116216]\n",
      "     -- cost=-0.00276154,   cost1=6.40383e-06, cost2=-0.00276794\n",
      "     -- mean(hitsP)=0.896468, mean(diffsP)=0.947171 mean(hitsA)=0.699424, mean(diffsA)=0.898123\n",
      "35: eta=0.281024 cost=-0.00276154 jtype=constrained costheta=-0.092 ps=[0.704493, -1.02773, -1.81188, 0.399783, 1.00637, 0.110613, 0.11373]\n",
      "     -- cost=-0.00276926,   cost1=1.25455e-05, cost2=-0.0027818\n",
      "     -- mean(hitsP)=0.898557, mean(diffsP)=0.947415 mean(hitsA)=0.695203, mean(diffsA)=0.907121\n",
      "36: eta=0.309127 cost=-0.00276926 jtype=constrained costheta=-0.068 ps=[0.865178, -0.884782, -1.765, 0.39979, 0.959837, 0.10633, 0.105194]\n",
      "     -- cost=-0.00278662,   cost1=1.45848e-07, cost2=-0.00278676\n",
      "     -- mean(hitsP)=0.900111, mean(diffsP)=0.946624 mean(hitsA)=0.700529, mean(diffsA)=0.911218\n",
      "37: eta=0.340039 cost=-0.00278662 jtype=Newton costheta=-0.051 ps=[0.87297, -0.917019, -1.8361, 0.399782, 0.978456, 0.104219, 0.102237]\n",
      "     -- cost=-0.0027844,   cost1=1.50842e-05, cost2=-0.00279949\n",
      "     -- mean(hitsP)=0.90271, mean(diffsP)=0.950857 mean(hitsA)=0.704777, mean(diffsA)=0.915469\n",
      "eta going down: new_cost-cost=2.2121e-06 and jumptype='constrained'\n",
      "38: eta=0.17002 cost=-0.00278662 jtype=constrained costheta=NaN ps=[0.87297, -0.917019, -1.8361, 0.399782, 0.978456, 0.104219, 0.102237]\n",
      "     -- cost=-0.00279074,   cost1=2.95923e-06, cost2=-0.0027937\n",
      "     -- mean(hitsP)=0.900237, mean(diffsP)=0.948539 mean(hitsA)=0.702421, mean(diffsA)=0.913926\n",
      "39: eta=0.187022 cost=-0.00279074 jtype=constrained costheta=-0.055 ps=[0.888368, -0.871293, -1.73717, 0.399799, 0.932568, 0.102304, 0.102903]\n",
      "     -- cost=-0.0025767,   cost1=0.000217266, cost2=-0.00279397\n",
      "     -- mean(hitsP)=0.879988, mean(diffsP)=0.949276 mean(hitsA)=0.694166, mean(diffsA)=0.913369\n",
      "eta going down: new_cost-cost=0.000214038 and jumptype='constrained'\n",
      "40: eta=0.0935109 cost=-0.00279074 jtype=constrained costheta=NaN ps=[0.888368, -0.871293, -1.73717, 0.399799, 0.932568, 0.102304, 0.102903]\n",
      "     -- cost=-0.00278437,   cost1=9.20368e-06, cost2=-0.00279357\n",
      "     -- mean(hitsP)=0.89571, mean(diffsP)=0.948768 mean(hitsA)=0.699981, mean(diffsA)=0.913615\n",
      "eta going down: new_cost-cost=6.36777e-06 and jumptype='constrained'\n",
      "41: eta=0.0467554 cost=-0.00279074 jtype=constrained costheta=NaN ps=[0.888368, -0.871293, -1.73717, 0.399799, 0.932568, 0.102304, 0.102903]\n",
      "     -- cost=-0.00279178,   cost1=1.56434e-06, cost2=-0.00279335\n",
      "     -- mean(hitsP)=0.89874, mean(diffsP)=0.948641 mean(hitsA)=0.701242, mean(diffsA)=0.913591\n",
      "42: eta=0.051431 cost=-0.00279178 jtype=constrained costheta=-0.049 ps=[0.913431, -0.85209, -1.72968, 0.399801, 0.909054, 0.101931, 0.100365]\n",
      "     -- cost=-0.00279385,   cost1=1.38553e-06, cost2=-0.00279523\n",
      "     -- mean(hitsP)=0.899626, mean(diffsP)=0.949495 mean(hitsA)=0.701622, mean(diffsA)=0.913992\n",
      "43: eta=0.0565741 cost=-0.00279385 jtype=constrained costheta=-0.035 ps=[0.922806, -0.835455, -1.70079, 0.399804, 0.893125, 0.100988, 0.0998749]\n",
      "     -- cost=-0.00279618,   cost1=1.20585e-06, cost2=-0.00279739\n",
      "     -- mean(hitsP)=0.898945, mean(diffsP)=0.950469 mean(hitsA)=0.701139, mean(diffsA)=0.914456\n",
      "44: eta=0.0622315 cost=-0.00279618 jtype=constrained costheta=-0.583 ps=[0.945185, -0.815851, -1.6832, 0.399805, 0.860513, 0.100096, 0.0970616]\n",
      "     -- cost=-0.00280007,   cost1=1.06239e-06, cost2=-0.00280114\n",
      "     -- mean(hitsP)=0.899693, mean(diffsP)=0.951857 mean(hitsA)=0.701425, mean(diffsA)=0.915567\n",
      "45: eta=0.0684546 cost=-0.00280007 jtype=constrained costheta=-0.060 ps=[0.960237, -0.795714, -1.65064, 0.399805, 0.834624, 0.0990722, 0.0956575]\n",
      "     -- cost=-0.00280418,   cost1=1.12427e-06, cost2=-0.0028053\n",
      "     -- mean(hitsP)=0.90005, mean(diffsP)=0.953246 mean(hitsA)=0.701499, mean(diffsA)=0.916955\n",
      "46: eta=0.0753001 cost=-0.00280418 jtype=constrained costheta=-0.244 ps=[0.968614, -0.776577, -1.61163, 0.399806, 0.805354, 0.0979094, 0.094779]\n",
      "     -- cost=-0.00280839,   cost1=1.40524e-06, cost2=-0.00280979\n",
      "     -- mean(hitsP)=0.900487, mean(diffsP)=0.955135 mean(hitsA)=0.701604, mean(diffsA)=0.91806\n",
      "47: eta=0.0828301 cost=-0.00280839 jtype=constrained costheta=-0.487 ps=[0.971505, -0.756908, -1.5636, 0.399806, 0.782808, 0.0966068, 0.0951607]\n",
      "     -- cost=-0.00281262,   cost1=1.4632e-06, cost2=-0.00281409\n",
      "     -- mean(hitsP)=0.90066, mean(diffsP)=0.957484 mean(hitsA)=0.701578, mean(diffsA)=0.918574\n",
      "48: eta=0.0911131 cost=-0.00281262 jtype=constrained costheta=-0.172 ps=[0.971367, -0.735959, -1.50752, 0.399806, 0.767273, 0.0951963, 0.0967597]\n",
      "     -- cost=-0.00281679,   cost1=1.33701e-06, cost2=-0.00281812\n",
      "     -- mean(hitsP)=0.900718, mean(diffsP)=0.9601 mean(hitsA)=0.701469, mean(diffsA)=0.91865\n",
      "49: eta=0.100224 cost=-0.00281679 jtype=constrained costheta=-0.107 ps=[0.970837, -0.712792, -1.44348, 0.399806, 0.755784, 0.0938224, 0.099238]\n",
      "     -- cost=-0.00282093,   cost1=1.33894e-06, cost2=-0.00282226\n",
      "     -- mean(hitsP)=0.900742, mean(diffsP)=0.962751 mean(hitsA)=0.701458, mean(diffsA)=0.918759\n",
      "50: eta=0.110247 cost=-0.00282093 jtype=constrained costheta=-0.082 ps=[0.969772, -0.687159, -1.37087, 0.399807, 0.739898, 0.0927312, 0.10196]\n",
      "     -- cost=-0.00282575,   cost1=1.38137e-06, cost2=-0.00282713\n",
      "     -- mean(hitsP)=0.900995, mean(diffsP)=0.965919 mean(hitsA)=0.701331, mean(diffsA)=0.918832\n",
      "51: eta=0.121272 cost=-0.00282575 jtype=constrained costheta=-0.059 ps=[0.968195, -0.658617, -1.28872, 0.399807, 0.718028, 0.0922916, 0.104861]\n",
      "     -- cost=-0.00283346,   cost1=1.91816e-06, cost2=-0.00283538\n",
      "     -- mean(hitsP)=0.901671, mean(diffsP)=0.970104 mean(hitsA)=0.701021, mean(diffsA)=0.920146\n",
      "52: eta=0.133399 cost=-0.00283346 jtype=constrained costheta=-0.049 ps=[0.965756, -0.627366, -1.19924, 0.399806, 0.679691, 0.0922042, 0.106832]\n",
      "     -- cost=-0.0028496,   cost1=5.46834e-07, cost2=-0.00285015\n",
      "     -- mean(hitsP)=0.900868, mean(diffsP)=0.975808 mean(hitsA)=0.700583, mean(diffsA)=0.924291\n",
      "53: eta=0.146739 cost=-0.0028496 jtype=constrained costheta=-0.079 ps=[0.964166, -0.592875, -1.10251, 0.399806, 0.628305, 0.0921045, 0.107656]\n",
      "     -- cost=-0.00286613,   cost1=2.71145e-07, cost2=-0.0028664\n",
      "     -- mean(hitsP)=0.899808, mean(diffsP)=0.981831 mean(hitsA)=0.700711, mean(diffsA)=0.9291\n",
      "54: eta=0.161412 cost=-0.00286613 jtype=constrained costheta=-0.092 ps=[0.965775, -0.552664, -0.982962, 0.399808, 0.602838, 0.0922478, 0.111933]\n",
      "     -- cost=-0.00286926,   cost1=5.43733e-07, cost2=-0.00286981\n",
      "     -- mean(hitsP)=0.901033, mean(diffsP)=0.985204 mean(hitsA)=0.700145, mean(diffsA)=0.928002\n",
      "55: eta=0.177554 cost=-0.00286926 jtype=Newton costheta=-0.027 ps=[0.967505, -0.550634, -0.977025, 0.399772, 0.607864, 0.0970432, 0.11233]\n",
      "     -- cost=-0.00286992,   cost1=9.6459e-07, cost2=-0.00287088\n",
      "     -- mean(hitsP)=0.901339, mean(diffsP)=0.986718 mean(hitsA)=0.70037, mean(diffsA)=0.927204\n",
      "About to break -- tol=1e-06, new_cost-cost=-6.53896e-07, eta=0.177554\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1×7 Array{Float64,2}:\n",
       " 0.967505  -0.550634  -0.977025  0.399772  0.607864  0.0970432  0.11233"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = [\"sW\", \"vW\", \"hW\", \"constant_excitation\", \"right_light_excitation\", \"target_period_excitation\", \"sigma\"]\n",
    "seed = [0.2,   -1.7, -1.7,      0.19,                0.5,                       1,                       0.1]\n",
    "seed = [0.2,   0.17,  0.17,      0.19,                0.5,                       1,                       0.1]\n",
    "seed = [0.2,   -1.7, -1.7,      -0.19,                0.5,                       1,                       0.1]\n",
    "seed = [0.2,   -1.7, -1.7,      0.19,                0.15,                       0.1,                       0.1]\n",
    "\n",
    "nPro=100; nAnti=100\n",
    "\n",
    "model_params = merge(model_params, Dict(:post_target_period=>0.5))\n",
    "\n",
    "bbox = Dict(:sW=>[0 3], :vW=>[-3 3], :hW=>[-3 3], :constant_excitation=>[0.01 0.4],\n",
    ":right_light_excitation=>[0.05 2], :target_period_excitation=>[0.05 2], :sigma=>[0.05 1])\n",
    "\n",
    "pars = bbox_Hessian_keyword_minimization(seed, args, bbox, \n",
    "(;params...) -> JJ(nPro, nAnti; seedrand=30, cbeta=0.003, verbose=true, merge(model_params, Dict(params))...),\n",
    "start_eta = 0.01, verbose=true)[1]\n",
    "\n",
    "pars'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6.281900 seconds (8.92 M allocations: 542.649 MB, 1.86% gc time)\n",
      "Pro % correct = 55.4%\n",
      "Anti % correct = 53.4% \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.04172470792281961"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------\n",
    "# NOW EVALUATE RESULTS HERE\n",
    "# --------------------\n",
    "\n",
    "my_params = make_dict([args; \"plot_list\"; \"post_target_period\"; \"rule_and_delay_period\"; \"dt\"], \n",
    "[pars; [[1:10;]]; 0.5; 0.8; 0.02], model_params)\n",
    "\n",
    "run_factor = 10\n",
    "\n",
    "proVs, antiVs = @time(run_ntrials(nPro*run_factor, nAnti*run_factor; plot_list=[], my_params...))\n",
    "\n",
    "if nPro>0;  @printf(\"Pro %% correct = %g%%\\n\", 100*length(find(proVs[1,:].>proVs[4,:]))/(nPro*run_factor)); end;\n",
    "if nAnti>0; @printf(\"Anti %% correct = %g%% \\n\", 100*length(find(antiVs[1,:].<antiVs[4,:]))/(nAnti*run_factor)); end;\n",
    "\n",
    "\n",
    "figure(3); clf();\n",
    "ax1 = subplot(2,1,1)\n",
    "h = plt[:hist](proVs[1,:]-proVs[4,:],-1:0.02:1)\n",
    "title(\"PRO Vr - Vl\")\n",
    "remove_xtick_labels(ax1)\n",
    "vlines(0, ylim()[1], ylim()[2])\n",
    "\n",
    "ax2 = subplot(2,1,2)\n",
    "h = plt[:hist](antiVs[1,:]-antiVs[4,:],-1:0.02:1)\n",
    "title(\"ANTI Vr - Vl\")\n",
    "vlines(0, ylim()[1], ylim()[2])\n",
    "\n",
    "JJ(nPro, nAnti; my_params...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Symbol,Any} with 29 entries:\n",
       "  :target_period          => 0.1\n",
       "  :right_light_pro_extra  => 0\n",
       "  :const_pro_bias         => 0\n",
       "  :beta                   => 1\n",
       "  :sigma                  => 0.08\n",
       "  :dW                     => 0\n",
       "  :anti_rule_strength     => 0.1\n",
       "  :init_add               => 0\n",
       "  :pro_rule_strength      => 0.1\n",
       "  :vw                     => -1.7\n",
       "  :sW                     => 0.2\n",
       "  :vW                     => -1.7\n",
       "  :noise                  => Any[]\n",
       "  :tau                    => 0.1\n",
       "  :theta                  => 1\n",
       "  :right_light_excitation => 0.5\n",
       "  :hW                     => -1.7\n",
       "  :hw                     => -1.7\n",
       "  :sw                     => 0.2\n",
       "  :constant_excitation    => 0.19\n",
       "  :rule_and_delay_period  => 0.4\n",
       "  :const_add              => 0\n",
       "  :nsteps                 => 2\n",
       "  :post_target_period     => 0.5\n",
       "  :input                  => 0\n",
       "  ⋮                       => ⋮"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7-element Array{Float64,1}:\n",
       "  0.764685\n",
       " -1.84917 \n",
       " -0.961235\n",
       "  0.399476\n",
       "  1.33592 \n",
       "  0.13114 \n",
       "  0.232522"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_pro_pars = pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition wallwrap(Any, Any) in module Main at In[90]:18 overwritten at In[92]:18.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'wallwrap :: Tuple{Any,Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition vector_wrap(Any, Any, Any) in module Main at In[90]:44 overwritten at In[92]:44.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'vector_wrap :: Tuple{Any,Any,Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition inverse_wall(Any, Any, Any) in module Main at In[90]:68 overwritten at In[92]:68.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'inverse_wall :: Tuple{Any,Any,Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition inverse_wall(Any, Any) in module Main at In[90]:91 overwritten at In[92]:91.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'inverse_wall :: Tuple{Any,Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition bbox_Hessian_keyword_minimization(Any, Any, Any, Any) in module Main at In[90]:193 overwritten at In[92]:193.\n",
      "WARNING: Method definition #bbox_Hessian_keyword_minimization(Array{Any, 1}, Main.#bbox_Hessian_keyword_minimization, Any, Any, Any, Any) in module Main overwritten.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'bbox_Hessian_keyword_minimization :: Tuple{Any,Any,Any,Any}' in module 'Main'.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bbox_Hessian_keyword_minimization"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################################\n",
    "#                                                    #\n",
    "#         BBOX_HESSIAN_KEYWORD_MINIMIZATION          #\n",
    "#                                                    #\n",
    "######################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "pdict = wallwrap(bdict, pdict)\n",
    "Given bdict, a dictionary of symbols to [minval, maxval] vectors, and pdict, a dictionary of symbols\n",
    "to values (or, alternatively, an Array of (Symbol, value) tuples], goes through each of the symbols in \n",
    "bdict and modifies the corresponding value in pdict putting it through a tanh so the final output lies \n",
    "within the limits in bdict.  Returns the new pdict.  Makes a copy of pdict so as not to modify the original.\n",
    "\"\"\"\n",
    "function wallwrap(bdict, epdict)\n",
    "    local pdict = two_level_copy(epdict)\n",
    "    if typeof(pdict)<:Array\n",
    "        pdict = Dict(pdict)\n",
    "    end\n",
    "\n",
    "    allkeys = keys(bdict)\n",
    "\n",
    "    for k in allkeys\n",
    "        local bbox = bdict[k]\n",
    "        d = 0.5*(bbox[2] - bbox[1])\n",
    "        m = 0.5*(bbox[2] + bbox[1])\n",
    "\n",
    "        pdict[k] = bbox[1] + d*(tanh((pdict[k]-m)/d)+1)\n",
    "    end\n",
    "    return pdict\n",
    "end\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "params = vector_wrap(bbox, args, eparams)\n",
    "Given bdict, a dictionary of symbols to [minval, maxval] vectors, args, an array of strings representing\n",
    "symbols, and params, an array of values corresponding to the args list, puts each param that has an entry \n",
    "in bdict through the tanh-walling mechanism, and returns the result. Does not modify the contents of the \n",
    "original params vector (or bdict or args).\n",
    "\"\"\"\n",
    "function vector_wrap(bbox, args, eparams)\n",
    "    local params = two_level_copy(eparams)\n",
    "    pdict = wallwrap(bbox, make_dict(args, params))\n",
    "    i=1; j=1\n",
    "    for i=1:length(args)\n",
    "        if typeof(args[i])<:Array\n",
    "            params[j:j+args[i][2]-1] = pdict[Symbol(args[i][1])]\n",
    "            j += args[i][2]-1\n",
    "        else\n",
    "            params[j] = pdict[Symbol(args[i])]\n",
    "        end\n",
    "    j = j+1\n",
    "    end\n",
    "    return params\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "params = inverse_wall(bdict, args, wparams)\n",
    "Given bdict, a dictionary of symbols to [minval, maxval] vectors, args, an array of strings representing\n",
    "symbols, and wparams, an array of values corresponding to the args list where each param that has an entry \n",
    "in bdict has alreadt been through the tanh-walling mechanism, UNwalls the ones that have a bdict entry and\n",
    "returns the result. Does not modify the contents of the original params vector (or bdict or args).\n",
    "\"\"\"\n",
    "function inverse_wall(bdict, args, wparams)\n",
    "    local params = two_level_copy(wparams)\n",
    "    pdict = inverse_wall(bdict, make_dict(args, params))\n",
    "    i=1; j=1\n",
    "    for i=1:length(args)\n",
    "        if typeof(args[i])<:Array\n",
    "            params[j:j+args[i][2]-1] = pdict[Symbol(args[i][1])]\n",
    "            j += args[i][2]-1\n",
    "        else\n",
    "            params[j] = pdict[Symbol(args[i])]\n",
    "        end\n",
    "        j = j+1\n",
    "    end\n",
    "    return params    \n",
    "end\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "pdict = inverse_wall(bdict, wdict)\n",
    "Given bdict, a dictionary of symbols to [minval, maxval] vectors, and wdict, a dictionary of symbols to values\n",
    "(or vectors of values)  UNwalls the ones that have a bdict entry and\n",
    "returns the result. Does not modify the contents of any dictionaries.\n",
    "\"\"\"\n",
    "function inverse_wall(bdict, wdict)\n",
    "    local pdict = two_level_copy(wdict)\n",
    "\n",
    "    allkeys = keys(bdict)\n",
    "    for k in allkeys\n",
    "        local bbox = bdict[k]\n",
    "        d = 0.5*(bbox[2] - bbox[1])\n",
    "        m = 0.5*(bbox[2] + bbox[1])\n",
    "\n",
    "        pdict[k] = m + d*0.5*log((pdict[k]-bbox[1])./(2*d - pdict[k] + bbox[1]))\n",
    "    end\n",
    "    return(pdict)\n",
    "end\n",
    "  \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "function bbox_Hessian_keyword_minimization(seed, args, bbox, func; wallwidth=NaN, start_eta=10, tol=1e-6, \n",
    "    maxiter=400, verbose=false)\n",
    "\n",
    "Like constrained_Hessian_minimization, but uses keyword_hessian!(). \n",
    "\n",
    "PARAMETERS:\n",
    "===========\n",
    "\n",
    "seed        column vector, representing the starting value of the parameters.\n",
    "\n",
    "args        List of strings identifying parameters for differentiation, e.g., [\"const_E\", \"w_self]\n",
    "\n",
    "bbox        If softbox=true (the default), should then be a Dict of Symbol=>[minval maxval] entries. An entry\n",
    "            in this Dict indicates that the corresponding parameter is to be bounded, as indicated by the associated \n",
    "            [minval maxval] vector. The bbox dictionary can have fewer entries than the number of parameters, and its\n",
    "            default value is Dict(), indicating an unbounded search.\n",
    "\n",
    "            If softbox=false, then bbox should be an nargs-by-2 matrix indicating the range for each argument,\n",
    "            with the minima (first column) and maxima (second column), and entries for ALL parameters.\n",
    "\n",
    "func        func must take only optional keyword args, and must \n",
    "            take nderivs=0, difforder=0  and declare any new matrices using ForwardDiffZeros() instead of zeros()\n",
    "\n",
    "\n",
    "OPTIONAL PARAMETERS:\n",
    "====================\n",
    "\n",
    "start_eta    Starting value of the radius.  It's good to start with somethibg biggish, if it is\n",
    "             too much, it'll quickly get cut down.\n",
    "\n",
    "tol=1e-6     Numerical tolerance. If a proposed jump produces a change in func that is less than\n",
    "             this, the minimization stops.\n",
    "\n",
    "maxiter=400  Maximum number of iterations to do before stopping\n",
    "\n",
    "verbose=false   If true, print out a report on each iteration of iteration number, radius size (eta),\n",
    "                what type jump was proposed (\"Newton\" means going straight to global min, \"constrained\" means jump has \n",
    "                norm eta, failed means that finding the minimum at a given radius somehow didn't work). Will also\n",
    "                print out the cosine of the angle between the proposed jump and the gradient.\n",
    "\n",
    "verbose_level   If less than 2, regular verbose output, if 2 or greater, very verbose, for debugging.\n",
    "\n",
    "softbox         If true, then bbox must be a Dict() and we use the tanh() mechanism for putting a fixed limit\n",
    "                on the parameters.\n",
    "\n",
    "hardbox=false   If true, ignores wallwidth, and just rests parameter values to the bounding box if they go outside it.\n",
    "                If false, adds cost function \"walls\" to implement the bounding box.\n",
    "\n",
    "walldith=NaN     Used for putting up cost function \"walls\" that implement the bounding box limits. Can be NaN.\n",
    "                If it is NaN, then the wallwidth is a constant factor of the range width for each argument. If not NaN, must\n",
    "                be an nargs-long vector that indicates the actual wall widths.\n",
    "\n",
    "wallwidth_factor=0.18   Only relevant if wallwidth is NaN, otherwise ignored. For each arg, the wall width\n",
    "                is going to be wall_width_factor*(bbox[i,2] - bbox[i,1])\n",
    "\n",
    "\n",
    "RETURNS:\n",
    "========\n",
    "\n",
    "params       A vector the size of seed that has the last values of the minimizing parameters for func\n",
    "trajectory   A (2+length(params))-by-nsteps matrix. Each column corresponds to an iteration step, and contains\n",
    "                 the value of eta used, the cost, and the value of the parameters at that iteration\n",
    "cost         Final value of objective function\n",
    "cpm_traj     A 2-by-nsteps matrix, containing reports from the contrained parabolic minimization at each timestep.\n",
    "             The first row is niters (how many iterations cpm's 1-d minimization ran for) and the second row is\n",
    "             Dlambda, the last change in the parameter being minimized in cpm's internal search\n",
    "\n",
    "\n",
    "EXAMPLE:\n",
    "========\n",
    "\n",
    "function tester(;x=5, y=10, z=20, nderivs=0, difforder=0)\n",
    "    return x^2*y + z/tanh(y)\n",
    "end\n",
    "\n",
    "params, trajectory = bbox_Hessian_keyword_minimization([0.5, 0.5], [\"x\", \"y\"], [1.1 2 ; 1.1 4], tester, \n",
    "    verbose=true, tol=1e-12, start_eta=1);\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "function bbox_Hessian_keyword_minimization(seed, args, bbox, func; start_eta=0.1, tol=1e-6, maxiter=400,\n",
    "    verbose=false, verbose_level=1, verbose_every=1, \n",
    "    softbox=true, hardbox=false, wallwidth=NaN, wallwidth_factor=0.18)\n",
    "\n",
    "      \n",
    "    \"\"\"\n",
    "    Given args, a list of string representing the arguments of interest, a bounding box for each,\n",
    "    and a Symbol=>value dictionary with the corresponding parameters, computes and returns a high cost for \n",
    "    being outside the bounding box\n",
    "    \"\"\"\n",
    "    function wall_cost(args, bbox; wallwidth=NaN, nderivs=0, difforder=0, pars...) \n",
    "        myparams = ForwardDiffZeros(length(pars), 1, nderivs=nderivs, difforder=difforder)\n",
    "        pars2 = Dict()\n",
    "        for i in [1:length(pars);]\n",
    "            pars2[string(pars[i][1])] = pars[i][2]\n",
    "        end\n",
    "        for i in [1:length(args);]\n",
    "            myparams[i] = pars2[args[i]]\n",
    "        end\n",
    "        \n",
    "        if isnan(wallwidth)\n",
    "            # We know that we're going to be taking hessian for params, so declare zeros accordingly:\n",
    "            wallwidth = ForwardDiffZeros(length(myparams), 1, nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "            for i in [1:length(myparams);]\n",
    "                wallwidth[i] = wallwidth_factor*(bbox[i,2]-bbox[i,1])\n",
    "            end\n",
    "        end\n",
    "\n",
    "        retval = 0\n",
    "        for i in [1:length(myparams);]\n",
    "            if myparams[i]<bbox[i,1]\n",
    "                retval += cosh((bbox[i,1]-myparams[i])/wallwidth[i])-1.0\n",
    "            elseif bbox[i,2] < myparams[i]\n",
    "                retval += cosh((myparams[i]-bbox[i,2])/wallwidth[i])-1.0                \n",
    "            end\n",
    "        end\n",
    "\n",
    "        return 2*retval\n",
    "    end\n",
    "\n",
    "    traj_increment = 100\n",
    "    params = 0  # Make sure to have this here so that params stays defined beyond the try/catch\n",
    "    if ( !(typeof(bbox)<:Dict) ); error(\"Currently only supporting softbox=true, bbox must be a Dict\"); end;\n",
    "    try\n",
    "        params = copy(inverse_wall(bbox, args, seed))\n",
    "    catch\n",
    "        error(\"Were all initial param values within the indicated walls?\")\n",
    "    end\n",
    "    eta = start_eta\n",
    "    trajectory = zeros(2+length(params), traj_increment); cpm_traj = zeros(2, traj_increment)\n",
    "\n",
    "    if verbose\n",
    "        @printf \"%d: eta=%g ps=\" 0 eta \n",
    "        print_vector(vector_wrap(bbox, args, params))\n",
    "        @printf \"\\n\"\n",
    "    end\n",
    "    \n",
    "    if softbox\n",
    "        if !(typeof(bbox)<:Dict); error(\"bhm: If softbox=true, then bbox must eb a Dict\"); end\n",
    "        cost, grad, hess = keyword_vgh((;pars...)->func(;wallwrap(bbox, pars)...), args, params)\n",
    "    elseif hardbox\n",
    "        cost, grad, hess = keyword_vgh((;pars...) -> func(;pars...), args, params)\n",
    "    else\n",
    "        cost, grad, hess = keyword_vgh((;pars...) -> func(;pars...) + wall_cost(args, bbox; wallwidth=wallwidth, pars...),\n",
    "            args, params)        \n",
    "    end\n",
    "        \n",
    "    chessdelta = zeros(size(params))\n",
    "\n",
    "    for i in [1:maxiter;]\n",
    "        if i > size(trajectory, 2)\n",
    "            trajectory = [trajectory zeros(2+length(params), traj_increment)]\n",
    "            cpm_traj   = [cpm_traj   zeros(2, traj_increment)]\n",
    "        end\n",
    "        trajectory[1:2, i]   = [eta;cost]\n",
    "        trajectory[3:end, i] = vector_wrap(bbox, args, params)\n",
    "        \n",
    "        hessdelta  = - inv(hess)*grad\n",
    "        try\n",
    "            if verbose && verbose_level >= 2\n",
    "                @printf(\"bhm: about to try cpm with grad : \"); print_vector_g(grad); print(\"\\n\")\n",
    "                @printf(\"bhm:   hess :\"); print_vector_g(hess[:]); print(\"\\n\");\n",
    "            end\n",
    "            if verbose && verbose_level >= 2\n",
    "                cpm_out = constrained_parabolic_minimization(hess, grad'', eta, \n",
    "                    maxiter=500, tol=1e-20, do_plot=true, verbose=true)                \n",
    "            else\n",
    "                cpm_out = constrained_parabolic_minimization(hess, grad'', eta, maxiter=500, tol=1e-20)\n",
    "            end\n",
    "            chessdelta = cpm_out[1]; cpm_traj[1,i] = cpm_out[5]; cpm_traj[2,i] = cpm_out[6]\n",
    "            jumptype = \"not failed\"\n",
    "        catch y\n",
    "            jumptype = \"failed\"\n",
    "            if verbose\n",
    "                @printf \"Constrained parabolic minimization failed with error %s\\n\" y\n",
    "                @printf \"\\n\"\n",
    "                @printf \"eta was %g\\n\" eta\n",
    "                @printf \"grad was\\n\"\n",
    "                print_vector(grad)\n",
    "                @printf \"\\n\\nhess was\\n\"\n",
    "                for k in [1:length(grad);]\n",
    "                    print_vector(hess[k,:])\n",
    "                    @printf \"\\n\"\n",
    "                end\n",
    "                @printf \"\\n\"\n",
    "                matwrite(\"error_report.mat\", Dict(\"grad\"=>grad, \"hess\"=>hess, \"eta\"=>eta))\n",
    "            end\n",
    "            break\n",
    "        end\n",
    "\n",
    "        if norm(hessdelta) <= eta\n",
    "            new_params = params + hessdelta\n",
    "            jumptype = \"Newton\"\n",
    "        elseif jumptype != \"failed\" \n",
    "            new_params = params + chessdelta\n",
    "            jumptype  = \"constrained\"\n",
    "        end\n",
    "\n",
    "        if jumptype != \"failed\"\n",
    "            if softbox\n",
    "                new_cost, new_grad, new_hess = \n",
    "                    keyword_vgh((;pars...) -> func(;wallwrap(bbox, pars)...), args, new_params)\n",
    "                if verbose && verbose_level >=2\n",
    "                    @printf(\"bhm: had new_params = : \"); print_vector_g(vector_wrap(bbox, args, params)); print(\"\\n\");\n",
    "                    @printf(\"bhm: and my bbox was : \"); print(bbox); print(\"\\n\")\n",
    "                    @printf(\"bhm: and my wallwrap output was : \"); print(wallwrap(bbox, make_dict(args, new_params))); print(\"\\n\")\n",
    "                    @printf(\"bhm: and this produced new_grad : \"); print_vector_g(new_grad); print(\"\\n\")\n",
    "                    @printf(\"bhm:   new_hess :\"); print_vector_g(new_hess[:]); print(\"\\n\");                                        \n",
    "                end\n",
    "            elseif hardbox\n",
    "                for p in [1:length(new_params);]\n",
    "                    if new_params[p] < bbox[p,1]; new_params[p] = bbox[p,1]; end\n",
    "                    if bbox[p,2] < new_params[p]; new_params[p] = bbox[p,2]; end\n",
    "                 end        \n",
    "                \n",
    "                new_cost, new_grad, new_hess = keyword_vgh((;pars...) -> func(;pars...), args, new_params)\n",
    "            else\n",
    "                new_cost, new_grad, new_hess = keyword_vgh((;pars...) -> func(;pars...) + \n",
    "                        wall_cost(args, bbox; wallwidth=wallwidth, pars...),\n",
    "                    args, new_params)                \n",
    "            end\n",
    "            \n",
    "            if abs(new_cost - cost) < tol || eta < tol\n",
    "                trajectory = trajectory[:,1:i]; cpm_traj = cpm_traj[:,1:i]\n",
    "                if verbose\n",
    "                    @printf(\"About to break -- tol=%g, new_cost-cost=%g, eta=%g\\n\", tol, new_cost-cost, eta)\n",
    "                end\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "\n",
    "        if jumptype == \"failed\" || new_cost >= cost  \n",
    "            if verbose\n",
    "                @printf(\"eta going down: new_cost-cost=%g and jumptype='%s'\\n\", new_cost-cost, jumptype)\n",
    "                if verbose_level >= 2\n",
    "                    nwp = vector_wrap(bbox, args, new_params); wp = vector_wrap(bbox, args, params)\n",
    "                    @printf(\"   vvv: proposed new params were : \"); print_vector_g(nwp); print(\"\\n\")\n",
    "                    @printf(\"   vvv: proposed delta params was : \"); print_vector_g(nwp-wp); print(\"\\n\")\n",
    "                    @printf(\"   vvv: grad was : \"); print_vector_g(grad); print(\"\\n\")\n",
    "                    costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "                    @printf(\"   vvv: costheta of proposed jump was %g\\n\", costheta)\n",
    "                end\n",
    "            end\n",
    "            eta = eta/2\n",
    "            costheta = NaN\n",
    "            if eta < tol\n",
    "                trajectory = trajectory[:,1:i]; cpm_traj = cpm_traj[:,1:i]\n",
    "                if verbose\n",
    "                    @printf(\"About to break -- tol=%g, new_cost-cost=%g, eta=%g\\n\", tol, new_cost-cost, eta)\n",
    "                end\n",
    "                break\n",
    "            end\n",
    "        else\n",
    "            eta = eta*1.1\n",
    "            costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "\n",
    "            params = new_params\n",
    "            cost = new_cost\n",
    "            grad = new_grad\n",
    "            hess = new_hess\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            if rem(i, verbose_every)==0\n",
    "                @printf \"%d: eta=%g cost=%g jtype=%s costheta=%.3f ps=\" i eta cost jumptype costheta\n",
    "                print_vector_g(vector_wrap(bbox, args, params))\n",
    "                @printf \"\\n\"\n",
    "                if verbose_level >= 3\n",
    "                    @printf \"    At this point, grad is =\"\n",
    "                    print_vector_g(grad)\n",
    "                    @printf \"\\n\"                \n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vector_wrap(bbox, args, params), trajectory, cost, cpm_traj\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# WORKS:\n",
    "# keyword_gradient((;params...) -> JJ(100; merge(model_params, Dict(params))...), [\"sw\", \"vw\", \"hw\"], [0.2, -1.7, -1.7])\n",
    "\n",
    "args = [\"sw\", \"vw\", \"hw\", \"constant_excitation\", \"right_light_excitation\", \"target_period_excitation\", \"sigma\"]\n",
    "seed = [0.2,   -1.7, -1.7,      0.19,                0.5,                       1,                       0.1]\n",
    "seed = [0.2,   0.17,  0.17,      0.19,                0.5,                       1,                       0.1]\n",
    "seed = [0.2,   -1.7, -1.7,      -0.19,                0.5,                       1,                       0.1]\n",
    "seed = [0.2,   -1.7, -1.7,      0.19,                0.15,                       0.1,                       0.1]\n",
    "\n",
    "bbox = [\n",
    "    -3    3 ;\n",
    "    -3    3 ; \n",
    "    -3    3 ;\n",
    "    0.1   0.4 ;\n",
    "    0.1   2.0 ;\n",
    "    0.1   2.0 ;\n",
    "    0.05  2.0 ;\n",
    "]\n",
    "\n",
    "pars = bbox_Hessian_keyword_minimization(seed, args, bbox, \n",
    "(;params...) -> JJ(100; seedrand=30, cbeta=0.001, verbose=true, merge(model_params, Dict(params))...),\n",
    "start_eta = 0.01, verbose=true)[1]\n",
    "\n",
    "# --------------------\n",
    "# NOW EVALUATE RESULTS\n",
    "# --------------------\n",
    "\n",
    "ntrials = 1000\n",
    "proVs, antiVs = @time(run_ntrials(ntrials; plot_list=[], make_dict(args, pars, model_params)...))\n",
    "\n",
    "@printf(\"Pro %% correct = %g%%\\n\", 100*length(find(proVs[1,:].>proVs[4,:]))/ntrials)\n",
    "@printf(\"Anti %% correct = %g%% \\n\", 100*length(find(antiVs[1,:].<antiVs[4,:]))/ntrials)\n",
    "\n",
    "\n",
    "figure(3); clf();\n",
    "ax1 = subplot(2,1,1)\n",
    "h = plt[:hist](proVs[1,:]-proVs[4,:],-1:0.02:0.1)\n",
    "title(\"PRO Vr - Vl\")\n",
    "remove_xtick_labels(ax1)\n",
    "vlines(0, ylim()[1], ylim()[2])\n",
    "\n",
    "ax2 = subplot(2,1,2)\n",
    "h = plt[:hist](antiVs[1,:]-antiVs[4,:],-1:0.02:1)\n",
    "title(\"ANTI Vr - Vl\")\n",
    "vlines(0, ylim()[1], ylim()[2])\n",
    "\n",
    "JJ(100; make_dict([args; \"plot_list\"], [pars; [[1:5;]]], model_params)...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[args pars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "ntrials = 1000\n",
    "proVs, antiVs = @time(run_ntrials(ntrials; plot_list=[], make_dict(args, pars, model_params)...))\n",
    "\n",
    "@printf(\"Pro %% correct = %g%%\\n\", 100*length(find(proVs[1,:].>proVs[4,:]))/ntrials)\n",
    "@printf(\"Anti %% correct = %g%% \\n\", 100*length(find(antiVs[1,:].<antiVs[4,:]))/ntrials)\n",
    "\n",
    "\n",
    "figure(3); clf();\n",
    "ax1 = subplot(2,1,1)\n",
    "h = plt[:hist](proVs[1,:]-proVs[4,:],-1:0.02:0.1)\n",
    "title(\"PRO Vr - Vl\")\n",
    "remove_xtick_labels(ax1)\n",
    "vlines(0, ylim()[1], ylim()[2])\n",
    "\n",
    "ax2 = subplot(2,1,2)\n",
    "h = plt[:hist](antiVs[1,:]-antiVs[4,:],-1:0.02:1)\n",
    "title(\"ANTI Vr - Vl\")\n",
    "vlines(0, ylim()[1], ylim()[2])\n",
    "\n",
    "JJ(100; make_dict([args; \"plot_list\"; \"dt\"], [pars; [[1:5;]]; 0.01], model_params)...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "JJ(100; make_dict([args; \"plot_list\"; \"dt\"], [pars; [[1:5;]]; 0.005], model_params)...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@time(ForwardDiff.hessian((x)->JJ(100; nderivs=length(x), difforder=2, \n",
    "make_dict([\"sw\", \"hw\", \"vw\", \"sigma\", \"gleak\", \"constant_excitation\", \"plot_list\"], \n",
    "[x[1], x[2], x[3], x[4], x[5], x[6], []], model_params)...), \n",
    "[0.2, -1.7, -1.7, 0.08, 0.25, 0.19]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@time(JJ(100; make_dict([\"sw\", \"hw\", \"vw\", \"plot_list\"], [0.2, -1.7, -1.7, []], model_params)...))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ntrials = 100\n",
    "proVs, antiVs = @time(run_ntrials(ntrials; plot_list=[], model_params...))\n",
    "\n",
    "@printf(\"Pro %% correct = %g%%\\n\", 100*length(find(proVs[1,:].>proVs[4,:]))/ntrials)\n",
    "@printf(\"Anti %% correct = %g%% \\n\", 100*length(find(antiVs[1,:].<antiVs[4,:]))/ntrials)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the distribution of VR - VL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ntrials = 500\n",
    "proVs, antiVs = @time(run_ntrials(ntrials; plot_list=[], model_params...))\n",
    "\n",
    "@printf(\"Pro %% correct = %g%%\\n\", 100*length(find(proVs[1,:].>proVs[4,:]))/ntrials)\n",
    "@printf(\"Anti %% correct = %g%% \\n\", 100*length(find(antiVs[1,:].<antiVs[4,:]))/ntrials)\n",
    "\n",
    "figure(1); clf();\n",
    "ax1 = subplot(2,1,1)\n",
    "h = plt[:hist](proVs[1,:]-proVs[4,:],-0.1:0.002:0.1)\n",
    "title(\"PRO Vr - Vl\")\n",
    "remove_xtick_labels(ax1)\n",
    "vlines(0, ylim()[1], ylim()[2])\n",
    "\n",
    "ax2 = subplot(2,1,2)\n",
    "h = plt[:hist](antiVs[1,:]-antiVs[4,:],-0.1:0.002:0.1)\n",
    "title(\"ANTI Vr - Vl\")\n",
    "vlines(0, ylim()[1], ylim()[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt = 0.0002\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "W = -4\n",
    "noise = 0\n",
    "sigma = 0.2\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    "    :noise=>0, :input=>0, :const_add=>0, :init_add=>0, :sigma=>sigma)\n",
    "\n",
    "clf()\n",
    "srand(20)\n",
    "startUs = randn(20,2)-3\n",
    "startUs = [-2 -4.1]\n",
    "\n",
    "for i=1:size(startUs,1)\n",
    "    forwardModel(startUs[i,:]; do_plot=true, clearfig=false, model_params...)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "typeof(model_params[:input])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- END of ProAnti section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing around with minimizing 2-d model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed = [-0.2, 0.2, 0.2, -1.5, 0.1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition JJ(Any) in module Main at In[47]:62 overwritten at In[48]:62.\n",
      "WARNING: Method definition #JJ(Array{Any, 1}, Main.#JJ, Any) in module Main overwritten.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- cost=0.0686324,   cost1=0.0687511, cost2=-0.00011865 :  mean(hits)=0.487796, mean(diffs)=0.0395499\n",
      "0: eta=1 ps=[-0.200, 0.200, 0.200, -1.500, 0.100]\n",
      "-- cost=0.0686324,   cost1=0.0687511, cost2=-0.00011865 :  mean(hits)=0.487796, mean(diffs)=0.0395499\n",
      "-- cost=0.0662608,   cost1=0.0663306, cost2=-6.98297e-05 :  mean(hits)=0.492453, mean(diffs)=0.0232766\n",
      "1: eta=1.1 cost=0.0662608 jtype=Newton costheta=-0.343 ps=[-0.112938, 0.349709, 0.101748, -1.26536, 0.103245]\n",
      "-- cost=0.0656768,   cost1=0.0657292, cost2=-5.24826e-05 :  mean(hits)=0.493623, mean(diffs)=0.0174942\n",
      "2: eta=1.21 cost=0.0656768 jtype=Newton costheta=-0.188 ps=[0.0772162, 0.616916, 0.0618108, -0.989811, 0.103702]\n",
      "-- cost=0.0658205,   cost1=0.0658659, cost2=-4.54283e-05 :  mean(hits)=0.493357, mean(diffs)=0.0151428\n",
      "eta going down: new_cost-cost=0.000143696 and jumptype='Newton'\n",
      "3: eta=0.605 cost=0.0656768 jtype=Newton costheta=NaN ps=[0.0772162, 0.616916, 0.0618108, -0.989811, 0.103702]\n",
      "-- cost=0.0658205,   cost1=0.0658659, cost2=-4.54283e-05 :  mean(hits)=0.493357, mean(diffs)=0.0151428\n",
      "eta going down: new_cost-cost=0.000143696 and jumptype='Newton'\n",
      "4: eta=0.3025 cost=0.0656768 jtype=Newton costheta=NaN ps=[0.0772162, 0.616916, 0.0618108, -0.989811, 0.103702]\n",
      "-- cost=0.0645488,   cost1=0.0646014, cost2=-5.25525e-05 :  mean(hits)=0.495832, mean(diffs)=0.0175175\n",
      "5: eta=0.33275 cost=0.0645488 jtype=constrained costheta=-0.719 ps=[0.274727, 0.41869, 0.0155098, -1.08603, 0.104242]\n",
      "-- cost=0.0614765,   cost1=0.0615494, cost2=-7.28586e-05 :  mean(hits)=0.501909, mean(diffs)=0.0242862\n",
      "6: eta=0.366025 cost=0.0614765 jtype=constrained costheta=-0.946 ps=[0.455695, 0.244242, 0.132879, -1.25726, 0.105988]\n",
      "-- cost=0.0486282,   cost1=0.0487824, cost2=-0.000154226 :  mean(hits)=0.529133, mean(diffs)=0.0514088\n",
      "7: eta=0.402628 cost=0.0486282 jtype=constrained costheta=-0.997 ps=[0.606886, 0.105449, 0.312739, -1.48064, 0.108562]\n",
      "-- cost=0.0133079,   cost1=0.0139258, cost2=-0.00061789 :  mean(hits)=0.631993, mean(diffs)=0.205963\n",
      "8: eta=0.44289 cost=0.0133079 jtype=constrained costheta=-1.000 ps=[0.755649, -0.026157, 0.504166, -1.74103, 0.111807]\n",
      "-- cost=0.000257413,   cost1=0.00210511, cost2=-0.00184769 :  mean(hits)=0.795881, mean(diffs)=0.615897\n",
      "9: eta=0.487179 cost=0.000257413 jtype=constrained costheta=-0.978 ps=[0.975464, -0.236981, 0.654068, -1.97877, 0.116327]\n",
      "-- cost=-0.00157669,   cost1=6.96102e-07, cost2=-0.00157738 :  mean(hits)=0.750834, mean(diffs)=0.525794\n",
      "10: eta=0.535897 cost=-0.00157669 jtype=Newton costheta=-0.587 ps=[1.02164, -0.0108635, 0.62631, -1.9228, 0.117996]\n",
      "-- cost=-0.00192434,   cost1=0.000676274, cost2=-0.00260062 :  mean(hits)=0.723995, mean(diffs)=0.866872\n",
      "11: eta=0.589487 cost=-0.00192434 jtype=constrained costheta=-0.796 ps=[0.821785, 0.247223, 0.818342, -2.23205, 0.123257]\n",
      "-- cost=-0.00271411,   cost1=6.39525e-05, cost2=-0.00277807 :  mean(hits)=0.742003, mean(diffs)=0.926022\n",
      "12: eta=0.648436 cost=-0.00271411 jtype=Newton costheta=-0.556 ps=[0.821891, 0.193543, 0.875512, -2.32724, 0.121511]\n",
      "-- cost=-0.00284202,   cost1=2.05373e-05, cost2=-0.00286256 :  mean(hits)=0.745468, mean(diffs)=0.954186\n",
      "13: eta=0.713279 cost=-0.00284202 jtype=Newton costheta=-0.248 ps=[0.72397, 0.0815275, 0.92699, -2.41553, 0.119098]\n",
      "-- cost=-0.00288506,   cost1=8.78345e-06, cost2=-0.00289385 :  mean(hits)=0.747036, mean(diffs)=0.964616\n",
      "14: eta=0.784607 cost=-0.00288506 jtype=Newton costheta=-0.210 ps=[0.603214, -0.0548918, 0.975406, -2.49512, 0.116614]\n",
      "-- cost=-0.00290096,   cost1=6.46895e-06, cost2=-0.00290743 :  mean(hits)=0.747457, mean(diffs)=0.969143\n",
      "15: eta=0.863068 cost=-0.00290096 jtype=Newton costheta=-0.147 ps=[0.375301, -0.293363, 1.03034, -2.58254, 0.1136]\n",
      "-- cost=-0.00291064,   cost1=1.10793e-05, cost2=-0.00292172 :  mean(hits)=0.746671, mean(diffs)=0.973907\n",
      "16: eta=0.949375 cost=-0.00291064 jtype=Newton costheta=-0.125 ps=[-0.101739, -0.779492, 1.10727, -2.71391, 0.109042]\n",
      "-- cost=-0.00209882,   cost1=0.000899802, cost2=-0.00299862 :  mean(hits)=0.779997, mean(diffs)=0.999541\n",
      "eta going down: new_cost-cost=0.000811819 and jumptype='constrained'\n",
      "17: eta=0.474687 cost=-0.00291064 jtype=constrained costheta=NaN ps=[-0.101739, -0.779492, 1.10727, -2.71391, 0.109042]\n",
      "-- cost=-0.00229665,   cost1=0.000648625, cost2=-0.00294527 :  mean(hits)=0.775468, mean(diffs)=0.981758\n",
      "eta going down: new_cost-cost=0.000613992 and jumptype='constrained'\n",
      "18: eta=0.237344 cost=-0.00291064 jtype=constrained costheta=NaN ps=[-0.101739, -0.779492, 1.10727, -2.71391, 0.109042]\n",
      "-- cost=-0.00286845,   cost1=4.55062e-05, cost2=-0.00291396 :  mean(hits)=0.756746, mean(diffs)=0.971319\n",
      "eta going down: new_cost-cost=4.21903e-05 and jumptype='constrained'\n",
      "19: eta=0.118672 cost=-0.00291064 jtype=constrained costheta=NaN ps=[-0.101739, -0.779492, 1.10727, -2.71391, 0.109042]\n",
      "-- cost=-0.00291069,   cost1=1.37804e-06, cost2=-0.00291207 :  mean(hits)=0.751174, mean(diffs)=0.97069\n",
      "20: eta=0.130539 cost=-0.00291069 jtype=constrained costheta=-0.090 ps=[-0.0962228, -0.781568, 1.15767, -2.78912, 0.110374]\n",
      "-- cost=-0.00292144,   cost1=7.15913e-06, cost2=-0.0029286 :  mean(hits)=0.747324, mean(diffs)=0.976199\n",
      "21: eta=0.143593 cost=-0.00292144 jtype=constrained costheta=-0.039 ps=[-0.0946473, -0.771467, 1.21975, -2.86746, 0.111792]\n",
      "-- cost=-0.00292227,   cost1=3.77572e-06, cost2=-0.00292604 :  mean(hits)=0.751943, mean(diffs)=0.975347\n",
      "22: eta=0.157952 cost=-0.00292227 jtype=constrained costheta=-0.035 ps=[-0.0923132, -0.770222, 1.27629, -2.95537, 0.11353]\n",
      "-- cost=-0.00292847,   cost1=9.24347e-06, cost2=-0.00293772 :  mean(hits)=0.74696, mean(diffs)=0.979239\n",
      "23: eta=0.173747 cost=-0.00292847 jtype=constrained costheta=-0.018 ps=[-0.0800473, -0.749842, 1.36128, -3.04012, 0.115212]\n",
      "-- cost=-0.00291325,   cost1=3.26152e-05, cost2=-0.00294587 :  mean(hits)=0.755711, mean(diffs)=0.981955\n",
      "eta going down: new_cost-cost=1.52223e-05 and jumptype='constrained'\n",
      "24: eta=0.0868737 cost=-0.00292847 jtype=constrained costheta=NaN ps=[-0.0800473, -0.749842, 1.36128, -3.04012, 0.115212]\n",
      "-- cost=-0.00292713,   cost1=9.25892e-06, cost2=-0.00293639 :  mean(hits)=0.753043, mean(diffs)=0.978795\n",
      "eta going down: new_cost-cost=1.34679e-06 and jumptype='constrained'\n",
      "25: eta=0.0434369 cost=-0.00292847 jtype=constrained costheta=NaN ps=[-0.0800473, -0.749842, 1.36128, -3.04012, 0.115212]\n",
      "-- cost=-0.00293064,   cost1=3.26848e-06, cost2=-0.00293391 :  mean(hits)=0.751808, mean(diffs)=0.97797\n",
      "26: eta=0.0477805 cost=-0.00293064 jtype=constrained costheta=-0.042 ps=[-0.0780774, -0.74942, 1.37648, -3.06588, 0.115771]\n",
      "-- cost=-0.00293452,   cost1=1.12552e-06, cost2=-0.00293565 :  mean(hits)=0.748939, mean(diffs)=0.97855\n",
      "27: eta=0.0525586 cost=-0.00293452 jtype=constrained costheta=-0.022 ps=[-0.0767747, -0.745522, 1.39617, -3.0931, 0.116353]\n",
      "-- cost=-0.00293575,   cost1=2.00775e-09, cost2=-0.00293575 :  mean(hits)=0.750045, mean(diffs)=0.978583\n",
      "28: eta=0.0578145 cost=-0.00293575 jtype=constrained costheta=-0.011 ps=[-0.0774677, -0.745455, 1.41562, -3.12338, 0.117064]\n",
      "-- cost=-0.00293654,   cost1=6.97725e-08, cost2=-0.00293661 :  mean(hits)=0.749736, mean(diffs)=0.978869\n",
      "29: eta=0.0635959 cost=-0.00293654 jtype=constrained costheta=-0.013 ps=[-0.0783256, -0.744805, 1.44056, -3.15536, 0.117805]\n",
      "-- cost=-0.00293718,   cost1=1.22753e-08, cost2=-0.00293719 :  mean(hits)=0.749889, mean(diffs)=0.979063\n",
      "30: eta=0.0699555 cost=-0.00293718 jtype=constrained costheta=-0.018 ps=[-0.0801053, -0.745193, 1.46612, -3.19054, 0.118677]\n",
      "-- cost=-0.00293768,   cost1=2.15906e-08, cost2=-0.0029377 :  mean(hits)=0.749853, mean(diffs)=0.979234\n",
      "31: eta=0.076951 cost=-0.00293768 jtype=constrained costheta=-0.096 ps=[-0.0821453, -0.745674, 1.49504, -3.22846, 0.119642]\n",
      "-- cost=-0.00293807,   cost1=1.7614e-08, cost2=-0.00293809 :  mean(hits)=0.749867, mean(diffs)=0.979363\n",
      "32: eta=0.0846461 cost=-0.00293807 jtype=constrained costheta=-0.009 ps=[-0.0852402, -0.747187, 1.52449, -3.26998, 0.120797]\n",
      "-- cost=-0.00293835,   cost1=2.79069e-08, cost2=-0.00293838 :  mean(hits)=0.749833, mean(diffs)=0.979461\n",
      "33: eta=0.0931108 cost=-0.00293835 jtype=constrained costheta=-0.005 ps=[-0.0894229, -0.749758, 1.55398, -3.31527, 0.122236]\n",
      "-- cost=-0.00293856,   cost1=3.56531e-08, cost2=-0.00293859 :  mean(hits)=0.749811, mean(diffs)=0.979531\n",
      "34: eta=0.102422 cost=-0.00293856 jtype=constrained costheta=-0.002 ps=[-0.0946094, -0.75342, 1.57897, -3.36357, 0.125003]\n",
      "-- cost=-0.00293869,   cost1=4.14224e-08, cost2=-0.00293873 :  mean(hits)=0.749796, mean(diffs)=0.979576\n",
      "35: eta=0.112664 cost=-0.00293869 jtype=Newton costheta=-0.001 ps=[-0.0998685, -0.757333, 1.60175, -3.41426, 0.121983]\n",
      "-- cost=-0.00293878,   cost1=8.36299e-09, cost2=-0.00293879 :  mean(hits)=0.749909, mean(diffs)=0.979596\n",
      "36: eta=0.12393 cost=-0.00293878 jtype=Newton costheta=-0.001 ps=[-0.103349, -0.759929, 1.61709, -3.44911, 0.119844]\n",
      "-- cost=-0.00293881,   cost1=8.37884e-08, cost2=-0.00293889 :  mean(hits)=0.749711, mean(diffs)=0.97963\n",
      "37: eta=0.136323 cost=-0.00293881 jtype=Newton costheta=-0.001 ps=[-0.108362, -0.763726, 1.63877, -3.49622, 0.116854]\n",
      "-- cost=-0.00293888,   cost1=4.20086e-12, cost2=-0.00293888 :  mean(hits)=0.749998, mean(diffs)=0.979628\n",
      "38: eta=0.149956 cost=-0.00293888 jtype=Newton costheta=-0.001 ps=[-0.109818, -0.764804, 1.64566, -3.51266, 0.115794]\n",
      "-- cost=-0.0029377,   cost1=1.92904e-06, cost2=-0.00293963 :  mean(hits)=0.748611, mean(diffs)=0.979877\n",
      "eta going down: new_cost-cost=1.18179e-06 and jumptype='constrained'\n",
      "39: eta=0.0749779 cost=-0.00293888 jtype=constrained costheta=NaN ps=[-0.109818, -0.764804, 1.64566, -3.51266, 0.115794]\n",
      "-- cost=-0.00293887,   cost1=1.04061e-07, cost2=-0.00293897 :  mean(hits)=0.749677, mean(diffs)=0.979657\n",
      "eta going down: new_cost-cost=1.78223e-08 and jumptype='constrained'\n",
      "40: eta=0.037489 cost=-0.00293888 jtype=constrained costheta=NaN ps=[-0.109818, -0.764804, 1.64566, -3.51266, 0.115794]\n",
      "-- cost=-0.00293891,   cost1=6.13699e-09, cost2=-0.00293891 :  mean(hits)=0.749922, mean(diffs)=0.979638\n",
      "41: eta=0.0412378 cost=-0.00293891 jtype=constrained costheta=-0.029 ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00209909,   cost1=0.000899869, cost2=-0.00299895 :  mean(hits)=0.779998, mean(diffs)=0.999652\n",
      "eta going down: new_cost-cost=0.000839821 and jumptype='constrained'\n",
      "42: eta=0.0206189 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00209908,   cost1=0.000899865, cost2=-0.00299894 :  mean(hits)=0.779998, mean(diffs)=0.999648\n",
      "eta going down: new_cost-cost=0.000839828 and jumptype='constrained'\n",
      "43: eta=0.0103095 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00209904,   cost1=0.000899838, cost2=-0.00299888 :  mean(hits)=0.779997, mean(diffs)=0.999625\n",
      "eta going down: new_cost-cost=0.000839869 and jumptype='constrained'\n",
      "44: eta=0.00515473 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00228001,   cost1=0.000672223, cost2=-0.00295223 :  mean(hits)=0.775927, mean(diffs)=0.984078\n",
      "eta going down: new_cost-cost=0.000658897 and jumptype='constrained'\n",
      "45: eta=0.00257737 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.0028976,   cost1=0.000100016, cost2=-0.00299761 :  mean(hits)=0.760001, mean(diffs)=0.999204\n",
      "eta going down: new_cost-cost=4.13094e-05 and jumptype='constrained'\n",
      "46: eta=0.00128868 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00289594,   cost1=9.69889e-05, cost2=-0.00299293 :  mean(hits)=0.759848, mean(diffs)=0.997645\n",
      "eta going down: new_cost-cost=4.29624e-05 and jumptype='constrained'\n",
      "47: eta=0.000644341 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00289846,   cost1=7.42642e-05, cost2=-0.00297273 :  mean(hits)=0.758618, mean(diffs)=0.990909\n",
      "eta going down: new_cost-cost=4.0444e-05 and jumptype='constrained'\n",
      "48: eta=0.000322171 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00291812,   cost1=3.3319e-05, cost2=-0.00295144 :  mean(hits)=0.755772, mean(diffs)=0.983815\n",
      "eta going down: new_cost-cost=2.07823e-05 and jumptype='constrained'\n",
      "49: eta=0.000161085 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00293241,   cost1=9.96215e-06, cost2=-0.00294237 :  mean(hits)=0.753156, mean(diffs)=0.980791\n",
      "eta going down: new_cost-cost=6.49675e-06 and jumptype='constrained'\n",
      "50: eta=8.05427e-05 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00293726,   cost1=2.5067e-06, cost2=-0.00293976 :  mean(hits)=0.751583, mean(diffs)=0.979921\n",
      "eta going down: new_cost-cost=1.65185e-06 and jumptype='constrained'\n",
      "51: eta=4.02713e-05 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00293853,   cost1=5.74493e-07, cost2=-0.0029391 :  mean(hits)=0.750758, mean(diffs)=0.979702\n",
      "eta going down: new_cost-cost=3.76922e-07 and jumptype='constrained'\n",
      "52: eta=2.01357e-05 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00293883,   cost1=1.15893e-07, cost2=-0.00293895 :  mean(hits)=0.75034, mean(diffs)=0.97965\n",
      "eta going down: new_cost-cost=7.3048e-08 and jumptype='constrained'\n",
      "53: eta=1.00678e-05 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.0029389,   cost1=1.71889e-08, cost2=-0.00293892 :  mean(hits)=0.750131, mean(diffs)=0.979639\n",
      "About to break -- tol=1e-08, new_cost-cost=7.49351e-09, eta=1.00678e-05\n",
      "-- cost=-0.00293891,   cost1=6.13699e-09, cost2=-0.00293891 :  mean(hits)=0.749922, mean(diffs)=0.979638\n",
      "-- cost=-0.00293891,   cost1=6.13699e-09, cost2=-0.00293891 :  mean(hits)=0.749922, mean(diffs)=0.979638\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1×2 Array{Float64,2}:\n",
       " 0.487796  0.749922"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following sequence leads to a situation where having only [-0.8, -0.8] as the single finalFluxPoint \n",
    "# leads to the minimization getting stuck.  Adding further finalFluxPoints solves the problem\n",
    "\n",
    "dt = 0.02\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "mW = -4\n",
    "sW = 0.2\n",
    "noise = 0\n",
    "input = 0\n",
    "sigma = 0.1\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :sW=>sW, :mW=>mW, :W=>[sW mW ; mW sW], :nsteps=>nsteps, \n",
    ":noise=>noise, :input=>input, :sigma=>sigma, :const_add=>0, :init_add=>0)\n",
    "\n",
    "\n",
    "# WORKING gradient:\n",
    "# ForwardDiff.gradient((x)->JJ(startU; do_plot=true, nderivs=length(x), difforder=1, \n",
    "#    make_dict([[\"init_add\" 2], \"const_add\"], x, model_params)...), [2.9, -2.9, 0.1])\n",
    "\n",
    "\n",
    "\n",
    "# The backward and costfunc functions should turn a single-scalar parameter W into the matrix W\n",
    "# backward always runs with no within-forward noise, i.e., sigma=0\n",
    "backward = (endpoint; do_plot=false, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if ~haskey(pars, :sW) || ~haskey(pars, :mW) \n",
    "        error(\"Need both sW and mW to determine weight matrix\")\n",
    "    end\n",
    "    pars=make_dict([\"W\"], [[pars[:sW] pars[:mW] ; pars[:mW] pars[:sW]]], pars);\n",
    "    backwardsModel(endpoint; do_plot=do_plot, make_dict([\"sigma\"], [0], pars)...)[1]\n",
    "end\n",
    "\n",
    "forward = (startpoint; pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if ~haskey(pars, :sW) || ~haskey(pars, :mW) \n",
    "        error(\"Need both sW and mW to determine weight matrix\")\n",
    "    end\n",
    "    pars=make_dict([\"W\"], [[pars[:sW] pars[:mW] ; pars[:mW] pars[:sW]]], pars);\n",
    "    forwardsModel(startpoint; pars...)\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "costfunc = (startpoints; do_plot=false, verbose=false, beta=0.003, nderivs=0, difforder=0, sr=26, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if ~haskey(pars, :sW) || ~haskey(pars, :mW) \n",
    "        error(\"Need both sW and mW to determine weight matrix\")\n",
    "    end\n",
    "    pars=make_dict([\"W\"], [[pars[:sW] pars[:mW] ; pars[:mW] pars[:sW]]], pars);\n",
    "    JJ(startpoints; seedrand=sr, beta=beta, \n",
    "        do_plot=do_plot, verbose=verbose, nderivs=nderivs, difforder=difforder, pars...)\n",
    "end\n",
    "\n",
    "function JJ(initUs; theta1=0.15, theta2=0.2, beta=0.003, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, pre_string=\"\", zero_last_sigmas=0, seedrand=NaN, params...)\n",
    "\n",
    "    if ~isnan(seedrand); srand(seedrand); end\n",
    "    \n",
    "    Vend = ForwardDiffZeros(size(initUs,1), size(initUs,2), nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:size(initUs,1)\n",
    "        if false # i>size(initUs,1) - zero_last_sigmas\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; sigma=0, nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)            \n",
    "        else\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)\n",
    "        end\n",
    "        Vend[i,:] = Ve\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend[:,1]-Vend[:,2])/theta1))\n",
    "    diffs = tanh.((Vend[:,1]-Vend[:,2])/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "\n",
    "    if do_plot\n",
    "        title(@sprintf(\"mean(hits)=%g, mean(diffs)=%g\", convert(Float64, mean(hits)), convert(Float64, mean(diffs))))\n",
    "    end\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"%s\", pre_string)\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2, mean(hits), mean(diffs)\n",
    "end\n",
    "  \n",
    "\n",
    "\n",
    "beta = 0.003\n",
    "args = [[\"start_add\" 2], \"const_add\", \"mW\", \"sW\"] \n",
    "seed = [-0.2, 0.2, 0.2, -1.5, 0.1] \n",
    "walls = Dict(:start_add=>[-5.1, 5.1], :sW=>[0, 5.1], :mW=>[-5.1, 5.1]) # \n",
    "# sr =  gives \n",
    "\n",
    "\n",
    "new_random_seed = true; if new_random_seed\n",
    "    sr = convert(Int64, round(time()))\n",
    "else\n",
    "    sr = old_sr\n",
    "end\n",
    "# sr = 1504716566\n",
    "old_sr = sr\n",
    "\n",
    "srand(sr)\n",
    "\n",
    "startU=randn(50,2)-3\n",
    "\n",
    "\n",
    "clf()\n",
    "print(\"seed = \"); print_vector_g(seed); print(\"\\n\")\n",
    "ocost, omhits, omdiffs = costfunc(startU; do_plot=true, sr=sr, verbose=true, make_dict(args, seed, model_params)...)\n",
    "\n",
    "\n",
    "params, traj, zz, cpm_traj = bbox_Hessian_keyword_minimization(seed, args, walls,  \n",
    "(;params...) -> costfunc(startU; beta=beta, sr=sr, do_plot=true, verbose=true, merge(model_params, Dict(params))...)[1], \n",
    "verbose=true, start_eta=1, tol=1e-8, softbox=true, maxiter=400 )\n",
    "\n",
    "# params, cost, ptraj, gtraj = fluxSense(costfunc, backward, model_params, startU, fluxFinalPoint, args, seed; \n",
    "#    start_eta=0.01, tol=1e-15, maxiter=400, verbose=true, report_every=1, do_plot=false, cost_limit=cost_limit) # cost_limit=-0.000935) # for beta=0.01\n",
    "\n",
    "# And show the final position\n",
    "figure(1); clf()\n",
    "cost, mhits, mdiffs = \n",
    "    costfunc(startU; beta=beta, do_plot=true, sr=sr, verbose=true, make_dict(args, params, model_params)...)\n",
    "\n",
    "repeat_results_in_fig2 = true; if repeat_results_in_fig2\n",
    "    figure(2); clf()\n",
    "    costfunc(startU; beta=beta, do_plot=true, sr=sr, verbose=true, \n",
    "        make_dict(args, params, merge(Dict(:fignum=>2), model_params))...)\n",
    "    figure(1); \n",
    "end\n",
    "params'\n",
    "\n",
    "# For beta=0, and ntrials=20, we collected a bunch of results and observed that it failes about half the time\n",
    "# WHEN the initial mean)hits) is below 0.5.  It never fails if the initial mean(hits) is above 0.5. \n",
    "# Seems like when it starts below 0.5, the fastest way to increase mean(hits) is to push it to 0.5 and floor it there.\n",
    "# The results were collected in \"Results.mat\"\n",
    "# res = [res ; omhits mhits]\n",
    "\n",
    "[omhits mhits]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of successful minimization including self-connection weights\n",
    "\n",
    "Amazing. Adding self-connection weights seems to make the minimization process even more robust: we can now start from start_add = [-0.2, 0.2], which usually gives mean(hits) < 0.5, and **still** reach a successful solution.  Without the self-connection weights that rarely happened, mean(hits) < 0.5 was a bad start, and start_add = [-0.2, 0.2] most often did not lead to success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed = [-0.2, 0.2, 0.2, -1.5, 0.1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition JJ(Any) in module Main at In[47]:62 overwritten at In[48]:62.\n",
      "WARNING: Method definition #JJ(Array{Any, 1}, Main.#JJ, Any) in module Main overwritten.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- cost=0.0686324,   cost1=0.0687511, cost2=-0.00011865 :  mean(hits)=0.487796, mean(diffs)=0.0395499\n",
      "0: eta=1 ps=[-0.200, 0.200, 0.200, -1.500, 0.100]\n",
      "-- cost=0.0686324,   cost1=0.0687511, cost2=-0.00011865 :  mean(hits)=0.487796, mean(diffs)=0.0395499\n",
      "-- cost=0.0662608,   cost1=0.0663306, cost2=-6.98297e-05 :  mean(hits)=0.492453, mean(diffs)=0.0232766\n",
      "1: eta=1.1 cost=0.0662608 jtype=Newton costheta=-0.343 ps=[-0.112938, 0.349709, 0.101748, -1.26536, 0.103245]\n",
      "-- cost=0.0656768,   cost1=0.0657292, cost2=-5.24826e-05 :  mean(hits)=0.493623, mean(diffs)=0.0174942\n",
      "2: eta=1.21 cost=0.0656768 jtype=Newton costheta=-0.188 ps=[0.0772162, 0.616916, 0.0618108, -0.989811, 0.103702]\n",
      "-- cost=0.0658205,   cost1=0.0658659, cost2=-4.54283e-05 :  mean(hits)=0.493357, mean(diffs)=0.0151428\n",
      "eta going down: new_cost-cost=0.000143696 and jumptype='Newton'\n",
      "3: eta=0.605 cost=0.0656768 jtype=Newton costheta=NaN ps=[0.0772162, 0.616916, 0.0618108, -0.989811, 0.103702]\n",
      "-- cost=0.0658205,   cost1=0.0658659, cost2=-4.54283e-05 :  mean(hits)=0.493357, mean(diffs)=0.0151428\n",
      "eta going down: new_cost-cost=0.000143696 and jumptype='Newton'\n",
      "4: eta=0.3025 cost=0.0656768 jtype=Newton costheta=NaN ps=[0.0772162, 0.616916, 0.0618108, -0.989811, 0.103702]\n",
      "-- cost=0.0645488,   cost1=0.0646014, cost2=-5.25525e-05 :  mean(hits)=0.495832, mean(diffs)=0.0175175\n",
      "5: eta=0.33275 cost=0.0645488 jtype=constrained costheta=-0.719 ps=[0.274727, 0.41869, 0.0155098, -1.08603, 0.104242]\n",
      "-- cost=0.0614765,   cost1=0.0615494, cost2=-7.28586e-05 :  mean(hits)=0.501909, mean(diffs)=0.0242862\n",
      "6: eta=0.366025 cost=0.0614765 jtype=constrained costheta=-0.946 ps=[0.455695, 0.244242, 0.132879, -1.25726, 0.105988]\n",
      "-- cost=0.0486282,   cost1=0.0487824, cost2=-0.000154226 :  mean(hits)=0.529133, mean(diffs)=0.0514088\n",
      "7: eta=0.402628 cost=0.0486282 jtype=constrained costheta=-0.997 ps=[0.606886, 0.105449, 0.312739, -1.48064, 0.108562]\n",
      "-- cost=0.0133079,   cost1=0.0139258, cost2=-0.00061789 :  mean(hits)=0.631993, mean(diffs)=0.205963\n",
      "8: eta=0.44289 cost=0.0133079 jtype=constrained costheta=-1.000 ps=[0.755649, -0.026157, 0.504166, -1.74103, 0.111807]\n",
      "-- cost=0.000257413,   cost1=0.00210511, cost2=-0.00184769 :  mean(hits)=0.795881, mean(diffs)=0.615897\n",
      "9: eta=0.487179 cost=0.000257413 jtype=constrained costheta=-0.978 ps=[0.975464, -0.236981, 0.654068, -1.97877, 0.116327]\n",
      "-- cost=-0.00157669,   cost1=6.96102e-07, cost2=-0.00157738 :  mean(hits)=0.750834, mean(diffs)=0.525794\n",
      "10: eta=0.535897 cost=-0.00157669 jtype=Newton costheta=-0.587 ps=[1.02164, -0.0108635, 0.62631, -1.9228, 0.117996]\n",
      "-- cost=-0.00192434,   cost1=0.000676274, cost2=-0.00260062 :  mean(hits)=0.723995, mean(diffs)=0.866872\n",
      "11: eta=0.589487 cost=-0.00192434 jtype=constrained costheta=-0.796 ps=[0.821785, 0.247223, 0.818342, -2.23205, 0.123257]\n",
      "-- cost=-0.00271411,   cost1=6.39525e-05, cost2=-0.00277807 :  mean(hits)=0.742003, mean(diffs)=0.926022\n",
      "12: eta=0.648436 cost=-0.00271411 jtype=Newton costheta=-0.556 ps=[0.821891, 0.193543, 0.875512, -2.32724, 0.121511]\n",
      "-- cost=-0.00284202,   cost1=2.05373e-05, cost2=-0.00286256 :  mean(hits)=0.745468, mean(diffs)=0.954186\n",
      "13: eta=0.713279 cost=-0.00284202 jtype=Newton costheta=-0.248 ps=[0.72397, 0.0815275, 0.92699, -2.41553, 0.119098]\n",
      "-- cost=-0.00288506,   cost1=8.78345e-06, cost2=-0.00289385 :  mean(hits)=0.747036, mean(diffs)=0.964616\n",
      "14: eta=0.784607 cost=-0.00288506 jtype=Newton costheta=-0.210 ps=[0.603214, -0.0548918, 0.975406, -2.49512, 0.116614]\n",
      "-- cost=-0.00290096,   cost1=6.46895e-06, cost2=-0.00290743 :  mean(hits)=0.747457, mean(diffs)=0.969143\n",
      "15: eta=0.863068 cost=-0.00290096 jtype=Newton costheta=-0.147 ps=[0.375301, -0.293363, 1.03034, -2.58254, 0.1136]\n",
      "-- cost=-0.00291064,   cost1=1.10793e-05, cost2=-0.00292172 :  mean(hits)=0.746671, mean(diffs)=0.973907\n",
      "16: eta=0.949375 cost=-0.00291064 jtype=Newton costheta=-0.125 ps=[-0.101739, -0.779492, 1.10727, -2.71391, 0.109042]\n",
      "-- cost=-0.00209882,   cost1=0.000899802, cost2=-0.00299862 :  mean(hits)=0.779997, mean(diffs)=0.999541\n",
      "eta going down: new_cost-cost=0.000811819 and jumptype='constrained'\n",
      "17: eta=0.474687 cost=-0.00291064 jtype=constrained costheta=NaN ps=[-0.101739, -0.779492, 1.10727, -2.71391, 0.109042]\n",
      "-- cost=-0.00229665,   cost1=0.000648625, cost2=-0.00294527 :  mean(hits)=0.775468, mean(diffs)=0.981758\n",
      "eta going down: new_cost-cost=0.000613992 and jumptype='constrained'\n",
      "18: eta=0.237344 cost=-0.00291064 jtype=constrained costheta=NaN ps=[-0.101739, -0.779492, 1.10727, -2.71391, 0.109042]\n",
      "-- cost=-0.00286845,   cost1=4.55062e-05, cost2=-0.00291396 :  mean(hits)=0.756746, mean(diffs)=0.971319\n",
      "eta going down: new_cost-cost=4.21903e-05 and jumptype='constrained'\n",
      "19: eta=0.118672 cost=-0.00291064 jtype=constrained costheta=NaN ps=[-0.101739, -0.779492, 1.10727, -2.71391, 0.109042]\n",
      "-- cost=-0.00291069,   cost1=1.37804e-06, cost2=-0.00291207 :  mean(hits)=0.751174, mean(diffs)=0.97069\n",
      "20: eta=0.130539 cost=-0.00291069 jtype=constrained costheta=-0.090 ps=[-0.0962228, -0.781568, 1.15767, -2.78912, 0.110374]\n",
      "-- cost=-0.00292144,   cost1=7.15913e-06, cost2=-0.0029286 :  mean(hits)=0.747324, mean(diffs)=0.976199\n",
      "21: eta=0.143593 cost=-0.00292144 jtype=constrained costheta=-0.039 ps=[-0.0946473, -0.771467, 1.21975, -2.86746, 0.111792]\n",
      "-- cost=-0.00292227,   cost1=3.77572e-06, cost2=-0.00292604 :  mean(hits)=0.751943, mean(diffs)=0.975347\n",
      "22: eta=0.157952 cost=-0.00292227 jtype=constrained costheta=-0.035 ps=[-0.0923132, -0.770222, 1.27629, -2.95537, 0.11353]\n",
      "-- cost=-0.00292847,   cost1=9.24347e-06, cost2=-0.00293772 :  mean(hits)=0.74696, mean(diffs)=0.979239\n",
      "23: eta=0.173747 cost=-0.00292847 jtype=constrained costheta=-0.018 ps=[-0.0800473, -0.749842, 1.36128, -3.04012, 0.115212]\n",
      "-- cost=-0.00291325,   cost1=3.26152e-05, cost2=-0.00294587 :  mean(hits)=0.755711, mean(diffs)=0.981955\n",
      "eta going down: new_cost-cost=1.52223e-05 and jumptype='constrained'\n",
      "24: eta=0.0868737 cost=-0.00292847 jtype=constrained costheta=NaN ps=[-0.0800473, -0.749842, 1.36128, -3.04012, 0.115212]\n",
      "-- cost=-0.00292713,   cost1=9.25892e-06, cost2=-0.00293639 :  mean(hits)=0.753043, mean(diffs)=0.978795\n",
      "eta going down: new_cost-cost=1.34679e-06 and jumptype='constrained'\n",
      "25: eta=0.0434369 cost=-0.00292847 jtype=constrained costheta=NaN ps=[-0.0800473, -0.749842, 1.36128, -3.04012, 0.115212]\n",
      "-- cost=-0.00293064,   cost1=3.26848e-06, cost2=-0.00293391 :  mean(hits)=0.751808, mean(diffs)=0.97797\n",
      "26: eta=0.0477805 cost=-0.00293064 jtype=constrained costheta=-0.042 ps=[-0.0780774, -0.74942, 1.37648, -3.06588, 0.115771]\n",
      "-- cost=-0.00293452,   cost1=1.12552e-06, cost2=-0.00293565 :  mean(hits)=0.748939, mean(diffs)=0.97855\n",
      "27: eta=0.0525586 cost=-0.00293452 jtype=constrained costheta=-0.022 ps=[-0.0767747, -0.745522, 1.39617, -3.0931, 0.116353]\n",
      "-- cost=-0.00293575,   cost1=2.00775e-09, cost2=-0.00293575 :  mean(hits)=0.750045, mean(diffs)=0.978583\n",
      "28: eta=0.0578145 cost=-0.00293575 jtype=constrained costheta=-0.011 ps=[-0.0774677, -0.745455, 1.41562, -3.12338, 0.117064]\n",
      "-- cost=-0.00293654,   cost1=6.97725e-08, cost2=-0.00293661 :  mean(hits)=0.749736, mean(diffs)=0.978869\n",
      "29: eta=0.0635959 cost=-0.00293654 jtype=constrained costheta=-0.013 ps=[-0.0783256, -0.744805, 1.44056, -3.15536, 0.117805]\n",
      "-- cost=-0.00293718,   cost1=1.22753e-08, cost2=-0.00293719 :  mean(hits)=0.749889, mean(diffs)=0.979063\n",
      "30: eta=0.0699555 cost=-0.00293718 jtype=constrained costheta=-0.018 ps=[-0.0801053, -0.745193, 1.46612, -3.19054, 0.118677]\n",
      "-- cost=-0.00293768,   cost1=2.15906e-08, cost2=-0.0029377 :  mean(hits)=0.749853, mean(diffs)=0.979234\n",
      "31: eta=0.076951 cost=-0.00293768 jtype=constrained costheta=-0.096 ps=[-0.0821453, -0.745674, 1.49504, -3.22846, 0.119642]\n",
      "-- cost=-0.00293807,   cost1=1.7614e-08, cost2=-0.00293809 :  mean(hits)=0.749867, mean(diffs)=0.979363\n",
      "32: eta=0.0846461 cost=-0.00293807 jtype=constrained costheta=-0.009 ps=[-0.0852402, -0.747187, 1.52449, -3.26998, 0.120797]\n",
      "-- cost=-0.00293835,   cost1=2.79069e-08, cost2=-0.00293838 :  mean(hits)=0.749833, mean(diffs)=0.979461\n",
      "33: eta=0.0931108 cost=-0.00293835 jtype=constrained costheta=-0.005 ps=[-0.0894229, -0.749758, 1.55398, -3.31527, 0.122236]\n",
      "-- cost=-0.00293856,   cost1=3.56531e-08, cost2=-0.00293859 :  mean(hits)=0.749811, mean(diffs)=0.979531\n",
      "34: eta=0.102422 cost=-0.00293856 jtype=constrained costheta=-0.002 ps=[-0.0946094, -0.75342, 1.57897, -3.36357, 0.125003]\n",
      "-- cost=-0.00293869,   cost1=4.14224e-08, cost2=-0.00293873 :  mean(hits)=0.749796, mean(diffs)=0.979576\n",
      "35: eta=0.112664 cost=-0.00293869 jtype=Newton costheta=-0.001 ps=[-0.0998685, -0.757333, 1.60175, -3.41426, 0.121983]\n",
      "-- cost=-0.00293878,   cost1=8.36299e-09, cost2=-0.00293879 :  mean(hits)=0.749909, mean(diffs)=0.979596\n",
      "36: eta=0.12393 cost=-0.00293878 jtype=Newton costheta=-0.001 ps=[-0.103349, -0.759929, 1.61709, -3.44911, 0.119844]\n",
      "-- cost=-0.00293881,   cost1=8.37884e-08, cost2=-0.00293889 :  mean(hits)=0.749711, mean(diffs)=0.97963\n",
      "37: eta=0.136323 cost=-0.00293881 jtype=Newton costheta=-0.001 ps=[-0.108362, -0.763726, 1.63877, -3.49622, 0.116854]\n",
      "-- cost=-0.00293888,   cost1=4.20086e-12, cost2=-0.00293888 :  mean(hits)=0.749998, mean(diffs)=0.979628\n",
      "38: eta=0.149956 cost=-0.00293888 jtype=Newton costheta=-0.001 ps=[-0.109818, -0.764804, 1.64566, -3.51266, 0.115794]\n",
      "-- cost=-0.0029377,   cost1=1.92904e-06, cost2=-0.00293963 :  mean(hits)=0.748611, mean(diffs)=0.979877\n",
      "eta going down: new_cost-cost=1.18179e-06 and jumptype='constrained'\n",
      "39: eta=0.0749779 cost=-0.00293888 jtype=constrained costheta=NaN ps=[-0.109818, -0.764804, 1.64566, -3.51266, 0.115794]\n",
      "-- cost=-0.00293887,   cost1=1.04061e-07, cost2=-0.00293897 :  mean(hits)=0.749677, mean(diffs)=0.979657\n",
      "eta going down: new_cost-cost=1.78223e-08 and jumptype='constrained'\n",
      "40: eta=0.037489 cost=-0.00293888 jtype=constrained costheta=NaN ps=[-0.109818, -0.764804, 1.64566, -3.51266, 0.115794]\n",
      "-- cost=-0.00293891,   cost1=6.13699e-09, cost2=-0.00293891 :  mean(hits)=0.749922, mean(diffs)=0.979638\n",
      "41: eta=0.0412378 cost=-0.00293891 jtype=constrained costheta=-0.029 ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00209909,   cost1=0.000899869, cost2=-0.00299895 :  mean(hits)=0.779998, mean(diffs)=0.999652\n",
      "eta going down: new_cost-cost=0.000839821 and jumptype='constrained'\n",
      "42: eta=0.0206189 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00209908,   cost1=0.000899865, cost2=-0.00299894 :  mean(hits)=0.779998, mean(diffs)=0.999648\n",
      "eta going down: new_cost-cost=0.000839828 and jumptype='constrained'\n",
      "43: eta=0.0103095 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00209904,   cost1=0.000899838, cost2=-0.00299888 :  mean(hits)=0.779997, mean(diffs)=0.999625\n",
      "eta going down: new_cost-cost=0.000839869 and jumptype='constrained'\n",
      "44: eta=0.00515473 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00228001,   cost1=0.000672223, cost2=-0.00295223 :  mean(hits)=0.775927, mean(diffs)=0.984078\n",
      "eta going down: new_cost-cost=0.000658897 and jumptype='constrained'\n",
      "45: eta=0.00257737 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.0028976,   cost1=0.000100016, cost2=-0.00299761 :  mean(hits)=0.760001, mean(diffs)=0.999204\n",
      "eta going down: new_cost-cost=4.13094e-05 and jumptype='constrained'\n",
      "46: eta=0.00128868 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00289594,   cost1=9.69889e-05, cost2=-0.00299293 :  mean(hits)=0.759848, mean(diffs)=0.997645\n",
      "eta going down: new_cost-cost=4.29624e-05 and jumptype='constrained'\n",
      "47: eta=0.000644341 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00289846,   cost1=7.42642e-05, cost2=-0.00297273 :  mean(hits)=0.758618, mean(diffs)=0.990909\n",
      "eta going down: new_cost-cost=4.0444e-05 and jumptype='constrained'\n",
      "48: eta=0.000322171 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00291812,   cost1=3.3319e-05, cost2=-0.00295144 :  mean(hits)=0.755772, mean(diffs)=0.983815\n",
      "eta going down: new_cost-cost=2.07823e-05 and jumptype='constrained'\n",
      "49: eta=0.000161085 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00293241,   cost1=9.96215e-06, cost2=-0.00294237 :  mean(hits)=0.753156, mean(diffs)=0.980791\n",
      "eta going down: new_cost-cost=6.49675e-06 and jumptype='constrained'\n",
      "50: eta=8.05427e-05 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00293726,   cost1=2.5067e-06, cost2=-0.00293976 :  mean(hits)=0.751583, mean(diffs)=0.979921\n",
      "eta going down: new_cost-cost=1.65185e-06 and jumptype='constrained'\n",
      "51: eta=4.02713e-05 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00293853,   cost1=5.74493e-07, cost2=-0.0029391 :  mean(hits)=0.750758, mean(diffs)=0.979702\n",
      "eta going down: new_cost-cost=3.76922e-07 and jumptype='constrained'\n",
      "52: eta=2.01357e-05 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.00293883,   cost1=1.15893e-07, cost2=-0.00293895 :  mean(hits)=0.75034, mean(diffs)=0.97965\n",
      "eta going down: new_cost-cost=7.3048e-08 and jumptype='constrained'\n",
      "53: eta=1.00678e-05 cost=-0.00293891 jtype=constrained costheta=NaN ps=[-0.11125, -0.765585, 1.66123, -3.53016, 0.116367]\n",
      "-- cost=-0.0029389,   cost1=1.71889e-08, cost2=-0.00293892 :  mean(hits)=0.750131, mean(diffs)=0.979639\n",
      "About to break -- tol=1e-08, new_cost-cost=7.49351e-09, eta=1.00678e-05\n",
      "-- cost=-0.00293891,   cost1=6.13699e-09, cost2=-0.00293891 :  mean(hits)=0.749922, mean(diffs)=0.979638\n",
      "-- cost=-0.00293891,   cost1=6.13699e-09, cost2=-0.00293891 :  mean(hits)=0.749922, mean(diffs)=0.979638\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1×2 Array{Float64,2}:\n",
       " 0.487796  0.749922"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following sequence leads to a situation where having only [-0.8, -0.8] as the single finalFluxPoint \n",
    "# leads to the minimization getting stuck.  Adding further finalFluxPoints solves the problem\n",
    "\n",
    "dt = 0.02\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "mW = -4\n",
    "sW = 0.2\n",
    "noise = 0\n",
    "input = 0\n",
    "sigma = 0.1\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :sW=>sW, :mW=>mW, :W=>[sW mW ; mW sW], :nsteps=>nsteps, \n",
    ":noise=>noise, :input=>input, :sigma=>sigma, :const_add=>0, :init_add=>0)\n",
    "\n",
    "\n",
    "# WORKING gradient:\n",
    "# ForwardDiff.gradient((x)->JJ(startU; do_plot=true, nderivs=length(x), difforder=1, \n",
    "#    make_dict([[\"init_add\" 2], \"const_add\"], x, model_params)...), [2.9, -2.9, 0.1])\n",
    "\n",
    "\n",
    "\n",
    "# The backward and costfunc functions should turn a single-scalar parameter W into the matrix W\n",
    "# backward always runs with no within-forward noise, i.e., sigma=0\n",
    "backward = (endpoint; do_plot=false, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if ~haskey(pars, :sW) || ~haskey(pars, :mW) \n",
    "        error(\"Need both sW and mW to determine weight matrix\")\n",
    "    end\n",
    "    pars=make_dict([\"W\"], [[pars[:sW] pars[:mW] ; pars[:mW] pars[:sW]]], pars);\n",
    "    backwardsModel(endpoint; do_plot=do_plot, make_dict([\"sigma\"], [0], pars)...)[1]\n",
    "end\n",
    "\n",
    "forward = (startpoint; pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if ~haskey(pars, :sW) || ~haskey(pars, :mW) \n",
    "        error(\"Need both sW and mW to determine weight matrix\")\n",
    "    end\n",
    "    pars=make_dict([\"W\"], [[pars[:sW] pars[:mW] ; pars[:mW] pars[:sW]]], pars);\n",
    "    forwardsModel(startpoint; pars...)\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "costfunc = (startpoints; do_plot=false, verbose=false, beta=0.003, nderivs=0, difforder=0, sr=26, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if ~haskey(pars, :sW) || ~haskey(pars, :mW) \n",
    "        error(\"Need both sW and mW to determine weight matrix\")\n",
    "    end\n",
    "    pars=make_dict([\"W\"], [[pars[:sW] pars[:mW] ; pars[:mW] pars[:sW]]], pars);\n",
    "    JJ(startpoints; seedrand=sr, beta=beta, \n",
    "        do_plot=do_plot, verbose=verbose, nderivs=nderivs, difforder=difforder, pars...)\n",
    "end\n",
    "\n",
    "function JJ(initUs; theta1=0.15, theta2=0.2, beta=0.003, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, pre_string=\"\", zero_last_sigmas=0, seedrand=NaN, params...)\n",
    "\n",
    "    if ~isnan(seedrand); srand(seedrand); end\n",
    "    \n",
    "    Vend = ForwardDiffZeros(size(initUs,1), size(initUs,2), nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:size(initUs,1)\n",
    "        if false # i>size(initUs,1) - zero_last_sigmas\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; sigma=0, nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)            \n",
    "        else\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)\n",
    "        end\n",
    "        Vend[i,:] = Ve\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend[:,1]-Vend[:,2])/theta1))\n",
    "    diffs = tanh.((Vend[:,1]-Vend[:,2])/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "\n",
    "    if do_plot\n",
    "        title(@sprintf(\"mean(hits)=%g, mean(diffs)=%g\", convert(Float64, mean(hits)), convert(Float64, mean(diffs))))\n",
    "    end\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"%s\", pre_string)\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2, mean(hits), mean(diffs)\n",
    "end\n",
    "  \n",
    "\n",
    "\n",
    "beta = 0.003\n",
    "args = [[\"start_add\" 2], \"const_add\", \"mW\", \"sW\"] \n",
    "seed = [-0.2, 0.2, 0.2, -1.5, 0.1] \n",
    "walls = Dict(:start_add=>[-5.1, 5.1], :sW=>[0, 5.1], :mW=>[-5.1, 5.1]) # \n",
    "# sr =  gives \n",
    "\n",
    "\n",
    "new_random_seed = true; if new_random_seed\n",
    "    sr = convert(Int64, round(time()))\n",
    "else\n",
    "    sr = old_sr\n",
    "end\n",
    "# sr = 1504716566\n",
    "old_sr = sr\n",
    "\n",
    "srand(sr)\n",
    "\n",
    "startU=randn(50,2)-3\n",
    "\n",
    "\n",
    "clf()\n",
    "print(\"seed = \"); print_vector_g(seed); print(\"\\n\")\n",
    "ocost, omhits, omdiffs = costfunc(startU; do_plot=true, sr=sr, verbose=true, make_dict(args, seed, model_params)...)\n",
    "\n",
    "\n",
    "params, traj, zz, cpm_traj = bbox_Hessian_keyword_minimization(seed, args, walls,  \n",
    "(;params...) -> costfunc(startU; beta=beta, sr=sr, do_plot=true, verbose=true, merge(model_params, Dict(params))...)[1], \n",
    "verbose=true, start_eta=1, tol=1e-8, softbox=true, maxiter=400 )\n",
    "\n",
    "# params, cost, ptraj, gtraj = fluxSense(costfunc, backward, model_params, startU, fluxFinalPoint, args, seed; \n",
    "#    start_eta=0.01, tol=1e-15, maxiter=400, verbose=true, report_every=1, do_plot=false, cost_limit=cost_limit) # cost_limit=-0.000935) # for beta=0.01\n",
    "\n",
    "# And show the final position\n",
    "figure(1); clf()\n",
    "cost, mhits, mdiffs = \n",
    "    costfunc(startU; beta=beta, do_plot=true, sr=sr, verbose=true, make_dict(args, params, model_params)...)\n",
    "\n",
    "repeat_results_in_fig2 = true; if repeat_results_in_fig2\n",
    "    figure(2); clf()\n",
    "    costfunc(startU; beta=beta, do_plot=true, sr=sr, verbose=true, \n",
    "        make_dict(args, params, merge(Dict(:fignum=>2), model_params))...)\n",
    "    figure(1); \n",
    "end\n",
    "params'\n",
    "\n",
    "# For beta=0, and ntrials=20, we collected a bunch of results and observed that it failes about half the time\n",
    "# WHEN the initial mean)hits) is below 0.5.  It never fails if the initial mean(hits) is above 0.5. \n",
    "# Seems like when it starts below 0.5, the fastest way to increase mean(hits) is to push it to 0.5 and floor it there.\n",
    "# The results were collected in \"Results.mat\"\n",
    "# res = [res ; omhits mhits]\n",
    "\n",
    "[omhits mhits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1504716566"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_sr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example that works about half the time-- seesm to follow the patter where initial mean(hits) < 0.5 means half the time go to mean(hits)=0.5 and get stuck there, whereas initial mean(hits)>=0.5 means success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed = [-0.2, 0.2, 0.2, -1.5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition JJ(Any) in module Main at In[17]:61 overwritten at In[19]:61.\n",
      "WARNING: Method definition #JJ(Array{Any, 1}, Main.#JJ, Any) in module Main overwritten.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- cost=0.0618702,   cost1=0.0619307, cost2=-6.04596e-05 :  mean(hits)=0.501141, mean(diffs)=0.0201532\n",
      "0: eta=1 ps=[-0.200, 0.200, 0.200, -1.500]\n",
      "-- cost=0.0618702,   cost1=0.0619307, cost2=-6.04596e-05 :  mean(hits)=0.501141, mean(diffs)=0.0201532\n",
      "-- cost=0.0609813,   cost1=0.0610149, cost2=-3.36142e-05 :  mean(hits)=0.502988, mean(diffs)=0.0112047\n",
      "1: eta=1.1 cost=0.0609813 jtype=Newton costheta=-0.368 ps=[-0.187942, 0.181484, -0.0195354, -1.39132]\n",
      "-- cost=0.0613172,   cost1=0.0613271, cost2=-9.90029e-06 :  mean(hits)=0.502357, mean(diffs)=0.0033001\n",
      "eta going down: new_cost-cost=0.000335981 and jumptype='Newton'\n",
      "2: eta=0.55 cost=0.0609813 jtype=Newton costheta=NaN ps=[-0.187942, 0.181484, -0.0195354, -1.39132]\n",
      "-- cost=0.0533101,   cost1=0.0533805, cost2=-7.04243e-05 :  mean(hits)=0.518958, mean(diffs)=0.0234748\n",
      "3: eta=0.605 cost=0.0533101 jtype=constrained costheta=-0.697 ps=[0.148346, -0.140224, 0.230154, -1.53175]\n",
      "-- cost=0.00648721,   cost1=0.00724268, cost2=-0.000755464 :  mean(hits)=0.664896, mean(diffs)=0.251821\n",
      "4: eta=0.6655 cost=0.00648721 jtype=constrained costheta=-0.996 ps=[0.353552, -0.315945, 0.585897, -1.89314]\n",
      "-- cost=-0.00125426,   cost1=0.000230058, cost2=-0.00148431 :  mean(hits)=0.765168, mean(diffs)=0.494771\n",
      "5: eta=0.73205 cost=-0.00125426 jtype=Newton costheta=-0.778 ps=[0.398471, -0.569932, 0.661777, -2.02273]\n",
      "-- cost=-0.00110447,   cost1=1.37151e-05, cost2=-0.00111819 :  mean(hits)=0.746297, mean(diffs)=0.372729\n",
      "eta going down: new_cost-cost=0.000149782 and jumptype='Newton'\n",
      "6: eta=0.366025 cost=-0.00125426 jtype=Newton costheta=NaN ps=[0.398471, -0.569932, 0.661777, -2.02273]\n",
      "-- cost=-0.00110447,   cost1=1.37151e-05, cost2=-0.00111819 :  mean(hits)=0.746297, mean(diffs)=0.372729\n",
      "eta going down: new_cost-cost=0.000149782 and jumptype='Newton'\n",
      "7: eta=0.183013 cost=-0.00125426 jtype=Newton costheta=NaN ps=[0.398471, -0.569932, 0.661777, -2.02273]\n",
      "-- cost=-0.00110447,   cost1=1.37151e-05, cost2=-0.00111819 :  mean(hits)=0.746297, mean(diffs)=0.372729\n",
      "eta going down: new_cost-cost=0.000149782 and jumptype='Newton'\n",
      "8: eta=0.0915063 cost=-0.00125426 jtype=Newton costheta=NaN ps=[0.398471, -0.569932, 0.661777, -2.02273]\n",
      "-- cost=-0.00154688,   cost1=4.1367e-05, cost2=-0.00158824 :  mean(hits)=0.756432, mean(diffs)=0.529414\n",
      "9: eta=0.100657 cost=-0.00154688 jtype=constrained costheta=-0.500 ps=[0.339325, -0.509753, 0.68099, -2.04598]\n",
      "-- cost=-0.001798,   cost1=1.4203e-05, cost2=-0.0018122 :  mean(hits)=0.753769, mean(diffs)=0.604068\n",
      "10: eta=0.110723 cost=-0.001798 jtype=constrained costheta=-0.954 ps=[0.286125, -0.453558, 0.715256, -2.09091]\n",
      "-- cost=-0.00207619,   cost1=2.66475e-06, cost2=-0.00207885 :  mean(hits)=0.751632, mean(diffs)=0.692952\n",
      "11: eta=0.121795 cost=-0.00207619 jtype=constrained costheta=-0.974 ps=[0.238178, -0.401162, 0.759385, -2.15064]\n",
      "-- cost=-0.00233075,   cost1=1.32111e-06, cost2=-0.00233207 :  mean(hits)=0.748851, mean(diffs)=0.777358\n",
      "12: eta=0.133974 cost=-0.00233075 jtype=constrained costheta=-0.922 ps=[0.200156, -0.35673, 0.813352, -2.22575]\n",
      "-- cost=-0.0025274,   cost1=6.29147e-06, cost2=-0.00253369 :  mean(hits)=0.747492, mean(diffs)=0.844564\n",
      "13: eta=0.147372 cost=-0.0025274 jtype=constrained costheta=-0.763 ps=[0.182843, -0.328748, 0.875797, -2.317]\n",
      "-- cost=-0.00267248,   cost1=2.55232e-06, cost2=-0.00267503 :  mean(hits)=0.748402, mean(diffs)=0.891678\n",
      "14: eta=0.162109 cost=-0.00267248 jtype=constrained costheta=-0.513 ps=[0.187217, -0.313812, 0.943893, -2.41878]\n",
      "-- cost=-0.00276642,   cost1=1.39128e-06, cost2=-0.00276781 :  mean(hits)=0.74882, mean(diffs)=0.922605\n",
      "15: eta=0.17832 cost=-0.00276642 jtype=constrained costheta=-0.414 ps=[0.199794, -0.303395, 1.02039, -2.52736]\n",
      "-- cost=-0.00282544,   cost1=9.90464e-09, cost2=-0.00282545 :  mean(hits)=0.7501, mean(diffs)=0.941817\n",
      "16: eta=0.196152 cost=-0.00282544 jtype=constrained costheta=-0.285 ps=[0.22659, -0.285909, 1.10701, -2.64072]\n",
      "-- cost=-0.00289147,   cost1=1.62338e-07, cost2=-0.00289163 :  mean(hits)=0.749597, mean(diffs)=0.963876\n",
      "17: eta=0.215767 cost=-0.00289147 jtype=constrained costheta=-0.790 ps=[0.249154, -0.27032, 1.20422, -2.76167]\n",
      "-- cost=-0.00293126,   cost1=1.47027e-07, cost2=-0.00293141 :  mean(hits)=0.750383, mean(diffs)=0.977137\n",
      "18: eta=0.237344 cost=-0.00293126 jtype=constrained costheta=-0.883 ps=[0.269609, -0.257936, 1.29073, -2.89745]\n",
      "-- cost=-0.00293542,   cost1=1.51272e-08, cost2=-0.00293543 :  mean(hits)=0.749877, mean(diffs)=0.978477\n",
      "19: eta=0.261078 cost=-0.00293542 jtype=Newton costheta=-0.069 ps=[0.3187, -0.211175, 1.31508, -2.9546]\n",
      "-- cost=-0.00293701,   cost1=6.57577e-09, cost2=-0.00293701 :  mean(hits)=0.750081, mean(diffs)=0.979005\n",
      "20: eta=0.287186 cost=-0.00293701 jtype=Newton costheta=-0.076 ps=[0.260131, -0.271745, 1.35753, -3.02038]\n",
      "-- cost=-0.00293771,   cost1=6.19265e-09, cost2=-0.00293772 :  mean(hits)=0.750079, mean(diffs)=0.979239\n",
      "21: eta=0.315904 cost=-0.00293771 jtype=Newton costheta=-0.030 ps=[0.325472, -0.209354, 1.39055, -3.08301]\n",
      "-- cost=-0.00293802,   cost1=2.11415e-07, cost2=-0.00293823 :  mean(hits)=0.75046, mean(diffs)=0.979411\n",
      "22: eta=0.347495 cost=-0.00293802 jtype=Newton costheta=-0.013 ps=[0.371607, -0.166556, 1.43658, -3.16823]\n",
      "-- cost=-0.00293832,   cost1=1.74097e-09, cost2=-0.00293832 :  mean(hits)=0.749958, mean(diffs)=0.979442\n",
      "23: eta=0.382244 cost=-0.00293832 jtype=Newton costheta=-0.002 ps=[0.383666, -0.155926, 1.45557, -3.22119]\n",
      "-- cost=-0.0029001,   cost1=6.93007e-05, cost2=-0.0029694 :  mean(hits)=0.758325, mean(diffs)=0.989799\n",
      "eta going down: new_cost-cost=3.82272e-05 and jumptype='constrained'\n",
      "24: eta=0.191122 cost=-0.00293832 jtype=constrained costheta=NaN ps=[0.383666, -0.155926, 1.45557, -3.22119]\n",
      "-- cost=-0.00293568,   cost1=4.40131e-06, cost2=-0.00294008 :  mean(hits)=0.752098, mean(diffs)=0.980028\n",
      "eta going down: new_cost-cost=2.64123e-06 and jumptype='constrained'\n",
      "25: eta=0.0955611 cost=-0.00293832 jtype=constrained costheta=NaN ps=[0.383666, -0.155926, 1.45557, -3.22119]\n",
      "-- cost=-0.00293834,   cost1=1.90498e-07, cost2=-0.00293853 :  mean(hits)=0.750436, mean(diffs)=0.979511\n",
      "26: eta=0.105117 cost=-0.00293834 jtype=constrained costheta=-0.005 ps=[0.38932, -0.152042, 1.51063, -3.2675]\n",
      "-- cost=-0.00293853,   cost1=1.13595e-09, cost2=-0.00293853 :  mean(hits)=0.749966, mean(diffs)=0.979509\n",
      "27: eta=0.115629 cost=-0.00293853 jtype=Newton costheta=-0.002 ps=[0.395298, -0.146756, 1.52057, -3.29715]\n",
      "-- cost=-0.00293821,   cost1=6.58622e-07, cost2=-0.00293887 :  mean(hits)=0.750812, mean(diffs)=0.979623\n",
      "eta going down: new_cost-cost=3.15695e-07 and jumptype='constrained'\n",
      "28: eta=0.0578145 cost=-0.00293853 jtype=constrained costheta=NaN ps=[0.395298, -0.146756, 1.52057, -3.29715]\n",
      "-- cost=-0.00293857,   cost1=3.24747e-08, cost2=-0.0029386 :  mean(hits)=0.75018, mean(diffs)=0.979534\n",
      "29: eta=0.0635959 cost=-0.00293857 jtype=constrained costheta=-0.003 ps=[0.39818, -0.144806, 1.55165, -3.32528]\n",
      "-- cost=-0.00293864,   cost1=6.45669e-09, cost2=-0.00293865 :  mean(hits)=0.75008, mean(diffs)=0.979549\n",
      "30: eta=0.0699555 cost=-0.00293864 jtype=Newton costheta=-0.001 ps=[0.405046, -0.138704, 1.56327, -3.35645]\n",
      "-- cost=-0.00293867,   cost1=4.14055e-08, cost2=-0.00293872 :  mean(hits)=0.750203, mean(diffs)=0.979572\n",
      "31: eta=0.076951 cost=-0.00293867 jtype=constrained costheta=-0.001 ps=[0.415647, -0.129059, 1.58082, -3.39364]\n",
      "-- cost=-0.00293873,   cost1=7.62131e-10, cost2=-0.00293873 :  mean(hits)=0.750028, mean(diffs)=0.979576\n",
      "32: eta=0.0846461 cost=-0.00293873 jtype=Newton costheta=-0.001 ps=[0.419668, -0.12547, 1.58841, -3.41297]\n",
      "-- cost=-0.00209856,   cost1=0.000900132, cost2=-0.0029987 :  mean(hits)=0.719998, mean(diffs)=0.999565\n",
      "eta going down: new_cost-cost=0.000840165 and jumptype='constrained'\n",
      "33: eta=0.0423231 cost=-0.00293873 jtype=constrained costheta=NaN ps=[0.419668, -0.12547, 1.58841, -3.41297]\n",
      "-- cost=-0.00293876,   cost1=6.45254e-09, cost2=-0.00293876 :  mean(hits)=0.75008, mean(diffs)=0.979587\n",
      "34: eta=0.0465554 cost=-0.00293876 jtype=constrained costheta=-0.002 ps=[0.414927, -0.130702, 1.60587, -3.43379]\n",
      "-- cost=-0.00289863,   cost1=0.000100054, cost2=-0.00299869 :  mean(hits)=0.739997, mean(diffs)=0.999562\n",
      "eta going down: new_cost-cost=4.01247e-05 and jumptype='constrained'\n",
      "35: eta=0.0232777 cost=-0.00293876 jtype=constrained costheta=NaN ps=[0.414927, -0.130702, 1.60587, -3.43379]\n",
      "-- cost=-0.00289868,   cost1=0.000100047, cost2=-0.00299872 :  mean(hits)=0.739998, mean(diffs)=0.999575\n",
      "eta going down: new_cost-cost=4.00782e-05 and jumptype='constrained'\n",
      "36: eta=0.0116388 cost=-0.00293876 jtype=constrained costheta=NaN ps=[0.414927, -0.130702, 1.60587, -3.43379]\n",
      "-- cost=-0.00289866,   cost1=0.000100045, cost2=-0.00299871 :  mean(hits)=0.739998, mean(diffs)=0.99957\n",
      "eta going down: new_cost-cost=4.00913e-05 and jumptype='constrained'\n",
      "37: eta=0.00581942 cost=-0.00293876 jtype=constrained costheta=NaN ps=[0.414927, -0.130702, 1.60587, -3.43379]\n",
      "-- cost=-0.00289858,   cost1=0.000100029, cost2=-0.00299861 :  mean(hits)=0.739999, mean(diffs)=0.999538\n",
      "eta going down: new_cost-cost=4.01714e-05 and jumptype='constrained'\n",
      "38: eta=0.00290971 cost=-0.00293876 jtype=constrained costheta=NaN ps=[0.414927, -0.130702, 1.60587, -3.43379]\n",
      "-- cost=-0.00289792,   cost1=9.97401e-05, cost2=-0.00299766 :  mean(hits)=0.740013, mean(diffs)=0.999219\n",
      "eta going down: new_cost-cost=4.08394e-05 and jumptype='constrained'\n",
      "39: eta=0.00145486 cost=-0.00293876 jtype=constrained costheta=NaN ps=[0.414927, -0.130702, 1.60587, -3.43379]\n",
      "-- cost=-0.00289509,   cost1=9.42936e-05, cost2=-0.00298938 :  mean(hits)=0.74029, mean(diffs)=0.996461\n",
      "eta going down: new_cost-cost=4.36676e-05 and jumptype='constrained'\n",
      "40: eta=0.000727428 cost=-0.00293876 jtype=constrained costheta=NaN ps=[0.414927, -0.130702, 1.60587, -3.43379]\n",
      "-- cost=-0.00290214,   cost1=6.428e-05, cost2=-0.00296642 :  mean(hits)=0.741983, mean(diffs)=0.988805\n",
      "eta going down: new_cost-cost=3.66203e-05 and jumptype='constrained'\n",
      "41: eta=0.000363714 cost=-0.00293876 jtype=constrained costheta=NaN ps=[0.414927, -0.130702, 1.60587, -3.43379]\n",
      "-- cost=-0.00292264,   cost1=2.53979e-05, cost2=-0.00294804 :  mean(hits)=0.74496, mean(diffs)=0.98268\n",
      "eta going down: new_cost-cost=1.61123e-05 and jumptype='constrained'\n",
      "42: eta=0.000181857 cost=-0.00293876 jtype=constrained costheta=NaN ps=[0.414927, -0.130702, 1.60587, -3.43379]\n",
      "-- cost=-0.00293407,   cost1=7.15022e-06, cost2=-0.00294122 :  mean(hits)=0.747326, mean(diffs)=0.980407\n",
      "eta going down: new_cost-cost=4.68551e-06 and jumptype='constrained'\n",
      "43: eta=9.09285e-05 cost=-0.00293876 jtype=constrained costheta=NaN ps=[0.414927, -0.130702, 1.60587, -3.43379]\n",
      "-- cost=-0.0029376,   cost1=1.75042e-06, cost2=-0.00293935 :  mean(hits)=0.748677, mean(diffs)=0.979784\n",
      "eta going down: new_cost-cost=1.15469e-06 and jumptype='constrained'\n",
      "44: eta=4.54642e-05 cost=-0.00293876 jtype=constrained costheta=NaN ps=[0.414927, -0.130702, 1.60587, -3.43379]\n",
      "-- cost=-0.0029385,   cost1=3.89949e-07, cost2=-0.00293889 :  mean(hits)=0.749376, mean(diffs)=0.97963\n",
      "eta going down: new_cost-cost=2.55089e-07 and jumptype='constrained'\n",
      "45: eta=2.27321e-05 cost=-0.00293876 jtype=constrained costheta=NaN ps=[0.414927, -0.130702, 1.60587, -3.43379]\n",
      "-- cost=-0.00293876,   cost1=1.51692e-12, cost2=-0.00293876 :  mean(hits)=0.750001, mean(diffs)=0.979587\n",
      "About to break -- tol=1e-08, new_cost-cost=-4.1584e-09, eta=2.27321e-05\n",
      "-- cost=-0.00293876,   cost1=6.45254e-09, cost2=-0.00293876 :  mean(hits)=0.75008, mean(diffs)=0.979587\n",
      "-- cost=-0.00293876,   cost1=6.45254e-09, cost2=-0.00293876 :  mean(hits)=0.75008, mean(diffs)=0.979587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1×2 Array{Float64,2}:\n",
       " 0.501141  0.75008"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following sequence leads to a situation where having only [-0.8, -0.8] as the single finalFluxPoint \n",
    "# leads to the minimization getting stuck.  Adding further finalFluxPoints solves the problem\n",
    "\n",
    "dt = 0.02\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "W = -4\n",
    "noise = 0\n",
    "input = 0\n",
    "sigma = 0.1\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    ":noise=>noise, :input=>input, :sigma=>sigma, :const_add=>0, :init_add=>0)\n",
    "\n",
    "\n",
    "# WORKING gradient:\n",
    "# ForwardDiff.gradient((x)->JJ(startU; do_plot=true, nderivs=length(x), difforder=1, \n",
    "#    make_dict([[\"init_add\" 2], \"const_add\"], x, model_params)...), [2.9, -2.9, 0.1])\n",
    "\n",
    "\n",
    "\n",
    "# The backward and costfunc functions should turn a single-scalar parameter W into the matrix W\n",
    "# backward always runs with no within-forward noise, i.e., sigma=0\n",
    "backward = (endpoint; do_plot=false, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;     \n",
    "    backwardsModel(endpoint; do_plot=do_plot, make_dict([\"sigma\"], [0], pars)...)[1]\n",
    "end\n",
    "\n",
    "forward = (startpoint; pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;     \n",
    "    forwardsModel(startpoint; pars...)\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "costfunc = (startpoints; do_plot=false, verbose=false, beta=0.003, nderivs=0, difforder=0, sr=26, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;         \n",
    "    JJ(startpoints; seedrand=sr, beta=beta, \n",
    "        do_plot=do_plot, verbose=verbose, nderivs=nderivs, difforder=difforder, pars...)\n",
    "end\n",
    "\n",
    "function JJ(initUs; theta1=0.15, theta2=0.2, beta=0.003, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, pre_string=\"\", zero_last_sigmas=0, seedrand=NaN, params...)\n",
    "\n",
    "    if ~isnan(seedrand); srand(seedrand); end\n",
    "    \n",
    "    Vend = ForwardDiffZeros(size(initUs,1), size(initUs,2), nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:size(initUs,1)\n",
    "        if false # i>size(initUs,1) - zero_last_sigmas\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; sigma=0, nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)            \n",
    "        else\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)\n",
    "        end\n",
    "        Vend[i,:] = Ve\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend[:,1]-Vend[:,2])/theta1))\n",
    "    diffs = tanh.((Vend[:,1]-Vend[:,2])/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "\n",
    "    if do_plot\n",
    "        title(@sprintf(\"mean(hits)=%g, mean(diffs)=%g\", convert(Float64, mean(hits)), convert(Float64, mean(diffs))))\n",
    "    end\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"%s\", pre_string)\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2, mean(hits), mean(diffs)\n",
    "end\n",
    "  \n",
    "\n",
    "\n",
    "fluxFinalPoint = [-0.8 -0.8; -0.6 -0.6 ; -0.4 -0.4; -0.2 -0.2; 0 0; 0.2 0.2]\n",
    "fluxFinalPoint = zeros(0,2);\n",
    "\n",
    "\n",
    "beta = 0.05\n",
    "args = [[\"start_add\" 2], \"const_add\", \"W\", \"sigma\"]\n",
    "seed = [0.1, 0.1, 2.1, -1, 0.1]\n",
    "walls = Dict(:start_add=>[-5.1, 5.1], :W=>[-5.1, 5.1], :sigma=>[-0.5, 0.5])\n",
    "# sr = 1504432803 causes a total mess with everything around the decision boundary; \n",
    "# sr = 1504432962 gets stuck at mean(hits)=0.66 but if we reduce the bounds of sigma, reaches 0.74\n",
    "\n",
    "\n",
    "beta = 0.0000001\n",
    "args = [[\"start_add\" 2], \"const_add\", \"W\"] \n",
    "seed = [0.1, 0.1, 2.1, -1] \n",
    "# This seed proves deadly, and always starts with mean(hits)<0.5:  seed = [-0.1, 0.1, 2.1, -1] \n",
    "walls = Dict(:start_add=>[-5.1, 5.1], :W=>[-5.1, 5.1]) # \n",
    "# sr = 1504433892 gives a mean(hits)=0.5 mess\n",
    "# sr = 1504433983 gives a mean(hits)=0.5 mess\n",
    "# sr = 1504434067 gives a mean(hits)=0.5 mess\n",
    "# sr = 1504434114 gives a mean(hits)=0.5 mess\n",
    "\n",
    "\n",
    "beta = 0.003\n",
    "args = [[\"start_add\" 2], \"const_add\", \"W\"] \n",
    "seed = [-0.2, 0.2, 0.2, -1.5] \n",
    "walls = Dict(:start_add=>[-5.1, 5.1], :W=>[-5.1, 5.1]) # \n",
    "# sr = 1504713552 gives a mean(hits)=0.5 mess\n",
    "# sr = 1504713626 gives a mean(hits)=0.5 mess\n",
    "# sr = 1504713708 gives a mean(hits)=0.5 mess\n",
    "# sr =  gives a mean(hits)=0.5 mess\n",
    "\n",
    "\n",
    "new_random_seed = true; if new_random_seed\n",
    "    sr = convert(Int64, round(time()))\n",
    "else\n",
    "    sr = old_sr\n",
    "end\n",
    "# sr = 1504649431\n",
    "old_sr = sr\n",
    "\n",
    "srand(sr)\n",
    "\n",
    "# THIS IS THE GOOD ONE FOR ALL THE COMMENTS ON sr NUMBERS ABOVE: startU=randn(50,2)-3\n",
    "startU=randn(50,2)-3\n",
    "\n",
    "\n",
    "clf()\n",
    "print(\"seed = \"); print_vector_g(seed); print(\"\\n\")\n",
    "ocost, omhits, omdiffs = costfunc(startU; do_plot=true, sr=sr, verbose=true, make_dict(args, seed, model_params)...)\n",
    "\n",
    "\n",
    "# :sigma=>[-0.3, 0.3] does fine but :sigma=>[-0.2, 0.2] gets stuck.\n",
    "# If we fix sigma at 0 it also gets stuck, but dynamics kind of odd, W a bit to big, or decrease dt\n",
    "params, traj, zz, cpm_traj = bbox_Hessian_keyword_minimization(seed, args, walls, # , :sigma=>[-0.2, 0.2]), \n",
    "(;params...) -> costfunc(startU; beta=beta, sr=sr, do_plot=false, verbose=true, merge(model_params, Dict(params))...)[1], \n",
    "verbose=true, start_eta=1, tol=1e-8, softbox=true, maxiter=400 )\n",
    "\n",
    "# params, cost, ptraj, gtraj = fluxSense(costfunc, backward, model_params, startU, fluxFinalPoint, args, seed; \n",
    "#    start_eta=0.01, tol=1e-15, maxiter=400, verbose=true, report_every=1, do_plot=false, cost_limit=cost_limit) # cost_limit=-0.000935) # for beta=0.01\n",
    "\n",
    "# And show the final position\n",
    "figure(1); clf()\n",
    "cost, mhits, mdiffs = \n",
    "    costfunc(startU; beta=beta, do_plot=true, sr=sr, verbose=true, make_dict(args, params, model_params)...)\n",
    "\n",
    "repeat_results_in_fig2 = true; if repeat_results_in_fig2\n",
    "    figure(2); clf()\n",
    "    costfunc(startU; beta=beta, do_plot=true, sr=sr, verbose=true, \n",
    "        make_dict(args, params, merge(Dict(:fignum=>2), model_params))...)\n",
    "    figure(1); \n",
    "end\n",
    "params'\n",
    "\n",
    "# For beta=0, and ntrials=20, we collected a bunch of results and observed that it failes about half the time\n",
    "# WHEN the initial mean)hits) is below 0.5.  It never fails if the initial mean(hits) is above 0.5. \n",
    "# Seems like when it starts below 0.5, the fastest way to increase mean(hits) is to push it to 0.5 and floor it there.\n",
    "# The results were collected in \"Results.mat\"\n",
    "# res = [res ; omhits mhits]\n",
    "\n",
    "[omhits mhits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1504713708"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Array{Any,1}:\n",
       " PyObject <matplotlib.lines.Line2D object at 0x33088fad0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "figure(2); clf();\n",
    "plot(cpm_traj[1,:], \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition param_path(Any) in module Main at In[258]:2 overwritten at In[260]:2.\n",
      "WARNING: Method definition #param_path(Array{Any, 1}, Main.#param_path, Any) in module Main overwritten.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "param_path (generic function with 1 method)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function param_path(ppath; k=1, do_plot=true, fignum=1, clearfig=true)\n",
    "    costhetas = zeros(1, size(ppath,2)-k-1)\n",
    "    deltas = ppath[:,2:end] - ppath[:,1:end-1]\n",
    "    for i=1:length(costhetas)\n",
    "        costhetas[i] = dot(deltas[:,i], deltas[:,i+k])/(norm(deltas[:,i])*norm(deltas[:,i+k]))\n",
    "    end;\n",
    "    \n",
    "    if do_plot\n",
    "        figure(fignum); if clearfig; clf(); end;\n",
    "        plot(costhetas', \".\")\n",
    "    end\n",
    "    return costhetas\n",
    "end\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×69 Array{Float64,2}:\n",
       " NaN  0.850328  0.775369  -0.952955  …  NaN  NaN  NaN  NaN  NaN  NaN  NaN"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_path(traj[3:end,:]; k=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpm: g (candidate indices) are : [1, 333, 444, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-7654187.550, -30616.750, 2518227.704, 7638879.175]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "1 : After searching for lambdas with efactor=3, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 361, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-30616750.199, 2449340.016, 30555516.699]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "2 : After searching for lambdas with efactor=12, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 340, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-122467000.798, 2081939.014, 122222066.796]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "3 : After searching for lambdas with efactor=48, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 335, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-489868003.190, 979736.006, 488888267.184]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "4 : After searching for lambdas with efactor=192, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 334, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-1959472012.760, -1959472.013, 1955553068.735]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "5 : After searching for lambdas with efactor=768, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 2, 12, 333, 656, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018, 0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-7837888051.040, -7814374386.887, -7579237745.356, -31351552.204, 7563561969.254, 7822212274.938]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "6 : After searching for lambdas with efactor=3072, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 1, 5, 10, 14, 18, 22, 26, 30, 34, 38, 41, 45, 48, 52, 55, 58, 62, 65, 68, 71, 73, 76, 79, 82, 84, 87, 89, 92, 94, 97, 99, 101, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 129, 131, 134, 136, 139, 142, 145, 148, 151, 155, 159, 165, 174, 333, 496, 504, 509, 513, 517, 520, 523, 526, 529, 531, 534, 536, 539, 541, 543, 545, 547, 549, 552, 555, 557, 559, 561, 563, 565, 568, 570, 572, 575, 577, 580, 582, 585, 588, 590, 593, 596, 599, 602, 605, 608, 612, 615, 618, 622, 625, 629, 633, 636, 640, 644, 648, 653, 657, 662, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-31351552204.161, -31351552204.161, -30975333577.711, -30505060294.648, -30128841668.198, -29752623041.748, -29376404415.298, -29000185788.849, -28623967162.399, -28247748535.949, -27871529909.499, -27589365939.661, -27213147313.211, -26930983343.374, -26554764716.924, -26272600747.087, -25990436777.249, -25614218150.799, -25332054180.962, -25049890211.124, -24767726241.287, -24579616928.062, -24297452958.224, -24015288988.387, -23733125018.550, -23545015705.325, -23262851735.487, -23074742422.262, -22792578452.425, -22604469139.200, -22322305169.362, -22134195856.137, -21946086542.912, -21663922573.075, -21475813259.850, -21287703946.625, -21099594633.400, -20911485320.175, -20723376006.950, -20535266693.725, -20347157380.500, -20159048067.275, -19970938754.050, -19782829440.825, -19594720127.600, -19312556157.763, -19124446844.538, -18842282874.700, -18654173561.476, -18372009591.638, -18089845621.801, -17807681651.963, -17525517682.126, -17243353712.288, -16867135085.838, -16490916459.388, -15926588519.714, -15080096610.201, -125406208.817, 15205502819.018, 15957940071.918, 16428213354.980, 16804431981.430, 17180650607.880, 17462814577.717, 17744978547.555, 18027142517.392, 18309306487.230, 18497415800.455, 18779579770.292, 18967689083.517, 19249853053.355, 19437962366.580, 19626071679.805, 19814180993.029, 20002290306.254, 20190399619.479, 20472563589.317, 20754727559.154, 20942836872.379, 21130946185.604, 21319055498.829, 21507164812.054, 21695274125.279, 21977438095.117, 22165547408.342, 22353656721.566, 22635820691.404, 22823930004.629, 23106093974.466, 23294203287.691, 23576367257.529, 23858531227.366, 24046640540.591, 24328804510.429, 24610968480.266, 24893132450.103, 25175296419.941, 25457460389.778, 25739624359.616, 26115842986.066, 26398006955.903, 26680170925.741, 27056389552.191, 27338553522.028, 27714772148.478, 28090990774.928, 28373154744.765, 28749373371.215, 29125591997.665, 29501810624.115, 29972083907.177, 30348302533.627, 30818575816.690, 31288849099.752]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "7 : After searching for lambdas with efactor=12288, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 12, 28, 43, 57, 69, 80, 91, 100, 109, 117, 125, 132, 139, 145, 151, 156, 161, 166, 171, 175, 179, 183, 187, 190, 194, 197, 200, 203, 206, 208, 211, 213, 216, 218, 220, 222, 226, 228, 230, 232, 235, 237, 241, 244, 249, 333, 419, 424, 427, 430, 433, 435, 438, 440, 442, 444, 446, 449, 451, 453, 456, 458, 461, 464, 467, 470, 473, 476, 480, 484, 488, 492, 496, 501, 506, 511, 516, 522, 528, 535, 542, 549, 558, 566, 576, 586, 598, 610, 623, 638, 655, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-125406208816.642, -121267803925.693, -115248305902.494, -109605026505.745, -104337965735.446, -99823342218.047, -95684937327.098, -91546532436.149, -88160564798.099, -84774597160.050, -81764848148.451, -78755099136.851, -76121568751.702, -73488038366.552, -71230726607.853, -68973414849.153, -67092321716.904, -65211228584.654, -63330135452.404, -61449042320.155, -59944167814.355, -58439293308.555, -56934418802.756, -55429544296.956, -54300888417.606, -52796013911.806, -51667358032.457, -50538702153.107, -49410046273.757, -48281390394.407, -47528953141.507, -46400297262.158, -45647860009.258, -44519204129.908, -43766766877.008, -43014329624.108, -42261892371.208, -40757017865.409, -40004580612.509, -39252143359.609, -38499706106.709, -37371050227.359, -36618612974.460, -35113738468.660, -33985082589.310, -32103989457.060, -501624835.267, 31853177039.427, 33734270171.677, 34862926051.027, 35991581930.376, 37120237809.726, 37872675062.626, 39001330941.976, 39753768194.876, 40506205447.775, 41258642700.675, 42011079953.575, 43139735832.925, 43892173085.825, 44644610338.725, 45773266218.074, 46525703470.974, 47654359350.324, 48783015229.674, 49911671109.024, 51040326988.373, 52168982867.723, 53297638747.073, 54802513252.873, 56307387758.672, 57812262264.472, 59317136770.272, 60822011276.071, 62703104408.321, 64584197540.571, 66465290672.820, 68346383805.070, 70603695563.770, 72861007322.469, 75494537707.619, 78128068092.768, 80761598477.918, 84147566115.967, 87157315127.566, 90919501392.066, 94681687656.565, 99196311173.964, 103710934691.363, 108601776835.212, 114245056231.961, 120640772881.610, 125155396399.009]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "8 : After searching for lambdas with efactor=49152, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 3, 63, 105, 135, 159, 177, 192, 204, 215, 224, 231, 238, 244, 249, 253, 257, 261, 265, 268, 270, 273, 275, 278, 280, 283, 285, 289, 333, 379, 382, 385, 387, 389, 391, 394, 396, 399, 402, 405, 409, 413, 418, 423, 429, 436, 443, 452, 462, 475, 490, 508, 531, 562, 604, 664, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-501624835266.569, -498615086254.969, -408322615906.987, -345117886663.399, -299971651489.408, -263854663350.215, -236766922245.820, -214193804658.825, -196135310589.228, -179581691025.432, -166037820473.234, -155503698932.636, -144969577392.038, -135940330357.240, -128415957828.242, -122396459805.043, -116376961781.844, -110357463758.645, -104337965735.446, -99823342218.047, -96813593206.448, -92298969689.049, -89289220677.449, -84774597160.050, -81764848148.451, -77250224631.052, -74240475619.452, -68220977596.253, -2006499341.066, 67217727925.720, 71732351443.119, 76246974960.519, 79256723972.118, 82266472983.717, 85276221995.317, 89790845512.716, 92800594524.315, 97315218041.714, 101829841559.114, 106344465076.513, 112363963099.711, 118383461122.910, 125907833651.909, 133432206180.907, 142461453215.706, 152995574756.303, 163529696296.901, 177073566849.099, 192122311907.096, 211685680482.492, 234258798069.488, 261346539173.882, 295958652807.276, 342609762487.067, 405814491730.654, 496106962078.637, 500621585596.036]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "9 : After searching for lambdas with efactor=196608, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 86, 185, 227, 251, 266, 277, 284, 290, 295, 298, 301, 304, 306, 308, 311, 333, 357, 359, 363, 365, 368, 372, 377, 382, 390, 401, 416, 439, 482, 581, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-2006499341066.275, -1494842009094.375, -898911704797.691, -646092787823.341, -501624835266.569, -411332364918.586, -345117886663.399, -302981400501.008, -266864412361.815, -236766922245.820, -218708428176.224, -200649934106.628, -182591440037.031, -170552443990.633, -158513447944.236, -140454953874.639, -8025997364.265, 136441955192.507, 148480951238.905, 172558943331.700, 184597939378.098, 202656433447.694, 226734425540.489, 256831915656.483, 286929405772.477, 335085389958.068, 401299868213.255, 491592338561.238, 630040793094.811, 888879208092.360, 1484809512389.044, 2002486342384.143]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "10 : After searching for lambdas with efactor=786432, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 148, 272, 297, 307, 313, 317, 321, 333, 347, 350, 354, 359, 370, 395, 519, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-8025997364265.102, -4486532526624.191, -1500861507117.573, -898911704797.691, -658131783869.738, -513663831312.966, -417351862941.785, -321039894570.604, -32103989457.060, 304987899842.074, 377221876120.459, 473533844491.641, 593923804955.618, 858781717976.366, 1460731520296.248, 4446402539802.866, 8009945369536.571]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "11 : After searching for lambdas with efactor=3.14573e+06, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 287, 318, 325, 327, 333, 342, 348, 379, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-32103989457060.406, -4558766502902.578, -1573095483395.958, -898911704797.692, -706287768055.330, -128415957828.242, 738391757512.394, 1316263567739.474, 4301934587246.098, 32039781478146.285]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "12 : After searching for lambdas with efactor=1.25829e+07, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 322, 330, 333, 337, 344, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-128415957828241.625, -4751390439644.944, -1669407451767.143, -513663831312.967, 1027327662625.934, 3724062777018.996, 128159125912585.141]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "13 : After searching for lambdas with efactor=5.03316e+07, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 331, 333, 336, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-513663831312966.500, -5136638313129.670, -2054655325251.868, 2568319156564.892, 512636503650340.563]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "14 : After searching for lambdas with efactor=2.01327e+08, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 333, 334, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018, 0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-2054655325251866.000, -8218621301007.472, -2054655325251.868, 2050546014601362.250]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "15 : After searching for lambdas with efactor=8.05306e+08, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-8218621301007464.000, 8202184058405449.000]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "16 : After searching for lambdas with efactor=3.22123e+09, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-32874485204029856.000, 32808736233621796.000]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "17 : After searching for lambdas with efactor=1.28849e+10, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-131497940816119424.000, 131234944934487184.000]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "18 : After searching for lambdas with efactor=5.15396e+10, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-525991763264477696.000, 524939779737948736.000]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "19 : After searching for lambdas with efactor=2.06158e+11, we found these : []\n",
      "cpm: g (candidate indices) are : [1, 667]\n",
      "cpm: and their corresponding costs are : [0.018, 0.018]\n",
      "cpm: and their corresponding lambdas are : [-2103967053057910784.000, 2099759118951794944.000]\n",
      "cpm: the minimum cost was : 0.0182817\n",
      "20 : After searching for lambdas with efactor=8.24634e+11, we found these : []\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "ArgumentError: collection must be non-empty",
     "output_type": "error",
     "traceback": [
      "ArgumentError: collection must be non-empty",
      "",
      " in findmin(::Array{Float64,1}) at ./array.jl:1254",
      " in indmin at ./array.jl:1294 [inlined]",
      " in #constrained_parabolic_minimization#777(::Float64, ::Bool, ::Bool, ::Bool, ::Float64, ::Int64, ::Float64, ::Float64, ::Float64, ::Int64, ::#constrained_parabolic_minimization, ::Array{Float64,2}, ::Array{Float64,2}, ::Float64) at ./In[227]:106",
      " in (::#kw##constrained_parabolic_minimization)(::Array{Any,1}, ::#constrained_parabolic_minimization, ::Array{Float64,2}, ::Array{Float64,2}, ::Float64) at ./<missing>:0"
     ]
    }
   ],
   "source": [
    "a = matread(\"error_report.mat\")\n",
    "hess = a[\"hess\"]\n",
    "eta = a[\"eta\"]\n",
    "grad = a[\"grad\"]\n",
    "\n",
    "chessdelta = constrained_parabolic_minimization(hess, grad'', eta, maxiter=500, max_efactor_tries=20, tol=1e-20, do_plot=true, verbose=true)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example doing a successful minimization of a 2d model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function JJ(initUs; theta1=0.15, theta2=0.2, beta=0.003, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, pre_string=\"\", zero_last_sigmas=0, seedrand=NaN, params...)\n",
    "\n",
    "    if ~isnan(seedrand); srand(seedrand); end\n",
    "    \n",
    "    Vend = ForwardDiffZeros(size(initUs,1), size(initUs,2), nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:size(initUs,1)\n",
    "        if i>size(initUs,1) - zero_last_sigmas\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; sigma=0, nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)            \n",
    "        else\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)\n",
    "        end\n",
    "        Vend[i,:] = Ve\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend[:,1]-Vend[:,2])/theta1))\n",
    "    diffs = tanh.((Vend[:,1]-Vend[:,2])/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"%s\", pre_string)\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The following sequence leads to a situation where having only [-0.8, -0.8] as the single finalFluxPoint \n",
    "# leads to the minimization getting stuck.  Adding further finalFluxPoints solves the problem\n",
    "#\n",
    "srand(12)   # 12 is perfect success; srand(11) gets stuck at mean(hits)=0.72\n",
    "startU=randn(100,2)-3\n",
    "startU=randn(100,2)-3\n",
    "sigma = 0\n",
    "\n",
    "# startU=0.1*randn(100,2)-3\n",
    "# startU=zeros(100,2)-3\n",
    "\n",
    "\n",
    "dt = 0.02\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "W = -4\n",
    "noise = 0\n",
    "input = 0\n",
    "sigma = 0\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    ":noise=>noise, :input=>input, :sigma=>sigma, :const_add=>0, :init_add=>0)\n",
    "\n",
    "\n",
    "# WORKING gradient:\n",
    "# ForwardDiff.gradient((x)->JJ(startU; do_plot=true, nderivs=length(x), difforder=1, \n",
    "#    make_dict([[\"init_add\" 2], \"const_add\"], x, model_params)...), [2.9, -2.9, 0.1])\n",
    "\n",
    "\n",
    "\n",
    "# The backward and costfunc functions should turn a single-scalar parameter W into the matrix W\n",
    "# backward always runs with no within-forward noise, i.e., sigma=0\n",
    "backward = (endpoint; do_plot=false, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;     \n",
    "    backwardsModel(endpoint; do_plot=do_plot, make_dict([\"sigma\"], [0], pars)...)[1]\n",
    "end\n",
    "\n",
    "\n",
    "beta = 0.003;\n",
    "\n",
    "costfunc = (startpoints; do_plot=false, verbose=false, nderivs=0, difforder=0, sr=26, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;         \n",
    "    JJ(startpoints; seedrand=sr, beta=beta, \n",
    "        do_plot=do_plot, verbose=verbose, nderivs=nderivs, difforder=difforder, pars...)\n",
    "end\n",
    "\n",
    "function JJ(initUs; theta1=0.15, theta2=0.2, beta=0.003, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, pre_string=\"\", zero_last_sigmas=0, seedrand=NaN, params...)\n",
    "\n",
    "    if ~isnan(seedrand); srand(seedrand); end\n",
    "    \n",
    "    Vend = ForwardDiffZeros(size(initUs,1), size(initUs,2), nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:size(initUs,1)\n",
    "        if false # i>size(initUs,1) - zero_last_sigmas\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; sigma=0, nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)            \n",
    "        else\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)\n",
    "        end\n",
    "        Vend[i,:] = Ve\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend[:,1]-Vend[:,2])/theta1))\n",
    "    diffs = tanh.((Vend[:,1]-Vend[:,2])/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"%s\", pre_string)\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "  \n",
    "\n",
    "\n",
    "if beta==0.003;     cost_limit = -0.00288\n",
    "elseif beta<0.001;  cost_limit = -0.0008\n",
    "elseif beta==0.001; cost_limit = -0.000935\n",
    "elseif beta==0.05;  cost_limit = -0.0485\n",
    "else\n",
    "    error(\"Don't know what cost limit goes with beta %g\\n\", beta)\n",
    "end\n",
    "\n",
    "fluxFinalPoint = [-0.8 -0.8; -0.6 -0.6 ; -0.4 -0.4; -0.2 -0.2; 0 0; 0.2 0.2]\n",
    "fluxFinalPoint = zeros(0,2);\n",
    "\n",
    "\n",
    "\n",
    "args = [[\"init_add\" 2], \"const_add\", \"W\", \"sigma\"]\n",
    "seed = [2, 2, 2.1, -1, 0.1]\n",
    "\n",
    "\n",
    "\n",
    "clf()\n",
    "print(\"seed = \"); print_vector_g(seed); print(\"\\n\")\n",
    "costfunc(startU; do_plot=true, verbose=true, make_dict(args, seed, model_params)...)\n",
    "\n",
    "# :sigma=>[-0.3, 0.3] does fine but :sigma=>[-0.2, 0.2] gets stuck.\n",
    "# If we fix sigma at 0 it also gets stuck, but dynamics kind of odd, W a bit to big, or decrease dt\n",
    "params, traj = bbox_Hessian_keyword_minimization(seed, args, Dict(:init_add=>[-5.1, 5.1], :sigma=>[-0.3, 0.3]), \n",
    "(;params...) -> costfunc(startU; do_plot=false, verbose=true, merge(model_params, Dict(params))...), \n",
    " verbose=true, start_eta=1, tol=1e-6, hardbox=true )\n",
    "\n",
    "# params, cost, ptraj, gtraj = fluxSense(costfunc, backward, model_params, startU, fluxFinalPoint, args, seed; \n",
    "#    start_eta=0.01, tol=1e-15, maxiter=400, verbose=true, report_every=1, do_plot=false, cost_limit=cost_limit) # cost_limit=-0.000935) # for beta=0.01\n",
    "\n",
    "# And show the final position\n",
    "clf()\n",
    "costfunc(startU; do_plot=true, verbose=true, make_dict(args, params, model_params)...)\n",
    "params'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf()\n",
    "startU=randn(4000,2)-3\n",
    "costfunc(startU; sr=NaN, do_plot=false, verbose=true, make_dict(args, params, model_params)...)\n",
    "params'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of getting stuck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function JJ(initUs; theta1=0.15, theta2=0.2, beta=0.003, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, pre_string=\"\", zero_last_sigmas=0, seedrand=NaN, params...)\n",
    "\n",
    "    if ~isnan(seedrand); srand(seedrand); end\n",
    "    \n",
    "    Vend = ForwardDiffZeros(size(initUs,1), size(initUs,2), nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:size(initUs,1)\n",
    "        if i>size(initUs,1) - zero_last_sigmas\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; sigma=0, nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)            \n",
    "        else\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)\n",
    "        end\n",
    "        Vend[i,:] = Ve\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend[:,1]-Vend[:,2])/theta1))\n",
    "    diffs = tanh.((Vend[:,1]-Vend[:,2])/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"%s\", pre_string)\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function JJ(initUs; theta1=0.15, theta2=0.2, beta=0.003, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, pre_string=\"\", zero_last_sigmas=0, seedrand=NaN, params...)\n",
    "\n",
    "    if ~isnan(seedrand); srand(seedrand); end\n",
    "    \n",
    "    Vend = ForwardDiffZeros(size(initUs,1), size(initUs,2), nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:size(initUs,1)\n",
    "        if false # i>size(initUs,1) - zero_last_sigmas\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; sigma=0, nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)            \n",
    "        else\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)\n",
    "        end\n",
    "        Vend[i,:] = Ve\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend[:,1]-Vend[:,2])/theta1))\n",
    "    diffs = tanh.((Vend[:,1]-Vend[:,2])/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"%s\", pre_string)\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "\n",
    "\n",
    "# The following sequence leads to a situation where having only [-0.8, -0.8] as the single finalFluxPoint \n",
    "# leads to the minimization getting stuck.  Adding further finalFluxPoints solves the problem\n",
    "#\n",
    "srand(11)\n",
    "startU=randn(100,2)-3\n",
    "startU=randn(100,2)-3\n",
    "\n",
    "dt = 0.02\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "W = -4\n",
    "noise = 0\n",
    "input = 0\n",
    "sigma = 0\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    ":noise=>noise, :input=>input, :sigma=>sigma, :const_add=>0, :init_add=>0)\n",
    "\n",
    "\n",
    "# WORKING gradient:\n",
    "# ForwardDiff.gradient((x)->JJ(startU; do_plot=true, nderivs=length(x), difforder=1, \n",
    "#    make_dict([[\"init_add\" 2], \"const_add\"], x, model_params)...), [2.9, -2.9, 0.1])\n",
    "\n",
    "\n",
    "\n",
    "# The backward and costfunc functions should turn a single-scalar parameter W into the matrix W\n",
    "# backward always runs with no within-forward noise, i.e., sigma=0\n",
    "backward = (endpoint; do_plot=false, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;     \n",
    "    backwardsModel(endpoint; do_plot=do_plot, make_dict([\"sigma\"], [0], pars)...)[1]\n",
    "end\n",
    "\n",
    "\n",
    "beta = 0.0001;\n",
    "beta = 0.003;\n",
    "beta = 0.003;\n",
    "beta=0\n",
    "\n",
    "costfunc = (startpoints; do_plot=false, verbose=false, nderivs=0, difforder=0, sr=26, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;         \n",
    "    JJ(startpoints; seedrand=sr, beta=beta, \n",
    "        do_plot=do_plot, verbose=verbose, nderivs=nderivs, difforder=difforder, pars...)\n",
    "end\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "if beta==0.003;     cost_limit = -0.00288\n",
    "elseif beta<0.001;  cost_limit = -0.0008\n",
    "elseif beta==0.001; cost_limit = -0.000935\n",
    "elseif beta==0.05;  cost_limit = -0.0485\n",
    "else\n",
    "    error(\"Don't know what cost limit goes with beta %g\\n\", beta)\n",
    "end\n",
    "\n",
    "fluxFinalPoint = [-0.8 -0.8; -0.6 -0.6 ; -0.4 -0.4; -0.2 -0.2; 0 0; 0.2 0.2]\n",
    "fluxFinalPoint = zeros(0,2);\n",
    "\n",
    "\n",
    "args = [[\"init_add\" 2], \"const_add\", \"W\"] # , \"sigma\"]\n",
    "seed = [2, 2, 2.1, -1] # , 0.1]\n",
    "\n",
    "\n",
    "\n",
    "clf()\n",
    "print(\"seed = \"); print_vector_g(seed); print(\"\\n\")\n",
    "costfunc(startU; do_plot=true, verbose=true, make_dict(args, seed, model_params)...)\n",
    "\n",
    "# :sigma=>[-0.3, 0.3] does fine but :sigma=>[-0.2, 0.2] gets stuck.\n",
    "# If we fix sigma at 0 it also gets stuck, but dynamics kind of odd, W a bit to big, or decrease dt\n",
    "params, traj = bbox_Hessian_keyword_minimization(seed, args, Dict(:init_add=>[-5.1, 5.1]), # , :sigma=>[-0.2, 0.2]), \n",
    "(;params...) -> costfunc(startU; do_plot=false, verbose=true, merge(model_params, Dict(params))...), \n",
    " verbose=true, start_eta=1, tol=1e-16, hardbox=true )\n",
    "\n",
    "# params, cost, ptraj, gtraj = fluxSense(costfunc, backward, model_params, startU, fluxFinalPoint, args, seed; \n",
    "#    start_eta=0.01, tol=1e-15, maxiter=400, verbose=true, report_every=1, do_plot=false, cost_limit=cost_limit) # cost_limit=-0.000935) # for beta=0.01\n",
    "\n",
    "# And show the final position\n",
    "clf()\n",
    "costfunc(startU; do_plot=true, verbose=true, make_dict(args, params, model_params)...)\n",
    "params'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function JJ(initUs; theta1=0.15, theta2=0.2, beta=0.003, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, pre_string=\"\", zero_last_sigmas=0, seedrand=NaN, params...)\n",
    "\n",
    "    if ~isnan(seedrand); srand(seedrand); end\n",
    "    \n",
    "    Vend = ForwardDiffZeros(size(initUs,1), size(initUs,2), nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:size(initUs,1)\n",
    "        if false # i>size(initUs,1) - zero_last_sigmas\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; sigma=0, nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)            \n",
    "        else\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)\n",
    "        end\n",
    "        Vend[i,:] = Ve\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend[:,1]-Vend[:,2])/theta1))\n",
    "    diffs = tanh.((Vend[:,1]-Vend[:,2])/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"%s\", pre_string)\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "\n",
    "\n",
    "# The following sequence leads to a situation where having only [-0.8, -0.8] as the single finalFluxPoint \n",
    "# leads to the minimization getting stuck.  Adding further finalFluxPoints solves the problem\n",
    "#\n",
    "srand(11)\n",
    "startU=randn(100,2)-3\n",
    "startU=randn(100,2)-3\n",
    "\n",
    "dt = 0.02\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "W = -4\n",
    "noise = 0\n",
    "input = 0\n",
    "sigma = 0\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    ":noise=>noise, :input=>input, :sigma=>sigma, :const_add=>0, :init_add=>0)\n",
    "\n",
    "\n",
    "# WORKING gradient:\n",
    "# ForwardDiff.gradient((x)->JJ(startU; do_plot=true, nderivs=length(x), difforder=1, \n",
    "#    make_dict([[\"init_add\" 2], \"const_add\"], x, model_params)...), [2.9, -2.9, 0.1])\n",
    "\n",
    "\n",
    "\n",
    "# The backward and costfunc functions should turn a single-scalar parameter W into the matrix W\n",
    "# backward always runs with no within-forward noise, i.e., sigma=0\n",
    "backward = (endpoint; do_plot=false, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;     \n",
    "    backwardsModel(endpoint; do_plot=do_plot, make_dict([\"sigma\"], [0], pars)...)[1]\n",
    "end\n",
    "\n",
    "\n",
    "beta = 0.0001;\n",
    "beta = 0.003;\n",
    "beta = 0.003;\n",
    "beta=0\n",
    "\n",
    "costfunc = (startpoints; do_plot=false, verbose=false, nderivs=0, difforder=0, sr=26, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;         \n",
    "    JJ(startpoints; seedrand=sr, beta=beta, \n",
    "        do_plot=do_plot, verbose=verbose, nderivs=nderivs, difforder=difforder, pars...)\n",
    "end\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "if beta==0.003;     cost_limit = -0.00288\n",
    "elseif beta<0.001;  cost_limit = -0.0008\n",
    "elseif beta==0.001; cost_limit = -0.000935\n",
    "elseif beta==0.05;  cost_limit = -0.0485\n",
    "else\n",
    "    error(\"Don't know what cost limit goes with beta %g\\n\", beta)\n",
    "end\n",
    "\n",
    "fluxFinalPoint = [-0.8 -0.8; -0.6 -0.6 ; -0.4 -0.4; -0.2 -0.2; 0 0; 0.2 0.2]\n",
    "fluxFinalPoint = zeros(0,2);\n",
    "\n",
    "\n",
    "args = [[\"init_add\" 2], \"const_add\", \"W\"] # , \"sigma\"]\n",
    "seed = [2, 2, 2.1, -1] # , 0.1]\n",
    "\n",
    "\n",
    "\n",
    "clf()\n",
    "print(\"seed = \"); print_vector_g(seed); print(\"\\n\")\n",
    "costfunc(startU; do_plot=true, verbose=true, make_dict(args, seed, model_params)...)\n",
    "\n",
    "# :sigma=>[-0.3, 0.3] does fine but :sigma=>[-0.2, 0.2] gets stuck.\n",
    "# If we fix sigma at 0 it also gets stuck, but dynamics kind of odd, W a bit to big, or decrease dt\n",
    "params, traj = bbox_Hessian_keyword_minimization(seed, args, Dict(:init_add=>[-5.1, 5.1], :W=>[-5.1, 5.1]), # , :sigma=>[-0.2, 0.2]), \n",
    "(;params...) -> costfunc(startU; do_plot=false, verbose=true, merge(model_params, Dict(params))...), \n",
    " verbose=true, start_eta=1, tol=1e-16, hardbox=true )\n",
    "\n",
    "# params, cost, ptraj, gtraj = fluxSense(costfunc, backward, model_params, startU, fluxFinalPoint, args, seed; \n",
    "#    start_eta=0.01, tol=1e-15, maxiter=400, verbose=true, report_every=1, do_plot=false, cost_limit=cost_limit) # cost_limit=-0.000935) # for beta=0.01\n",
    "\n",
    "# And show the final position\n",
    "clf()\n",
    "costfunc(startU; do_plot=true, verbose=true, make_dict(args, params, model_params)...)\n",
    "params'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(args)\n",
    "\n",
    "params'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf()\n",
    "costfunc(startU; do_plot=true, verbose=true, make_dict(args, [2, 1.7, 1.3, -1.4, -0.5], model_params)...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure(2)\n",
    "clf()\n",
    "plot(traj[1,:], \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf()\n",
    "cost, grad, hess = keyword_gradient((;params...) -> \n",
    "costfunc(startU; do_plot=true, verbose=true, merge(model_params, Dict(params))...), args, [0.6, -0.6, 1.5, -2])\n",
    "\n",
    "\n",
    "# costfunc(startU; do_plot=true, verbose=true, make_dict(args, [0.6, -0.6, 1.5, -2], model_params)...)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf()\n",
    "cost, gradB, hessB = keyword_vgh((;params...) -> \n",
    "costfunc(startU; do_plot=true, verbose=true, merge(merge(model_params, Dict(params)), Dict(:dt=>0.005, :nsteps=>201))...), \n",
    "args, [0.6, -0.6, 1.5, -2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[gradA gradB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf()\n",
    "costfunc(startU; do_plot=true, verbose=true, \n",
    "merge(make_dict(args, params, model_params), Dict(:dt=>0.005, :nsteps=>201))...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure(2);\n",
    "clf()\n",
    "subplot(2,1,1); plot(cost', \".\")\n",
    "subplot(2,1,2); \n",
    "guys = 4:5\n",
    "ng = sqrt(sum(gtraj[guys,:].*gtraj[guys,:],1))\n",
    "plot(sum(gtraj[guys,1:end-1].*gtraj[guys,2:end],1)'./(ng[1:end-1].*ng[2:end]), \".\")\n",
    "grid(true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beginning of attempt at finding saddle points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt = 0.02\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "W = -4\n",
    "noise = 0\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    "    :noise=>noise, :noise=>noise, :const_add=>0, :init_add=>0)\n",
    "\n",
    "@time(trust_region_Hessian_minimization([-2.1, -2.1], \n",
    "    (x)->forwardModel(x; do_plot=false, nderivs=2, difforder=1, dUdt_mag_only=true, model_params...), \n",
    "verbose=false, start_eta=0.1, tol=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "backward = (endpoint; do_plot=false, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;     \n",
    "    backwardsModel(endpoint; do_plot=do_plot, make_dict([\"sigma\"], [0], pars)...)[1]\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hessian_fluxSense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "function bbox_Hessian_keyword_minimization(seed, args, bbox, func; wallwidth=NaN, start_eta=10, tol=1e-6, \n",
    "    maxiter=400, verbose=false)\n",
    "\n",
    "Like constrained_Hessian_minimization, but uses keyword_hessian!(). \n",
    "\n",
    "PARAMETERS:\n",
    "===========\n",
    "\n",
    "seed        column vector, representing the starting value of the parameters.\n",
    "\n",
    "args        List of strings identifying parameters for differentiation, e.g., [\"const_E\", \"w_self]\n",
    "\n",
    "bbox        An nargs-by-2 matrix indicating the range for each argument,\n",
    "            with the minima (first column) and maxima (second column).\n",
    "\n",
    "func        func must take only optional keyword args, and must \n",
    "            take nderivs=0, difforder=0  and declare any new matrices using ForwardDiffZeros() instead of zeros()\n",
    "\n",
    "\n",
    "OPTIONAL PARAMETERS:\n",
    "====================\n",
    "\n",
    "start_eta=10 Starting value of the radius.  It's good to start with somethibg biggish, if it is\n",
    "             too much, it'll quickly get cut down.\n",
    "\n",
    "tol=1e-6     Numerical tolerance. If a proposed jump produces a change in func that is less than\n",
    "             this, the minimization stops.\n",
    "\n",
    "maxiter=400  Maximum number of iterations to do before stopping\n",
    "\n",
    "verbose=false   If true, print out a report on each iteration of iteration number, radius size (eta),\n",
    "                what type jump was proposed (\"Newton\" means going straight to global min, \"constrained\" means jump has \n",
    "                norm eta, failed means that finding the minimum at a given radius somehow didn't work). Will also\n",
    "                print out the cosine of the angle between the proposed jump and the gradient.\n",
    "\n",
    "hardbox=false   If true, ignores wallwidth, and just rests parameter values to the bounding box if they go outside it.\n",
    "                If false, adds cost function \"walls\" to implement the bounding box.\n",
    "\n",
    "walldith=NaN     Used for putting up cost function \"walls\" that implement the bounding box limits. Can be NaN.\n",
    "                If it is NaN, then the wallwidth is a constant factor of the range width for each argument. If not NaN, must\n",
    "                be an nargs-long vector that indicates the actual wall widths.\n",
    "\n",
    "wallwidth_factor=0.18   Only relevant if wallwidth is NaN, otherwise ignored. For each arg, the wall width\n",
    "                is going to be wall_width_factor*(bbox[i,2] - bbox[i,1])\n",
    "\n",
    "\n",
    "RETURNS:\n",
    "========\n",
    "\n",
    "params       A vector the size of seed that has the last values of the minimizing parameters for func\n",
    "trajectory   A (2+length(params))-by-nsteps matrix. Each column corresponds to an iteration step, and contains\n",
    "                 the value of eta used, the cost, and the value of the parameters at that iteration\n",
    "cost         Final value of objective function\n",
    "\n",
    "\n",
    "EXAMPLE:\n",
    "========\n",
    "\n",
    "function tester(;x=5, y=10, z=20, nderivs=0, difforder=0)\n",
    "    return x^2*y + z/tanh(y)\n",
    "end\n",
    "\n",
    "params, trajectory = bbox_Hessian_keyword_minimization([0.5, 0.5], [\"x\", \"y\"], [1.1 2 ; 1.1 4], tester, \n",
    "    verbose=true, tol=1e-12, start_eta=1);\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# function bbox_Hessian_keyword_minimization(seed, args, bbox, func; \n",
    "    \n",
    "start_eta=10 \n",
    "tol=1e-6 \n",
    "maxiter=400\n",
    "verbose=false\n",
    "verbose_every=1 \n",
    "wallwidth=NaN \n",
    "wallwidth_factor=0.18\n",
    "hardbox=false\n",
    "\n",
    "\n",
    "    \n",
    "    traj_increment = 100\n",
    "    params = seed\n",
    "    eta = start_eta\n",
    "    trajectory = zeros(2+length(params), traj_increment)\n",
    "\n",
    "    if verbose\n",
    "        @printf \"%d: eta=%g ps=\" 0 eta \n",
    "        print_vector_g(params)\n",
    "        @printf \"\\n\"\n",
    "    end\n",
    "    \n",
    "    if hardbox\n",
    "        cost, grad, hess = keyword_vgh((;pars...) -> func(;pars...), args, params)\n",
    "    else\n",
    "        cost, grad, hess = keyword_vgh((;pars...) -> func(;pars...) + wall_cost(args, bbox; wallwidth=wallwidth, pars...),\n",
    "            args, params)        \n",
    "    end\n",
    "        \n",
    "    chessdelta = zeros(size(params))\n",
    "\n",
    "    for i=1:maxiter\n",
    "        if i > size(trajectory, 2)\n",
    "            trajectory = [trajectory zeros(2+length(params), traj_increment)]\n",
    "        end\n",
    "        trajectory[1:2, i]   = [eta;cost]\n",
    "        trajectory[3:end, i] = params\n",
    "        \n",
    "        hessdelta  = - inv(hess)*grad\n",
    "        try\n",
    "            chessdelta = constrained_parabolic_minimization(hess, grad'', eta)[1]\n",
    "            jumptype = \"not failed\"\n",
    "        catch y\n",
    "            jumptype = \"failed\"\n",
    "            if verbose\n",
    "                @printf \"Constrained parabolic minimization failed with error %s\\n\" y\n",
    "                @printf \"\\n\"\n",
    "                @printf \"eta was %g\\n\" eta\n",
    "                @printf \"grad was\\n\"\n",
    "                print_vector(grad)\n",
    "                @printf \"\\n\\nhess was\\n\"\n",
    "                for k in [1:length(grad);]\n",
    "                    print_vector(hess[k,:])\n",
    "                    @printf \"\\n\"\n",
    "                end\n",
    "                @printf \"\\n\"\n",
    "            end\n",
    "        end\n",
    "\n",
    "        if norm(hessdelta) <= eta\n",
    "            new_params = params + hessdelta\n",
    "            jumptype = \"Newton\"\n",
    "        elseif jumptype != \"failed\" \n",
    "            new_params = params + chessdelta\n",
    "            jumptype  = \"constrained\"\n",
    "        end\n",
    "\n",
    "        if jumptype != \"failed\"\n",
    "            if hardbox\n",
    "                for p in [1:length(new_params);]\n",
    "                    if new_params[p] < bbox[p,1]; new_params[p] = bbox[p,1]; end\n",
    "                    if bbox[p,2] < new_params[p]; new_params[p] = bbox[p,2]; end\n",
    "                 end        \n",
    "                \n",
    "                new_cost, new_grad, new_hess = keyword_vgh((;pars...) -> func(;pars...), args, new_params)\n",
    "            else\n",
    "                new_cost, new_grad, new_hess = keyword_vgh((;pars...) -> func(;pars...) + \n",
    "                        wall_cost(args, bbox; wallwidth=wallwidth, pars...),\n",
    "                    args, new_params)                \n",
    "            end\n",
    "            \n",
    "            if abs(new_cost - cost) < tol || eta < tol\n",
    "                trajectory = trajectory[:,1:i]\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "\n",
    "        if jumptype == \"failed\" || new_cost >= cost  \n",
    "            eta = eta/2\n",
    "            costheta = NaN\n",
    "            if eta < tol\n",
    "                trajectory = trajectory[:,1:i]\n",
    "                break\n",
    "            end\n",
    "        else\n",
    "            eta = eta*1.1\n",
    "            costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "\n",
    "            params = new_params\n",
    "            cost = new_cost\n",
    "            grad = new_grad\n",
    "            hess = new_hess\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            if rem(i, verbose_every)==0\n",
    "                @printf \"%d: eta=%g cost=%.4f jtype=%s costheta=%.3f ps=\" i eta cost jumptype costheta\n",
    "                print_vector(params)\n",
    "                @printf \"\\n\"\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return params, trajectory, cost\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    Given args, a list of string representing the arguments of interest, a bounding box for each,\n",
    "    and a Symbol=>value dictionary with the corresponding parameters, computes and returns a high cost for \n",
    "    being outside the bounding box\n",
    "    \"\"\"\n",
    "    function wall_cost(args, bbox; wallwidth=NaN, nderivs=0, difforder=0, pars...) \n",
    "        myparams = ForwardDiffZeros(length(pars), 1, nderivs=nderivs, difforder=difforder)\n",
    "        pars2 = Dict()\n",
    "        for i in [1:length(pars);]\n",
    "            pars2[string(pars[i][1])] = pars[i][2]\n",
    "        end\n",
    "        for i in [1:length(args);]\n",
    "            myparams[i] = pars2[args[i]]\n",
    "        end\n",
    "        \n",
    "        if isnan(wallwidth)\n",
    "            # We know that we're going to be taking hessian for params, so declare zeros accordingly:\n",
    "            wallwidth = ForwardDiffZeros(length(myparams), 1, nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "            for i in [1:length(myparams);]\n",
    "                wallwidth[i] = wallwidth_factor*(bbox[i,2]-bbox[i,1])\n",
    "            end\n",
    "        end\n",
    "\n",
    "        retval = 0\n",
    "        for i in [1:length(myparams);]\n",
    "            if myparams[i]<bbox[i,1]\n",
    "                retval += cosh((bbox[i,1]-myparams[i])/wallwidth[i])-1.0\n",
    "            elseif bbox[i,2] < myparams[i]\n",
    "                retval += cosh((myparams[i]-bbox[i,2])/wallwidth[i])-1.0                \n",
    "            end\n",
    "        end\n",
    "\n",
    "        return 2*retval\n",
    "    end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf()\n",
    "costfunc(startU; do_plot=true, verbose=true, make_dict([\"sigma\"], [0.1], make_dict(args, params, model_params))...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working on figuring out the weird trajectories. Probably a dt thing.  The fourth one is the weird one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forward = (startpoint; do_plot=false, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;     \n",
    "    forwardModel(startpoint; do_plot=do_plot, pars...)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTION DEFINITION: fluxSense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The main fluxSense() function containing the main minimization loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function fluxSense(costfunc, backward, paramsDict, startUs, ends, args, seed; start_eta=0.01, tol=1e-15, \n",
    "    maxiter=400, verbose=true, do_plot=false, cost_limit=[], report_fluxless_grad=false, report_every=1)\n",
    "   \n",
    "    if do_plot; clf(); end;\n",
    "\n",
    "    params = seed\n",
    "    eta    = start_eta\n",
    "\n",
    "    if ~(typeof(ends)<:Array); ends = [ends]; end\n",
    "    U0 = zeros(size(ends))\n",
    "    for j in 1:size(ends,1)\n",
    "        # @printf(\"model params is \"); print(model_params); print(\"\\n\")\n",
    "        # @printf(\"ends[j,:] is \"); print(ends[j,:]); print(\"\\n\")        \n",
    "        U0[j,:] = backward(ends[j,:]; tol=1e-25, do_plot=false, make_dict(args, params, model_params)...)'\n",
    "        # @printf(\"U0[j,:] is \"); print(U0[j,:]); print(\"\\n\")        \n",
    "    end\n",
    "    \n",
    "    if length(ends)>0\n",
    "        @printf(\"U0[end,:] is \"); print_vector_g(U0[end,:]); @printf(\"\\n\")\n",
    "    end\n",
    "    \n",
    "    cost, grad, hess = \n",
    "        vgh((x)->costfunc([startUs;U0]; do_plot=do_plot, nderivs=length(x), difforder=2, \n",
    "            make_dict(args, x, model_params)...), params)\n",
    "    \n",
    "    if verbose && report_fluxless_grad\n",
    "        fcost, fgrad, fhess = \n",
    "        vgh((x)->costfunc(startUs; do_plot=false, nderivs=length(x), difforder=2, \n",
    "                make_dict(args, x, model_params)...), params)\n",
    "        @printf(\"      ### grad without flux track = \"); print_vector_g(fgrad); @printf(\"\\n\")    \n",
    "    end\n",
    "\n",
    "    if verbose\n",
    "        @printf(\"Initial cost, grad, hess:\\n\")\n",
    "        @printf(\"   cost = %g\\n\", cost)\n",
    "        @printf(\"   grad = \"); print_vector_g(grad); print(\"\\n\")\n",
    "        @printf(\"   hess = \"); print_vector_g(hess); print(\"\\n\")\n",
    "    end\n",
    "\n",
    "    delta_params=0\n",
    "    ptrajectory = zeros(length(seed), maxiter); \n",
    "    gtrajectory = zeros(length(seed), maxiter); \n",
    "    ctrajectory = zeros(1, maxiter);\n",
    "    \n",
    "    for i in [1:maxiter;]         \n",
    "        my_verbose = verbose && rem(i, report_every)==0\n",
    "\n",
    "        new_params = params - eta*grad/(sqrt(sum(grad.*grad)))\n",
    "        delta_params = new_params - params\n",
    "\n",
    "        new_cost, new_grad, new_hess = \n",
    "        vgh((x)->costfunc([startUs;U0]; do_plot=false, verbose=my_verbose, pre_string=\"   newpars>> \",\n",
    "                zero_last_sigmas=size(U0,1), nderivs=length(x), difforder=2, make_dict(args, x, model_params)...), \n",
    "                new_params)\n",
    "\n",
    "        if my_verbose\n",
    "            @printf(\"delta_params=\"); print_vector_g(delta_params); @printf(\"\\n\"); \n",
    "            @printf(\"new_cost=%g  cost=%g   delta_cost=%g\\n\", new_cost, cost, new_cost-cost)\n",
    "        end\n",
    "        \n",
    "        if abs(new_cost - cost) < tol\n",
    "            @printf(\"\\n===\\nChange in cost was less than the tolerance %g\\n===\\n\", tol)\n",
    "            ptrajectory=ptrajectory[:,1:i-1]; gtrajectory=gtrajectory[:,1:i-1]; ctrajectory=ctrajectory[1:i-1]\n",
    "            break\n",
    "        end\n",
    "        if (length(cost_limit)>0 && cost < cost_limit)\n",
    "            @printf(\"\\n===\\nCost was less than the cost limit %g\\n===\\n\", cost_limit)\n",
    "            ptrajectory=ptrajectory[:,1:i-1]; gtrajectory=gtrajectory[:,1:i-1]; ctrajectory=ctrajectory[1:i-1]\n",
    "            break\n",
    "        end\n",
    "        \n",
    "        if new_cost >= cost\n",
    "            eta = eta/2\n",
    "            costheta = NaN\n",
    "        else\n",
    "            eta = eta*1.2\n",
    "            costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "\n",
    "            params = new_params\n",
    "    \n",
    "            for j in 1:size(ends,1)\n",
    "                U0[j,:] = backward(ends[j,:]; do_plot=false, make_dict(args, params, model_params)...)'\n",
    "            end\n",
    "            if my_verbose && length(ends)>0\n",
    "                @printf(\"U0[end,:] is \"); print_vector_g(U0[end,:]); @printf(\"\\n\")\n",
    "            end\n",
    "            cost, grad, hess = \n",
    "                vgh((x)->costfunc([startUs;U0]; do_plot=do_plot, verbose=my_verbose, nderivs=length(x), difforder=2, \n",
    "                    zero_last_sigmas=size(U0,1), make_dict(args, x, model_params)...), params)\n",
    "\n",
    "        end\n",
    "        \n",
    "        ptrajectory[:,i] = params\n",
    "        gtrajectory[:,i] = grad\n",
    "        ctrajectory[i]   = cost\n",
    "\n",
    "        if my_verbose\n",
    "            @printf \"%d: eta=%g cost=%g costheta=%g ps=\" i eta cost  costheta\n",
    "            print_vector(params)\n",
    "            @printf \"\\n\"\n",
    "            @printf(\"grad=\"); print_vector_g(grad); @printf(\"\\n\")\n",
    "            if report_fluxless_grad\n",
    "                fcost, fgrad, fhess = \n",
    "                vgh((x)->costfunc(startUs; do_plot=false, verbose=false, nderivs=length(x), difforder=2, \n",
    "                        make_dict(args, x, model_params)...), params)\n",
    "            @printf(\"      ### grad without flux track = \"); print_vector_g(fgrad); @printf(\"\\n\")    \n",
    "            end            \n",
    "        end\n",
    "    end    \n",
    "\n",
    "    return params, ctrajectory, ptrajectory, gtrajectory\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORKING EXAMPLE:   1-D example of using fluxSense()\n",
    "\n",
    "## after defining fluxSense(), run the next two cells in order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the cost function. It passes most keyword params down to the forward and backward models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function J(initUs; theta1=0.15, theta2=0.2, beta=0.01, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, params...)\n",
    "\n",
    "    Vend = ForwardDiffZeros(length(initUs), 1, nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:length(initUs)\n",
    "        Ue, Ve, U, V = forwardModel(initUs[i]; nderivs=nderivs, difforder=difforder, \n",
    "            do_plot=do_plot, clearfig=false, params...)\n",
    "        Vend[i] = Ve[1]\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend-0.5)/theta1))\n",
    "    diffs = tanh((Vend-0.5)/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now setup, run fluxSense(), and display results. Example is only 1-d so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "backward = (endpoint; do_plot=false, pars...) -> backwardsModel(endpoint; do_plot=do_plot, pars...)[1]\n",
    "\n",
    "costfunc = (startpoints; do_plot=false, verbose=false, nderivs=0, difforder=0, pars...) -> \n",
    "J(startpoints; do_plot=do_plot, verbose=verbose, beta=0.01, nderivs=nderivs, difforder=difforder, pars...)\n",
    "\n",
    "dt = 0.01\n",
    "t = 0:dt:2\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "noise = 0*randn(1, nsteps)\n",
    "noise = 0.02*sin(2*pi*3*t); noise=reshape(noise, 1, nsteps)\n",
    "\n",
    "W = 4.1\n",
    "const_add = -2\n",
    "init_add=0\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>W, :nsteps=>nsteps, \n",
    "    :noise=>noise, :noise=>noise, :const_add=>const_add, :init_add=>init_add)\n",
    "\n",
    "srand(20)  \n",
    "startUs = randn(20, 1)       # The starting values\n",
    "# startUs = [randn(10,1)+2;randn(10,1)-2]\n",
    "\n",
    "\n",
    "args = [\"init_add\", \"const_add\", \"W\"]\n",
    "seed = [0, -2, 4.1]\n",
    "\n",
    "# Do an initial run plotting to show the starting position\n",
    "clf()\n",
    "costfunc(startUs; do_plot=true, verbose=true, model_params...)\n",
    "\n",
    "\n",
    "fluxFinalPoint = convert(Float64, 0)  # The final value of the pinned output\n",
    "#\n",
    "# If you remove the fluxFinalPoint, by un-commenting the following line, it gets stuck. But\n",
    "# it is also true that if you make beta=0 (in the constfunc() definition in line 4 above) then t\n",
    "# that also solves the sticking problem.  If we had beta=0 until after our hits are what we want, \n",
    "# would we ever need fluxPoint?\n",
    "#\n",
    "fluxFinalPoint = [];\n",
    "\n",
    "params, cost = fluxSense(costfunc, backward, model_params, startUs, fluxFinalPoint, args, seed; \n",
    "start_eta=0.01, tol=1e-15, maxiter=400, verbose=true, report_fluxless_grad=false, do_plot=true, cost_limit=-0.00959)\n",
    "\n",
    "# And show the final position\n",
    "clf()\n",
    "costfunc(startUs; do_plot=true, verbose=true, make_dict(args, params, model_params)...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---  END --- 1d example of using fluxSense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of forwards and backwards models\n",
    "\n",
    "### Inverting time even through a sinusoidal noise, with added noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt = 0.01\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "noise = 0*randn(1, nsteps)\n",
    "noise = 0.2*sin(2*pi*3*t); noise=reshape(noise, 1, nsteps)\n",
    "W = [0.5]\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>W, :nsteps=>nsteps, \n",
    "    :noise=>noise, :noise=>noise, :const_add=>-0.15, :init_add=>0.3)\n",
    "clf();\n",
    "\n",
    "srand(10)\n",
    "\n",
    "startUs = -0.5\n",
    "Uend, Vend, U, V =forwardModel(startUs; do_plot=true, clearfig=false, model_params...)\n",
    "Ustart, Vstart = backwardsModel(Uend;  do_plot=true, clearfig=false, tol=1e-15, model_params...)\n",
    "\n",
    "[startUs Ustart]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD: scripts on the path to writing fluxSense() as a function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the cost function. It passes most keyword params down to the forward and backward models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function J(initUs; theta1=0.15, theta2=0.2, beta=0.01, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, params...)\n",
    "\n",
    "    Vend = ForwardDiffZeros(length(initUs), 1, nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:length(initUs)\n",
    "        Ue, Ve, U, V = forwardModel(initUs[i]; nderivs=nderivs, difforder=difforder, \n",
    "            do_plot=do_plot, clearfig=false, params...)\n",
    "        Vend[i] = Ve[1]\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend-0.5)/theta1))\n",
    "    diffs = tanh((Vend-0.5)/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An example of some code that does differentiation. This cell not necessary for running the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# An example of a standard setup which we'll try to modify to try to get 75% correct\n",
    "\n",
    "dt = 0.01\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "noise = 0*randn(1, nsteps)\n",
    "noise = 0.02*sin(2*pi*3*t); noise=reshape(noise, 1, nsteps)\n",
    "\n",
    "W = 4.1\n",
    "const_add = -2\n",
    "init_add=0\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>W, :nsteps=>nsteps, \n",
    ":noise=>noise, :noise=>noise, :const_add=>const_add, :init_add=>init_add)\n",
    "\n",
    "srand(10)\n",
    "startUs = randn(40, 1)\n",
    "J(startUs; do_plot=true, verbose=true, model_params...)\n",
    "\n",
    "# --- now while taking the derivative ---\n",
    "args = [\"init_add\", \"const_add\", \"W\"]\n",
    "seed = [init_add, const_add, W]\n",
    "\n",
    "ForwardDiff.gradient((x)->J(startUs; do_plot=true, nderivs=length(x), difforder=1, verbose=true, make_dict(args, x, model_params)...), seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main adaptive step with gradient and keywords loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################################\n",
    "#                                                   #\n",
    "#                                                   #\n",
    "#     ADAPTIVE GRADIENT FOR KEYWORD VERSION         #\n",
    "#                                                   #\n",
    "#                                                   #\n",
    "#####################################################\n",
    "\n",
    "\n",
    "# -----------------  FORWARD MODEL SETUP ---------------\n",
    "dt = 0.01\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "noise = 0*randn(1, nsteps)\n",
    "noise = 0.02*sin(2*pi*3*t); noise=reshape(noise, 1, nsteps)\n",
    "\n",
    "W = 4.1\n",
    "const_add = -2\n",
    "init_add=0\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>W, :nsteps=>nsteps, \n",
    ":noise=>noise, :noise=>noise, :const_add=>const_add, :init_add=>init_add)\n",
    "\n",
    "\n",
    "# ----------------  CRITICAL INDICATION OF PARAMETERS TO OPTIMIZE IS HERE: -----\n",
    "args = [\"init_add\", \"const_add\", \"W\"]\n",
    "seed = [0, -2, 4.1]\n",
    "\n",
    "fluxFinalPoint = convert(Float64, 0)  # The final value of the pinned output\n",
    "\n",
    "srand(10)  \n",
    "startUs = randn(200, 1)       # The starting values\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "clf()\n",
    "\n",
    "start_eta = 0.01\n",
    "tol = 1e-15\n",
    "maxiter = 400\n",
    "verbose = true\n",
    "do_plot=false\n",
    "\n",
    "# -------\n",
    "\n",
    "params = seed\n",
    "eta    = start_eta\n",
    "\n",
    "\n",
    "U0 = backwardsModel(fluxFinalPoint; do_plot=false, make_dict(args, params, model_params)...)[1]\n",
    "J([startUs;U0]; verbose=true, do_plot=true, make_dict(args, params, model_params)...)\n",
    "\n",
    "\n",
    "cost, grad, hess = \n",
    "    vgh((x)->J([startUs;U0]; do_plot=false, nderivs=length(x), difforder=2, make_dict(args, x, model_params)...), params)\n",
    "\n",
    "@printf(\"Initial cost, grad, hess:\\n\")\n",
    "print_vector_g(:cost)\n",
    "print_vector_g(:grad)\n",
    "print_vector_g(:hess)\n",
    "delta_params=0\n",
    "\n",
    "\n",
    "for i in 1:maxiter         \n",
    "        new_params = params - eta*grad/(sqrt(sum(grad.*grad)))\n",
    "        delta_params = new_params - params\n",
    "        print_vector_g(:delta_params)\n",
    "        new_cost, new_grad, new_hess = \n",
    "            vgh((x)->J([startUs;U0]; do_plot=false, verbose=false,\n",
    "                nderivs=length(x), difforder=2, make_dict(args, x, model_params)...), new_params)\n",
    "        @printf(\"new_cost=%g  cost=%g   delta_cost=%g\\n\", new_cost, cost, new_cost-cost)\n",
    "        if abs(new_cost - cost) < tol\n",
    "            break\n",
    "        end\n",
    "\n",
    "        if new_cost >= cost\n",
    "            eta = eta/2\n",
    "            costheta = NaN\n",
    "        else\n",
    "            eta = eta*1.2\n",
    "            costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "\n",
    "            params = new_params\n",
    "    \n",
    "            U0 = backwardsModel(fluxFinalPoint; do_plot=false, make_dict(args, params, model_params)...)[1]\n",
    "            cost, grad, hess = \n",
    "                vgh((x)->J([startUs;U0]; do_plot=do_plot, verbose=true,\n",
    "                    nderivs=length(x), difforder=2, make_dict(args, x, model_params)...), params)\n",
    "\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            @printf \"%d: eta=%g cost=%g costheta=%g ps=\" i eta cost  costheta\n",
    "            print_vector(params)\n",
    "            @printf \"\\n\"\n",
    "        end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A cell to plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# J(params; initUs=[startUs;U0], verbose=true, do_plot=true)\n",
    "clf()\n",
    "Ve = zeros(length(startUs),1)\n",
    "for i=1:length(startUs)\n",
    "    Ue, Vee, U, V = forwardModel(startUs[i]; do_plot=true, clearfig=false, make_dict(args, params, model_params)...)\n",
    "    Ve[i] = Vee[1]\n",
    "end\n",
    "\n",
    "@printf(\"\\n\\nFinal result produces %d hits out of %d trials for %.1f per cent correct\\n\\n\", length(find(Ve.>0.5)), \n",
    "    length(Ve), 100*length(find(Ve.>0.5))/length(Ve))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- END Complete keyword-driven adaptive gradient version of FluxSense minimizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete adaptive gradient version of FluxSense minimizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################################\n",
    "#                                                   #\n",
    "#                                                   #\n",
    "#     In this cell we define J(x::Vector[3])        #\n",
    "#     Not full keyword version yet.                 #\n",
    "#                                                   #\n",
    "#     Next cell has the adaptive gradient procedure #\n",
    "#                                                   #\n",
    "#     Run the third cell to see results             #\n",
    "#                                                   #\n",
    "#                                                   #\n",
    "#####################################################\n",
    "\n",
    "dt = 0.01\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "\n",
    "W = [4]\n",
    "k = -2\n",
    "init_k = 0\n",
    "\n",
    "noise = 0.2*sin(2*pi*3*t); noise = reshape(noise, 1, nsteps)\n",
    "\n",
    "mypars = Dict(:dt=>dt, :tau=>tau, :nsteps=>nsteps)\n",
    "\n",
    "srand(10)\n",
    "startUs = 2*randn(200,1)\n",
    "# for i=1:length(startUs)\n",
    "#    Uend, Vend, U, V = forwardModel(startUs[i]; noise=noise+k, W=W, do_plot=true, clearfig=false, params...)\n",
    "#end\n",
    "\n",
    "# backwardsModel([1.2*0]; do_plot=true, clearfig=false, params...)\n",
    "\n",
    "function J(x; initUs=startUs, theta1=0.15, theta2=0.2, beta=0.05, verbose=false,\n",
    "    nderivs=0, difforder=0, do_plot=true)\n",
    "    \n",
    "    Vend = ForwardDiffZeros(length(initUs), 1, nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    k = x[1]\n",
    "    W = x[2]\n",
    "    init_k = x[3]\n",
    "    \n",
    "    for i=1:length(initUs)\n",
    "        Ue, Ve, U, V = forwardModel(initUs[i]+init_k; noise=noise+k, W=[W], \n",
    "        nderivs=nderivs, difforder=difforder, do_plot=do_plot, clearfig=false, mypars...)\n",
    "        Vend[i] = Ve[1]\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend-0.5)/theta1))\n",
    "    diffs = tanh((Vend-0.5)/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "\n",
    "\n",
    "# WORKS:\n",
    "# ForwardDiff.gradient((x)->forwardModel(x[1]; noise=noise+k, W=[x[2]], \n",
    "#    do_plot=true, clearfig=true, nderivs=2, difforder=1, params...)[1][1], [-2.1, 4])\n",
    "\n",
    "ForwardDiff.gradient((x)->J(x; nderivs=3, difforder=1), [-2, 4.1, 0])\n",
    "# J([-2.1, 4])\n",
    "\n",
    "\n",
    "# ForwardDiff.derivative((x)->forwardModel(startUs[1]; noise=noise+k, W=[x], \n",
    "#    do_plot=true, clearfig=true, nderivs=1, difforder=1, params...)[1], 4.5995)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################################\n",
    "#                                                   #\n",
    "#                                                   #\n",
    "#     ADAPTIVE GRADIENT VERSION                     #\n",
    "#                                                   #\n",
    "#                                                   #\n",
    "#####################################################\n",
    "\n",
    "# This is all BEFORE makign J fully keyword-value driven\n",
    "\n",
    "seed = [-2, 4.1, 0]   # params are constant add, W, and init_add.\n",
    "start_eta = 0.01\n",
    "tol = 1e-15\n",
    "maxiter = 400\n",
    "verbose = true\n",
    "\n",
    "params = seed\n",
    "eta = start_eta\n",
    "\n",
    "U0 = backwardsModel([1.2*0]; noise=noise+params[1], W=[params[2]], do_plot=true, mypars...)[1] - params[3]\n",
    "\n",
    "J(params; initUs=[startUs;U0], verbose=true)\n",
    "\n",
    "cost, grad, hess = vgh((x)->J(x; initUs=[startUs;U0], nderivs=length(params), difforder=2, verbose=true), params)\n",
    "\n",
    "@printf(\"Initial cost, grad, hess:\\n\")\n",
    "print_vector_g(:cost)\n",
    "print_vector_g(:grad)\n",
    "print_vector_g(:hess)\n",
    "delta_params=0\n",
    "\n",
    "for i in 1:maxiter         \n",
    "        new_params = params - eta*grad/(sqrt(sum(grad.*grad)))\n",
    "        delta_params = new_params - params\n",
    "        print_vector_g(:delta_params)\n",
    "        new_cost, new_grad, new_hess = \n",
    "            vgh((x)->J(x; initUs=[startUs;U0], nderivs=length(params), difforder=2, verbose=false, do_plot=false), \n",
    "                new_params)        \n",
    "        @printf(\"new_cost=%g  cost=%g   delta_cost=%g\\n\", new_cost, cost, new_cost-cost)\n",
    "        if abs(new_cost - cost) < tol\n",
    "            break\n",
    "        end\n",
    "\n",
    "        if new_cost >= cost\n",
    "            eta = eta/2\n",
    "            costheta = NaN\n",
    "        else\n",
    "            eta = eta*1.2\n",
    "            costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "\n",
    "            params = new_params\n",
    "    \n",
    "            U0 = backwardsModel([1.2*0]; noise=noise+params[1], W=[params[2]], do_plot=false, mypars...)[1] - params[3]\n",
    "            cost, grad, hess = \n",
    "                vgh((x)->J(x; initUs=[startUs;U0], nderivs=length(params), difforder=2, verbose=true, do_plot=false), params)\n",
    "\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            @printf \"%d: eta=%g cost=%g costheta=%g ps=\" i eta cost  costheta\n",
    "            print_vector(params)\n",
    "            @printf \"\\n\"\n",
    "        end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# J(params; initUs=[startUs;U0], verbose=true, do_plot=true)\n",
    "clf()\n",
    "Ve = zeros(length(startUs),1)\n",
    "for i=1:length(startUs)\n",
    "    Ue, Vee, U, V = forwardModel(startUs[i]+params[3]; noise=params[1], W=[params[2]],     \n",
    "        do_plot=true, clearfig=false, tau=0.1, nsteps=201, dt=0.01)\n",
    "    Ve[i] = Vee[1]\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------END OF: complete adaptive gradient version of FluxSense minimizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf()\n",
    "J(params; initUs=[startUs;U0], verbose=true, do_plot=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "length(find(Ve.>0.5))/200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mypars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################################\n",
    "#                                                   #\n",
    "#                                                   #\n",
    "#     HESSIAN VERSION                               #\n",
    "#                                                   #\n",
    "#                                                   #\n",
    "#####################################################\n",
    "\n",
    "\n",
    "seed = [-2, 4.1]\n",
    "start_eta = 0.0000001\n",
    "tol = 1e-15\n",
    "maxiter = 400\n",
    "verbose = true\n",
    "\n",
    "params = seed\n",
    "eta = start_eta\n",
    "\n",
    "U0 = backwardsModel([1.2*0]; noise=noise+params[1], W=[params[2]], do_plot=true, mypars...)[1]\n",
    "\n",
    "J(params; initUs=[startUs;U0], verbose=true)\n",
    "\n",
    "cost, grad, hess = vgh((x)->J(x; initUs=[startUs;U0], nderivs=length(params), difforder=2, verbose=true), params)\n",
    "\n",
    "@printf(\"Initial cost, grad, hess:\\n\")\n",
    "print_vector_g(:cost)\n",
    "print_vector_g(:grad)\n",
    "print_vector_g(:hess)\n",
    "\n",
    "for i in 1:maxiter\n",
    "        hathess    = hess + eye(length(grad), length(grad))/eta        \n",
    "        new_params = params - inv(hathess)*grad\n",
    "        new_cost, new_grad, new_hess = \n",
    "            vgh((x)->J(x; initUs=[startUs;U0], nderivs=length(params), difforder=2, verbose=true), new_params)\n",
    "            \n",
    "        if abs(new_cost - cost) < tol\n",
    "            # break\n",
    "        end\n",
    "\n",
    "        if new_cost >= cost\n",
    "            eta = eta/2\n",
    "            costheta = NaN\n",
    "        else\n",
    "            eta = eta*1.1\n",
    "            costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "\n",
    "            params = new_params\n",
    "    \n",
    "            U0 = backwardsModel([1.2*0]; noise=noise+params[1], W=[params[2]], do_plot=false, mypars...)[1]\n",
    "            cost, grad, hess = \n",
    "                vgh((x)->J(x; initUs=[startUs;U0], nderivs=length(params), difforder=2, verbose=true), params)\n",
    "\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            @printf \"%d: eta=%g cost=%g costheta=%g ps=\" i eta cost  costheta\n",
    "            print_vector(params)\n",
    "            @printf \"\\n\"\n",
    "        end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = [-2, 4.1]\n",
    "start_eta = 10\n",
    "tol = 1e-15\n",
    "maxiter = 400\n",
    "\n",
    "params = seed\n",
    "eta = start_eta\n",
    "\n",
    "# backwardsModel([0.5]; noise=noise+params[1], W=params[2], params...)\n",
    "\n",
    "cost, grad, hess = vgh(func, params)\n",
    "\n",
    "\n",
    "    if verbose && verbose_level >= 2\n",
    "        @printf(\"Initial cost, grad, hess:\\n\")\n",
    "        print_vector_g(:cost)\n",
    "        print_vector_g(:grad)\n",
    "        print_vector_g(:hess)\n",
    "    end\n",
    "    \n",
    "\n",
    "    for i in [1:maxiter;]\n",
    "        hathess    = hess + eye(length(grad), length(grad))/eta        \n",
    "        new_params = params - inv(hathess)*grad\n",
    "        new_cost, new_grad, new_hess = vgh(func, new_params)\n",
    "            \n",
    "        if abs(new_cost - cost) < tol\n",
    "            break\n",
    "        end\n",
    "\n",
    "        if new_cost >= cost\n",
    "            eta = eta/2\n",
    "            costheta = NaN\n",
    "        else\n",
    "            eta = eta*1.1\n",
    "            costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "\n",
    "            params = new_params\n",
    "            cost = new_cost\n",
    "            grad = new_grad\n",
    "            hess = new_hess\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            @printf \"%d: eta=%.3f cost=%.4f costheta=%.3f ps=\" i eta cost  costheta\n",
    "            print_vector(params)\n",
    "            @printf \"\\n\"\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return params, cost\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trust_region_Hessian_minimization([-2, 4.1], (x)->J(x; nderivs=2, difforder=2, verbose=true), verbose=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt = 0.01\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "\n",
    "noise = 3.5*sin(2*pi*3*t); noise=reshape(noise, 1, nsteps)\n",
    "W = [0.5]\n",
    "\n",
    "params = Dict(:dt=>dt, :tau=>tau, :nsteps=>nsteps, :noise=>noise)\n",
    "\n",
    "function J(x; nderivs=0, difforder=0)\n",
    "    startU = x[1]\n",
    "    W = x[2]\n",
    "    \n",
    "    Uend, Vend, U, V = forwardModel(startU; do_plot=true, W=[W], nderivs=nderivs, difforder=difforder, params...)\n",
    "    \n",
    "    return (Vend[1]-0.5)^2\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trust_region_Hessian_minimization([-0.5, 0.5], (x) -> J(x;nderivs=2, difforder=2), verbose=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD STUFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- BEGIN --- Old example that gets stuck: too large beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The following sequence leads to a situation where having only [-0.8, -0.8] as the single finalFluxPoint \n",
    "# leads to the minimization getting stuck.  Adding further finalFluxPoints solves the problem.\n",
    "#\n",
    "# Reducing beta in the cost function J() from 0.01 to 0.003 also eliminated the problem.  \n",
    "#\n",
    "srand(10)\n",
    "startU=randn(100,2)-3\n",
    "startU=randn(100,2)-3\n",
    "\n",
    "# startU=randn(100,2)-3\n",
    "\n",
    "dt = 0.02\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "W = -4\n",
    "noise = 0\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    "    :noise=>noise, :noise=>noise, :const_add=>0, :init_add=>0)\n",
    "\n",
    "clf(); subplot(2,1,1)\n",
    "for i in 1:size(startU,1)\n",
    "    Uend, Vend, U, V = forwardModel(startU[i,:]; do_plot=true, clearfig=false, model_params...)\n",
    "end\n",
    "\n",
    "Ustarthat, Vstarthat, Uhatm, Vhat, costs = backwardsModel([-0.8, -0.8]; do_plot=true, clearfig=false, \n",
    "tol=1e-50, maxiter=800, model_params...)\n",
    "\n",
    "subplot(2,1,2)\n",
    "plot(t, costs, \".-\")\n",
    "\n",
    "\n",
    "function JJ(initUs; theta1=0.15, theta2=0.2, beta=0.01, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, pre_string=\"\", params...)\n",
    "\n",
    "    Vend = ForwardDiffZeros(size(initUs,1), size(initUs,2), nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:size(initUs,1)\n",
    "        Ue, Ve, U, V = forwardModel(initUs[i,:]; nderivs=nderivs, difforder=difforder, \n",
    "            do_plot=do_plot, clearfig=false, params...)\n",
    "        Vend[i,:] = Ve\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend[:,1]-Vend[:,2])/theta1))\n",
    "    diffs = tanh.((Vend[:,1]-Vend[:,2])/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"%s\", pre_string)\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "  \n",
    "clf();\n",
    "JJ(startU; do_plot=true, model_params...)\n",
    "\n",
    "# WORKING gradient:\n",
    "# ForwardDiff.gradient((x)->JJ(startU; do_plot=true, nderivs=length(x), difforder=1, \n",
    "#    make_dict([[\"init_add\" 2], \"const_add\"], x, model_params)...), [2.9, -2.9, 0.1])\n",
    "\n",
    "\n",
    "\n",
    "# The backward and costfunc functions should turn a single-scalar parameter W into the matrix W\n",
    "backward = (endpoint; do_plot=false, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;     \n",
    "    backwardsModel(endpoint; do_plot=do_plot, pars...)[1]\n",
    "end\n",
    "\n",
    "costfunc = (startpoints; do_plot=false, verbose=false, nderivs=0, difforder=0, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;         \n",
    "    JJ(startpoints; do_plot=do_plot, verbose=verbose, nderivs=nderivs, difforder=difforder, pars...)\n",
    "end\n",
    "\n",
    "\n",
    "fluxFinalPoint = [-0.8 -0.8]  # ; -0.6 -0.6 ; -0.4 -0.4; -0.2 -0.2; 0 0; 0.2 0.2]\n",
    "\n",
    "args = [[\"init_add\" 2], \"const_add\"] # , \"W\"]\n",
    "seed = [0.001, 0.001, 0] # , -4]\n",
    "# seed = [1.190, -1.178, 2.000]\n",
    "\n",
    "params, cost = fluxSense(costfunc, backward, model_params, startU, fluxFinalPoint, args, seed; \n",
    "start_eta=0.01, tol=1e-15, maxiter=400, verbose=true, do_plot=false, cost_limit=-0.00935) # cost_limit=-0.000935) # for beta=0.01\n",
    "\n",
    "# And show the final position\n",
    "clf()\n",
    "costfunc(startU; do_plot=true, verbose=true, make_dict(args, params, model_params)...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- END --- Old example that gets stuck: too large beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of getting stuck without a flux point even with beta=0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function JJ(initUs; theta1=0.15, theta2=0.2, beta=0.003, verbose=false, nderivs=0, difforder=0, \n",
    "    do_plot=false, pre_string=\"\", zero_last_sigmas=0, seedrand=NaN, params...)\n",
    "\n",
    "    if ~isnan(seedrand); srand(seedrand); end\n",
    "    \n",
    "    Vend = ForwardDiffZeros(size(initUs,1), size(initUs,2), nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "    if do_plot; clf(); end;\n",
    "    \n",
    "    for i=1:size(initUs,1)\n",
    "        if i>size(initUs,1) - zero_last_sigmas\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; sigma=0, nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)            \n",
    "        else\n",
    "            Ue, Ve, U, V = forwardModel(initUs[i,:]; nderivs=nderivs, difforder=difforder, \n",
    "                do_plot=do_plot, clearfig=false, params...)\n",
    "        end\n",
    "        Vend[i,:] = Ve\n",
    "    end\n",
    "    \n",
    "    hits = 0.5*(1 + tanh.((Vend[:,1]-Vend[:,2])/theta1))\n",
    "    diffs = tanh.((Vend[:,1]-Vend[:,2])/theta2).^2\n",
    "    \n",
    "    cost1 = (mean(hits) - 0.75).^2 \n",
    "    cost2 = -beta*mean(diffs)\n",
    "    \n",
    "    if verbose\n",
    "        @printf(\"%s\", pre_string)\n",
    "        @printf(\"-- cost=%g,   cost1=%g, cost2=%g :  mean(hits)=%g, mean(diffs)=%g\\n\", \n",
    "            convert(Float64, cost1+cost2), convert(Float64, cost1), convert(Float64, cost2),\n",
    "            convert(Float64, mean(hits)), convert(Float64, mean(diffs)))\n",
    "    end\n",
    "    \n",
    "    return cost1 + cost2\n",
    "end\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "srand(11)\n",
    "startU=randn(100,2)-3\n",
    "startU=randn(100,2)-3\n",
    "\n",
    "\n",
    "# startU=0.1*randn(100,2)-3\n",
    "# startU=zeros(100,2)-3\n",
    "\n",
    "\n",
    "dt = 0.005  # If we go to dt=0.02, it doesn't get stuck\n",
    "t = 0:dt:1\n",
    "tau = 0.1\n",
    "nsteps = length(t)\n",
    "t = t[1:nsteps]\n",
    "\n",
    "W = -4\n",
    "noise = 0\n",
    "input = 0\n",
    "sigma = 0.1\n",
    "\n",
    "\n",
    "model_params = Dict(:dt=>dt, :tau=>tau, :W=>[0 W; W 0], :nsteps=>nsteps, \n",
    ":noise=>noise, :input=>input, :sigma=>sigma, :const_add=>0, :init_add=>0)\n",
    "\n",
    "\n",
    "\n",
    "# The backward and costfunc functions should turn a single-scalar parameter W into the matrix W\n",
    "# backward always runs with no within-forward noise, i.e., sigma=0\n",
    "backward = (endpoint; do_plot=false, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;     \n",
    "    backwardsModel(endpoint; do_plot=do_plot, make_dict([\"sigma\"], [0], pars)...)[1]\n",
    "end\n",
    "\n",
    "\n",
    "beta=0\n",
    "\n",
    "costfunc = (startpoints; do_plot=false, verbose=false, nderivs=0, difforder=0, sr=26, pars...) -> begin\n",
    "    pars = Dict(pars)\n",
    "    if haskey(pars, :W); \n",
    "        W=pars[:W];   # mess with it only if it is not already a matrix:\n",
    "        if length(W)==1; pars=make_dict([\"W\"], [[0 W;W 0]], pars); end;\n",
    "    end;         \n",
    "    JJ(startpoints; seedrand=sr, beta=beta, \n",
    "        do_plot=do_plot, verbose=verbose, nderivs=nderivs, difforder=difforder, pars...)\n",
    "end\n",
    "\n",
    "if beta==0.003;     cost_limit = -0.00288\n",
    "elseif beta<0.001;  cost_limit = -0.0008\n",
    "elseif beta==0.001; cost_limit = -0.000935\n",
    "elseif beta==0.05;  cost_limit = -0.0485\n",
    "else\n",
    "    error(\"Don't know what cost limit goes with beta %g\\n\", beta)\n",
    "end\n",
    "\n",
    "fluxFinalPoint = zeros(0,2);\n",
    "\n",
    "args = [[\"init_add\" 2], \"const_add\", \"W\"]\n",
    "\n",
    "seed = [0.001, 0.001, 0, -4]\n",
    "\n",
    "\n",
    "# Alternatively, start right from the sticking point:\n",
    "seed = [4.74063,  -4.68228,  2.73165,  -5.6783]\n",
    "\n",
    "# Walls are big enough that we never hit them, so it is immaterial:\n",
    "bbox = [\n",
    "    -15        15  ;\n",
    "    -15        15  ;\n",
    "    -15        15  ;\n",
    "    -20.5  20.5  ; \n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clf()\n",
    "costfunc(startU; do_plot=true, verbose=true, make_dict(args, seed, model_params)...)\n",
    "\n",
    "\n",
    "# YOU CAN DO EITHER THIS:\n",
    "params, traj = bbox_Hessian_keyword_minimization(seed, args, bbox, \n",
    "(;params...) -> costfunc(startU; verbose=true, merge(model_params, Dict(params))...), \n",
    "verbose=true, start_eta=0.01, tol=1e-10, hardbox=true )\n",
    "\n",
    "# OR THIS:  (both get stuck)\n",
    "# fluxFinalPoint = [-0.1 -0.1]\n",
    "# params, cost, ptraj, gtraj = fluxSense(costfunc, backward, model_params, startU, fluxFinalPoint, args, seed; \n",
    "# start_eta=0.01, tol=1e-15, maxiter=400, verbose=true, report_every=1, do_plot=true, cost_limit=cost_limit) # cost_limit=-0.000935) # for beta=0.01\n",
    "\n",
    "# And show the final position\n",
    "clf()\n",
    "costfunc(startU; do_plot=true, verbose=true, make_dict(args, params, model_params)...)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.2",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
