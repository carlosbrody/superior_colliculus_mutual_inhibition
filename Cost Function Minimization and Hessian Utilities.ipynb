{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utility functions related to cost function minimizations**\n",
    "\n",
    "The main function defined here to be heavily used elsewhere is **bbox_hessian_keyword_minimization()**.\n",
    "\n",
    "Important functions needed to work with bbox_hessian_keyword_minimization() and also defined here are\n",
    "\n",
    "* **ForwarDiffZeros()**.  If you're going to declare vectors or matrices inside a function that you want to take derivatives of, *do not* use zeros(). Use ForwardDiffZeros() instead. If you dont' declare vectors or matrices, you won't need it. Be careful. Failing to use ForwardDiffZeros() does not produce an error: your derivatives and hessians simply come back as zeros, and if you don't think of this issue, the reason can feel very mysterious.\n",
    "\n",
    "* **make_dict()**. If you're working with keyword-value pairs, you will want to manipulate sets of those. make_dict() is a function that helps you do that, for example, for merging current selected parameter values with a superset of default values (see below).  In addition, make_dict() is central to the gradient- and hessian-taking functions defined here that operate on keyword-value pairs. The reason is that make_dict() can take a vector (as one of its parameters)and turn it into keyword-value pairs, and we need this transformation to work with the ForwardDiff package, since that package only operates on functions of vectors.\n",
    "\n",
    "Two other functions often used are **vgh()** (a wrapper to fund the value, gradient, and hessian of a scalar function of a vector) and **keyword_vgh()** (a wrapper like vgh() except it operates on functions that take only keyword-value pairs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">TABLE OF CONTENTS</h1>\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "**Updates to the table of contents are periodic, but run the cell below to first start or force an update.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('make_table_of_contents.js')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition @javascript_str(ANY<:Any) in module Main at In[5]:1 overwritten at In[6]:1.\n"
     ]
    }
   ],
   "source": [
    "macro javascript_str(s) display(\"text/javascript\", s); end\n",
    "\n",
    "javascript\"\"\"\n",
    "$.getScript('make_table_of_contents.js')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make_dict() and ForwardDiffZeros()\n",
    "\n",
    "The ForwardDiff package takes gradients of functions of vectors. `make_dict()` was originally written as an internal function to keyword_vgh(), essentially for turning keyword-value pairs into a vector that ForwardDiff could work with. The main goal was to make gradient-taking flexible, so the user could easily switch between different chosen parameters. However, make_dict() turns out to be useful externally, for the user, also.\n",
    "\n",
    "The basic usage of make_dict is to take a list of strings, and a vector of numeric values of the same length, and turn those into a dictionary that Julia can use when passing paramaters. Thus, for example,\n",
    "\n",
    "> `tester(;a=10, b=20)`\n",
    "\n",
    "is equivalent to\n",
    "\n",
    "\n",
    "> `tester(;make_dict([\"a\", \"b\"], [10, 20])...)`\n",
    "\n",
    "which can be used to pass all the various desired parameter values as a single vector, which is what ForwardDiff needs, and is how keyword_vgh() works.\n",
    "\n",
    "> (**An aside on Julia symbols and passing sets of keyword-values to functions**: The `...` is Julia-speak for \"this argument contains a set of multiple keyword-value pairs.\"  In Julia, that can be either a dictionary of Symbol=>value pairs, or a list of (Symbol, value) tuples). \n",
    "\n",
    "> `make_dict()` itself returns a dictionary, so `make_dict([\"a\", \"b\"], [10, 20])` returns `Dict(:a=>10, :b=>20)`.  A Julia Symbol stands for a variable; you can go back and forth between strings and Symbols by using, for example, `Symbol(\"a\")` to get `:a`, or use `string(:a)` to get `\"a\"`.)\n",
    "\n",
    "\n",
    "## Typical user-level usage of make_dict()\n",
    "\n",
    "The typical thing a user will use make_dict() for is to merge paramater values with a superset of default parameter values. For example, suppose you have defined a scalar function\n",
    "\n",
    "> `function tester(;a=1, b=2, c=3, d=4)`\n",
    "\n",
    "You can decide you want your default parameter values to be as defined here:\n",
    "\n",
    "> `defaults = Dict(:a=>10, :b=20, :c=>30, :d=>40)`\n",
    "\n",
    "Given that, you can call `tester()` with this set of values by calling `tester(;defaults...)`.\n",
    "\n",
    "\n",
    "Now suppose you've done a minimization search over two of these paramaters. Let's say that you indicate your choice of those parameters in `args = [\"a\", \"c\"]`. And let's say the resulting values for them are in the two-long vector `pars`.  You want to call `tester()` with the default parameter values _except_ for whatever is indicated in `args` and `pars`. To do that, you use the optional third argument of `make_dict()` as follows:\n",
    "\n",
    "> `tester(;make_dict(args, pars, defaults)...)`\n",
    "\n",
    "\n",
    "See the examples below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition make_dict(Any, Any) in module Main at In[2]:46 overwritten at In[42]:46.\n",
      "WARNING: Method definition make_dict(Any, Any, Any) in module Main at In[2]:46 overwritten at In[42]:46.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'make_dict :: Union{Tuple{Any,Any,Any},Tuple{Any,Any}}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition convert(Type{Float64}, ForwardDiff.Dual) in module Main at In[2]:96 overwritten at In[42]:96.\n",
      "WARNING: Method definition convert(Array{Float64, N<:Any}, Array{ForwardDiff.Dual, N<:Any}) in module Main at In[2]:98 overwritten at In[42]:98.\n",
      "WARNING: Method definition ForwardDiffZeros(Any, Any) in module Main at In[2]:138 overwritten at In[42]:138.\n",
      "WARNING: Method definition #ForwardDiffZeros(Array{Any, 1}, Main.#ForwardDiffZeros, Any, Any) in module Main overwritten.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'ForwardDiffZeros :: Tuple{Any,Any}' in module 'Main'.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ForwardDiffZeros"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@include_me hessian_utils.jl\n",
    "        \n",
    "\"\"\"\n",
    "dict = make_dict(argstrings, x, [starting_dict=Dict()] )\n",
    "\n",
    "Given a list of strings, and a list of values, makes a dictionary of Symbols to values, with the Symbols \n",
    "corresponding to each of the strings.  Mostly used to pass arguments as a keyword-value set into a function.\n",
    "If one of the elements of argstrings is *not* a string, but is instead a 2-long list, the first element of that \n",
    "list should be a string, and the second element of that list should be a positive integer. This will be \n",
    "interpreted as \"don't take only one value, take this number of values and this parameter will be a vector\"\n",
    "\n",
    "# PARAMS:\n",
    "\n",
    "* argstrings     A list of strings. Each element may also be a two-long list of a string, positive integer, e.g., [\"this\" 3]\n",
    "\n",
    "* x              A vector of numeric values. Its length must be such that all the strings in argstrings\n",
    "                 can take their corresponding element(s), sequentially, from x\n",
    "\n",
    "* starting_dict  An optional initial dictionary to work with.  Any key in this starting dictionary matching an argstring\n",
    "                 will be replaced by the new value. Keys not matched will remain.\n",
    "\n",
    "# RETURNS:\n",
    "\n",
    "dict             The symbol dictionary.\n",
    "\n",
    "\n",
    "# EXAMPLES:\n",
    "\n",
    ">> make_dict([\"this\", \"that\", [\"there\", 2]], [10, 20, 3, 4])\n",
    "\n",
    "Dict{Any,Any} with 3 entries:\n",
    "  :this  => 10\n",
    "  :that  => 20\n",
    "  :there => [3,4]\n",
    "\n",
    ">> make_dict([\"doo\", \"gaa\"], [10, 20], Dict(:blob=>100, :gaa=>-44))\n",
    "\n",
    "Dict{Symbol,Int64} with 3 entries:\n",
    "  :gaa  => 20\n",
    "  :blob => 100\n",
    "  :doo  => 10\n",
    "\n",
    "\"\"\"\n",
    "function make_dict(args, x, starting_dict=Dict())\n",
    "    # For error diagnostics, check that the length of the param vector specified in args matches the length of x\n",
    "    nargs = 0\n",
    "    for i in [1:length(args);]\n",
    "        if typeof(args[i])==String # if the entry in args is a string, then there's one corresponding scalar entry in x0\n",
    "            nargs += 1\n",
    "        else\n",
    "            nargs += args[i][2]    # otherwise, the entry in args should be a  [varnamestring, nvals] vector, \n",
    "            # indicating that the next nvals entries in x0 are all a single vector, belonging to variable\n",
    "            # with name varnamestring. \n",
    "        end\n",
    "    end\n",
    "    if nargs != length(x)\n",
    "        error(\"Oy! args and x must indicate the same total number of variables!\")\n",
    "    end\n",
    "\n",
    "    \n",
    "    # ---- done error-checking, now main function\n",
    "    \n",
    "    kwargs = starting_dict;\n",
    "    i = 1; j=1\n",
    "    while i<=length(args)\n",
    "        if typeof(args[i])==String\n",
    "            kwargs = merge(kwargs, Dict(Symbol(args[i]) => x[j]))\n",
    "        else\n",
    "            if length(args[i]) == 2\n",
    "                extra = args[i][2]-1\n",
    "                kwargs = merge(kwargs, Dict(Symbol(args[i][1]) => x[j:(j+extra)]))\n",
    "                j = j+extra\n",
    "            else\n",
    "                error(\"Each element of the args vector must be either a string, or a 2-long vector, first element a string, second integer\")\n",
    "            end            \n",
    "        end\n",
    "        i = i+1; j=j+1\n",
    "    end\n",
    "    return kwargs\n",
    "end \n",
    "\n",
    "\n",
    "using ForwardDiff\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# We define functions to convert Duals, the variable types used by ForwardDiff, \n",
    "# to Floats. This is useful if we want to print out the value of a variable \n",
    "# (since print doesn't know how to Duals). Note that after being converted to a Float, no\n",
    "# differentiation by ForwardDiff can happen!  e.g. after\n",
    "#     x = convert(Float64, y)\n",
    "# ForwardDiff can still differentiate y, but it can't differentiate x\n",
    "# \"\"\"\n",
    "\n",
    "import Base.convert\n",
    "convert(::Type{Float64}, x::ForwardDiff.Dual) = Float64(x.value)\n",
    "function convert(::Array{Float64}, x::Array{ForwardDiff.Dual}) \n",
    "    y = zeros(size(x)); \n",
    "    for i in 1:prod(size(x)) \n",
    "        y[i] = convert(Float64, x[i]) \n",
    "    end\n",
    "    return y\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "function M = ForwardDiffZeros(m, n; nderivs=0, difforder=0)\n",
    "\n",
    "Use instead of zeros(). Creates a matrix of zeros, of size m rows by n columns, with elements appropriate for \n",
    "differentiation by ForwardDiff. If nderivs==0 or difforder==0 then the elements will be regular\n",
    "Float64, not ForwardDiff types.\n",
    "\n",
    "PARAMETERS:\n",
    "===========\n",
    "\n",
    "m        Integer, number of rows\n",
    "\n",
    "n        Integer, number of columns\n",
    "\n",
    "\n",
    "OPTIONAL PARAMETERS:\n",
    "====================\n",
    "\n",
    "nderivs=0       The number of variables that we'll be differentiating with respect to. In other\n",
    "                words, this number is equal to the length of the gradient. If this is left as zero (the default) then \n",
    "                the data type will be regular Float64\n",
    "\n",
    "difforder=0     The order of the derivative we will want to take.  Zero means nothing, stick with\n",
    "                regular Float64, 1 means gradient, 2 means hessian\n",
    "\n",
    "RETURNS:\n",
    "========\n",
    "\n",
    "An m-by-n matrix of zeros that can be used with Forward Diff.\n",
    "\n",
    "\"\"\"\n",
    "function ForwardDiffZeros(m, n; nderivs=0, difforder=0)\n",
    "    if nderivs == 0 || difforder == 0\n",
    "        return zeros(m, n)\n",
    "    elseif difforder == 1\n",
    "        return zeros(ForwardDiff.Dual{nderivs, Float64}, m , n)\n",
    "    elseif difforder == 2\n",
    "        return zeros(ForwardDiff.Dual{nderivs, ForwardDiff.Dual{nderivs, Float64}}, m, n)\n",
    "    else\n",
    "        error(\"Don't know how to do that order of derivatives!\", nderivs)\n",
    "    end\n",
    "end\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of using make_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tester():\n",
      "a=1\n",
      "b=2\n",
      "c=3\n",
      "d=4\n",
      "\n",
      "tester(;defaults...):\n",
      "a=10\n",
      "b=20\n",
      "c=30\n",
      "d=40\n",
      "\n",
      "make_dict([\"a\", \"c\"], [-5, -6]):\n",
      "Dict{Any,Any}(Pair{Any,Any}(:c,-6),Pair{Any,Any}(:a,-5))\n",
      "\n",
      "tester(;make_dict([\"a\", \"c\"], [-5, -6])...) changes a and c but doesn't use what is in defaults:\n",
      "a=-5\n",
      "b=2\n",
      "c=-6\n",
      "d=4\n",
      "\n",
      "tester(;make_dict([\"a\", \"c\"], [-5, -6], defaults)...) changes a and c and *does* use what is in defaults:\n",
      "a=-5\n",
      "b=20\n",
      "c=-6\n",
      "d=40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "function tester(;a=1, b=2, c=3, d=4)\n",
    "    @printf(\"a=%g\\n\", a)\n",
    "    @printf(\"b=%g\\n\", b)\n",
    "    @printf(\"c=%g\\n\", c)\n",
    "    @printf(\"d=%g\\n\", d)\n",
    "end\n",
    "\n",
    "defaults = Dict(:a=>10, :b=>20, :c=>30, :d=>40)\n",
    "\n",
    "@printf(\"tester():\\n\")\n",
    "tester()\n",
    "@printf(\"\\n\")\n",
    "\n",
    "@printf(\"tester(;defaults...):\\n\")\n",
    "tester(;defaults...)\n",
    "@printf(\"\\n\")\n",
    "\n",
    "@printf(\"make_dict([\\\"a\\\", \\\"c\\\"], [-5, -6]):\\n\")\n",
    "print(make_dict([\"a\", \"c\"], [-5, -6]))\n",
    "@printf(\"\\n\\n\")\n",
    "\n",
    "@printf(\"tester(;make_dict([\\\"a\\\", \\\"c\\\"], [-5, -6])...) changes a and c but doesn't use what is in defaults:\\n\")\n",
    "tester(;make_dict([\"a\", \"c\"], [-5, -6])...)\n",
    "@printf(\"\\n\")\n",
    "\n",
    "@printf(\"tester(;make_dict([\\\"a\\\", \\\"c\\\"], [-5, -6], defaults)...) changes a and c and *does* use what is in defaults:\\n\")\n",
    "tester(;make_dict([\"a\", \"c\"], [-5, -6], defaults)...)\n",
    "@printf(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vgh() and keyword_vgh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition vgh(Any, Any) in module Main at In[4]:21 overwritten at In[43]:21.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'vgh :: Tuple{Any,Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition keyword_vgh(Any, Any, Any) in module Main at In[4]:73 overwritten at In[43]:73.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'keyword_vgh :: Tuple{Any,Any,Any}' in module 'Main'.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "keyword_vgh"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@include_me hessian_utils.jl\n",
    "\n",
    "using ForwardDiff\n",
    "\n",
    "\"\"\"\n",
    "function value, gradient, hessian = vgh(func, x0)\n",
    "\n",
    "Wrapper for ForwardDiff.hessian!() that computes and returns all three of a function's value, gradient, and hessian.\n",
    "\n",
    "EXAMPLE:\n",
    "========\n",
    "\n",
    "function tester(x::Vector)\n",
    "\n",
    "    return sum(x.*x)\n",
    "end\n",
    "\n",
    "value, grad, hess = vgh(tester, [10, 3.1])\n",
    "\"\"\"\n",
    "function vgh(func, x0)\n",
    "    out = DiffBase.HessianResult(x0)             \n",
    "    ForwardDiff.hessian!(out, func, x0)\n",
    "    value    = DiffBase.value(out)\n",
    "    gradient = DiffBase.gradient(out)\n",
    "    hessian  = DiffBase.hessian(out)\n",
    "    \n",
    "    return value, gradient, hessian    \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "function value, gradient, hessian = keyword_vgh(func, args, x0)\n",
    "\n",
    "Wrapper for vgh() that computes and returns all three of a function's value, gradient, and hessian, but now\n",
    "uses make_dict() to apply it to a function that only takes keyword-value pairs. \n",
    "\n",
    "*Note that func MUST also take the keyword parameters nderivs and difforder*. If you declare any vectors or \n",
    "matrices inside func() (or inside any function inside func()), use ForwardDiffZeros with these two parameters, \n",
    "do NOT use zeros(). Your gradients will come out as zero is you use zeros().\n",
    "\n",
    "# PARAMETERS\n",
    "\n",
    "* func    A function that takes keyword-value pairs only, including nderivs and difforder.  I.e., it must be a function declared as `function func(; nderivs=0, difforder=0, other_kw_value_pairs)` or as `function func(; nderivs=0, difforder=0, other_kw_value_pairs_dict...)`\n",
    "* args    A list of strings indicating names of variables to work with\n",
    "* x0      A vector with the value of the variables indicates in args.  **See make_dict() for how to pass both scalars and vectors as variables**\n",
    "\n",
    "# IMPORTANT JULIA BUG\n",
    "\n",
    "If you modify func, it is possible that keyword_vgh() will still work on the previously defined version. AACK!  \n",
    "That's horrible! Alice Yoon's tip on the workaround: instead of func(), use (;params...) -> func(; params...) and then\n",
    "everything will be fine. Perhaps this bug will be fixed in Julia 0.6\n",
    "\n",
    "# EXAMPLE:\n",
    "\n",
    "function tester(;a=10, b=20, c=30, nderivs=0, difforder=0)\n",
    "    M = ForwardDiffZeros(3, 3; nderivs=nderivs, difforder=difforder)\n",
    "    M[1,1] = a^2*10\n",
    "    M[2,2] = b*20\n",
    "    M[3,3] = a*sqrt(c)*30.1\n",
    "    return trace(M)\n",
    "end\n",
    "\n",
    "value, grad, hess = keyword_vgh(tester, [\"a\", \"c\"], [10, 3.1])\n",
    "\n",
    "value, grad, hess = keyword_vgh((;params...) -> tester(;params...), [\"a\", \"c\"], [10, 3.1])\n",
    "\n",
    "\"\"\"\n",
    "function keyword_vgh(func, args, x0)\n",
    "\n",
    "    value, gradient, hessian = vgh(x -> func(;nderivs=length(x), difforder=2, make_dict(args, x)...), x0)\n",
    "\n",
    "    return value, gradient, hessian    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### examples using vgh() and keyword_vgh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vgh()\n",
      "value=109.61, grad=[20.0,6.2], hess[:]=[2.0,0.0,0.0,2.0]\n",
      "503.6298243750066\n",
      "\n",
      "-------\n",
      "\n",
      "Using keyword_vgh()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition tester(Array{T<:Any, 1}) in module Main at In[25]:3 overwritten at In[26]:3.\n",
      "WARNING: Method definition tester2() in module Main at In[25]:14 overwritten at In[26]:14.\n",
      "WARNING: Method definition #tester2(Array{Any, 1}, Main.#tester2) in module Main overwritten.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value=500.382, grad=[36.0475,0.120908,0.00299771,0.00439664], hess[:]=[8.20239,0.0154713,-0.000215955,-0.000316734,0.0154713,-2.9215e-5,-7.24338e-7,-1.06236e-6,-0.000215955,-7.24338e-7,0.00199846,-2.63396e-8,-0.000316734,-1.06236e-6,-2.63396e-8,0.00199843]\n"
     ]
    }
   ],
   "source": [
    "function tester(x::Vector)\n",
    "\n",
    "    return sum(x.*x)\n",
    "end\n",
    "\n",
    "value, grad, hess = vgh(tester, [10, 3.1])\n",
    "\n",
    "@printf(\"Using vgh()\\n\")\n",
    "@printf(\"value=%g, grad=\", value); print(grad[:]); @printf(\", hess[:]=\"); print(hess[:]); print(\"\\n\");\n",
    "\n",
    "# -------------\n",
    "\n",
    "function tester2(;a=10, b=20, c=30, d=[2, 3], nderivs=0, difforder=0)\n",
    "    M = ForwardDiffZeros(3, 3; nderivs=nderivs, difforder=difforder)\n",
    "    M[1,1] = a^2*3\n",
    "    M[1,2:3] = d\n",
    "    M[2,2] = b*20\n",
    "    M[3,3] = a*sqrt(c)*1.1\n",
    "    return sqrt(sum(M[:].*M[:]))\n",
    "end\n",
    "\n",
    "print(tester2())\n",
    "\n",
    "value, grad, hess = keyword_vgh((;params...) -> tester2(;params...), [\"a\", \"c\", [\"d\" 2]], [10, 3.1, 1.5, 2.2])\n",
    "\n",
    "@printf(\"\\n\\n-------\\n\\nUsing keyword_vgh()\\n\")\n",
    "@printf(\"value=%g, grad=\", value); print(grad[:]); @printf(\", hess[:]=\"); print(hess[:]); print(\"\\n\");\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# various other keyword and gradient functions, in practice largely unused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#@include_me hessian_utils.jl\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "#                                                                           #\n",
    "#      OLD AND LARGELY UNUSED KEYWORD GRADIENTS AND HESSIANS                #\n",
    "#                                                                           #\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "function keyword_gradient(func, args, x0)\n",
    "\n",
    "Same as ForwardDiff.gradient except that func() must be a function taking only optional \n",
    "keyword arguments, and the derivative is taken with respect to an arbitrarily chosen set of \n",
    "those, indicated by a list of strings.\n",
    "\n",
    "In addition, func *MUST* take optional keyword args nderivs=0 and difforder=0, and within it,\n",
    "if matrices or vectors of zeros are declared, use ForwardDiffZeros() instead of zeros().\n",
    "\n",
    "PARAMETERS:\n",
    "===========\n",
    "\n",
    "func        A scalar function taking only optional keyword arguments, including nderivs=0 and difforder=0\n",
    "\n",
    "args        A list of strings indicating which keyword arguments to differentiate. These strings must\n",
    "            match the keyword names in func()   For example, func(;this=10, that=20) would mean that \n",
    "            \"this\" and \"that\" are allowable elements in args.\n",
    "\n",
    "x0          A vector of floats, same length as args, representing the values of these args at which the\n",
    "            derivatives will be taken.\n",
    "\n",
    "RETURNS:\n",
    "========\n",
    "\n",
    "grad        The gradient of func w.r.t. args\n",
    "\n",
    "\n",
    "EXAMPLE:\n",
    "========\n",
    "\n",
    "function tester(;a=10, b=20, c=30, nderivs=0, difforder=0)\n",
    "    M = ForwardDiffZeros(3, 3; nderivs=nderivs, difforder=difforder)\n",
    "    M[1,1] = a^2*10\n",
    "    M[2,2] = b*20\n",
    "    M[3,3] = a*sqrt(c)*30.1\n",
    "    return trace(M)\n",
    "end\n",
    "\n",
    "grad_a_c = keyword_gradient((;pars...) -> tester(;pars...), [\"a\", \"c\"], [10, 3.1])  # note initial values must be floats\n",
    "\n",
    "grad_b_c = keyword_gradient((;pars...) -> tester(;pars...), [\"b\", \"c\"], [10, 3.1]) \n",
    "\n",
    "\"\"\"\n",
    "function keyword_gradient(func, args, x0)\n",
    "    \n",
    "    ans = ForwardDiff.gradient(x -> func(;nderivs=length(x0), difforder=1, make_dict(args, x)...), x0)\n",
    "    \n",
    "    return ans\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "function keyword_gradient!(out, func, args, x0)\n",
    "\n",
    "Same as keyword_gradient, but puts the result in mutable out. See keyword_gradient() for documentation.\n",
    "\n",
    "EXAMPLE:\n",
    "========\n",
    "\n",
    "function tester(;a=10, b=20, c=30, nderivs=0, difforder=0)\n",
    "    M = ForwardDiffZeros(3, 3; nderivs=nderivs, difforder=difforder)\n",
    "    M[1,1] = a^2*10\n",
    "    M[2,2] = b*20\n",
    "    M[3,3] = a*sqrt(c)*30.1\n",
    "    return trace(M)\n",
    "end\n",
    "\n",
    "out = DiffBase.GradientResult([10, 3.1])  # out must be same length as whatever we will differentiate w.r.t.\n",
    "keyword_gradient!(out, (;pars...) -> tester(;pars...), [\"a\", \"c\"], [10, 3.1])  # note initial values must be floats\n",
    "grad_a_c = DiffBase.gradient(out)\n",
    "value    = DiffBase.value(out)\n",
    "\n",
    "out = DiffBase.GradientResult([10, 3.1, 20])  # out must be same length as whatever we will differentiate w.r.t.\n",
    "keyword_gradient!(out, (;pars...) -> tester(;pars...), [\"a\", \"b\", \"c\"], [10, 20, 3.1])  # note initial values must be floats\n",
    "grad_a_b_c = DiffBase.gradient(out)\n",
    "\n",
    "\"\"\"\n",
    "function keyword_gradient!(out, func, args, x0)\n",
    "\n",
    "    if length(args) != length(x0)\n",
    "        error(\"Oy! args and x0 must be the same length!\")\n",
    "    end\n",
    "\n",
    "    ForwardDiff.gradient!(out, x -> func(;nderivs=length(x0), difforder=1, make_dict(args, x)...), x0)\n",
    "    \n",
    "    return \n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "function keyword_hessian(func, args, x0)\n",
    "\n",
    "Same as ForwardDiff.hessian except that func() must be a function taking only optional \n",
    "keyword arguments, and the derivative is taken with respect to an arbitrarily chosen set of \n",
    "those, indicated by a list of strings.\n",
    "\n",
    "In addition, func *MUST* take optional keyword args nderivs=0 and difforder=0, and within it,\n",
    "if matrices or vectors of zeros are declared, use ForwardDiffZeros() instead of zeros().\n",
    "\n",
    "PARAMETERS:\n",
    "===========\n",
    "\n",
    "func        A scalar function taking only optional keyword arguments, including nderivs=0 and difforder=0\n",
    "\n",
    "args        A list of strings indicating which keyword arguments to differentiate. These strings must\n",
    "            match the keyword names in func()   For example, func(;this=10, that=20) would mean that \n",
    "            \"this\" and \"that\" are allowable elements in args.\n",
    "\n",
    "x0          A vector of floats, same length as args, representing the values of these args at which the\n",
    "            derivatives will be taken.\n",
    "\n",
    "RETURNS:\n",
    "========\n",
    "\n",
    "grad        The gradient of func w.r.t. args\n",
    "\n",
    "\n",
    "EXAMPLE:\n",
    "========\n",
    "\n",
    "function tester(;a=10, b=20, c=30, nderivs=0, difforder=0)\n",
    "    M = ForwardDiffZeros(3, 3; nderivs=nderivs, difforder=difforder)\n",
    "    M[1,1] = a^2*10\n",
    "    M[2,2] = b*20\n",
    "    M[3,3] = a*sqrt(c)*30.1\n",
    "    return trace(M)\n",
    "end\n",
    "\n",
    "hess_b_c = keyword_hessian((;pars...) -> tester(;pars...), [\"b\", \"c\"], [10, 3.1])  # note initial values must be floats\n",
    "\n",
    "hess_a_b_c = keyword_hessian((;pars...) -> tester(;pars...), [\"a\", \"b\", c\"], [10, 2, 3.1]) \n",
    "\n",
    "\"\"\"\n",
    "function keyword_hessian(func, args, x0)\n",
    "\n",
    "    if length(args) != length(x0)\n",
    "        error(\"Oy! args and x0 must be the same length!\")\n",
    "    end\n",
    "\n",
    "    ans = ForwardDiff.hessian(x -> func(;nderivs=length(x0), difforder=2, make_dict(args, x)...), x0)\n",
    "    \n",
    "    return ans\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "function keyword_hessian!(out, func, args, x0)\n",
    "\n",
    "Same as keyword_hessian, but puts the result in mutable out. See keyword_hessian() for documentation.\n",
    "\n",
    "EXAMPLE:\n",
    "========\n",
    "\n",
    "function tester(;a=10, b=20, c=30, nderivs=0, difforder=0)\n",
    "    M = ForwardDiffZeros(3, 3; nderivs=nderivs, difforder=difforder)\n",
    "    M[1,1] = a^2*10\n",
    "    M[2,2] = b*20\n",
    "    M[3,3] = a*sqrt(c)*30.1\n",
    "    return trace(M)\n",
    "end\n",
    "\n",
    "out = DiffBase.HessianResult([10, 3.1])  # out must be same length as whatever we will differentiate w.r.t.\n",
    "keyword_hessian!(out, (;pars...) -> tester(;pars...), [\"a\", \"c\"], [10, 3.1])  # note initial values must be floats\n",
    "hess_a_c = DiffBase.hessian(out)\n",
    "grad_a_c = DiffBase.gradient(out)\n",
    "value    = DiffBase.value(out)\n",
    "\n",
    "out = DiffBase.HessianResult([10, 3.1, 20])  # out must be same length as whatever we will differentiate w.r.t.\n",
    "keyword_hessian!(out, (;pars...) -> tester(;pars...), [\"a\", \"b\", \"c\"], [10, 20, 3.1])  # note initial values must be floats\n",
    "hess_a_b_c = DiffBase.hessian(out)\n",
    "\n",
    "\"\"\"\n",
    "function keyword_hessian!(out, func, args, x0)\n",
    "    nargs = 0\n",
    "    for i in [1:length(args);]\n",
    "        if typeof(args[i])==String\n",
    "            nargs += 1\n",
    "        else\n",
    "            nargs += args[i][2]\n",
    "        end\n",
    "    end\n",
    "    if nargs != length(x0)\n",
    "        error(\"Oy! args and x0 must be the same length!\")\n",
    "    end\n",
    "\n",
    "    ForwardDiff.hessian!(out, x -> func(;nderivs=length(x0), difforder=2, make_dict(args, x)...), x0)\n",
    "    \n",
    "    return \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Various utilities for taking gradients and for minimizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#@include_me hessian_utils.jl\n",
    "\n",
    "\"\"\"\n",
    "function constrained_Hessian_minimization(seed, func; start_eta=10, tol=1e-6, maxiter=400,\n",
    "    verbose=false)\n",
    "\n",
    "LARGELY SUPERSEDED BY BBOX_HESSIAN_KEYWORD_MINIMIZATION\n",
    "\n",
    "Minimizes a function using. At each step, it computes the Hessian approximation to the function, \n",
    "and then asks, according to the corresponding parabolic approximation: is the global minimum within a\n",
    "radius eta? If so, try to jump to it. If not, try to jump to the point at a distance eta from the \n",
    "current point that would minimize the function.  If the attempted jump leads to an increase in the function,\n",
    "then the jump is rejected and eta is reduced by a factor of 2. If the attempted jump reduces the function,\n",
    "then it is accepted and eta is increased by a factor of 1.1.  This proceeds until the change in the function\n",
    "after a proposed jump would be less than tol, or the iteration number has reached maxiter, whichever happens\n",
    "first.  Returns the minimizing parameters.\n",
    "\n",
    "PARAMETERS:\n",
    "===========\n",
    "\n",
    "seed        column vector, representing the starting value of the parameters.\n",
    "\n",
    "func        Function that takes a vector and returns a scalar.  If you want to\n",
    "            work with a function that tales mpre parameterrs and returns more than one \n",
    "            output, you can use something like\n",
    "\n",
    "                    x -> orig_func(x, other_params)[1]\n",
    "\n",
    "            You only need the \"[1]\" part of the orig_func returns more outputs than a scalar. \n",
    "\n",
    "OPTIONAL PARAMETERS:\n",
    "====================\n",
    "\n",
    "start_eta=10 Starting value of the radius.  It's good to start with somethibg biggish, if it is\n",
    "             too much, it'll quickly get cut down.\n",
    "\n",
    "tol=1e-6     Numerical tolerance. If a proposed jump produces a change in func that is less than\n",
    "             this, the minimization stops.\n",
    "\n",
    "maxiter=400  Maximum number of iterations to do before stopping\n",
    "\n",
    "verbose=false   If true, print out a report on each iteration of iteration number, radius size (eta),\n",
    "                what type jump was proposed (\"Newton\" means going straight to global min, \"constrained\" means jump has \n",
    "                norm eta, failed means that finding the minimum at a given radius somehow didn't work). Will also\n",
    "                print out the cosine of the angle between the proposed jump and the gradient.\n",
    "\n",
    "RETURNS:\n",
    "========\n",
    "\n",
    "params       A vector the size of seed that has the last values of the minimizing parameters for func\n",
    "\n",
    "\"\"\"\n",
    "function constrained_Hessian_minimization(seed, func; start_eta=10, tol=1e-6, maxiter=400,\n",
    "    verbose=false)\n",
    "\n",
    "    params = seed\n",
    "    eta = start_eta\n",
    "\n",
    "    out = DiffBase.HessianResult(params)\n",
    "    ForwardDiff.hessian!(out, func, params)\n",
    "    cost = DiffBase.value(out)\n",
    "    grad = DiffBase.gradient(out)\n",
    "    hess = DiffBase.hessian(out)\n",
    "\n",
    "    chessdelta = zeros(size(params))\n",
    "\n",
    "    for i in [1:maxiter;]\n",
    "        hessdelta  = - inv(hess)*grad\n",
    "        try\n",
    "            chessdelta = constrained_parabolic_minimization(hess, grad'', eta)[1]\n",
    "            jumptype = \"not failed\"\n",
    "        catch\n",
    "            jumptype = \"failed\"\n",
    "        end\n",
    "\n",
    "        if norm(hessdelta) <= eta\n",
    "            new_params = params + hessdelta\n",
    "            jumptype = \"Newton\"\n",
    "        elseif jumptype != \"failed\" \n",
    "            new_params = params + chessdelta\n",
    "            jumptype  = \"constrained\"\n",
    "        end\n",
    "\n",
    "        if jumptype != \"failed\"\n",
    "            ForwardDiff.hessian!(out, func, new_params)\n",
    "            new_cost = DiffBase.value(out)\n",
    "            new_grad = DiffBase.gradient(out)\n",
    "            new_hess = DiffBase.hessian(out)\n",
    "            \n",
    "            if abs(new_cost - cost) < tol\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "\n",
    "        if jumptype == \"failed\" || new_cost >= cost\n",
    "            eta = eta/2\n",
    "            costheta = NaN\n",
    "        else\n",
    "            eta = eta*1.1\n",
    "            costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "\n",
    "            params = new_params\n",
    "            cost = new_cost\n",
    "            grad = new_grad\n",
    "            hess = new_hess\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            @printf \"%d: eta=%.3f cost=%.4f jtype=%s costheta=%.3f ps=\" i eta cost jumptype costheta\n",
    "            print_vector(params)\n",
    "            @printf \"\\n\"\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return params\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function adaptive_gradient_minimization(seed, func; start_eta=0.1, tol=1e-6, maxiter=400,\n",
    "    verbose=false)\n",
    "    \n",
    "    params = seed\n",
    "    eta = start_eta\n",
    "\n",
    "    out = DiffBase.GradientResult(params)\n",
    "    ForwardDiff.gradient!(out, func, params)\n",
    "    cost = DiffBase.value(out)\n",
    "    grad = DiffBase.gradient(out)\n",
    "\n",
    "    for i in [1:maxiter;]\n",
    "        new_params = params - eta*grad\n",
    "\n",
    "        ForwardDiff.gradient!(out, func, new_params)\n",
    "        new_cost = DiffBase.value(out)\n",
    "        new_grad = DiffBase.gradient(out)\n",
    "\n",
    "        if abs(new_cost - cost) < tol\n",
    "            break\n",
    "        end\n",
    "    \n",
    "        if new_cost >= cost\n",
    "            eta = eta/2\n",
    "        else\n",
    "            eta = eta*1.1\n",
    "            params = new_params\n",
    "            cost = new_cost\n",
    "            grad = new_grad\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            @printf \"%d: eta=%.3f cost=%.4f ps=[\" i eta cost \n",
    "            for p in [1:length(params);]\n",
    "                @printf \"%.3f\" params[p]\n",
    "                if p<length(params) @printf \", \"; end\n",
    "            end\n",
    "            @printf \"]\\n\"\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return params\n",
    "end\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "#                                                                           #\n",
    "#                   TRUST_REGION_HESSIAN_MINIMIZATION                       #\n",
    "#                                                                           #\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "function trust_region_Hessian_minimization(seed, func; start_eta=10, tol=1e-6, maxiter=400,\n",
    "    verbose=false)\n",
    "\n",
    "(below, x stands for delta_x, the step from the current x=x0 position at which the cost = const)\n",
    "\n",
    "cost = 0.5*x'*H*x + grad*x + const\n",
    "\n",
    "dcost/dx = H*x + grad  ;   dcost/dx = 0  ==> x =  - inv(H)*grad\n",
    "\n",
    "Trust-region says have a parameter lambda, and replace H with hat{H} = H +  I/eta.  \n",
    "When eta is very large, this is equivalent to a straight Newton method jump, \n",
    "because hat{H} ~= H.  But when eta is small, this is more like a small gradient\n",
    "descent step, because for small eta inv(hat{H}) ~= eta and therefore the delta x is like \n",
    "-eta*grad.  So, if the cost function is going down, make eta larger, and if it is going\n",
    "up, make eta a lot smaller. Just like we do in other adaptive methods\n",
    "\n",
    "PARAMETERS:\n",
    "===========\n",
    "\n",
    "seed        column vector, representing the starting value of the parameters.\n",
    "\n",
    "func        Function that takes a vector and returns a scalar.  If you want to\n",
    "            work with a function that tales more parameterrs and returns more than one \n",
    "            output, you can use something like\n",
    "\n",
    "                    x -> orig_func(x, other_params)[1]\n",
    "\n",
    "            You only need the \"[1]\" part if the orig_func returns more outputs than a scalar. \n",
    "\n",
    "OPTIONAL PARAMETERS:\n",
    "====================\n",
    "\n",
    "start_eta=10    Starting value of eta.  It's good to start with somethibg biggish, if it is\n",
    "                too much, it'll quickly get cut down.\n",
    "\n",
    "tol=1e-15       Numerical tolerance. If a proposed jump produces a change in func that is less than\n",
    "               this, the minimization stops.\n",
    "\n",
    "maxiter=400    Maximum number of iterations to do before stopping\n",
    "\n",
    "verbose=false   If true, print out a report on each iteration of iteration number, radius size (eta),\n",
    "                what type jump was proposed (\"Newton\" means going straight to global min, \"constrained\" means jump has \n",
    "                norm eta, failed means that finding the minimum at a given radius somehow didn't work). Will also\n",
    "                print out the cosine of the angle between the proposed jump and the gradient.\n",
    "\n",
    "RETURNS:\n",
    "========\n",
    "\n",
    "params       A vector the size of seed that has the last values of the minimizing parameters for func\n",
    "\n",
    "\"\"\"\n",
    "function trust_region_Hessian_minimization(seed, func; start_eta=10, tol=1e-15, maxiter=400,\n",
    "    verbose=false, verbose_level=1)\n",
    "\n",
    "    params = seed\n",
    "    eta = start_eta\n",
    "\n",
    "    cost, grad, hess = vgh(func, params)\n",
    "    if verbose && verbose_level >= 2\n",
    "        @printf(\"Initial cost, grad, hess:\\n\")\n",
    "        print_vector_g(:cost)\n",
    "        print_vector_g(:grad)\n",
    "        print_vector_g(:hess)\n",
    "    end\n",
    "    \n",
    "\n",
    "    for i in [1:maxiter;]\n",
    "        hathess    = hess + eye(length(grad), length(grad))/eta        \n",
    "        new_params = params - inv(hathess)*grad\n",
    "        new_cost, new_grad, new_hess = vgh(func, new_params)\n",
    "            \n",
    "        if abs(new_cost - cost) < tol\n",
    "            break\n",
    "        end\n",
    "\n",
    "        if new_cost >= cost\n",
    "            eta = eta/2\n",
    "            costheta = NaN\n",
    "        else\n",
    "            eta = eta*1.1\n",
    "            costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "\n",
    "            params = new_params\n",
    "            cost = new_cost\n",
    "            grad = new_grad\n",
    "            hess = new_hess\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            @printf \"%d: eta=%.3f cost=%.4f costheta=%.3f ps=\" i eta cost  costheta\n",
    "            print_vector(params)\n",
    "            @printf \"\\n\"\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return params, cost\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BBOX_HESSIAN_KEYWORD_MINIMIZATION()\n",
    "\n",
    "The main function here does parameter optimization, i.e. searching for the minimum of a scalar function of a vector-valued set of payameters. It uses ForwardDiff to compute and use information about gradients and Hessians, and uses the soft tanh() wall method to keep selected parameters strictly within desired limits during the search.\n",
    "\n",
    "The main search function, called bbox_hessian_keyword_minimization(), takes four obligatory parameters (and many optional ones, see its documentation below for details):\n",
    "\n",
    "* seed, a vector with the starting value of some parameters\n",
    "* args, a list of strings, same length as seed, indicating the parameter names associated with each value in seed. \n",
    "* func, a function that returns a scalar and takes only keyword-value argument pairs. All of the strings in args must indicate keyword names for func() that func() knows about.\n",
    "* bbox, a dictionary where each key is a Symbol indicating a parameter name, and the values are two-long vectors, whose elements indicate the desired minimum and maximum of the range for that parameter, respectively. Any key in this dictionary must also be, in string form, in args. But not all entries in args, need be in bbox, the missing ones are assumed to have no bounds. bbox could even be an empty dictionary, indicating no bounds on any parameter.\n",
    "\n",
    "bbox_hessian_keyword_minimization() will start from seed, will search for parameter values that minimize fun, and will return those, along with a variety of information that is diagnostic regarding the search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, some helper functions for bbox_hessian_keyword_minimization()\n",
    "\n",
    "These are functions that help with the soft tanh() wall method for implementing the parameter bounding box.\n",
    "\n",
    "### The soft tanh() wall method for putting bounds on parameter values\n",
    "\n",
    "The basic idea of the soft tanh() method is the following:  Let's say we have a scalar function $f(x)$, and we want to find its minimum subject to $x_0 <= x <= x_1$.\n",
    "\n",
    "Let's say that $m$ is the midpoint of the range, $m = (x_0+x_1)/2$, and $d$ is the width of the range, $d = x_1 - x_0$. We're going to use the function $g()$\n",
    "\n",
    "$$\n",
    "   g(x) = x_0 + (x_1-x_0)*\\frac{1}{1 + \\exp(-\\frac{x-m}{d})}\n",
    "$$\n",
    "\n",
    "Note that $g()$ ranges from a strict minimum of $x_0$ to a strict maximum of $x_1$, and it is monotonic in $x$, so for any value of $x$ that is within the range, we can obtain $g^{-1}(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "PyPlot.Figure(PyObject <matplotlib.figure.Figure object at 0x32e86b490>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using PyPlot\n",
    "\n",
    "function g(x, x0, x1)\n",
    "    m = 0.5*(x0+x1)\n",
    "    d = x1-x0\n",
    "    \n",
    "    return x0 + (x1-x0)./(1 + exp(-(x-m)/d))\n",
    "end\n",
    "\n",
    "h = figure(1); clf(); h[:set_size_inches](3, 2); \n",
    "x0 = 3; x1 = 10; x = -60:0.1:60\n",
    "\n",
    "plot(x, g(x, x0, x1), \"-\"); \n",
    "\n",
    "xlabel(\"xi\"); ylabel(\"x = g(xi)\"); hlines([x0, x1], xlim()[1], xlim()[2]); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our strategy:**\n",
    "\n",
    "Now here is our strategy: instead of minimizing the function $f(x)$ subject to the range constraints, we will instead  minimize the function $f(g(\\xi))$ subject to *no* constraints on $\\xi$. \n",
    "\n",
    "To start the minimization, if we wanted to start at $x = x_{seed}$, we first compute $\\xi_{seed} = g^{-1}(x_{seed})$, and we start at that value of $\\xi_{seed}$. That will correspond to having started at $x_{seed}$, as desired.\n",
    "\n",
    "For every value of $\\xi$ there is a corresponding value of $x = g(\\xi)$. And no matter where $\\xi$ ranges, its corresponding value of $x$ will be strictly bounded within the desired limits.  Note that \n",
    "\n",
    "$$\n",
    "    \\frac{{\\rm d}f}{{\\rm d}x} = \\frac{{\\rm d}f}{{\\rm d}g} \\cdot \\frac{{\\rm d}g}{{\\rm d}\\xi}\n",
    "$$\n",
    "\n",
    "Therefore, as $x$ approaches the rangle limits (which corresponds to $|\\xi - m| >> d$), we will be approaching the flat part of $g()$ and therefore $\\frac{{\\rm d}g}{{\\rm d}\\xi}$ will approach zero and so will $\\frac{{\\rm d}f}{{\\rm d}x}$: this is what prevents the search from going beyond the bounds. Note also that in each iteration of  bbox_hessian_keyword_minimization() we ask for the step that would minimize the cost function *given a certain step magnitude*. This means we will avoid directions that have little impact on the cost function.\n",
    "\n",
    "\n",
    "\n",
    "**The helper functions are**:\n",
    "\n",
    "* **pdict()** takes a dictionary of range limits, and a dictionary of current free range parameter values and returns a dictionary of the corresponding \"walled\" values, constrained to lie within the desired bounds. (I.e., puts values through tanh())\n",
    "* **vector_wrap()** does the same as pdict, except that instead of the dictionary of current free range parameter values, it takes a list of strings and a corresponding vector of values, and returns a vector.\n",
    "* **inverse_wall()** is the inverse of the above two operations: if passed a dictionary of ranges and a dictionary of constrained values, it returns a dictionary of the corresponding free range values. If passed a list of strings and a vector or the corresponding constrained values, returns a vector of the corresponding free-range, unconstrained values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inverse_wall"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@include_me hessian_utils.jl\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "pdict = wallwrap(bdict, pdict)\n",
    "Given bdict, a dictionary of symbols to [minval, maxval] vectors, and pdict, a dictionary of symbols\n",
    "to values (or, alternatively, an Array of (Symbol, value) tuples], goes through each of the symbols in \n",
    "bdict and modifies the corresponding value in pdict putting it through a tanh so the final output lies \n",
    "within the limits in bdict.  Returns the new pdict.  Makes a copy of pdict so as not to modify the original.\n",
    "\"\"\"\n",
    "function wallwrap(bdict, epdict)\n",
    "    local pdict = two_level_copy(epdict)  # Must be very careful here! I got bit by the bug of forgetting that without\n",
    "    # an explicit copy() call, Julia does not make copies of the contents of arrays or dictionaries, making it\n",
    "    # easy to inadvertently modify something one did not intend to perturb.  Note the two_level_copy() call, \n",
    "    # necessary to make sure we don't mess up the content of the caller's dictionary.\n",
    "    \n",
    "    if typeof(pdict)<:Array\n",
    "        pdict = Dict(pdict)\n",
    "    end\n",
    "\n",
    "    allkeys = keys(bdict)\n",
    "\n",
    "    for k in allkeys\n",
    "        local bbox = bdict[k]\n",
    "        d = 0.5*(bbox[2] - bbox[1])\n",
    "        m = 0.5*(bbox[2] + bbox[1])\n",
    "\n",
    "        pdict[k] = bbox[1] + d*(tanh((pdict[k]-m)/d)+1)\n",
    "    end\n",
    "    return pdict\n",
    "end\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "params = vector_wrap(bbox, args, eparams)\n",
    "Given bdict, a dictionary of symbols to [minval, maxval] vectors, args, an array of strings representing\n",
    "symbols, and params, an array of values corresponding to the args list, puts each param that has an entry \n",
    "in bdict through the tanh-walling mechanism, and returns the result. Does not modify the contents of the \n",
    "original params vector (or bdict or args).\n",
    "\"\"\"\n",
    "function vector_wrap(bbox, args, eparams)\n",
    "    local params = two_level_copy(eparams)\n",
    "    pdict = wallwrap(bbox, make_dict(args, params))\n",
    "    i=1; j=1\n",
    "    for i=1:length(args)\n",
    "        if typeof(args[i])<:Array\n",
    "            params[j:j+args[i][2]-1] = pdict[Symbol(args[i][1])]\n",
    "            j += args[i][2]-1\n",
    "        else\n",
    "            params[j] = pdict[Symbol(args[i])]\n",
    "        end\n",
    "    j = j+1\n",
    "    end\n",
    "    return params\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "params = inverse_wall(bdict, args, wparams)\n",
    "Given bdict, a dictionary of symbols to [minval, maxval] vectors, args, an array of strings representing\n",
    "symbols, and wparams, an array of values corresponding to the args list where each param that has an entry \n",
    "in bdict has alreadt been through the tanh-walling mechanism, UNwalls the ones that have a bdict entry and\n",
    "returns the result. Does not modify the contents of the original params vector (or bdict or args).\n",
    "\"\"\"\n",
    "function inverse_wall(bdict, args, wparams)\n",
    "    local params = two_level_copy(wparams)\n",
    "    pdict = inverse_wall(bdict, make_dict(args, params))\n",
    "    i=1; j=1\n",
    "    for i=1:length(args)\n",
    "        if typeof(args[i])<:Array\n",
    "            params[j:j+args[i][2]-1] = pdict[Symbol(args[i][1])]\n",
    "            j += args[i][2]-1\n",
    "        else\n",
    "            params[j] = pdict[Symbol(args[i])]\n",
    "        end\n",
    "        j = j+1\n",
    "    end\n",
    "    return params    \n",
    "end\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "pdict = inverse_wall(bdict, wdict)\n",
    "Given bdict, a dictionary of symbols to [minval, maxval] vectors, and wdict, a dictionary of symbols to values\n",
    "(or vectors of values)  UNwalls the ones that have a bdict entry and\n",
    "returns the result. Does not modify the contents of any dictionaries.\n",
    "\"\"\"\n",
    "function inverse_wall(bdict, wdict)\n",
    "    local pdict = two_level_copy(wdict)\n",
    "\n",
    "    allkeys = keys(bdict)\n",
    "    for k in allkeys\n",
    "        local bbox = bdict[k]\n",
    "        d = 0.5*(bbox[2] - bbox[1])\n",
    "        m = 0.5*(bbox[2] + bbox[1])\n",
    "\n",
    "        pdict[k] = m + d*0.5*log((pdict[k]-bbox[1])./(2*d - pdict[k] + bbox[1]))\n",
    "    end\n",
    "    return(pdict)\n",
    "end\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The main bbox_hessian_keyword_minimization() function\n",
    "\n",
    "Minimizes a scalar function $f()$ of keyword-value pairs, by searching for the parameter values that produce the smallest output. At each step of the minimization, uses ForwardDiff to compute the gradient $\\bf g$ and the Hessian $H$ at the current set of parameter values ${\\bf p_0}$; and then uses constrained_parabolic_minimization() to find the the parameter values ${\\bf p}$, such that $|{\\bf p} - {\\bf p_0}| <= \\eta$, that would minimize the parabolic approximation to the cost function\n",
    "\n",
    "$$\n",
    "J({\\bf p}) = J({\\bf p_o}) + {\\bf g}^T ({\\bf p} - {\\bf p_0}) + \\frac{1}{2} ({\\bf p} - {\\bf p_0})^T \\cdot H \\cdot ({\\bf p} - {\\bf p_0})\n",
    "$$\n",
    "\n",
    "If the cost $f({\\bf p}) < f({\\bf p_0})$, then the step is accepted, the current parameter values become ${\\bf p}$ and $\\eta$ is increased slightly. Otherwise the step is not accepted, current parameter values are not changed, and $\\eta$ is cut by a factor of 2.  Note than when $\\eta$ is very small, this becomes gradient descent. It is only when $\\eta$ is larger that the Hessian begins to play a role.\n",
    "\n",
    "A parameter step for which $|{\\bf p} - {\\bf p_0}| < \\eta$ is called a \"Newton\" jump (it is Newton's method). A step in which $|{\\bf p} - {\\bf p_0}| = \\eta$ is called a \"constrained\" jump. If the `verbose=true option` is selected, then the diagnostic information printed out at each step includes the cosine of the angle between the step taken and the gradient (when this is close to -1, we're doing gradient descent; far from -1, we're using the Hessian information).\n",
    "\n",
    "**FAILURE MODES:**  At each step of the search iteration, constrained_parabolic_minimization(), which itself involves a search, may fail. For diagnostics on this, a matrix called `cpm_traj` is returned. Its first row is the number of iterations run by constrained_parabolic_minimization()'s internal search, at each step of the overall search. If this number is equal to the maximum requested (currently hardcoded as 500 iterations), that means that constrained_parabolic_minimization() returned only because it ran into its maximum iteration limit, not because it was successful, and is therefore a sign of a step that may have been taken in a poor direction.\n",
    "\n",
    "**RETURNING MULTIPLE OUTPUTS IN $f()$:** In the implementation below, $f()$ must either return a scalar, or the first output it returns must be a scalar. That scalar is what will be minimized. The trajectory across the minimization of any further outputs that f() returns will be available in ftraj (see help documentation for `bbox_Hessian_keyword_minimization()` below). **Note that you might want to convert some of those outputs into\n",
    "Float64s, so they don't return as ForwardDiff Duals.**  See example in the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition bbox_Hessian_keyword_minimization(Any, Any, Any, Any) in module Main at In[75]:105 overwritten at In[77]:107.\n",
      "WARNING: Method definition #bbox_Hessian_keyword_minimization(Array{Any, 1}, Main.#bbox_Hessian_keyword_minimization, Any, Any, Any, Any) in module Main overwritten.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'bbox_Hessian_keyword_minimization :: Tuple{Any,Any,Any,Any}' in module 'Main'.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bbox_Hessian_keyword_minimization"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@include_me hessian_utils.jl\n",
    "\n",
    "\n",
    "try\n",
    "    Pkg.installed(\"JLD\")\n",
    "catch\n",
    "    Pkg.add(\"JLD\")\n",
    "end\n",
    "using JLD\n",
    "using MAT\n",
    "\n",
    "\n",
    "######################################################\n",
    "#                                                    #\n",
    "#         BBOX_HESSIAN_KEYWORD_MINIMIZATION          #\n",
    "#                                                    #\n",
    "######################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "function bbox_Hessian_keyword_minimization(seed, args, bbox, func; start_eta=10, tol=1e-6, \n",
    "maxiter=400, verbose=false, report_file=\"\")\n",
    "\n",
    "Like constrained_Hessian_minimization, but uses keyword_hessian!(). \n",
    "\n",
    "# PARAMETERS:\n",
    "\n",
    "- seed        column vector, representing the starting value of the parameters.\n",
    "\n",
    "- args        List of strings identifying parameters for differentiation, e.g., [\"const_E\", \"w_self]\n",
    "\n",
    "- bbox        If softbox=true (the default), should then be a Dict of Symbol=>[minval maxval] entries. An entry\n",
    "            in this Dict indicates that the corresponding parameter is to be bounded, as indicated by the associated \n",
    "            [minval maxval] vector. The bbox dictionary can have fewer entries than the number of parameters, and its\n",
    "            default value is Dict(), indicating an unbounded search.\n",
    "                If softbox=false, then bbox should be an nargs-by-2 matrix indicating the range for each argument,\n",
    "            with the minima (first column) and maxima (second column), and entries for ALL parameters.\n",
    "\n",
    "- func        func must take only optional keyword args, and must \n",
    "            take nderivs=0, difforder=0  and declare any new matrices using ForwardDiffZeros() instead of zeros().\n",
    "            func must either return a scalar, or the first output it returns must be a scalar. \n",
    "            That scalar is what will be minimized. The trajectory across the minimization of \n",
    "            any further outputs that f() returns will be available in ftraj (see RETURNS below)\n",
    "\n",
    "\n",
    "# OPTIONAL PARAMETERS:\n",
    "\n",
    "- start_eta    Starting value of the radius.  It's good to start with somethibg biggish, if it is\n",
    "             too much, it'll quickly get cut down.\n",
    "\n",
    "- tol=1e-6     Numerical tolerance. If a proposed jump produces a change in func that is less than\n",
    "             this, the minimization stops.\n",
    "\n",
    "- maxiter=400  Maximum number of iterations to do before stopping\n",
    "\n",
    "- verbose=false   If true, print out a report on each iteration of iteration number, radius size (eta),\n",
    "                what type jump was proposed (\"Newton\" means going straight to global min, \"constrained\" means jump has \n",
    "                norm eta, failed means that finding the minimum at a given radius somehow didn't work). Will also\n",
    "                print out the cosine of the angle between the proposed jump and the gradient.\n",
    "\n",
    "- verbose_level   If less than 2, regular verbose output, if 2 or greater, very verbose, for debugging.\n",
    "\n",
    "- softbox       If true, then bbox must be a Dict() and we use the tanh() mechanism for putting a fixed limit\n",
    "                on the parameters. NO LONGER SUPPORTING ANYTHING OTHER THAN softbox=true (which is the default)\n",
    "\n",
    "- report_file   If non-empty, at each iteration timestep will write into this file outputs trajectory, \n",
    "                (which contains eta, cost, and parameters), cpm_traj, and ftraj (which contains gradient, hessian, \n",
    "                and further cost function outputs).  The file must be a JLD file, and so will end with a .jld extension.\n",
    "                To load the saved dictionary, simply do D = load(filename)  (we have already called \"using JLD\" for you.)\n",
    "\n",
    "\n",
    "\n",
    "# RETURNS:\n",
    "\n",
    "- params       A vector the size of seed that has the last values of the minimizing parameters for func\n",
    "- trajectory   A (2+length(params))-by-nsteps matrix. Each column corresponds to an iteration step, and contains\n",
    "                 the value of eta used, the cost, and the value of the parameters at that iteration\n",
    "- cost         Final value of objective function\n",
    "- cpm_traj     A 2-by-nsteps matrix, containing reports from the contrained parabolic minimization at each timestep.\n",
    "             The first row is niters (how many iterations cpm's 1-d minimization ran for) and the second row is\n",
    "             Dlambda, the last change in the parameter being minimized in cpm's internal search\n",
    "- ftraj     Further components for the trajectory, will be an Array{Any}(3, nsteps). First row is gradient,\n",
    "            second row is Hessian, third row is second-and-further outputs of func, each one at each step of\n",
    "            the minimization. **NOTE** that if these further outputs contain variables that are being minimized, \n",
    "            they'll come out as ForwardDiff Duals, which you might not want!  So, for example, you might want to\n",
    "            convert vectors and matrices into Float64s before returning them in those extra outputs. E.g.,\n",
    "            if you want to return sum(err.*err) as the scalar to be minimized, and also return err, in your \n",
    "            cost function you would write   \" return sum(err.*err), Array{Float64}(err) \".   That way the first,\n",
    "            scalar output can still be differentiated, for minimization, and the second one comes out in readable form.\n",
    "\n",
    "\n",
    "\n",
    "# EXAMPLE:  (see also a more complete example in Cost Function Minimization and Hessian Utilities.ipynb)\n",
    "\n",
    "```\n",
    "function tester(;x=5, y=10, z=20, nderivs=0, difforder=0)\n",
    "    return x^2*y + z/tanh(y)\n",
    "end\n",
    "\n",
    "params, trajectory = bbox_Hessian_keyword_minimization([0.5, 0.5], [\"x\", \"y\"], [1.1 2 ; 1.1 4], tester, \n",
    "    verbose=true, tol=1e-12, start_eta=1);\n",
    "```\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "function bbox_Hessian_keyword_minimization(seed, args, bbox, func; start_eta=0.1, tol=1e-6, maxiter=400,\n",
    "    verbose=false, verbose_level=1, verbose_every=1, softbox=true, hardbox=false, report_file=\"\")\n",
    "\n",
    "    # --- check that saving will be done to a .jld file ---\n",
    "    if length(report_file)>0 && splitext(report_file)[2] != \".jld\"\n",
    "        if splitext(report_file)[2] == \"\"\n",
    "            report_file = resport_file * \".jld\"\n",
    "        else\n",
    "            error(\"Sorry, report_file can only write to JLD files, the extension has to be .jld\")\n",
    "        end\n",
    "    end\n",
    "\n",
    "    \n",
    "    # --------- Initializing the trajectory trace and function wrapper--------\n",
    " \n",
    "    traj_increment = 100\n",
    "    params = 0  # Make sure to have this here so that params stays defined beyond the try/catch\n",
    "    if ( !(typeof(bbox)<:Dict) ); error(\"Currently only supporting softbox=true, bbox must be a Dict\"); end;\n",
    "    try\n",
    "        params = copy(inverse_wall(bbox, args, seed))\n",
    "    catch\n",
    "        error(\"Were all initial param values within the indicated walls?\")\n",
    "    end\n",
    "    eta = start_eta\n",
    "    trajectory = zeros(2+length(params), traj_increment); cpm_traj = zeros(2, traj_increment)\n",
    "    \n",
    "    ftraj = Array{Any}(3,0)  # will hold gradient, hessian, and further_out,  per iteration\n",
    "\n",
    "    further_out =[];  # We define this variable here so it will be available for stashing further outputs from func\n",
    "    \n",
    "    # Now we define a wrapper around func() to do three things: (a) wallwrap parameters using the softbox method;\n",
    "    # (b) return as the desired scalar the first output of func; (c) stash in further_out any further outputs of func\n",
    "    internal_func = (;pars...) -> begin\n",
    "        fresults = func(;wallwrap(bbox, pars)...)   # note use of bbox external to this begin...end\n",
    "        if typeof(fresults)<:Tuple\n",
    "            answer = fresults[1]\n",
    "            further_out = fresults[2:end]\n",
    "        else\n",
    "            answer = fresults\n",
    "        end\n",
    "        return answer  # we assume that the first output of func() will always be a scalar, and that's what we return for ForwardDiff\n",
    "    end\n",
    "\n",
    "    # --------- END Initializing the trajectory trace --------\n",
    "\n",
    "    if verbose\n",
    "        @printf \"%d: eta=%g ps=\" 0 eta \n",
    "        print_vector(vector_wrap(bbox, args, params))\n",
    "        @printf \"\\n\"\n",
    "    end\n",
    "    \n",
    "    if softbox\n",
    "        if !(typeof(bbox)<:Dict); error(\"bhm: If softbox=true, then bbox must eb a Dict\"); end\n",
    "        cost, grad, hess = keyword_vgh(internal_func, args, params)  # further_out will be mutated\n",
    "    elseif hardbox\n",
    "        error(\"Sorry, no longer supporting hardbox=true\")\n",
    "    else\n",
    "        error(\"Sorry, no longer supporting softbox=false\")\n",
    "    end\n",
    "        \n",
    "    chessdelta = zeros(size(params))\n",
    "    \n",
    "    i=0  # here so variable i is available outside the loop\n",
    "    for i in [1:maxiter;]\n",
    "        if i > size(trajectory, 2)\n",
    "            trajectory = [trajectory zeros(2+length(params), traj_increment)]\n",
    "            cpm_traj   = [cpm_traj   zeros(2, traj_increment)]\n",
    "        end\n",
    "        trajectory[1:2, i]   = [eta;cost]\n",
    "        trajectory[3:end, i] = vector_wrap(bbox, args, params)\n",
    "        ftraj = [ftraj [grad, hess, further_out]]\n",
    "\n",
    "        if length(report_file)>0\n",
    "            save(report_file, Dict(\"traj\"=>trajectory[:,1:i], \"cpm_traj\"=>cpm_traj[:,1:i], \"ftraj\"=>ftraj))\n",
    "        end\n",
    "        \n",
    "        hessdelta  = - inv(hess)*grad\n",
    "        try\n",
    "            if verbose && verbose_level >= 2\n",
    "                @printf(\"bhm: about to try cpm with grad : \"); print_vector_g(grad); print(\"\\n\")\n",
    "                @printf(\"bhm:   hess :\"); print_vector_g(hess[:]); print(\"\\n\");\n",
    "            end\n",
    "            if verbose && verbose_level >= 2\n",
    "                cpm_out = constrained_parabolic_minimization(hess, grad'', eta, \n",
    "                    maxiter=500, tol=1e-20, do_plot=true, verbose=true)                \n",
    "            else\n",
    "                cpm_out = constrained_parabolic_minimization(hess, grad'', eta, maxiter=500, tol=1e-20)\n",
    "            end\n",
    "            chessdelta = cpm_out[1]; cpm_traj[1,i] = cpm_out[5]; cpm_traj[2,i] = cpm_out[6]\n",
    "            jumptype = \"not failed\"\n",
    "        catch y\n",
    "            jumptype = \"failed\"\n",
    "            if verbose\n",
    "                @printf \"Constrained parabolic minimization failed with error %s\\n\" y\n",
    "                @printf \"\\n\"\n",
    "                @printf \"eta was %g\\n\" eta\n",
    "                @printf \"grad was\\n\"\n",
    "                print_vector(grad)\n",
    "                @printf \"\\n\\nhess was\\n\"\n",
    "                for k in [1:length(grad);]\n",
    "                    print_vector(hess[k,:])\n",
    "                    @printf \"\\n\"\n",
    "                end\n",
    "                @printf \"\\n\"\n",
    "                matwrite(\"error_report.mat\", Dict(\"grad\"=>grad, \"hess\"=>hess, \"eta\"=>eta))\n",
    "            end\n",
    "            break\n",
    "        end\n",
    "\n",
    "        if norm(hessdelta) <= eta\n",
    "            new_params = params + hessdelta\n",
    "            jumptype = \"Newton\"\n",
    "        elseif jumptype != \"failed\" \n",
    "            new_params = params + chessdelta\n",
    "            jumptype  = \"constrained\"\n",
    "        end\n",
    "\n",
    "        if jumptype != \"failed\"\n",
    "            new_cost, new_grad, new_hess = keyword_vgh(internal_func, args, new_params)   # further_out may mutate\n",
    "            if verbose && verbose_level >=2\n",
    "                @printf(\"bhm: had new_params = : \"); print_vector_g(vector_wrap(bbox, args, params)); print(\"\\n\");\n",
    "                @printf(\"bhm: and my bbox was : \"); print(bbox); print(\"\\n\")\n",
    "                @printf(\"bhm: and my wallwrap output was : \"); print(wallwrap(bbox, make_dict(args, new_params))); print(\"\\n\")\n",
    "                @printf(\"bhm: and this produced new_grad : \"); print_vector_g(new_grad); print(\"\\n\")\n",
    "                @printf(\"bhm:   new_hess :\"); print_vector_g(new_hess[:]); print(\"\\n\");                                        \n",
    "            end\n",
    "            \n",
    "            if abs(new_cost - cost) < tol || eta < tol\n",
    "                if verbose\n",
    "                    @printf(\"About to break -- tol=%g, new_cost-cost=%g, eta=%g\\n\", tol, new_cost-cost, eta)\n",
    "                end\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "\n",
    "        if jumptype == \"failed\" || new_cost >= cost  \n",
    "            if verbose\n",
    "                @printf(\"eta going down: new_cost-cost=%g and jumptype='%s'\\n\", new_cost-cost, jumptype)\n",
    "                if verbose_level >= 2\n",
    "                    nwp = vector_wrap(bbox, args, new_params); wp = vector_wrap(bbox, args, params)\n",
    "                    @printf(\"   vvv: proposed new params were : \"); print_vector_g(nwp); print(\"\\n\")\n",
    "                    @printf(\"   vvv: proposed delta params was : \"); print_vector_g(nwp-wp); print(\"\\n\")\n",
    "                    @printf(\"   vvv: grad was : \"); print_vector_g(grad); print(\"\\n\")\n",
    "                    costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "                    @printf(\"   vvv: costheta of proposed jump was %g\\n\", costheta)\n",
    "                end\n",
    "            end\n",
    "            eta = eta/2\n",
    "            costheta = NaN\n",
    "            if eta < tol\n",
    "                if verbose\n",
    "                    @printf(\"About to break -- tol=%g, new_cost-cost=%g, eta=%g\\n\", tol, new_cost-cost, eta)\n",
    "                end\n",
    "                break\n",
    "            end\n",
    "        else\n",
    "            eta = eta*1.1\n",
    "            costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "\n",
    "            params = new_params\n",
    "            cost = new_cost\n",
    "            grad = new_grad\n",
    "            hess = new_hess\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            if rem(i, verbose_every)==0\n",
    "                @printf \"%d: eta=%g cost=%g jtype=%s costheta=%.3f ps=\" i eta cost jumptype costheta\n",
    "                print_vector_g(vector_wrap(bbox, args, params))\n",
    "                @printf \"\\n\"\n",
    "                if verbose_level >= 3\n",
    "                    @printf \"    At this point, grad is =\"\n",
    "                    print_vector_g(grad)\n",
    "                    @printf \"\\n\"                \n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    trajectory = trajectory[:,1:i]; cpm_traj = cpm_traj[:,1:i]\n",
    "    if length(report_file)>0\n",
    "        save(report_file, Dict(\"traj\"=>trajectory, \"cpm_traj\"=>cpm_traj, \"ftraj\"=>ftraj))\n",
    "    end\n",
    "    \n",
    "    return vector_wrap(bbox, args, params), trajectory, cost, cpm_traj, ftraj\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic example of using bbox_Hessian_keyword_minimization()\n",
    "\n",
    "We'll fit a sigmoid to some data generated from a sigmoid with noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run this cell once before the example to make sure to load all necessary functions\n",
    "\n",
    "include(\"general_utils.jl\")\n",
    "include(\"constrained_parabolic_minimization.jl\")\n",
    "include(\"hessian_utils.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition JJ(Any, Any) in module Main at In[133]:26 overwritten at In[134]:26.\n",
      "WARNING: Method definition #JJ(Array{Any, 1}, Main.#JJ, Any, Any) in module Main overwritten.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PyObject <matplotlib.text.Text object at 0x3326d9390>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pygui(true)\n",
    "\n",
    "npoints = 1000; srand(400)\n",
    "args = [\"baseline\", \"amplitude\", \"threshold\", \"slope\"]\n",
    "\n",
    "# Generating values for our four params:\n",
    "params = [1 5 0.5 0.8]\n",
    "\n",
    "# Make some points and plot them\n",
    "x = rand(npoints, 1)*6-3\n",
    "y = params[1] + params[2]*0.5*(tanh((x-params[3])/params[4])+1) + randn(npoints,1)*2\n",
    "figure(1); clf();\n",
    "plot(x, y, \".\")\n",
    "\n",
    "# Starting values for the four params. Plot the corresponding curve they generate\n",
    "seed = [8, 3.1, 0, 0.01]\n",
    "xx = -3:0.01:3\n",
    "plot(xx, seed[1] + seed[2]*0.5*(tanh((xx-seed[3])/seed[4])+1), \"g-\")\n",
    "\n",
    "# Cost function.  Note that it takes nderivs and difforder as parameters. First output is the scalar\n",
    "# that will be minimized, and we also returns a second output whose trajectory will be stashed \n",
    "# by bbox in ftraj as a diagnostic during the minimization.\n",
    "function JJ(x, y; baseline=0, amplitude=1, threshold=0, slope=1, do_plot=false, fignum=1, clearfig=true,\n",
    "    nderivs=0, difforder=0)\n",
    "\n",
    "    if do_plot\n",
    "        figure(fignum);\n",
    "        if clearfig; clf(); end;\n",
    "        xx = -3:0.01:3; x2=ForwardDiffZeros(size(xx,1), size(xx,2), nderivs=nderivs, difforder=difforder)\n",
    "        for i=1:length(xx); x2[i]=xx[i]; end; xx= x2\n",
    "        \n",
    "        plot(x, y, \".\")\n",
    "        plot(xx, baseline + amplitude*0.5*(tanh((xx-threshold)/slope)+1), \"r-\")\n",
    "    end\n",
    "\n",
    "    yhat =  baseline + amplitude*0.5*(tanh((x-threshold)/slope)+1) \n",
    "    err = yhat - y\n",
    "    return sum(err.*err), Array{Float64}(err)    # Note first output, the scalar to be minimized,\n",
    "    # may be ForwardDiff Duals during the minimization, which is fine, so it can be differentiated.\n",
    "    # The second one we cast into regular Float64 so it comes out readable.\n",
    "end\n",
    "\n",
    "\n",
    "if ~isdir(\"Trash\"); mkdir(\"Trash\"); end;  # we're going to put the iteration-step by iteration-step report file there\n",
    "\n",
    "bbox = Dict(:baseline=>[-2, 10], :slope=>[0.001 5])\n",
    "func = (;pars...) -> JJ(x, y; do_plot=false, pars...)\n",
    "\n",
    "opars, traj, cost, cpm_traj, ftraj = bbox_Hessian_keyword_minimization(seed, args, bbox, func,\n",
    "verbose=false, verbose_level=1, softbox=true, start_eta=1, report_file=\"Trash/example_report.jld\")\n",
    "\n",
    "# Note that the gradient at step i of the minimization will be available as ftraj[1,i], the hessian will be \n",
    "# in ftraj[2,i], and the error vector, which is the first of the extra outputs of JJ(), will be in ftraj[3,i][1].\n",
    "# In our example JJ() produced only one extra output; a second extra output would be in ftraj[3,i][2], and so on.\n",
    "\n",
    "# Plot the resulting curve, and report both final and generating params\n",
    "plot(xx, opars[1] + opars[2]*0.5*(tanh((xx-opars[3])/opars[4])+1), \"r-\")\n",
    "[opars' ; params]\n",
    "xlabel(\"x\"); ylabel(\"y\"); title(\"green is sigmoid with starting params, red is end\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000×1 Array{Float64,2}:\n",
       "  2.73188\n",
       "  4.10574\n",
       "  5.0182 \n",
       "  3.36893\n",
       "  5.4196 \n",
       "  7.32563\n",
       "  7.44422\n",
       " 10.5383 \n",
       "  5.67542\n",
       "  8.44268\n",
       "  4.12478\n",
       "  6.14751\n",
       "  7.77823\n",
       "  ⋮      \n",
       "  5.76898\n",
       "  8.79183\n",
       "  5.19711\n",
       "  8.98922\n",
       "  5.34472\n",
       "  5.57774\n",
       "  5.13125\n",
       "  8.50657\n",
       "  4.48849\n",
       "  8.56634\n",
       "  8.80441\n",
       "  5.11256"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err = ftraj[3,1][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000×1 Array{Float64,2}:\n",
       "  2.73188\n",
       "  4.10574\n",
       "  5.0182 \n",
       "  3.36893\n",
       "  5.4196 \n",
       "  7.32563\n",
       "  7.44422\n",
       " 10.5383 \n",
       "  5.67542\n",
       "  8.44268\n",
       "  4.12478\n",
       "  6.14751\n",
       "  7.77823\n",
       "  ⋮      \n",
       "  5.76898\n",
       "  8.79183\n",
       "  5.19711\n",
       "  8.98922\n",
       "  5.34472\n",
       "  5.57774\n",
       "  5.13125\n",
       "  8.50657\n",
       "  4.48849\n",
       "  8.56634\n",
       "  8.80441\n",
       "  5.11256"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = load(\"Trash/example_report.jld\")\n",
    "G[\"ftraj\"][3,1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.2",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
