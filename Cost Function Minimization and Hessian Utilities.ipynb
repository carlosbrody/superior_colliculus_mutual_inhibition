{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utility functions related to cost function minimizations**\n",
    "\n",
    "The main function defined here to be heavily used elsewhere is **bbox_hessian_keyword_minimization()**.\n",
    "\n",
    "Important functions needed to work with bbox_hessian_keyword_minimization() and also defined here are\n",
    "\n",
    "* **ForwarDiffZeros()**.  If you're going to declare vectors or matrices inside a function that you want to take derivatives of, *do not* use zeros(). Use ForwardDiffZeros() instead. If you dont' declare vectors or matrices, you won't need it. Be careful. Failing to use ForwardDiffZeros() does not produce an error: your derivatives and hessians simply come back as zeros, and if you don't think of this issue, the reason can feel very mysterious.\n",
    "\n",
    "* **make_dict()**. If you're working with keyword-value pairs, you will want to manipulate sets of those. make_dict() is a function that helps you do that, for example, for merging current selected parameter values with a superset of default values.  In addition, make_dict() is central to the gradient- and hessian-taking functions defined here that operate on keyword-value pairs. The reason is that make_dict() can take a vector (as one of its parameters)and turn it into keyword-value pairs, and we need this transformation to work with the ForwardDiff package, since that package only operates on functions of vectors.\n",
    "\n",
    "Two other functions often used are **vgh()** (a wrapper to fund the value, gradient, and hessian of a scalar function of a vector) and **keyword_vgh()** (a wrapper like vgh() except it operates on functions that take only keyword-value pairs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">TABLE OF CONTENTS</h1>\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "**Updates to the table of contents are periodic, but run the cell below to first start or force an update.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://sites.google.com/site/brodylabhome/files/make_table_of_contents.js')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition @javascript_str(ANY<:Any) in module Main at In[28]:1 overwritten at In[29]:1.\n"
     ]
    }
   ],
   "source": [
    "macro javascript_str(s) display(\"text/javascript\", s); end\n",
    "\n",
    "javascript\"\"\"\n",
    "$.getScript('https://sites.google.com/site/brodylabhome/files/make_table_of_contents.js')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make_dict() and ForwardDiffZeros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#@include_me hessian_utils.jl\n",
    "        \n",
    "\"\"\"\n",
    "dict = make_dict(argstrings, x, [starting_dict=Dict()] )\n",
    "\n",
    "Given a list of strings, and a list of values, makes a dictionary of Symbols to values, with the Symbols \n",
    "corresponding to each of the strings.  Mostly used to pass arguments as a keyword-value set into a function.\n",
    "If one of the elements of argstrings is *not* a string, but is instead a 2-long list, the first element of that \n",
    "list should be a string, and the second element of that list should be a positive integer. This will be \n",
    "interpreted as \"don't take only one value, take this number of values and this parameter will be a vector\"\n",
    "\n",
    "PARAMS:\n",
    "=======\n",
    "\n",
    "argstrings       A list of strings. Each element may also be a two-long list of a string, positive integer,\n",
    "                 e.g., [\"this\" 3]\n",
    "\n",
    "x                A vector of numeric values. Its length must be such that all the strings in argstrings\n",
    "                 can take their corresponding element(s), sequentially, from x\n",
    "\n",
    "starting_dict    An initial dictionary to work with.  Any key in this starting dictionary matching an argstring\n",
    "                 will be replaced by the new value. Keys not matched will remain.\n",
    "\n",
    "RETURNS:\n",
    "========\n",
    "\n",
    "dict             The symbol dictionary.\n",
    "\n",
    "\n",
    "EXAMPLES:\n",
    "=========\n",
    "\n",
    ">> make_dict([\"this\", \"that\", [\"there\", 2]], [10, 20, 3, 4])\n",
    "\n",
    "Dict{Any,Any} with 3 entries:\n",
    "  :this  => 10\n",
    "  :that  => 20\n",
    "  :there => [3,4]\n",
    "\n",
    ">> make_dict([\"doo\", \"gaa\"], [10, 20], Dict(:blob=>100, :gaa=>-44))\n",
    "\n",
    "Dict{Symbol,Int64} with 3 entries:\n",
    "  :gaa  => 20\n",
    "  :blob => 100\n",
    "  :doo  => 10\n",
    "\n",
    "\"\"\"\n",
    "function make_dict(args, x, starting_dict=Dict())\n",
    "    kwargs = starting_dict;\n",
    "    i = 1; j=1\n",
    "    while i<=length(args)\n",
    "        if typeof(args[i])==String\n",
    "            kwargs = merge(kwargs, Dict(Symbol(args[i]) => x[j]))\n",
    "        else\n",
    "            if length(args[i]) == 2\n",
    "                extra = args[i][2]-1\n",
    "                kwargs = merge(kwargs, Dict(Symbol(args[i][1]) => x[j:(j+extra)]))\n",
    "                j = j+extra\n",
    "            else\n",
    "                error(\"Each element of the args vector must be either a string, or a 2-long vector, first element a string, second integer\")\n",
    "            end            \n",
    "        end\n",
    "        i = i+1; j=j+1\n",
    "    end\n",
    "    return kwargs\n",
    "end \n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "function M = ForwardDiffZeros(m, n; nderivs=0, difforder=0)\n",
    "\n",
    "Use instead of zeros(). Creates a matrix of zeros, of size m rows by n columns, with elements appropriate for \n",
    "differentiation by ForwardDiff. If nderivs==0 or difforder==0 then the elements will be regular\n",
    "Float64, not ForwardDiff types.\n",
    "\n",
    "PARAMETERS:\n",
    "===========\n",
    "\n",
    "m        Integer, number of rows\n",
    "\n",
    "n        Integer, number of columns\n",
    "\n",
    "\n",
    "OPTIONAL PARAMETERS:\n",
    "====================\n",
    "\n",
    "nderivs=0       The number of variables that we'll be differentiating with respect to. In other\n",
    "                words, this number is equal to the length of the gradient. If this is left as zero (the default) then \n",
    "                the data type will be regular Float64\n",
    "\n",
    "difforder=0     The order of the derivative we will want to take.  Zero means nothing, stick with\n",
    "                regular Float64, 1 means gradient, 2 means hessian\n",
    "\n",
    "RETURNS:\n",
    "========\n",
    "\n",
    "An m-by-n matrix of zeros that can be used with Forward Diff.\n",
    "\n",
    "\"\"\"\n",
    "function ForwardDiffZeros(m, n; nderivs=0, difforder=0)\n",
    "    if nderivs == 0 || difforder == 0\n",
    "        return zeros(m, n)\n",
    "    elseif difforder == 1\n",
    "        return zeros(ForwardDiff.Dual{nderivs, Float64}, m , n)\n",
    "    elseif difforder == 2\n",
    "        return zeros(ForwardDiff.Dual{nderivs, ForwardDiff.Dual{nderivs, Float64}}, m, n)\n",
    "    else\n",
    "        error(\"Don't know how to do that order of derivatives!\", nderivs)\n",
    "    end\n",
    "end\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vgh() and keyword_vgh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#@include_me hessian_utils.jl\n",
    "\n",
    "\"\"\"\n",
    "function value, gradient, hessian = vgh(func, x0)\n",
    "\n",
    "Wrapper for ForwardDiff.hessian!() that computes and returns all three of a function's value, gradient, and hessian.\n",
    "\n",
    "EXAMPLE:\n",
    "========\n",
    "\n",
    "function tester(x::Vector)\n",
    "\n",
    "    return sum(x.*x)\n",
    "end\n",
    "\n",
    "value, grad, hess = vgh(tester, [10, 3.1])\n",
    "\"\"\"\n",
    "function vgh(func, x0)\n",
    "    out = DiffBase.HessianResult(x0)             \n",
    "    ForwardDiff.hessian!(out, func, x0)\n",
    "    value    = DiffBase.value(out)\n",
    "    gradient = DiffBase.gradient(out)\n",
    "    hessian  = DiffBase.hessian(out)\n",
    "    \n",
    "    return value, gradient, hessian    \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "#                                                                           #\n",
    "#                   KEYWORD GRADIENTS AND HESSIANS                          #\n",
    "#                                                                           #\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "function keyword_gradient(func, args, x0)\n",
    "\n",
    "Same as ForwardDiff.gradient except that func() must be a function taking only optional \n",
    "keyword arguments, and the derivative is taken with respect to an arbitrarily chosen set of \n",
    "those, indicated by a list of strings.\n",
    "\n",
    "In addition, func *MUST* take optional keyword args nderivs=0 and difforder=0, and within it,\n",
    "if matrices or vectors of zeros are declared, use ForwardDiffZeros() instead of zeros().\n",
    "\n",
    "PARAMETERS:\n",
    "===========\n",
    "\n",
    "func        A scalar function taking only optional keyword arguments, including nderivs=0 and difforder=0\n",
    "\n",
    "args        A list of strings indicating which keyword arguments to differentiate. These strings must\n",
    "            match the keyword names in func()   For example, func(;this=10, that=20) would mean that \n",
    "            \"this\" and \"that\" are allowable elements in args.\n",
    "\n",
    "x0          A vector of floats, same length as args, representing the values of these args at which the\n",
    "            derivatives will be taken.\n",
    "\n",
    "RETURNS:\n",
    "========\n",
    "\n",
    "grad        The gradient of func w.r.t. args\n",
    "\n",
    "\n",
    "EXAMPLE:\n",
    "========\n",
    "\n",
    "function tester(;a=10, b=20, c=30, nderivs=0, difforder=0)\n",
    "    M = ForwardDiffZeros(3, 3; nderivs=nderivs, difforder=difforder)\n",
    "    M[1,1] = a^2*10\n",
    "    M[2,2] = b*20\n",
    "    M[3,3] = a*sqrt(c)*30.1\n",
    "    return trace(M)\n",
    "end\n",
    "\n",
    "grad_a_c = keyword_gradient((;pars...) -> tester(;pars...), [\"a\", \"c\"], [10, 3.1])  # note initial values must be floats\n",
    "\n",
    "grad_b_c = keyword_gradient((;pars...) -> tester(;pars...), [\"b\", \"c\"], [10, 3.1]) \n",
    "\n",
    "\"\"\"\n",
    "function keyword_gradient(func, args, x0)\n",
    "    \n",
    "    ans = ForwardDiff.gradient(x -> func(;nderivs=length(x0), difforder=1, make_dict(args, x)...), x0)\n",
    "    \n",
    "    return ans\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "function keyword_gradient!(out, func, args, x0)\n",
    "\n",
    "Same as keyword_gradient, but puts the result in mutable out. See keyword_gradient() for documentation.\n",
    "\n",
    "EXAMPLE:\n",
    "========\n",
    "\n",
    "function tester(;a=10, b=20, c=30, nderivs=0, difforder=0)\n",
    "    M = ForwardDiffZeros(3, 3; nderivs=nderivs, difforder=difforder)\n",
    "    M[1,1] = a^2*10\n",
    "    M[2,2] = b*20\n",
    "    M[3,3] = a*sqrt(c)*30.1\n",
    "    return trace(M)\n",
    "end\n",
    "\n",
    "out = DiffBase.GradientResult([10, 3.1])  # out must be same length as whatever we will differentiate w.r.t.\n",
    "keyword_gradient!(out, (;pars...) -> tester(;pars...), [\"a\", \"c\"], [10, 3.1])  # note initial values must be floats\n",
    "grad_a_c = DiffBase.gradient(out)\n",
    "value    = DiffBase.value(out)\n",
    "\n",
    "out = DiffBase.GradientResult([10, 3.1, 20])  # out must be same length as whatever we will differentiate w.r.t.\n",
    "keyword_gradient!(out, (;pars...) -> tester(;pars...), [\"a\", \"b\", \"c\"], [10, 20, 3.1])  # note initial values must be floats\n",
    "grad_a_b_c = DiffBase.gradient(out)\n",
    "\n",
    "\"\"\"\n",
    "function keyword_gradient!(out, func, args, x0)\n",
    "\n",
    "    if length(args) != length(x0)\n",
    "        error(\"Oy! args and x0 must be the same length!\")\n",
    "    end\n",
    "\n",
    "    ForwardDiff.gradient!(out, x -> func(;nderivs=length(x0), difforder=1, make_dict(args, x)...), x0)\n",
    "    \n",
    "    return \n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "function keyword_hessian(func, args, x0)\n",
    "\n",
    "Same as ForwardDiff.hessian except that func() must be a function taking only optional \n",
    "keyword arguments, and the derivative is taken with respect to an arbitrarily chosen set of \n",
    "those, indicated by a list of strings.\n",
    "\n",
    "In addition, func *MUST* take optional keyword args nderivs=0 and difforder=0, and within it,\n",
    "if matrices or vectors of zeros are declared, use ForwardDiffZeros() instead of zeros().\n",
    "\n",
    "PARAMETERS:\n",
    "===========\n",
    "\n",
    "func        A scalar function taking only optional keyword arguments, including nderivs=0 and difforder=0\n",
    "\n",
    "args        A list of strings indicating which keyword arguments to differentiate. These strings must\n",
    "            match the keyword names in func()   For example, func(;this=10, that=20) would mean that \n",
    "            \"this\" and \"that\" are allowable elements in args.\n",
    "\n",
    "x0          A vector of floats, same length as args, representing the values of these args at which the\n",
    "            derivatives will be taken.\n",
    "\n",
    "RETURNS:\n",
    "========\n",
    "\n",
    "grad        The gradient of func w.r.t. args\n",
    "\n",
    "\n",
    "EXAMPLE:\n",
    "========\n",
    "\n",
    "function tester(;a=10, b=20, c=30, nderivs=0, difforder=0)\n",
    "    M = ForwardDiffZeros(3, 3; nderivs=nderivs, difforder=difforder)\n",
    "    M[1,1] = a^2*10\n",
    "    M[2,2] = b*20\n",
    "    M[3,3] = a*sqrt(c)*30.1\n",
    "    return trace(M)\n",
    "end\n",
    "\n",
    "hess_b_c = keyword_hessian((;pars...) -> tester(;pars...), [\"b\", \"c\"], [10, 3.1])  # note initial values must be floats\n",
    "\n",
    "hess_a_b_c = keyword_hessian((;pars...) -> tester(;pars...), [\"a\", \"b\", c\"], [10, 2, 3.1]) \n",
    "\n",
    "\"\"\"\n",
    "function keyword_hessian(func, args, x0)\n",
    "\n",
    "    if length(args) != length(x0)\n",
    "        error(\"Oy! args and x0 must be the same length!\")\n",
    "    end\n",
    "\n",
    "    ans = ForwardDiff.hessian(x -> func(;nderivs=length(x0), difforder=2, make_dict(args, x)...), x0)\n",
    "    \n",
    "    return ans\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "function keyword_hessian!(out, func, args, x0)\n",
    "\n",
    "Same as keyword_hessian, but puts the result in mutable out. See keyword_hessian() for documentation.\n",
    "\n",
    "EXAMPLE:\n",
    "========\n",
    "\n",
    "function tester(;a=10, b=20, c=30, nderivs=0, difforder=0)\n",
    "    M = ForwardDiffZeros(3, 3; nderivs=nderivs, difforder=difforder)\n",
    "    M[1,1] = a^2*10\n",
    "    M[2,2] = b*20\n",
    "    M[3,3] = a*sqrt(c)*30.1\n",
    "    return trace(M)\n",
    "end\n",
    "\n",
    "out = DiffBase.HessianResult([10, 3.1])  # out must be same length as whatever we will differentiate w.r.t.\n",
    "keyword_hessian!(out, (;pars...) -> tester(;pars...), [\"a\", \"c\"], [10, 3.1])  # note initial values must be floats\n",
    "hess_a_c = DiffBase.hessian(out)\n",
    "grad_a_c = DiffBase.gradient(out)\n",
    "value    = DiffBase.value(out)\n",
    "\n",
    "out = DiffBase.HessianResult([10, 3.1, 20])  # out must be same length as whatever we will differentiate w.r.t.\n",
    "keyword_hessian!(out, (;pars...) -> tester(;pars...), [\"a\", \"b\", \"c\"], [10, 20, 3.1])  # note initial values must be floats\n",
    "hess_a_b_c = DiffBase.hessian(out)\n",
    "\n",
    "\"\"\"\n",
    "function keyword_hessian!(out, func, args, x0)\n",
    "    nargs = 0\n",
    "    for i in [1:length(args);]\n",
    "        if typeof(args[i])==String\n",
    "            nargs += 1\n",
    "        else\n",
    "            nargs += args[i][2]\n",
    "        end\n",
    "    end\n",
    "    if nargs != length(x0)\n",
    "        error(\"Oy! args and x0 must be the same length!\")\n",
    "    end\n",
    "\n",
    "    ForwardDiff.hessian!(out, x -> func(;nderivs=length(x0), difforder=2, make_dict(args, x)...), x0)\n",
    "    \n",
    "    return \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "function value, gradient, hessian = keyword_vgh(func, args, x0)\n",
    "\n",
    "Wrapper for keyword_hessian!() that computes and returns all three of a function's value, gradient, and hessian.\n",
    "\n",
    "EXAMPLE:\n",
    "========\n",
    "\n",
    "function tester(;a=10, b=20, c=30, nderivs=0, difforder=0)\n",
    "    M = ForwardDiffZeros(3, 3; nderivs=nderivs, difforder=difforder)\n",
    "    M[1,1] = a^2*10\n",
    "    M[2,2] = b*20\n",
    "    M[3,3] = a*sqrt(c)*30.1\n",
    "    return trace(M)\n",
    "end\n",
    "\n",
    "value, grad, hess = keyword_vgh(tester, [\"a\", \"c\"], [10, 3.1])\n",
    "\"\"\"\n",
    "function keyword_vgh(func, args, x0)\n",
    "    out = DiffBase.HessianResult(x0)\n",
    "    keyword_hessian!(out, func, args, x0)\n",
    "    value    = DiffBase.value(out)\n",
    "    gradient = DiffBase.gradient(out)\n",
    "    hessian  = DiffBase.hessian(out)\n",
    "    \n",
    "    return value, gradient, hessian    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Various utilities for taking gradients and for minimizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keyword_vgh"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@include_me hessian_utils.jl\n",
    "\n",
    "\"\"\"\n",
    "function constrained_Hessian_minimization(seed, func; start_eta=10, tol=1e-6, maxiter=400,\n",
    "    verbose=false)\n",
    "\n",
    "LARGELY SUPERSEDED BY BBOX_HESSIAN_KEYWORD_MINIMIZATION\n",
    "\n",
    "Minimizes a function using. At each step, it computes the Hessian approximation to the function, \n",
    "and then asks, according to the corresponding parabolic approximation: is the global minimum within a\n",
    "radius eta? If so, try to jump to it. If not, try to jump to the point at a distance eta from the \n",
    "current point that would minimize the function.  If the attempted jump leads to an increase in the function,\n",
    "then the jump is rejected and eta is reduced by a factor of 2. If the attempted jump reduces the function,\n",
    "then it is accepted and eta is increased by a factor of 1.1.  This proceeds until the change in the function\n",
    "after a proposed jump would be less than tol, or the iteration number has reached maxiter, whichever happens\n",
    "first.  Returns the minimizing parameters.\n",
    "\n",
    "PARAMETERS:\n",
    "===========\n",
    "\n",
    "seed        column vector, representing the starting value of the parameters.\n",
    "\n",
    "func        Function that takes a vector and returns a scalar.  If you want to\n",
    "            work with a function that tales mpre parameterrs and returns more than one \n",
    "            output, you can use something like\n",
    "\n",
    "                    x -> orig_func(x, other_params)[1]\n",
    "\n",
    "            You only need the \"[1]\" part of the orig_func returns more outputs than a scalar. \n",
    "\n",
    "OPTIONAL PARAMETERS:\n",
    "====================\n",
    "\n",
    "start_eta=10 Starting value of the radius.  It's good to start with somethibg biggish, if it is\n",
    "             too much, it'll quickly get cut down.\n",
    "\n",
    "tol=1e-6     Numerical tolerance. If a proposed jump produces a change in func that is less than\n",
    "             this, the minimization stops.\n",
    "\n",
    "maxiter=400  Maximum number of iterations to do before stopping\n",
    "\n",
    "verbose=false   If true, print out a report on each iteration of iteration number, radius size (eta),\n",
    "                what type jump was proposed (\"Newton\" means going straight to global min, \"constrained\" means jump has \n",
    "                norm eta, failed means that finding the minimum at a given radius somehow didn't work). Will also\n",
    "                print out the cosine of the angle between the proposed jump and the gradient.\n",
    "\n",
    "RETURNS:\n",
    "========\n",
    "\n",
    "params       A vector the size of seed that has the last values of the minimizing parameters for func\n",
    "\n",
    "\"\"\"\n",
    "function constrained_Hessian_minimization(seed, func; start_eta=10, tol=1e-6, maxiter=400,\n",
    "    verbose=false)\n",
    "\n",
    "    params = seed\n",
    "    eta = start_eta\n",
    "\n",
    "    out = DiffBase.HessianResult(params)\n",
    "    ForwardDiff.hessian!(out, func, params)\n",
    "    cost = DiffBase.value(out)\n",
    "    grad = DiffBase.gradient(out)\n",
    "    hess = DiffBase.hessian(out)\n",
    "\n",
    "    chessdelta = zeros(size(params))\n",
    "\n",
    "    for i in [1:maxiter;]\n",
    "        hessdelta  = - inv(hess)*grad\n",
    "        try\n",
    "            chessdelta = constrained_parabolic_minimization(hess, grad'', eta)[1]\n",
    "            jumptype = \"not failed\"\n",
    "        catch\n",
    "            jumptype = \"failed\"\n",
    "        end\n",
    "\n",
    "        if norm(hessdelta) <= eta\n",
    "            new_params = params + hessdelta\n",
    "            jumptype = \"Newton\"\n",
    "        elseif jumptype != \"failed\" \n",
    "            new_params = params + chessdelta\n",
    "            jumptype  = \"constrained\"\n",
    "        end\n",
    "\n",
    "        if jumptype != \"failed\"\n",
    "            ForwardDiff.hessian!(out, func, new_params)\n",
    "            new_cost = DiffBase.value(out)\n",
    "            new_grad = DiffBase.gradient(out)\n",
    "            new_hess = DiffBase.hessian(out)\n",
    "            \n",
    "            if abs(new_cost - cost) < tol\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "\n",
    "        if jumptype == \"failed\" || new_cost >= cost\n",
    "            eta = eta/2\n",
    "            costheta = NaN\n",
    "        else\n",
    "            eta = eta*1.1\n",
    "            costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "\n",
    "            params = new_params\n",
    "            cost = new_cost\n",
    "            grad = new_grad\n",
    "            hess = new_hess\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            @printf \"%d: eta=%.3f cost=%.4f jtype=%s costheta=%.3f ps=\" i eta cost jumptype costheta\n",
    "            print_vector(params)\n",
    "            @printf \"\\n\"\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return params\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function adaptive_gradient_minimization(seed, func; start_eta=0.1, tol=1e-6, maxiter=400,\n",
    "    verbose=false)\n",
    "    \n",
    "    params = seed\n",
    "    eta = start_eta\n",
    "\n",
    "    out = DiffBase.GradientResult(params)\n",
    "    ForwardDiff.gradient!(out, func, params)\n",
    "    cost = DiffBase.value(out)\n",
    "    grad = DiffBase.gradient(out)\n",
    "\n",
    "    for i in [1:maxiter;]\n",
    "        new_params = params - eta*grad\n",
    "\n",
    "        ForwardDiff.gradient!(out, func, new_params)\n",
    "        new_cost = DiffBase.value(out)\n",
    "        new_grad = DiffBase.gradient(out)\n",
    "\n",
    "        if abs(new_cost - cost) < tol\n",
    "            break\n",
    "        end\n",
    "    \n",
    "        if new_cost >= cost\n",
    "            eta = eta/2\n",
    "        else\n",
    "            eta = eta*1.1\n",
    "            params = new_params\n",
    "            cost = new_cost\n",
    "            grad = new_grad\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            @printf \"%d: eta=%.3f cost=%.4f ps=[\" i eta cost \n",
    "            for p in [1:length(params);]\n",
    "                @printf \"%.3f\" params[p]\n",
    "                if p<length(params) @printf \", \"; end\n",
    "            end\n",
    "            @printf \"]\\n\"\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return params\n",
    "end\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "#                                                                           #\n",
    "#                   TRUST_REGION_HESSIAN_MINIMIZATION                       #\n",
    "#                                                                           #\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "function trust_region_Hessian_minimization(seed, func; start_eta=10, tol=1e-6, maxiter=400,\n",
    "    verbose=false)\n",
    "\n",
    "(below, x stands for delta_x, the step from the current x=x0 position at which the cost = const)\n",
    "\n",
    "cost = 0.5*x'*H*x + grad*x + const\n",
    "\n",
    "dcost/dx = H*x + grad  ;   dcost/dx = 0  ==> x =  - inv(H)*grad\n",
    "\n",
    "Trust-region says have a parameter lambda, and replace H with hat{H} = H +  I/eta.  \n",
    "When eta is very large, this is equivalent to a straight Newton method jump, \n",
    "because hat{H} ~= H.  But when eta is small, this is more like a small gradient\n",
    "descent step, because for small eta inv(hat{H}) ~= eta and therefore the delta x is like \n",
    "-eta*grad.  So, if the cost function is going down, make eta larger, and if it is going\n",
    "up, make eta a lot smaller. Just like we do in other adaptive methods\n",
    "\n",
    "PARAMETERS:\n",
    "===========\n",
    "\n",
    "seed        column vector, representing the starting value of the parameters.\n",
    "\n",
    "func        Function that takes a vector and returns a scalar.  If you want to\n",
    "            work with a function that tales more parameterrs and returns more than one \n",
    "            output, you can use something like\n",
    "\n",
    "                    x -> orig_func(x, other_params)[1]\n",
    "\n",
    "            You only need the \"[1]\" part if the orig_func returns more outputs than a scalar. \n",
    "\n",
    "OPTIONAL PARAMETERS:\n",
    "====================\n",
    "\n",
    "start_eta=10    Starting value of eta.  It's good to start with somethibg biggish, if it is\n",
    "                too much, it'll quickly get cut down.\n",
    "\n",
    "tol=1e-15       Numerical tolerance. If a proposed jump produces a change in func that is less than\n",
    "               this, the minimization stops.\n",
    "\n",
    "maxiter=400    Maximum number of iterations to do before stopping\n",
    "\n",
    "verbose=false   If true, print out a report on each iteration of iteration number, radius size (eta),\n",
    "                what type jump was proposed (\"Newton\" means going straight to global min, \"constrained\" means jump has \n",
    "                norm eta, failed means that finding the minimum at a given radius somehow didn't work). Will also\n",
    "                print out the cosine of the angle between the proposed jump and the gradient.\n",
    "\n",
    "RETURNS:\n",
    "========\n",
    "\n",
    "params       A vector the size of seed that has the last values of the minimizing parameters for func\n",
    "\n",
    "\"\"\"\n",
    "function trust_region_Hessian_minimization(seed, func; start_eta=10, tol=1e-15, maxiter=400,\n",
    "    verbose=false, verbose_level=1)\n",
    "\n",
    "    params = seed\n",
    "    eta = start_eta\n",
    "\n",
    "    cost, grad, hess = vgh(func, params)\n",
    "    if verbose && verbose_level >= 2\n",
    "        @printf(\"Initial cost, grad, hess:\\n\")\n",
    "        print_vector_g(:cost)\n",
    "        print_vector_g(:grad)\n",
    "        print_vector_g(:hess)\n",
    "    end\n",
    "    \n",
    "\n",
    "    for i in [1:maxiter;]\n",
    "        hathess    = hess + eye(length(grad), length(grad))/eta        \n",
    "        new_params = params - inv(hathess)*grad\n",
    "        new_cost, new_grad, new_hess = vgh(func, new_params)\n",
    "            \n",
    "        if abs(new_cost - cost) < tol\n",
    "            break\n",
    "        end\n",
    "\n",
    "        if new_cost >= cost\n",
    "            eta = eta/2\n",
    "            costheta = NaN\n",
    "        else\n",
    "            eta = eta*1.1\n",
    "            costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "\n",
    "            params = new_params\n",
    "            cost = new_cost\n",
    "            grad = new_grad\n",
    "            hess = new_hess\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            @printf \"%d: eta=%.3f cost=%.4f costheta=%.3f ps=\" i eta cost  costheta\n",
    "            print_vector(params)\n",
    "            @printf \"\\n\"\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return params, cost\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BBOX_HESSIAN_KEYWORD_MINIMIZATION()\n",
    "\n",
    "The main function here does parameter optimization, i.e. searching for the minimum of a scalar function of a vector-valued set of payameters. It uses ForwardDiff to compute and use information about gradients and Hessians, and uses the soft tanh() wall method to keep selected parameters strictly within desired limits during the search.\n",
    "\n",
    "The main search function, called bbox_hessian_keyword_minimization(), takes four obligatory parameters (and many optional ones, see its documentation below for details):\n",
    "\n",
    "* seed, a vector with the starting value of some parameters\n",
    "* args, a list of strings, same length as seed, indicating the parameter names associated with each value in seed. \n",
    "* func, a function that returns a scalar and takes only keyword-value argument pairs. All of the strings in args must indicate keyword names for func() that func() knows about.\n",
    "* bbox, a dictionary where each key is a Symbol indicating a parameter name, and the values are two-long vectors, whose elements indicate the desired minimum and maximum of the range for that parameter, respectively. Any key in this dictionary must also be, in string form, in args. But not all entries in args, need be in bbox, the missing ones are assumed to have no bounds. bbox could even be an empty dictionary, indicating no bounds on any parameter.\n",
    "\n",
    "bbox_hessian_keyword_minimization() will start from seed, will search for parameter values that minimize fun, and will return those, along with a variety of information that is diagnostic regarding the search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, some helper functions for bbox_hessian_keyword_minimization()\n",
    "\n",
    "These are functions that help with the soft tanh() wall method for implementing the parameter bounding box.\n",
    "\n",
    "### The soft tanh() wall method for putting bounds on parameter values\n",
    "\n",
    "The basic idea of the soft tanh() method is the following:  Let's say we have a scalar function $f(x)$, and we want to find its minimum subject to $x_0 <= x <= x_1$.\n",
    "\n",
    "Let's say that $m$ is the midpoint of the range, $m = (x_0+x_1)/2$, and $d$ is the width of the range, $d = x_1 - x_0$. We're going to use the function $g()$\n",
    "\n",
    "$$\n",
    "   g(x) = x_0 + (x_1-x_0)*\\frac{1}{1 + \\exp(-\\frac{x-m}{d})}\n",
    "$$\n",
    "\n",
    "Note that $g()$ ranges from a strict minimum of $x_0$ to a strict maximum of $x_1$, and it is monotonic in $x$, so for any value of $x$ that is within the range, we can obtain $g^{-1}(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "PyPlot.Figure(PyObject <matplotlib.figure.Figure object at 0x33157ed10>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition g(Any, Any, Any) in module Main at In[26]:4 overwritten at In[27]:4.\n"
     ]
    }
   ],
   "source": [
    "using PyPlot\n",
    "\n",
    "function g(x, x0, x1)\n",
    "    m = 0.5*(x0+x1)\n",
    "    d = x1-x0\n",
    "    \n",
    "    return x0 + (x1-x0)./(1 + exp(-(x-m)/d))\n",
    "end\n",
    "\n",
    "h = figure(1); clf(); h[:set_size_inches](3, 2); \n",
    "x0 = 3; x1 = 10; x = -60:0.1:60\n",
    "\n",
    "plot(x, g(x, x0, x1), \"-\"); \n",
    "\n",
    "xlabel(\"xi\"); ylabel(\"x = g(xi)\"); hlines([x0, x1], xlim()[1], xlim()[2]); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our strategy:**\n",
    "\n",
    "Now here is our strategy: instead of minimizing the function $f(x)$ subject to the range constraints, we will instead  minimize the function $f(g(\\xi))$ subject to *no* constraints on $\\xi$. \n",
    "\n",
    "To start the minimization, if we wanted to start at $x = x_{seed}$, we first compute $\\xi_{seed} = g^{-1}(x_{seed})$, and we start at that value of $\\xi_{seed}$. That will correspond to having started at $x_{seed}$, as desired.\n",
    "\n",
    "For every value of $\\xi$ there is a corresponding value of $x = g(\\xi)$. And no matter where $\\xi$ ranges, its corresponding value of $x$ will be strictly bounded within the desired limits.  Note that \n",
    "\n",
    "$$\n",
    "    \\frac{{\\rm d}f}{{\\rm d}x} = \\frac{{\\rm d}f}{{\\rm d}g} \\cdot \\frac{{\\rm d}g}{{\\rm d}\\xi}\n",
    "$$\n",
    "\n",
    "Therefore, as $x$ approaches the rangle limits (which corresponds to $|\\xi - m| >> d$), we will be approaching the flat part of $g()$ and therefore $\\frac{{\\rm d}g}{{\\rm d}\\xi}$ will approach zero and so will $\\frac{{\\rm d}f}{{\\rm d}x}$: this is what prevents the search from going beyond the bounds. Note also that in each iteration of  bbox_hessian_keyword_minimization() we ask for the step that would minimize the cost function *given a certain step magnitude*. This means we will avoid directions that have little impact on the cost function.\n",
    "\n",
    "\n",
    "\n",
    "**The helper functions are**:\n",
    "\n",
    "* **pdict()** takes a dictionary of range limits, and a dictionary of current free range parameter values and returns a dictionary of the corresponding \"walled\" values, constrained to lie within the desired bounds. (I.e., puts values through tanh())\n",
    "* **vector_wrap()** does the same as pdict, except that instead of the dictionary of current free range parameter values, it takes a list of strings and a corresponding vector of values, and returns a vector.\n",
    "* **inverse_wall()** is the inverse of the above two operations: if passed a dictionary of ranges and a dictionary of constrained values, it returns a dictionary of the corresponding free range values. If passed a list of strings and a vector or the corresponding constrained values, returns a vector of the corresponding free-range, unconstrained values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#@include_me hessian_utils.jl\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "pdict = wallwrap(bdict, pdict)\n",
    "Given bdict, a dictionary of symbols to [minval, maxval] vectors, and pdict, a dictionary of symbols\n",
    "to values (or, alternatively, an Array of (Symbol, value) tuples], goes through each of the symbols in \n",
    "bdict and modifies the corresponding value in pdict putting it through a tanh so the final output lies \n",
    "within the limits in bdict.  Returns the new pdict.  Makes a copy of pdict so as not to modify the original.\n",
    "\"\"\"\n",
    "function wallwrap(bdict, epdict)\n",
    "    local pdict = two_level_copy(epdict)  # Must be very careful here! I got bit by the bug of forgetting that without\n",
    "    # an explicit copy() call, Julia does not make copies of the contents of arrays or dictionaries, making it\n",
    "    # easy to inadvertently modify something one did not intend to perturb.  Note the two_level_copy() call, \n",
    "    # necessary to make sure we don't mess up the content of the caller's dictionary.\n",
    "    \n",
    "    if typeof(pdict)<:Array\n",
    "        pdict = Dict(pdict)\n",
    "    end\n",
    "\n",
    "    allkeys = keys(bdict)\n",
    "\n",
    "    for k in allkeys\n",
    "        local bbox = bdict[k]\n",
    "        d = 0.5*(bbox[2] - bbox[1])\n",
    "        m = 0.5*(bbox[2] + bbox[1])\n",
    "\n",
    "        pdict[k] = bbox[1] + d*(tanh((pdict[k]-m)/d)+1)\n",
    "    end\n",
    "    return pdict\n",
    "end\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "params = vector_wrap(bbox, args, eparams)\n",
    "Given bdict, a dictionary of symbols to [minval, maxval] vectors, args, an array of strings representing\n",
    "symbols, and params, an array of values corresponding to the args list, puts each param that has an entry \n",
    "in bdict through the tanh-walling mechanism, and returns the result. Does not modify the contents of the \n",
    "original params vector (or bdict or args).\n",
    "\"\"\"\n",
    "function vector_wrap(bbox, args, eparams)\n",
    "    local params = two_level_copy(eparams)\n",
    "    pdict = wallwrap(bbox, make_dict(args, params))\n",
    "    i=1; j=1\n",
    "    for i=1:length(args)\n",
    "        if typeof(args[i])<:Array\n",
    "            params[j:j+args[i][2]-1] = pdict[Symbol(args[i][1])]\n",
    "            j += args[i][2]-1\n",
    "        else\n",
    "            params[j] = pdict[Symbol(args[i])]\n",
    "        end\n",
    "    j = j+1\n",
    "    end\n",
    "    return params\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "params = inverse_wall(bdict, args, wparams)\n",
    "Given bdict, a dictionary of symbols to [minval, maxval] vectors, args, an array of strings representing\n",
    "symbols, and wparams, an array of values corresponding to the args list where each param that has an entry \n",
    "in bdict has alreadt been through the tanh-walling mechanism, UNwalls the ones that have a bdict entry and\n",
    "returns the result. Does not modify the contents of the original params vector (or bdict or args).\n",
    "\"\"\"\n",
    "function inverse_wall(bdict, args, wparams)\n",
    "    local params = two_level_copy(wparams)\n",
    "    pdict = inverse_wall(bdict, make_dict(args, params))\n",
    "    i=1; j=1\n",
    "    for i=1:length(args)\n",
    "        if typeof(args[i])<:Array\n",
    "            params[j:j+args[i][2]-1] = pdict[Symbol(args[i][1])]\n",
    "            j += args[i][2]-1\n",
    "        else\n",
    "            params[j] = pdict[Symbol(args[i])]\n",
    "        end\n",
    "        j = j+1\n",
    "    end\n",
    "    return params    \n",
    "end\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "pdict = inverse_wall(bdict, wdict)\n",
    "Given bdict, a dictionary of symbols to [minval, maxval] vectors, and wdict, a dictionary of symbols to values\n",
    "(or vectors of values)  UNwalls the ones that have a bdict entry and\n",
    "returns the result. Does not modify the contents of any dictionaries.\n",
    "\"\"\"\n",
    "function inverse_wall(bdict, wdict)\n",
    "    local pdict = two_level_copy(wdict)\n",
    "\n",
    "    allkeys = keys(bdict)\n",
    "    for k in allkeys\n",
    "        local bbox = bdict[k]\n",
    "        d = 0.5*(bbox[2] - bbox[1])\n",
    "        m = 0.5*(bbox[2] + bbox[1])\n",
    "\n",
    "        pdict[k] = m + d*0.5*log((pdict[k]-bbox[1])./(2*d - pdict[k] + bbox[1]))\n",
    "    end\n",
    "    return(pdict)\n",
    "end\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The main bbox_hessian_keyword_minimization() function\n",
    "\n",
    "Minimizes a scalar function $f()$ of keyword-value pairs, by searching for the parameter values that produce the smallest output. At each step of the minimization, uses ForwardDiff to compute the gradient $\\bf g$ and the Hessian $H$ at the current set of parameter values ${\\bf p_0}$; and then uses constrained_parabolic_minimization() to find the the parameter values ${\\bf p}$, such that $|{\\bf p} - {\\bf p_0}| <= \\eta$, that would minimize the parabolic approximation to the cost function\n",
    "\n",
    "$$\n",
    "J({\\bf p}) = J({\\bf p_o}) + {\\bf g}^T ({\\bf p} - {\\bf p_0}) + \\frac{1}{2} ({\\bf p} - {\\bf p_0})^T \\cdot H \\cdot ({\\bf p} - {\\bf p_0})\n",
    "$$\n",
    "\n",
    "If the cost $f({\\bf p}) < f({\\bf p_0})$, then the step is accepted, the current parameter values become ${\\bf p}$ and $\\eta$ is increased slightly. Otherwise the step is not accepted, current parameter values are not changed, and $\\eta$ is cut by a factor of 2.  Note than when $\\eta$ is very small, this becomes gradient descent. It is only when $\\eta$ is larger that the Hessian begins to play a role.\n",
    "\n",
    "A parameter step for which $|{\\bf p} - {\\bf p_0}| < \\eta$ is called a \"Newton\" jump (it is Newton's method). A step in which $|{\\bf p} - {\\bf p_0}| = \\eta$ is called a \"constrained\" jump. If the `verbose=true option` is selected, then the diagnostic information printed out at each step includes the cosine of the angle between the step taken and the gradient (when this is close to -1, we're doing gradient descent; far from -1, we're using the Hessian information).\n",
    "\n",
    "**FAILURE MODES:**  At each step of the search iteration, constrained_parabolic_minimization(), which itself involves a search, may fail. For diagnostics on this, a matrix called `cpm_traj` is returned. Its first row is the number of iterations run by constrained_parabolic_minimization()'s internal search, at each step of the overall search. If this number is equal to the maximum requested (currently hardcoded as 500 iterations), that means that constrained_parabolic_minimization() returned only because it ran into its maximum iteration limit, not because it was successful, and is therefore a sign of a step that may have been taken in a poor direction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bbox_Hessian_keyword_minimization"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@include_me hessian_utils.jl\n",
    "\n",
    "\n",
    "######################################################\n",
    "#                                                    #\n",
    "#         BBOX_HESSIAN_KEYWORD_MINIMIZATION          #\n",
    "#                                                    #\n",
    "######################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "function bbox_Hessian_keyword_minimization(seed, args, bbox, func; wallwidth=NaN, start_eta=10, tol=1e-6, \n",
    "    maxiter=400, verbose=false)\n",
    "\n",
    "Like constrained_Hessian_minimization, but uses keyword_hessian!(). \n",
    "\n",
    "# PARAMETERS:\n",
    "\n",
    "- seed        column vector, representing the starting value of the parameters.\n",
    "\n",
    "- args        List of strings identifying parameters for differentiation, e.g., [\"const_E\", \"w_self]\n",
    "\n",
    "- bbox        If softbox=true (the default), should then be a Dict of Symbol=>[minval maxval] entries. An entry\n",
    "            in this Dict indicates that the corresponding parameter is to be bounded, as indicated by the associated \n",
    "            [minval maxval] vector. The bbox dictionary can have fewer entries than the number of parameters, and its\n",
    "            default value is Dict(), indicating an unbounded search.\n",
    "                If softbox=false, then bbox should be an nargs-by-2 matrix indicating the range for each argument,\n",
    "            with the minima (first column) and maxima (second column), and entries for ALL parameters.\n",
    "\n",
    "- func        func must take only optional keyword args, and must \n",
    "            take nderivs=0, difforder=0  and declare any new matrices using ForwardDiffZeros() instead of zeros()\n",
    "\n",
    "\n",
    "# OPTIONAL PARAMETERS:\n",
    "\n",
    "- start_eta    Starting value of the radius.  It's good to start with somethibg biggish, if it is\n",
    "             too much, it'll quickly get cut down.\n",
    "\n",
    "- tol=1e-6     Numerical tolerance. If a proposed jump produces a change in func that is less than\n",
    "             this, the minimization stops.\n",
    "\n",
    "- maxiter=400  Maximum number of iterations to do before stopping\n",
    "\n",
    "- verbose=false   If true, print out a report on each iteration of iteration number, radius size (eta),\n",
    "                what type jump was proposed (\"Newton\" means going straight to global min, \"constrained\" means jump has \n",
    "                norm eta, failed means that finding the minimum at a given radius somehow didn't work). Will also\n",
    "                print out the cosine of the angle between the proposed jump and the gradient.\n",
    "\n",
    "- verbose_level   If less than 2, regular verbose output, if 2 or greater, very verbose, for debugging.\n",
    "\n",
    "- softbox         If true, then bbox must be a Dict() and we use the tanh() mechanism for putting a fixed limit\n",
    "                on the parameters.\n",
    "\n",
    "- hardbox=false   If true, ignores wallwidth, and just rests parameter values to the bounding box if they go outside it.\n",
    "                If false, adds cost function \"walls\" to implement the bounding box.\n",
    "\n",
    "- walldith=NaN     Used for putting up cost function \"walls\" that implement the bounding box limits. Can be NaN.\n",
    "                If it is NaN, then the wallwidth is a constant factor of the range width for each argument. If not NaN, must\n",
    "                be an nargs-long vector that indicates the actual wall widths.\n",
    "\n",
    "- wallwidth_factor=0.18   Only relevant if wallwidth is NaN, otherwise ignored. For each arg, the wall width\n",
    "                is going to be wall_width_factor*(bbox[i,2] - bbox[i,1])\n",
    "\n",
    "\n",
    "# RETURNS:\n",
    "\n",
    "- params       A vector the size of seed that has the last values of the minimizing parameters for func\n",
    "- trajectory   A (2+length(params))-by-nsteps matrix. Each column corresponds to an iteration step, and contains\n",
    "                 the value of eta used, the cost, and the value of the parameters at that iteration\n",
    "- cost         Final value of objective function\n",
    "- cpm_traj     A 2-by-nsteps matrix, containing reports from the contrained parabolic minimization at each timestep.\n",
    "             The first row is niters (how many iterations cpm's 1-d minimization ran for) and the second row is\n",
    "             Dlambda, the last change in the parameter being minimized in cpm's internal search\n",
    "\n",
    "\n",
    "# EXAMPLE:\n",
    "\n",
    "```\n",
    "function tester(;x=5, y=10, z=20, nderivs=0, difforder=0)\n",
    "    return x^2*y + z/tanh(y)\n",
    "end\n",
    "\n",
    "params, trajectory = bbox_Hessian_keyword_minimization([0.5, 0.5], [\"x\", \"y\"], [1.1 2 ; 1.1 4], tester, \n",
    "    verbose=true, tol=1e-12, start_eta=1);\n",
    "```\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "function bbox_Hessian_keyword_minimization(seed, args, bbox, func; start_eta=0.1, tol=1e-6, maxiter=400,\n",
    "    verbose=false, verbose_level=1, verbose_every=1, \n",
    "    softbox=true, hardbox=false, wallwidth=NaN, wallwidth_factor=0.18)\n",
    "\n",
    "      \n",
    "    \"\"\"\n",
    "    Given args, a list of string representing the arguments of interest, a bounding box for each,\n",
    "    and a Symbol=>value dictionary with the corresponding parameters, computes and returns a high cost for \n",
    "    being outside the bounding box\n",
    "    \"\"\"\n",
    "    function wall_cost(args, bbox; wallwidth=NaN, nderivs=0, difforder=0, pars...) \n",
    "        myparams = ForwardDiffZeros(length(pars), 1, nderivs=nderivs, difforder=difforder)\n",
    "        pars2 = Dict()\n",
    "        for i in [1:length(pars);]\n",
    "            pars2[string(pars[i][1])] = pars[i][2]\n",
    "        end\n",
    "        for i in [1:length(args);]\n",
    "            myparams[i] = pars2[args[i]]\n",
    "        end\n",
    "        \n",
    "        if isnan(wallwidth)\n",
    "            # We know that we're going to be taking hessian for params, so declare zeros accordingly:\n",
    "            wallwidth = ForwardDiffZeros(length(myparams), 1, nderivs=nderivs, difforder=difforder)\n",
    "\n",
    "            for i in [1:length(myparams);]\n",
    "                wallwidth[i] = wallwidth_factor*(bbox[i,2]-bbox[i,1])\n",
    "            end\n",
    "        end\n",
    "\n",
    "        retval = 0\n",
    "        for i in [1:length(myparams);]\n",
    "            if myparams[i]<bbox[i,1]\n",
    "                retval += cosh((bbox[i,1]-myparams[i])/wallwidth[i])-1.0\n",
    "            elseif bbox[i,2] < myparams[i]\n",
    "                retval += cosh((myparams[i]-bbox[i,2])/wallwidth[i])-1.0                \n",
    "            end\n",
    "        end\n",
    "\n",
    "        return 2*retval\n",
    "    end\n",
    "\n",
    "    traj_increment = 100\n",
    "    params = 0  # Make sure to have this here so that params stays defined beyond the try/catch\n",
    "    if ( !(typeof(bbox)<:Dict) ); error(\"Currently only supporting softbox=true, bbox must be a Dict\"); end;\n",
    "    try\n",
    "        params = copy(inverse_wall(bbox, args, seed))\n",
    "    catch\n",
    "        error(\"Were all initial param values within the indicated walls?\")\n",
    "    end\n",
    "    eta = start_eta\n",
    "    trajectory = zeros(2+length(params), traj_increment); cpm_traj = zeros(2, traj_increment)\n",
    "\n",
    "    if verbose\n",
    "        @printf \"%d: eta=%g ps=\" 0 eta \n",
    "        print_vector(vector_wrap(bbox, args, params))\n",
    "        @printf \"\\n\"\n",
    "    end\n",
    "    \n",
    "    if softbox\n",
    "        if !(typeof(bbox)<:Dict); error(\"bhm: If softbox=true, then bbox must eb a Dict\"); end\n",
    "        cost, grad, hess = keyword_vgh((;pars...)->func(;wallwrap(bbox, pars)...), args, params)\n",
    "    elseif hardbox\n",
    "        cost, grad, hess = keyword_vgh((;pars...) -> func(;pars...), args, params)\n",
    "    else\n",
    "        cost, grad, hess = keyword_vgh((;pars...) -> func(;pars...) + wall_cost(args, bbox; wallwidth=wallwidth, pars...),\n",
    "            args, params)        \n",
    "    end\n",
    "        \n",
    "    chessdelta = zeros(size(params))\n",
    "    \n",
    "    i=0  # here so variable i is available outside the loop\n",
    "    for i in [1:maxiter;]\n",
    "        if i > size(trajectory, 2)\n",
    "            trajectory = [trajectory zeros(2+length(params), traj_increment)]\n",
    "            cpm_traj   = [cpm_traj   zeros(2, traj_increment)]\n",
    "        end\n",
    "        trajectory[1:2, i]   = [eta;cost]\n",
    "        trajectory[3:end, i] = vector_wrap(bbox, args, params)\n",
    "        \n",
    "        hessdelta  = - inv(hess)*grad\n",
    "        try\n",
    "            if verbose && verbose_level >= 2\n",
    "                @printf(\"bhm: about to try cpm with grad : \"); print_vector_g(grad); print(\"\\n\")\n",
    "                @printf(\"bhm:   hess :\"); print_vector_g(hess[:]); print(\"\\n\");\n",
    "            end\n",
    "            if verbose && verbose_level >= 2\n",
    "                cpm_out = constrained_parabolic_minimization(hess, grad'', eta, \n",
    "                    maxiter=500, tol=1e-20, do_plot=true, verbose=true)                \n",
    "            else\n",
    "                cpm_out = constrained_parabolic_minimization(hess, grad'', eta, maxiter=500, tol=1e-20)\n",
    "            end\n",
    "            chessdelta = cpm_out[1]; cpm_traj[1,i] = cpm_out[5]; cpm_traj[2,i] = cpm_out[6]\n",
    "            jumptype = \"not failed\"\n",
    "        catch y\n",
    "            jumptype = \"failed\"\n",
    "            if verbose\n",
    "                @printf \"Constrained parabolic minimization failed with error %s\\n\" y\n",
    "                @printf \"\\n\"\n",
    "                @printf \"eta was %g\\n\" eta\n",
    "                @printf \"grad was\\n\"\n",
    "                print_vector(grad)\n",
    "                @printf \"\\n\\nhess was\\n\"\n",
    "                for k in [1:length(grad);]\n",
    "                    print_vector(hess[k,:])\n",
    "                    @printf \"\\n\"\n",
    "                end\n",
    "                @printf \"\\n\"\n",
    "                matwrite(\"error_report.mat\", Dict(\"grad\"=>grad, \"hess\"=>hess, \"eta\"=>eta))\n",
    "            end\n",
    "            break\n",
    "        end\n",
    "\n",
    "        if norm(hessdelta) <= eta\n",
    "            new_params = params + hessdelta\n",
    "            jumptype = \"Newton\"\n",
    "        elseif jumptype != \"failed\" \n",
    "            new_params = params + chessdelta\n",
    "            jumptype  = \"constrained\"\n",
    "        end\n",
    "\n",
    "        if jumptype != \"failed\"\n",
    "            if softbox\n",
    "                new_cost, new_grad, new_hess = \n",
    "                    keyword_vgh((;pars...) -> func(;wallwrap(bbox, pars)...), args, new_params)\n",
    "                if verbose && verbose_level >=2\n",
    "                    @printf(\"bhm: had new_params = : \"); print_vector_g(vector_wrap(bbox, args, params)); print(\"\\n\");\n",
    "                    @printf(\"bhm: and my bbox was : \"); print(bbox); print(\"\\n\")\n",
    "                    @printf(\"bhm: and my wallwrap output was : \"); print(wallwrap(bbox, make_dict(args, new_params))); print(\"\\n\")\n",
    "                    @printf(\"bhm: and this produced new_grad : \"); print_vector_g(new_grad); print(\"\\n\")\n",
    "                    @printf(\"bhm:   new_hess :\"); print_vector_g(new_hess[:]); print(\"\\n\");                                        \n",
    "                end\n",
    "            elseif hardbox\n",
    "                for p in [1:length(new_params);]\n",
    "                    if new_params[p] < bbox[p,1]; new_params[p] = bbox[p,1]; end\n",
    "                    if bbox[p,2] < new_params[p]; new_params[p] = bbox[p,2]; end\n",
    "                 end        \n",
    "                \n",
    "                new_cost, new_grad, new_hess = keyword_vgh((;pars...) -> func(;pars...), args, new_params)\n",
    "            else\n",
    "                new_cost, new_grad, new_hess = keyword_vgh((;pars...) -> func(;pars...) + \n",
    "                        wall_cost(args, bbox; wallwidth=wallwidth, pars...),\n",
    "                    args, new_params)                \n",
    "            end\n",
    "            \n",
    "            if abs(new_cost - cost) < tol || eta < tol\n",
    "                if verbose\n",
    "                    @printf(\"About to break -- tol=%g, new_cost-cost=%g, eta=%g\\n\", tol, new_cost-cost, eta)\n",
    "                end\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "\n",
    "        if jumptype == \"failed\" || new_cost >= cost  \n",
    "            if verbose\n",
    "                @printf(\"eta going down: new_cost-cost=%g and jumptype='%s'\\n\", new_cost-cost, jumptype)\n",
    "                if verbose_level >= 2\n",
    "                    nwp = vector_wrap(bbox, args, new_params); wp = vector_wrap(bbox, args, params)\n",
    "                    @printf(\"   vvv: proposed new params were : \"); print_vector_g(nwp); print(\"\\n\")\n",
    "                    @printf(\"   vvv: proposed delta params was : \"); print_vector_g(nwp-wp); print(\"\\n\")\n",
    "                    @printf(\"   vvv: grad was : \"); print_vector_g(grad); print(\"\\n\")\n",
    "                    costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "                    @printf(\"   vvv: costheta of proposed jump was %g\\n\", costheta)\n",
    "                end\n",
    "            end\n",
    "            eta = eta/2\n",
    "            costheta = NaN\n",
    "            if eta < tol\n",
    "                if verbose\n",
    "                    @printf(\"About to break -- tol=%g, new_cost-cost=%g, eta=%g\\n\", tol, new_cost-cost, eta)\n",
    "                end\n",
    "                break\n",
    "            end\n",
    "        else\n",
    "            eta = eta*1.1\n",
    "            costheta = dot(new_params-params, grad)/(norm(new_params-params)*norm(grad))\n",
    "\n",
    "            params = new_params\n",
    "            cost = new_cost\n",
    "            grad = new_grad\n",
    "            hess = new_hess\n",
    "        end\n",
    "\n",
    "        if verbose\n",
    "            if rem(i, verbose_every)==0\n",
    "                @printf \"%d: eta=%g cost=%g jtype=%s costheta=%.3f ps=\" i eta cost jumptype costheta\n",
    "                print_vector_g(vector_wrap(bbox, args, params))\n",
    "                @printf \"\\n\"\n",
    "                if verbose_level >= 3\n",
    "                    @printf \"    At this point, grad is =\"\n",
    "                    print_vector_g(grad)\n",
    "                    @printf \"\\n\"                \n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    trajectory = trajectory[:,1:i]; cpm_traj = cpm_traj[:,1:i]\n",
    "    return vector_wrap(bbox, args, params), trajectory, cost, cpm_traj\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.2",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
